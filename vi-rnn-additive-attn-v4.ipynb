{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import train_and_eval, count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params required for generating data loaders \n",
    "\n",
    "SRC_LANG = 'vi'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture params \n",
    "NETWORK_TYPE = 'rnn'\n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 \n",
    "DEC_DROPOUT = 0.2  \n",
    "ATTENTION_TYPE = 'additive'\n",
    "\n",
    "# training params  \n",
    "NUM_EPOCHS = 15 \n",
    "LR = 0.0001 \n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = False\n",
    "\n",
    "# name the model and experiment \n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    EXPERIMENT_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    EXPERIMENT_NAME = '{}-cnn'.format(SRC_LANG)\n",
    "MODEL_NAME = '{}-{}'.format(EXPERIMENT_NAME, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n",
    "          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n",
    "          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n",
    "          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n",
    "          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n",
    "          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "if ATTENTION_TYPE == 'without': \n",
    "    # without attention \n",
    "    decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                         targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)\n",
    "    \n",
    "else: \n",
    "    # with attention \n",
    "    decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                             num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, \n",
    "                             src_max_sentence_len=SRC_MAX_SENTENCE_LEN, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                             dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                    vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.29, Train BLEU: 0.00, Val BLEU: 0.00, Minutes Elapsed: 0.13\n",
      "Sampling from val predictions...\n",
      "Source: chúng_ta những nạn_nhân cần đến tất_cả mọi người . <EOS>\n",
      "Reference: we victims need everyone . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> caption caption caption caption elitist elitist elitist freely freely\n",
      "Attention Weights: tensor([[0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027]])\n",
      "\n",
      "Epoch: 0.05, Train Loss: 0.00, Val Loss: 6.38, Train BLEU: 0.00, Val BLEU: 0.03, Minutes Elapsed: 2.69\n",
      "Sampling from val predictions...\n",
      "Source: bởi_vì tại thời_điểm đó , toà_án hiến_pháp liên_bang đã quy_định\n",
      "Reference: because in the mean time , the german constitutional\n",
      "Model: <SOS> and , , , , , , , ,\n",
      "Attention Weights: tensor([[5.7219e-08, 8.3849e-06, 1.1718e-03, 1.8554e-02, 8.8084e-02, 1.6820e-01,\n",
      "         2.1600e-01, 2.8887e-01, 1.9591e-01, 2.3204e-02],\n",
      "        [3.0807e-05, 2.9487e-04, 3.4421e-03, 1.7298e-02, 4.7326e-02, 7.6239e-02,\n",
      "         9.9193e-02, 1.5470e-01, 2.6057e-01, 3.4091e-01],\n",
      "        [6.7604e-05, 4.5313e-04, 3.9083e-03, 1.7099e-02, 4.3866e-02, 6.9138e-02,\n",
      "         8.9534e-02, 1.4005e-01, 2.4918e-01, 3.8671e-01],\n",
      "        [7.2524e-05, 4.6308e-04, 3.8667e-03, 1.6738e-02, 4.2768e-02, 6.7357e-02,\n",
      "         8.7328e-02, 1.3711e-01, 2.4740e-01, 3.9690e-01],\n",
      "        [7.3656e-05, 4.6334e-04, 3.8369e-03, 1.6577e-02, 4.2333e-02, 6.6672e-02,\n",
      "         8.6488e-02, 1.3601e-01, 2.4676e-01, 4.0078e-01],\n",
      "        [7.4413e-05, 4.6433e-04, 3.8269e-03, 1.6511e-02, 4.2139e-02, 6.6355e-02,\n",
      "         8.6091e-02, 1.3548e-01, 2.4640e-01, 4.0267e-01],\n",
      "        [7.5110e-05, 4.6605e-04, 3.8269e-03, 1.6487e-02, 4.2047e-02, 6.6190e-02,\n",
      "         8.5877e-02, 1.3516e-01, 2.4615e-01, 4.0372e-01],\n",
      "        [7.5705e-05, 4.6781e-04, 3.8301e-03, 1.6479e-02, 4.2000e-02, 6.6097e-02,\n",
      "         8.5749e-02, 1.3497e-01, 2.4596e-01, 4.0437e-01],\n",
      "        [7.6273e-05, 4.6972e-04, 3.8356e-03, 1.6482e-02, 4.1979e-02, 6.6043e-02,\n",
      "         8.5669e-02, 1.3483e-01, 2.4581e-01, 4.0480e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.10, Train Loss: 0.00, Val Loss: 6.21, Train BLEU: 0.00, Val BLEU: 1.30, Minutes Elapsed: 5.19\n",
      "Sampling from val predictions...\n",
      "Source: các gia_đình hàng_xóm nghe kể về ý_tưởng này . <EOS>\n",
      "Reference: and my neighboring homes heard about this idea .\n",
      "Model: <SOS> and &apos;s the the . . . . .\n",
      "Attention Weights: tensor([[8.3157e-06, 4.1053e-03, 8.9195e-02, 2.5529e-01, 2.5606e-01, 2.0147e-01,\n",
      "         1.3048e-01, 5.1371e-02, 9.3558e-03, 2.6709e-03],\n",
      "        [8.7240e-05, 1.3469e-02, 1.0965e-01, 2.1270e-01, 2.0989e-01, 1.7924e-01,\n",
      "         1.3910e-01, 8.2685e-02, 3.1731e-02, 2.1444e-02],\n",
      "        [1.4440e-04, 5.4484e-03, 2.8936e-02, 5.1302e-02, 5.0970e-02, 4.5558e-02,\n",
      "         4.0251e-02, 3.4335e-02, 3.8145e-02, 7.0491e-01],\n",
      "        [1.2625e-05, 3.3452e-04, 1.8714e-03, 3.4943e-03, 3.4164e-03, 3.0291e-03,\n",
      "         2.7818e-03, 2.7626e-03, 5.7113e-03, 9.7659e-01],\n",
      "        [4.1132e-06, 1.0739e-04, 6.6137e-04, 1.2866e-03, 1.2309e-03, 1.0732e-03,\n",
      "         9.9927e-04, 1.0725e-03, 3.0761e-03, 9.9049e-01],\n",
      "        [2.9000e-06, 7.7782e-05, 5.0731e-04, 1.0065e-03, 9.4808e-04, 8.1702e-04,\n",
      "         7.6582e-04, 8.5745e-04, 2.8801e-03, 9.9214e-01],\n",
      "        [2.6825e-06, 7.3945e-05, 4.9757e-04, 9.9549e-04, 9.2817e-04, 7.9399e-04,\n",
      "         7.4687e-04, 8.5620e-04, 3.1001e-03, 9.9201e-01],\n",
      "        [2.7211e-06, 7.6566e-05, 5.2383e-04, 1.0513e-03, 9.7381e-04, 8.2920e-04,\n",
      "         7.8154e-04, 9.0801e-04, 3.4061e-03, 9.9145e-01],\n",
      "        [2.8364e-06, 8.0897e-05, 5.5815e-04, 1.1209e-03, 1.0340e-03, 8.7796e-04,\n",
      "         8.2844e-04, 9.6995e-04, 3.6978e-03, 9.9083e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.14, Train Loss: 0.00, Val Loss: 6.15, Train BLEU: 0.00, Val BLEU: 1.36, Minutes Elapsed: 7.69\n",
      "Sampling from val predictions...\n",
      "Source: bây_giờ tôi muốn giới_thiệu các bạn với những người em_trai\n",
      "Reference: now i &apos;d like to introduce you to my\n",
      "Model: <SOS> and , , , , , , , ,\n",
      "Attention Weights: tensor([[1.7282e-06, 3.5568e-05, 1.3156e-03, 3.7219e-02, 1.7338e-01, 3.2576e-01,\n",
      "         3.0054e-01, 1.5262e-01, 9.1044e-03, 2.7114e-05],\n",
      "        [1.7126e-05, 3.0989e-04, 6.4648e-03, 7.2837e-02, 1.9150e-01, 2.8140e-01,\n",
      "         2.6474e-01, 1.6437e-01, 1.8301e-02, 6.2360e-05],\n",
      "        [1.4059e-04, 1.7037e-03, 1.8358e-02, 9.8321e-02, 1.8429e-01, 2.3224e-01,\n",
      "         2.2533e-01, 1.7845e-01, 5.9534e-02, 1.6301e-03],\n",
      "        [5.3611e-04, 3.6287e-03, 2.3545e-02, 9.4603e-02, 1.6717e-01, 2.0627e-01,\n",
      "         2.0373e-01, 1.7875e-01, 1.0259e-01, 1.9182e-02],\n",
      "        [7.2082e-04, 3.8620e-03, 2.1698e-02, 8.5845e-02, 1.5745e-01, 1.9778e-01,\n",
      "         1.9591e-01, 1.7526e-01, 1.1587e-01, 4.5603e-02],\n",
      "        [7.7392e-04, 3.7384e-03, 1.9924e-02, 8.0012e-02, 1.5156e-01, 1.9314e-01,\n",
      "         1.9125e-01, 1.7196e-01, 1.2031e-01, 6.7338e-02],\n",
      "        [7.8888e-04, 3.5844e-03, 1.8576e-02, 7.5803e-02, 1.4721e-01, 1.8962e-01,\n",
      "         1.8759e-01, 1.6900e-01, 1.2230e-01, 8.5535e-02],\n",
      "        [7.9071e-04, 3.4500e-03, 1.7576e-02, 7.2698e-02, 1.4385e-01, 1.8678e-01,\n",
      "         1.8459e-01, 1.6644e-01, 1.2323e-01, 1.0059e-01],\n",
      "        [7.8703e-04, 3.3349e-03, 1.6794e-02, 7.0266e-02, 1.4112e-01, 1.8438e-01,\n",
      "         1.8204e-01, 1.6420e-01, 1.2368e-01, 1.1340e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.19, Train Loss: 0.00, Val Loss: 6.14, Train BLEU: 0.00, Val BLEU: 1.44, Minutes Elapsed: 10.19\n",
      "Sampling from val predictions...\n",
      "Source: cô cũng đã có bản_sao của bức ảnh . <EOS>\n",
      "Reference: she also had <UNK> . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> and &apos;s is the the . . . .\n",
      "Attention Weights: tensor([[3.1292e-05, 3.6202e-03, 9.7324e-02, 3.3806e-01, 3.9314e-01, 1.3732e-01,\n",
      "         2.6389e-02, 3.2184e-03, 5.8511e-04, 3.1986e-04],\n",
      "        [1.8756e-04, 1.2437e-02, 1.4713e-01, 3.3343e-01, 3.6238e-01, 1.2223e-01,\n",
      "         2.0536e-02, 1.5367e-03, 1.1547e-04, 1.8871e-05],\n",
      "        [3.5653e-04, 2.0291e-02, 1.5693e-01, 2.9468e-01, 3.1801e-01, 1.5951e-01,\n",
      "         4.4742e-02, 5.0739e-03, 3.6289e-04, 4.0808e-05],\n",
      "        [1.1531e-03, 2.9656e-02, 1.3656e-01, 2.1912e-01, 2.4009e-01, 1.9036e-01,\n",
      "         1.1862e-01, 4.6776e-02, 1.2559e-02, 5.1102e-03],\n",
      "        [9.0547e-04, 1.5486e-02, 6.8923e-02, 1.1573e-01, 1.3175e-01, 1.2594e-01,\n",
      "         1.1540e-01, 1.0272e-01, 1.0496e-01, 2.1819e-01],\n",
      "        [1.3627e-04, 1.9128e-03, 9.5038e-03, 1.7779e-02, 2.1168e-02, 2.2875e-02,\n",
      "         2.7258e-02, 4.3927e-02, 1.2164e-01, 7.3380e-01],\n",
      "        [1.0386e-05, 1.2707e-04, 7.6334e-04, 1.6898e-03, 2.1364e-03, 2.6388e-03,\n",
      "         4.1546e-03, 1.2055e-02, 7.5898e-02, 9.0053e-01],\n",
      "        [2.4940e-06, 2.8171e-05, 1.9460e-04, 4.9247e-04, 6.5117e-04, 8.8994e-04,\n",
      "         1.7054e-03, 7.0606e-03, 6.4324e-02, 9.2465e-01],\n",
      "        [1.4451e-06, 1.5578e-05, 1.1463e-04, 3.1086e-04, 4.2050e-04, 6.0692e-04,\n",
      "         1.2819e-03, 6.1633e-03, 6.2924e-02, 9.2816e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.24, Train Loss: 0.00, Val Loss: 6.09, Train BLEU: 0.00, Val BLEU: 1.22, Minutes Elapsed: 12.69\n",
      "Sampling from val predictions...\n",
      "Source: bức này được chụp vài tuần sau sự_kiện 11/9 ,\n",
      "Reference: this one was taken just weeks after 9 /\n",
      "Model: <SOS> so i , , , , , , ,\n",
      "Attention Weights: tensor([[1.0457e-06, 1.5149e-05, 6.0338e-04, 2.2053e-02, 1.4643e-01, 4.2314e-01,\n",
      "         3.2781e-01, 7.0959e-02, 8.1638e-03, 8.2759e-04],\n",
      "        [4.2169e-06, 7.0708e-05, 2.3426e-03, 4.7131e-02, 1.8065e-01, 3.9584e-01,\n",
      "         2.9627e-01, 7.0941e-02, 6.4913e-03, 2.6289e-04],\n",
      "        [9.4693e-06, 1.7831e-04, 4.9891e-03, 6.2688e-02, 1.8065e-01, 3.1932e-01,\n",
      "         2.8208e-01, 1.2204e-01, 2.5975e-02, 2.0665e-03],\n",
      "        [1.2596e-04, 9.5308e-04, 8.7100e-03, 4.9105e-02, 1.1280e-01, 1.7860e-01,\n",
      "         1.9239e-01, 1.6182e-01, 1.4628e-01, 1.4921e-01],\n",
      "        [1.1593e-04, 5.2860e-04, 3.1579e-03, 1.5769e-02, 3.9052e-02, 6.7206e-02,\n",
      "         8.0280e-02, 8.3599e-02, 1.4264e-01, 5.6765e-01],\n",
      "        [7.1836e-05, 2.7961e-04, 1.4950e-03, 7.5187e-03, 1.9850e-02, 3.6024e-02,\n",
      "         4.5431e-02, 5.2141e-02, 1.1660e-01, 7.2059e-01],\n",
      "        [5.5962e-05, 2.0487e-04, 1.0556e-03, 5.3935e-03, 1.4757e-02, 2.7476e-02,\n",
      "         3.5596e-02, 4.2847e-02, 1.0765e-01, 7.6496e-01],\n",
      "        [4.9239e-05, 1.7497e-04, 8.8792e-04, 4.5953e-03, 1.2844e-02, 2.4236e-02,\n",
      "         3.1873e-02, 3.9402e-02, 1.0485e-01, 7.8108e-01],\n",
      "        [4.5942e-05, 1.6063e-04, 8.0980e-04, 4.2333e-03, 1.1997e-02, 2.2811e-02,\n",
      "         3.0273e-02, 3.8047e-02, 1.0443e-01, 7.8719e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.29, Train Loss: 0.00, Val Loss: 6.06, Train BLEU: 0.00, Val BLEU: 2.00, Minutes Elapsed: 15.19\n",
      "Sampling from val predictions...\n",
      "Source: trước_tiên , bạn phải mang đến cho họ sự bảo_mật\n",
      "Reference: first , you have to offer them <UNK> .\n",
      "Model: <SOS> i , , , , , , , ,\n",
      "Attention Weights: tensor([[2.6359e-06, 5.9158e-05, 1.4391e-03, 2.7887e-02, 1.5529e-01, 2.3204e-01,\n",
      "         3.3462e-01, 1.9430e-01, 5.0176e-02, 4.1809e-03],\n",
      "        [1.3446e-05, 3.0760e-04, 5.4670e-03, 5.6093e-02, 1.8316e-01, 2.3615e-01,\n",
      "         2.8934e-01, 1.8256e-01, 4.4576e-02, 2.3322e-03],\n",
      "        [2.0934e-05, 4.7237e-04, 7.3419e-03, 5.3104e-02, 1.3759e-01, 1.7423e-01,\n",
      "         2.2129e-01, 2.0359e-01, 1.4693e-01, 5.5428e-02],\n",
      "        [1.5129e-05, 1.1361e-04, 7.9439e-04, 4.2160e-03, 1.1913e-02, 1.6806e-02,\n",
      "         2.7642e-02, 4.1169e-02, 1.2720e-01, 7.7013e-01],\n",
      "        [4.0656e-06, 1.9490e-05, 1.0138e-04, 5.2009e-04, 1.7202e-03, 2.6797e-03,\n",
      "         5.4974e-03, 1.0879e-02, 6.7424e-02, 9.1115e-01],\n",
      "        [3.1265e-06, 1.3651e-05, 6.6951e-05, 3.4415e-04, 1.2005e-03, 1.9329e-03,\n",
      "         4.2750e-03, 9.2237e-03, 6.5316e-02, 9.1762e-01],\n",
      "        [3.1651e-06, 1.3539e-05, 6.5637e-05, 3.3875e-04, 1.2034e-03, 1.9580e-03,\n",
      "         4.4383e-03, 9.7945e-03, 6.9322e-02, 9.1286e-01],\n",
      "        [3.4590e-06, 1.4757e-05, 7.1406e-05, 3.6860e-04, 1.3147e-03, 2.1436e-03,\n",
      "         4.8883e-03, 1.0804e-02, 7.4098e-02, 9.0629e-01],\n",
      "        [3.8461e-06, 1.6475e-05, 7.9853e-05, 4.1171e-04, 1.4661e-03, 2.3873e-03,\n",
      "         5.4363e-03, 1.1943e-02, 7.8822e-02, 8.9943e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.34, Train Loss: 0.00, Val Loss: 6.04, Train BLEU: 0.00, Val BLEU: 2.23, Minutes Elapsed: 17.67\n",
      "Sampling from val predictions...\n",
      "Source: đây là những ý_tưởng cần_thiết mà một đất_nước đã bị\n",
      "Reference: these are the ideals that a war-torn libya needs\n",
      "Model: <SOS> it &apos;s a a a a a a a\n",
      "Attention Weights: tensor([[8.1807e-04, 4.2498e-02, 5.2140e-01, 3.9098e-01, 4.3869e-02, 4.0526e-04,\n",
      "         2.7698e-05, 5.3590e-06, 6.8273e-07, 2.7185e-07],\n",
      "        [4.3248e-03, 9.0189e-02, 5.3184e-01, 3.2518e-01, 4.7631e-02, 7.6097e-04,\n",
      "         6.6216e-05, 1.3734e-05, 2.0928e-06, 9.0415e-07],\n",
      "        [2.2520e-03, 3.7600e-02, 2.4581e-01, 4.2283e-01, 2.7163e-01, 1.7383e-02,\n",
      "         2.0320e-03, 4.1716e-04, 3.8149e-05, 1.2422e-05],\n",
      "        [1.0343e-03, 4.4474e-03, 1.7510e-02, 6.3187e-02, 2.2266e-01, 2.5937e-01,\n",
      "         2.0730e-01, 1.6194e-01, 4.2664e-02, 1.9886e-02],\n",
      "        [2.0720e-04, 8.9477e-04, 4.0513e-03, 2.0703e-02, 1.0846e-01, 2.0928e-01,\n",
      "         2.3817e-01, 2.4812e-01, 1.0692e-01, 6.3190e-02],\n",
      "        [1.4516e-04, 6.7461e-04, 3.3066e-03, 1.8503e-02, 1.0056e-01, 2.0151e-01,\n",
      "         2.3506e-01, 2.4988e-01, 1.1747e-01, 7.2893e-02],\n",
      "        [1.4427e-04, 6.7255e-04, 3.2965e-03, 1.8452e-02, 9.9866e-02, 2.0082e-01,\n",
      "         2.3472e-01, 2.4979e-01, 1.1839e-01, 7.3854e-02],\n",
      "        [1.4298e-04, 6.6978e-04, 3.2971e-03, 1.8489e-02, 9.9390e-02, 1.9987e-01,\n",
      "         2.3387e-01, 2.4923e-01, 1.1974e-01, 7.5292e-02],\n",
      "        [1.3870e-04, 6.5766e-04, 3.2773e-03, 1.8528e-02, 9.9037e-02, 1.9884e-01,\n",
      "         2.3278e-01, 2.4837e-01, 1.2133e-01, 7.7040e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.38, Train Loss: 0.00, Val Loss: 5.99, Train BLEU: 0.00, Val BLEU: 2.75, Minutes Elapsed: 20.16\n",
      "Sampling from val predictions...\n",
      "Source: có một người phụ_nữ địa_phương tuyệt_vời đã hướng_dẫn chúng_tôi .\n",
      "Reference: we had an amazing local woman who guided us\n",
      "Model: <SOS> so &apos;s is the the the . . <EOS>\n",
      "Attention Weights: tensor([[1.1766e-05, 1.6281e-03, 7.7843e-02, 5.0699e-01, 3.3741e-01, 7.4112e-02,\n",
      "         1.7175e-03, 2.4495e-04, 2.8698e-05, 8.2875e-06],\n",
      "        [1.6196e-04, 8.2038e-03, 1.2017e-01, 4.5895e-01, 3.3688e-01, 7.3145e-02,\n",
      "         2.2353e-03, 2.3913e-04, 1.5733e-05, 2.6149e-06],\n",
      "        [1.2515e-04, 7.2213e-03, 1.1189e-01, 3.6483e-01, 3.3615e-01, 1.6235e-01,\n",
      "         1.4132e-02, 2.9679e-03, 2.9092e-04, 5.0445e-05],\n",
      "        [4.7738e-05, 4.4748e-04, 4.7278e-03, 1.5168e-02, 1.8068e-02, 3.3904e-02,\n",
      "         2.2382e-02, 4.8073e-02, 1.5776e-01, 6.9942e-01],\n",
      "        [1.0438e-06, 4.8754e-06, 3.8656e-05, 1.2309e-04, 1.5416e-04, 4.9599e-04,\n",
      "         6.2262e-04, 3.7698e-03, 6.0166e-02, 9.3462e-01],\n",
      "        [6.8526e-07, 3.0507e-06, 2.4177e-05, 7.4608e-05, 9.4444e-05, 3.5654e-04,\n",
      "         5.6343e-04, 4.0052e-03, 6.7032e-02, 9.2785e-01],\n",
      "        [8.2227e-07, 3.6061e-06, 2.7857e-05, 8.2903e-05, 1.0478e-04, 4.0220e-04,\n",
      "         6.7748e-04, 4.7389e-03, 7.3249e-02, 9.2071e-01],\n",
      "        [1.0150e-06, 4.4227e-06, 3.3580e-05, 9.7174e-05, 1.2253e-04, 4.7137e-04,\n",
      "         8.2574e-04, 5.5982e-03, 7.9454e-02, 9.1339e-01],\n",
      "        [1.2406e-06, 5.3303e-06, 3.9423e-05, 1.1122e-04, 1.3964e-04, 5.3091e-04,\n",
      "         9.4756e-04, 6.2271e-03, 8.3354e-02, 9.0864e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.43, Train Loss: 0.00, Val Loss: 5.89, Train BLEU: 0.00, Val BLEU: 3.04, Minutes Elapsed: 22.62\n",
      "Sampling from val predictions...\n",
      "Source: đây là một bản_vẽ cách để khảo_sát toàn xã_hội bởi_lẽ\n",
      "Reference: this is a blueprint how to survey your society\n",
      "Model: <SOS> it &apos;s a a a a , , ,\n",
      "Attention Weights: tensor([[0.0541, 0.4531, 0.3497, 0.0727, 0.0493, 0.0179, 0.0026, 0.0004, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0902, 0.5222, 0.3071, 0.0468, 0.0247, 0.0076, 0.0012, 0.0002, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0204, 0.2503, 0.3222, 0.1230, 0.1560, 0.1010, 0.0218, 0.0040, 0.0007,\n",
      "         0.0006],\n",
      "        [0.0011, 0.0062, 0.0132, 0.0205, 0.0796, 0.2727, 0.2488, 0.1784, 0.0753,\n",
      "         0.1044],\n",
      "        [0.0001, 0.0003, 0.0009, 0.0024, 0.0124, 0.0709, 0.1311, 0.2167, 0.2106,\n",
      "         0.3546],\n",
      "        [0.0000, 0.0002, 0.0005, 0.0016, 0.0087, 0.0522, 0.1076, 0.2028, 0.2336,\n",
      "         0.3926],\n",
      "        [0.0000, 0.0002, 0.0005, 0.0016, 0.0083, 0.0486, 0.1017, 0.1977, 0.2387,\n",
      "         0.4027],\n",
      "        [0.0000, 0.0002, 0.0006, 0.0017, 0.0084, 0.0476, 0.0992, 0.1947, 0.2399,\n",
      "         0.4077],\n",
      "        [0.0001, 0.0002, 0.0006, 0.0017, 0.0084, 0.0469, 0.0981, 0.1935, 0.2416,\n",
      "         0.4088]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.48, Train Loss: 0.00, Val Loss: 5.84, Train BLEU: 0.00, Val BLEU: 2.49, Minutes Elapsed: 25.09\n",
      "Sampling from val predictions...\n",
      "Source: đủ để trồng <UNK> triệu cây cà_chua . <EOS> <PAD>\n",
      "Reference: that &apos;s enough space to plant <UNK> million tomato\n",
      "Model: <SOS> we , , , , . . . <EOS>\n",
      "Attention Weights: tensor([[0.5705, 0.3861, 0.0112, 0.0291, 0.0019, 0.0007, 0.0004, 0.0002, 0.0001,\n",
      "         0.0000],\n",
      "        [0.2897, 0.5732, 0.0453, 0.0887, 0.0027, 0.0004, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0007, 0.0051, 0.0090, 0.8024, 0.1100, 0.0459, 0.0191, 0.0061, 0.0016,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0002, 0.0004, 0.3368, 0.1461, 0.1915, 0.2078, 0.0990, 0.0182,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0076, 0.0144, 0.0694, 0.2765, 0.4591, 0.1730,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0007, 0.0021, 0.0141, 0.0950, 0.4165, 0.4716,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0003, 0.0010, 0.0070, 0.0528, 0.3265, 0.6124,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0003, 0.0009, 0.0062, 0.0461, 0.3006, 0.6459,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0003, 0.0010, 0.0063, 0.0455, 0.2940, 0.6529,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.53, Train Loss: 0.00, Val Loss: 5.77, Train BLEU: 0.00, Val BLEU: 3.75, Minutes Elapsed: 27.59\n",
      "Sampling from val predictions...\n",
      "Source: peter <UNK> là một giáo_sư triết_học trước khi làm công_việc\n",
      "Reference: peter <UNK> was a professor of philosophy before becoming\n",
      "Model: <SOS> but we we we we the the , the\n",
      "Attention Weights: tensor([[1.0446e-03, 8.9460e-01, 1.0348e-01, 8.3351e-04, 1.8015e-05, 2.4278e-05,\n",
      "         5.5912e-06, 1.3471e-06, 1.1400e-06, 5.8245e-07],\n",
      "        [1.6179e-03, 9.0266e-01, 9.5224e-02, 4.8784e-04, 4.5768e-06, 2.3822e-06,\n",
      "         2.5679e-07, 3.2492e-08, 1.7214e-08, 6.7119e-09],\n",
      "        [6.8701e-06, 8.1341e-01, 1.8623e-01, 3.4490e-04, 8.0103e-07, 3.8665e-07,\n",
      "         2.8461e-08, 3.1339e-09, 1.7809e-09, 7.2869e-10],\n",
      "        [5.3901e-07, 2.0118e-01, 7.7351e-01, 2.5129e-02, 1.1293e-04, 7.4057e-05,\n",
      "         1.8909e-06, 5.5528e-08, 1.9241e-08, 4.2645e-09],\n",
      "        [4.7231e-07, 1.0270e-02, 4.9072e-01, 3.3483e-01, 3.2930e-02, 1.2297e-01,\n",
      "         7.9903e-03, 2.1464e-04, 6.7148e-05, 9.4258e-06],\n",
      "        [1.0964e-07, 5.3943e-04, 6.3700e-02, 1.3475e-01, 5.2784e-02, 5.9271e-01,\n",
      "         1.3615e-01, 1.1329e-02, 6.6124e-03, 1.4300e-03],\n",
      "        [3.7853e-08, 1.1532e-04, 1.3260e-02, 4.3133e-02, 3.0291e-02, 5.4058e-01,\n",
      "         2.5509e-01, 5.0407e-02, 4.9058e-02, 1.8068e-02],\n",
      "        [2.5599e-08, 6.0512e-05, 6.1116e-03, 2.3763e-02, 2.0984e-02, 4.3914e-01,\n",
      "         2.7880e-01, 8.1877e-02, 1.0048e-01, 4.8780e-02],\n",
      "        [2.1572e-08, 4.9385e-05, 4.3869e-03, 1.8415e-02, 1.7880e-02, 3.8469e-01,\n",
      "         2.7507e-01, 9.6681e-02, 1.3028e-01, 7.2546e-02]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=100, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=True, print_attn=True, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_learning_curve(experiment_results[0]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(experiment_results)[['best_val_loss', 'best_val_bleu', 'runtime', \n",
    "                                       'total_params', 'trainable_params', 'dt_created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model and test \n",
    "checkpoint = torch.load('model_checkpoints/{}.pth.tar'.format(MODEL_NAME), map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
