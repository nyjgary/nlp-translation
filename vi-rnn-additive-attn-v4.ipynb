{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import train_and_eval, count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params required for generating data loaders \n",
    "\n",
    "SRC_LANG = 'vi'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture params \n",
    "NETWORK_TYPE = 'rnn'\n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 \n",
    "DEC_DROPOUT = 0.2  \n",
    "ATTENTION_TYPE = 'additive'\n",
    "\n",
    "# training params  \n",
    "NUM_EPOCHS = 15 \n",
    "LR = 0.0001 \n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = False\n",
    "\n",
    "# name the model and experiment \n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    EXPERIMENT_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    EXPERIMENT_NAME = '{}-cnn'.format(SRC_LANG)\n",
    "MODEL_NAME = '{}-{}'.format(EXPERIMENT_NAME, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n",
    "          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n",
    "          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n",
    "          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n",
    "          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n",
    "          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "if ATTENTION_TYPE == 'without': \n",
    "    # without attention \n",
    "    decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                         targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)\n",
    "    \n",
    "else: \n",
    "    # with attention \n",
    "    decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                             num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, \n",
    "                             src_max_sentence_len=SRC_MAX_SENTENCE_LEN, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                             dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                    vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.29, Train BLEU: 0.00, Val BLEU: 0.00, Minutes Elapsed: 0.13\n",
      "Sampling from val predictions...\n",
      "Source: chúng_ta những nạn_nhân cần đến tất_cả mọi người . <EOS>\n",
      "Reference: we victims need everyone . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> caption caption caption caption elitist elitist elitist freely freely\n",
      "Attention Weights: tensor([[0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027],\n",
      "        [0.0986, 0.0983, 0.0985, 0.1000, 0.1001, 0.1001, 0.1007, 0.1006, 0.1005,\n",
      "         0.1027]])\n",
      "\n",
      "Epoch: 0.05, Train Loss: 0.00, Val Loss: 6.38, Train BLEU: 0.00, Val BLEU: 0.03, Minutes Elapsed: 2.69\n",
      "Sampling from val predictions...\n",
      "Source: bởi_vì tại thời_điểm đó , toà_án hiến_pháp liên_bang đã quy_định\n",
      "Reference: because in the mean time , the german constitutional\n",
      "Model: <SOS> and , , , , , , , ,\n",
      "Attention Weights: tensor([[5.7219e-08, 8.3849e-06, 1.1718e-03, 1.8554e-02, 8.8084e-02, 1.6820e-01,\n",
      "         2.1600e-01, 2.8887e-01, 1.9591e-01, 2.3204e-02],\n",
      "        [3.0807e-05, 2.9487e-04, 3.4421e-03, 1.7298e-02, 4.7326e-02, 7.6239e-02,\n",
      "         9.9193e-02, 1.5470e-01, 2.6057e-01, 3.4091e-01],\n",
      "        [6.7604e-05, 4.5313e-04, 3.9083e-03, 1.7099e-02, 4.3866e-02, 6.9138e-02,\n",
      "         8.9534e-02, 1.4005e-01, 2.4918e-01, 3.8671e-01],\n",
      "        [7.2524e-05, 4.6308e-04, 3.8667e-03, 1.6738e-02, 4.2768e-02, 6.7357e-02,\n",
      "         8.7328e-02, 1.3711e-01, 2.4740e-01, 3.9690e-01],\n",
      "        [7.3656e-05, 4.6334e-04, 3.8369e-03, 1.6577e-02, 4.2333e-02, 6.6672e-02,\n",
      "         8.6488e-02, 1.3601e-01, 2.4676e-01, 4.0078e-01],\n",
      "        [7.4413e-05, 4.6433e-04, 3.8269e-03, 1.6511e-02, 4.2139e-02, 6.6355e-02,\n",
      "         8.6091e-02, 1.3548e-01, 2.4640e-01, 4.0267e-01],\n",
      "        [7.5110e-05, 4.6605e-04, 3.8269e-03, 1.6487e-02, 4.2047e-02, 6.6190e-02,\n",
      "         8.5877e-02, 1.3516e-01, 2.4615e-01, 4.0372e-01],\n",
      "        [7.5705e-05, 4.6781e-04, 3.8301e-03, 1.6479e-02, 4.2000e-02, 6.6097e-02,\n",
      "         8.5749e-02, 1.3497e-01, 2.4596e-01, 4.0437e-01],\n",
      "        [7.6273e-05, 4.6972e-04, 3.8356e-03, 1.6482e-02, 4.1979e-02, 6.6043e-02,\n",
      "         8.5669e-02, 1.3483e-01, 2.4581e-01, 4.0480e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.10, Train Loss: 0.00, Val Loss: 6.21, Train BLEU: 0.00, Val BLEU: 1.30, Minutes Elapsed: 5.19\n",
      "Sampling from val predictions...\n",
      "Source: các gia_đình hàng_xóm nghe kể về ý_tưởng này . <EOS>\n",
      "Reference: and my neighboring homes heard about this idea .\n",
      "Model: <SOS> and &apos;s the the . . . . .\n",
      "Attention Weights: tensor([[8.3157e-06, 4.1053e-03, 8.9195e-02, 2.5529e-01, 2.5606e-01, 2.0147e-01,\n",
      "         1.3048e-01, 5.1371e-02, 9.3558e-03, 2.6709e-03],\n",
      "        [8.7240e-05, 1.3469e-02, 1.0965e-01, 2.1270e-01, 2.0989e-01, 1.7924e-01,\n",
      "         1.3910e-01, 8.2685e-02, 3.1731e-02, 2.1444e-02],\n",
      "        [1.4440e-04, 5.4484e-03, 2.8936e-02, 5.1302e-02, 5.0970e-02, 4.5558e-02,\n",
      "         4.0251e-02, 3.4335e-02, 3.8145e-02, 7.0491e-01],\n",
      "        [1.2625e-05, 3.3452e-04, 1.8714e-03, 3.4943e-03, 3.4164e-03, 3.0291e-03,\n",
      "         2.7818e-03, 2.7626e-03, 5.7113e-03, 9.7659e-01],\n",
      "        [4.1132e-06, 1.0739e-04, 6.6137e-04, 1.2866e-03, 1.2309e-03, 1.0732e-03,\n",
      "         9.9927e-04, 1.0725e-03, 3.0761e-03, 9.9049e-01],\n",
      "        [2.9000e-06, 7.7782e-05, 5.0731e-04, 1.0065e-03, 9.4808e-04, 8.1702e-04,\n",
      "         7.6582e-04, 8.5745e-04, 2.8801e-03, 9.9214e-01],\n",
      "        [2.6825e-06, 7.3945e-05, 4.9757e-04, 9.9549e-04, 9.2817e-04, 7.9399e-04,\n",
      "         7.4687e-04, 8.5620e-04, 3.1001e-03, 9.9201e-01],\n",
      "        [2.7211e-06, 7.6566e-05, 5.2383e-04, 1.0513e-03, 9.7381e-04, 8.2920e-04,\n",
      "         7.8154e-04, 9.0801e-04, 3.4061e-03, 9.9145e-01],\n",
      "        [2.8364e-06, 8.0897e-05, 5.5815e-04, 1.1209e-03, 1.0340e-03, 8.7796e-04,\n",
      "         8.2844e-04, 9.6995e-04, 3.6978e-03, 9.9083e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.14, Train Loss: 0.00, Val Loss: 6.15, Train BLEU: 0.00, Val BLEU: 1.36, Minutes Elapsed: 7.69\n",
      "Sampling from val predictions...\n",
      "Source: bây_giờ tôi muốn giới_thiệu các bạn với những người em_trai\n",
      "Reference: now i &apos;d like to introduce you to my\n",
      "Model: <SOS> and , , , , , , , ,\n",
      "Attention Weights: tensor([[1.7282e-06, 3.5568e-05, 1.3156e-03, 3.7219e-02, 1.7338e-01, 3.2576e-01,\n",
      "         3.0054e-01, 1.5262e-01, 9.1044e-03, 2.7114e-05],\n",
      "        [1.7126e-05, 3.0989e-04, 6.4648e-03, 7.2837e-02, 1.9150e-01, 2.8140e-01,\n",
      "         2.6474e-01, 1.6437e-01, 1.8301e-02, 6.2360e-05],\n",
      "        [1.4059e-04, 1.7037e-03, 1.8358e-02, 9.8321e-02, 1.8429e-01, 2.3224e-01,\n",
      "         2.2533e-01, 1.7845e-01, 5.9534e-02, 1.6301e-03],\n",
      "        [5.3611e-04, 3.6287e-03, 2.3545e-02, 9.4603e-02, 1.6717e-01, 2.0627e-01,\n",
      "         2.0373e-01, 1.7875e-01, 1.0259e-01, 1.9182e-02],\n",
      "        [7.2082e-04, 3.8620e-03, 2.1698e-02, 8.5845e-02, 1.5745e-01, 1.9778e-01,\n",
      "         1.9591e-01, 1.7526e-01, 1.1587e-01, 4.5603e-02],\n",
      "        [7.7392e-04, 3.7384e-03, 1.9924e-02, 8.0012e-02, 1.5156e-01, 1.9314e-01,\n",
      "         1.9125e-01, 1.7196e-01, 1.2031e-01, 6.7338e-02],\n",
      "        [7.8888e-04, 3.5844e-03, 1.8576e-02, 7.5803e-02, 1.4721e-01, 1.8962e-01,\n",
      "         1.8759e-01, 1.6900e-01, 1.2230e-01, 8.5535e-02],\n",
      "        [7.9071e-04, 3.4500e-03, 1.7576e-02, 7.2698e-02, 1.4385e-01, 1.8678e-01,\n",
      "         1.8459e-01, 1.6644e-01, 1.2323e-01, 1.0059e-01],\n",
      "        [7.8703e-04, 3.3349e-03, 1.6794e-02, 7.0266e-02, 1.4112e-01, 1.8438e-01,\n",
      "         1.8204e-01, 1.6420e-01, 1.2368e-01, 1.1340e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.19, Train Loss: 0.00, Val Loss: 6.14, Train BLEU: 0.00, Val BLEU: 1.44, Minutes Elapsed: 10.19\n",
      "Sampling from val predictions...\n",
      "Source: cô cũng đã có bản_sao của bức ảnh . <EOS>\n",
      "Reference: she also had <UNK> . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> and &apos;s is the the . . . .\n",
      "Attention Weights: tensor([[3.1292e-05, 3.6202e-03, 9.7324e-02, 3.3806e-01, 3.9314e-01, 1.3732e-01,\n",
      "         2.6389e-02, 3.2184e-03, 5.8511e-04, 3.1986e-04],\n",
      "        [1.8756e-04, 1.2437e-02, 1.4713e-01, 3.3343e-01, 3.6238e-01, 1.2223e-01,\n",
      "         2.0536e-02, 1.5367e-03, 1.1547e-04, 1.8871e-05],\n",
      "        [3.5653e-04, 2.0291e-02, 1.5693e-01, 2.9468e-01, 3.1801e-01, 1.5951e-01,\n",
      "         4.4742e-02, 5.0739e-03, 3.6289e-04, 4.0808e-05],\n",
      "        [1.1531e-03, 2.9656e-02, 1.3656e-01, 2.1912e-01, 2.4009e-01, 1.9036e-01,\n",
      "         1.1862e-01, 4.6776e-02, 1.2559e-02, 5.1102e-03],\n",
      "        [9.0547e-04, 1.5486e-02, 6.8923e-02, 1.1573e-01, 1.3175e-01, 1.2594e-01,\n",
      "         1.1540e-01, 1.0272e-01, 1.0496e-01, 2.1819e-01],\n",
      "        [1.3627e-04, 1.9128e-03, 9.5038e-03, 1.7779e-02, 2.1168e-02, 2.2875e-02,\n",
      "         2.7258e-02, 4.3927e-02, 1.2164e-01, 7.3380e-01],\n",
      "        [1.0386e-05, 1.2707e-04, 7.6334e-04, 1.6898e-03, 2.1364e-03, 2.6388e-03,\n",
      "         4.1546e-03, 1.2055e-02, 7.5898e-02, 9.0053e-01],\n",
      "        [2.4940e-06, 2.8171e-05, 1.9460e-04, 4.9247e-04, 6.5117e-04, 8.8994e-04,\n",
      "         1.7054e-03, 7.0606e-03, 6.4324e-02, 9.2465e-01],\n",
      "        [1.4451e-06, 1.5578e-05, 1.1463e-04, 3.1086e-04, 4.2050e-04, 6.0692e-04,\n",
      "         1.2819e-03, 6.1633e-03, 6.2924e-02, 9.2816e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.24, Train Loss: 0.00, Val Loss: 6.09, Train BLEU: 0.00, Val BLEU: 1.22, Minutes Elapsed: 12.69\n",
      "Sampling from val predictions...\n",
      "Source: bức này được chụp vài tuần sau sự_kiện 11/9 ,\n",
      "Reference: this one was taken just weeks after 9 /\n",
      "Model: <SOS> so i , , , , , , ,\n",
      "Attention Weights: tensor([[1.0457e-06, 1.5149e-05, 6.0338e-04, 2.2053e-02, 1.4643e-01, 4.2314e-01,\n",
      "         3.2781e-01, 7.0959e-02, 8.1638e-03, 8.2759e-04],\n",
      "        [4.2169e-06, 7.0708e-05, 2.3426e-03, 4.7131e-02, 1.8065e-01, 3.9584e-01,\n",
      "         2.9627e-01, 7.0941e-02, 6.4913e-03, 2.6289e-04],\n",
      "        [9.4693e-06, 1.7831e-04, 4.9891e-03, 6.2688e-02, 1.8065e-01, 3.1932e-01,\n",
      "         2.8208e-01, 1.2204e-01, 2.5975e-02, 2.0665e-03],\n",
      "        [1.2596e-04, 9.5308e-04, 8.7100e-03, 4.9105e-02, 1.1280e-01, 1.7860e-01,\n",
      "         1.9239e-01, 1.6182e-01, 1.4628e-01, 1.4921e-01],\n",
      "        [1.1593e-04, 5.2860e-04, 3.1579e-03, 1.5769e-02, 3.9052e-02, 6.7206e-02,\n",
      "         8.0280e-02, 8.3599e-02, 1.4264e-01, 5.6765e-01],\n",
      "        [7.1836e-05, 2.7961e-04, 1.4950e-03, 7.5187e-03, 1.9850e-02, 3.6024e-02,\n",
      "         4.5431e-02, 5.2141e-02, 1.1660e-01, 7.2059e-01],\n",
      "        [5.5962e-05, 2.0487e-04, 1.0556e-03, 5.3935e-03, 1.4757e-02, 2.7476e-02,\n",
      "         3.5596e-02, 4.2847e-02, 1.0765e-01, 7.6496e-01],\n",
      "        [4.9239e-05, 1.7497e-04, 8.8792e-04, 4.5953e-03, 1.2844e-02, 2.4236e-02,\n",
      "         3.1873e-02, 3.9402e-02, 1.0485e-01, 7.8108e-01],\n",
      "        [4.5942e-05, 1.6063e-04, 8.0980e-04, 4.2333e-03, 1.1997e-02, 2.2811e-02,\n",
      "         3.0273e-02, 3.8047e-02, 1.0443e-01, 7.8719e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.29, Train Loss: 0.00, Val Loss: 6.06, Train BLEU: 0.00, Val BLEU: 2.00, Minutes Elapsed: 15.19\n",
      "Sampling from val predictions...\n",
      "Source: trước_tiên , bạn phải mang đến cho họ sự bảo_mật\n",
      "Reference: first , you have to offer them <UNK> .\n",
      "Model: <SOS> i , , , , , , , ,\n",
      "Attention Weights: tensor([[2.6359e-06, 5.9158e-05, 1.4391e-03, 2.7887e-02, 1.5529e-01, 2.3204e-01,\n",
      "         3.3462e-01, 1.9430e-01, 5.0176e-02, 4.1809e-03],\n",
      "        [1.3446e-05, 3.0760e-04, 5.4670e-03, 5.6093e-02, 1.8316e-01, 2.3615e-01,\n",
      "         2.8934e-01, 1.8256e-01, 4.4576e-02, 2.3322e-03],\n",
      "        [2.0934e-05, 4.7237e-04, 7.3419e-03, 5.3104e-02, 1.3759e-01, 1.7423e-01,\n",
      "         2.2129e-01, 2.0359e-01, 1.4693e-01, 5.5428e-02],\n",
      "        [1.5129e-05, 1.1361e-04, 7.9439e-04, 4.2160e-03, 1.1913e-02, 1.6806e-02,\n",
      "         2.7642e-02, 4.1169e-02, 1.2720e-01, 7.7013e-01],\n",
      "        [4.0656e-06, 1.9490e-05, 1.0138e-04, 5.2009e-04, 1.7202e-03, 2.6797e-03,\n",
      "         5.4974e-03, 1.0879e-02, 6.7424e-02, 9.1115e-01],\n",
      "        [3.1265e-06, 1.3651e-05, 6.6951e-05, 3.4415e-04, 1.2005e-03, 1.9329e-03,\n",
      "         4.2750e-03, 9.2237e-03, 6.5316e-02, 9.1762e-01],\n",
      "        [3.1651e-06, 1.3539e-05, 6.5637e-05, 3.3875e-04, 1.2034e-03, 1.9580e-03,\n",
      "         4.4383e-03, 9.7945e-03, 6.9322e-02, 9.1286e-01],\n",
      "        [3.4590e-06, 1.4757e-05, 7.1406e-05, 3.6860e-04, 1.3147e-03, 2.1436e-03,\n",
      "         4.8883e-03, 1.0804e-02, 7.4098e-02, 9.0629e-01],\n",
      "        [3.8461e-06, 1.6475e-05, 7.9853e-05, 4.1171e-04, 1.4661e-03, 2.3873e-03,\n",
      "         5.4363e-03, 1.1943e-02, 7.8822e-02, 8.9943e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.34, Train Loss: 0.00, Val Loss: 6.04, Train BLEU: 0.00, Val BLEU: 2.23, Minutes Elapsed: 17.67\n",
      "Sampling from val predictions...\n",
      "Source: đây là những ý_tưởng cần_thiết mà một đất_nước đã bị\n",
      "Reference: these are the ideals that a war-torn libya needs\n",
      "Model: <SOS> it &apos;s a a a a a a a\n",
      "Attention Weights: tensor([[8.1807e-04, 4.2498e-02, 5.2140e-01, 3.9098e-01, 4.3869e-02, 4.0526e-04,\n",
      "         2.7698e-05, 5.3590e-06, 6.8273e-07, 2.7185e-07],\n",
      "        [4.3248e-03, 9.0189e-02, 5.3184e-01, 3.2518e-01, 4.7631e-02, 7.6097e-04,\n",
      "         6.6216e-05, 1.3734e-05, 2.0928e-06, 9.0415e-07],\n",
      "        [2.2520e-03, 3.7600e-02, 2.4581e-01, 4.2283e-01, 2.7163e-01, 1.7383e-02,\n",
      "         2.0320e-03, 4.1716e-04, 3.8149e-05, 1.2422e-05],\n",
      "        [1.0343e-03, 4.4474e-03, 1.7510e-02, 6.3187e-02, 2.2266e-01, 2.5937e-01,\n",
      "         2.0730e-01, 1.6194e-01, 4.2664e-02, 1.9886e-02],\n",
      "        [2.0720e-04, 8.9477e-04, 4.0513e-03, 2.0703e-02, 1.0846e-01, 2.0928e-01,\n",
      "         2.3817e-01, 2.4812e-01, 1.0692e-01, 6.3190e-02],\n",
      "        [1.4516e-04, 6.7461e-04, 3.3066e-03, 1.8503e-02, 1.0056e-01, 2.0151e-01,\n",
      "         2.3506e-01, 2.4988e-01, 1.1747e-01, 7.2893e-02],\n",
      "        [1.4427e-04, 6.7255e-04, 3.2965e-03, 1.8452e-02, 9.9866e-02, 2.0082e-01,\n",
      "         2.3472e-01, 2.4979e-01, 1.1839e-01, 7.3854e-02],\n",
      "        [1.4298e-04, 6.6978e-04, 3.2971e-03, 1.8489e-02, 9.9390e-02, 1.9987e-01,\n",
      "         2.3387e-01, 2.4923e-01, 1.1974e-01, 7.5292e-02],\n",
      "        [1.3870e-04, 6.5766e-04, 3.2773e-03, 1.8528e-02, 9.9037e-02, 1.9884e-01,\n",
      "         2.3278e-01, 2.4837e-01, 1.2133e-01, 7.7040e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.38, Train Loss: 0.00, Val Loss: 5.99, Train BLEU: 0.00, Val BLEU: 2.75, Minutes Elapsed: 20.16\n",
      "Sampling from val predictions...\n",
      "Source: có một người phụ_nữ địa_phương tuyệt_vời đã hướng_dẫn chúng_tôi .\n",
      "Reference: we had an amazing local woman who guided us\n",
      "Model: <SOS> so &apos;s is the the the . . <EOS>\n",
      "Attention Weights: tensor([[1.1766e-05, 1.6281e-03, 7.7843e-02, 5.0699e-01, 3.3741e-01, 7.4112e-02,\n",
      "         1.7175e-03, 2.4495e-04, 2.8698e-05, 8.2875e-06],\n",
      "        [1.6196e-04, 8.2038e-03, 1.2017e-01, 4.5895e-01, 3.3688e-01, 7.3145e-02,\n",
      "         2.2353e-03, 2.3913e-04, 1.5733e-05, 2.6149e-06],\n",
      "        [1.2515e-04, 7.2213e-03, 1.1189e-01, 3.6483e-01, 3.3615e-01, 1.6235e-01,\n",
      "         1.4132e-02, 2.9679e-03, 2.9092e-04, 5.0445e-05],\n",
      "        [4.7738e-05, 4.4748e-04, 4.7278e-03, 1.5168e-02, 1.8068e-02, 3.3904e-02,\n",
      "         2.2382e-02, 4.8073e-02, 1.5776e-01, 6.9942e-01],\n",
      "        [1.0438e-06, 4.8754e-06, 3.8656e-05, 1.2309e-04, 1.5416e-04, 4.9599e-04,\n",
      "         6.2262e-04, 3.7698e-03, 6.0166e-02, 9.3462e-01],\n",
      "        [6.8526e-07, 3.0507e-06, 2.4177e-05, 7.4608e-05, 9.4444e-05, 3.5654e-04,\n",
      "         5.6343e-04, 4.0052e-03, 6.7032e-02, 9.2785e-01],\n",
      "        [8.2227e-07, 3.6061e-06, 2.7857e-05, 8.2903e-05, 1.0478e-04, 4.0220e-04,\n",
      "         6.7748e-04, 4.7389e-03, 7.3249e-02, 9.2071e-01],\n",
      "        [1.0150e-06, 4.4227e-06, 3.3580e-05, 9.7174e-05, 1.2253e-04, 4.7137e-04,\n",
      "         8.2574e-04, 5.5982e-03, 7.9454e-02, 9.1339e-01],\n",
      "        [1.2406e-06, 5.3303e-06, 3.9423e-05, 1.1122e-04, 1.3964e-04, 5.3091e-04,\n",
      "         9.4756e-04, 6.2271e-03, 8.3354e-02, 9.0864e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.43, Train Loss: 0.00, Val Loss: 5.89, Train BLEU: 0.00, Val BLEU: 3.04, Minutes Elapsed: 22.62\n",
      "Sampling from val predictions...\n",
      "Source: đây là một bản_vẽ cách để khảo_sát toàn xã_hội bởi_lẽ\n",
      "Reference: this is a blueprint how to survey your society\n",
      "Model: <SOS> it &apos;s a a a a , , ,\n",
      "Attention Weights: tensor([[0.0541, 0.4531, 0.3497, 0.0727, 0.0493, 0.0179, 0.0026, 0.0004, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0902, 0.5222, 0.3071, 0.0468, 0.0247, 0.0076, 0.0012, 0.0002, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0204, 0.2503, 0.3222, 0.1230, 0.1560, 0.1010, 0.0218, 0.0040, 0.0007,\n",
      "         0.0006],\n",
      "        [0.0011, 0.0062, 0.0132, 0.0205, 0.0796, 0.2727, 0.2488, 0.1784, 0.0753,\n",
      "         0.1044],\n",
      "        [0.0001, 0.0003, 0.0009, 0.0024, 0.0124, 0.0709, 0.1311, 0.2167, 0.2106,\n",
      "         0.3546],\n",
      "        [0.0000, 0.0002, 0.0005, 0.0016, 0.0087, 0.0522, 0.1076, 0.2028, 0.2336,\n",
      "         0.3926],\n",
      "        [0.0000, 0.0002, 0.0005, 0.0016, 0.0083, 0.0486, 0.1017, 0.1977, 0.2387,\n",
      "         0.4027],\n",
      "        [0.0000, 0.0002, 0.0006, 0.0017, 0.0084, 0.0476, 0.0992, 0.1947, 0.2399,\n",
      "         0.4077],\n",
      "        [0.0001, 0.0002, 0.0006, 0.0017, 0.0084, 0.0469, 0.0981, 0.1935, 0.2416,\n",
      "         0.4088]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.48, Train Loss: 0.00, Val Loss: 5.84, Train BLEU: 0.00, Val BLEU: 2.49, Minutes Elapsed: 25.09\n",
      "Sampling from val predictions...\n",
      "Source: đủ để trồng <UNK> triệu cây cà_chua . <EOS> <PAD>\n",
      "Reference: that &apos;s enough space to plant <UNK> million tomato\n",
      "Model: <SOS> we , , , , . . . <EOS>\n",
      "Attention Weights: tensor([[0.5705, 0.3861, 0.0112, 0.0291, 0.0019, 0.0007, 0.0004, 0.0002, 0.0001,\n",
      "         0.0000],\n",
      "        [0.2897, 0.5732, 0.0453, 0.0887, 0.0027, 0.0004, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0007, 0.0051, 0.0090, 0.8024, 0.1100, 0.0459, 0.0191, 0.0061, 0.0016,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0002, 0.0004, 0.3368, 0.1461, 0.1915, 0.2078, 0.0990, 0.0182,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0076, 0.0144, 0.0694, 0.2765, 0.4591, 0.1730,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0007, 0.0021, 0.0141, 0.0950, 0.4165, 0.4716,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0003, 0.0010, 0.0070, 0.0528, 0.3265, 0.6124,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0003, 0.0009, 0.0062, 0.0461, 0.3006, 0.6459,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0003, 0.0010, 0.0063, 0.0455, 0.2940, 0.6529,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.53, Train Loss: 0.00, Val Loss: 5.77, Train BLEU: 0.00, Val BLEU: 3.75, Minutes Elapsed: 27.59\n",
      "Sampling from val predictions...\n",
      "Source: peter <UNK> là một giáo_sư triết_học trước khi làm công_việc\n",
      "Reference: peter <UNK> was a professor of philosophy before becoming\n",
      "Model: <SOS> but we we we we the the , the\n",
      "Attention Weights: tensor([[1.0446e-03, 8.9460e-01, 1.0348e-01, 8.3351e-04, 1.8015e-05, 2.4278e-05,\n",
      "         5.5912e-06, 1.3471e-06, 1.1400e-06, 5.8245e-07],\n",
      "        [1.6179e-03, 9.0266e-01, 9.5224e-02, 4.8784e-04, 4.5768e-06, 2.3822e-06,\n",
      "         2.5679e-07, 3.2492e-08, 1.7214e-08, 6.7119e-09],\n",
      "        [6.8701e-06, 8.1341e-01, 1.8623e-01, 3.4490e-04, 8.0103e-07, 3.8665e-07,\n",
      "         2.8461e-08, 3.1339e-09, 1.7809e-09, 7.2869e-10],\n",
      "        [5.3901e-07, 2.0118e-01, 7.7351e-01, 2.5129e-02, 1.1293e-04, 7.4057e-05,\n",
      "         1.8909e-06, 5.5528e-08, 1.9241e-08, 4.2645e-09],\n",
      "        [4.7231e-07, 1.0270e-02, 4.9072e-01, 3.3483e-01, 3.2930e-02, 1.2297e-01,\n",
      "         7.9903e-03, 2.1464e-04, 6.7148e-05, 9.4258e-06],\n",
      "        [1.0964e-07, 5.3943e-04, 6.3700e-02, 1.3475e-01, 5.2784e-02, 5.9271e-01,\n",
      "         1.3615e-01, 1.1329e-02, 6.6124e-03, 1.4300e-03],\n",
      "        [3.7853e-08, 1.1532e-04, 1.3260e-02, 4.3133e-02, 3.0291e-02, 5.4058e-01,\n",
      "         2.5509e-01, 5.0407e-02, 4.9058e-02, 1.8068e-02],\n",
      "        [2.5599e-08, 6.0512e-05, 6.1116e-03, 2.3763e-02, 2.0984e-02, 4.3914e-01,\n",
      "         2.7880e-01, 8.1877e-02, 1.0048e-01, 4.8780e-02],\n",
      "        [2.1572e-08, 4.9385e-05, 4.3869e-03, 1.8415e-02, 1.7880e-02, 3.8469e-01,\n",
      "         2.7507e-01, 9.6681e-02, 1.3028e-01, 7.2546e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.58, Train Loss: 0.00, Val Loss: 5.67, Train BLEU: 0.00, Val BLEU: 3.52, Minutes Elapsed: 30.08\n",
      "Sampling from val predictions...\n",
      "Source: đó là thời_điểm tuyệt_vọng nhất trong cuộc_đời tôi . <EOS>\n",
      "Reference: this was one of the lowest points in my\n",
      "Model: <SOS> it &apos;s a the the . . . .\n",
      "Attention Weights: tensor([[1.8613e-01, 7.8057e-01, 3.2145e-02, 1.1084e-03, 4.0639e-05, 2.9356e-06,\n",
      "         5.0399e-07, 3.0161e-07, 3.6258e-08, 8.5195e-09],\n",
      "        [3.9610e-02, 7.5760e-01, 1.9241e-01, 1.0146e-02, 2.1940e-04, 8.7719e-06,\n",
      "         7.3705e-07, 3.1033e-07, 4.6370e-08, 9.7483e-09],\n",
      "        [3.1201e-04, 1.8002e-02, 3.5146e-01, 5.5716e-01, 6.6564e-02, 5.6784e-03,\n",
      "         5.5388e-04, 2.4234e-04, 2.2225e-05, 2.2705e-06],\n",
      "        [1.2947e-05, 2.6223e-04, 1.2604e-02, 4.4706e-01, 4.3659e-01, 8.8613e-02,\n",
      "         1.0611e-02, 4.1626e-03, 8.2793e-05, 1.7899e-06],\n",
      "        [5.1065e-06, 5.0826e-05, 1.2875e-03, 6.3951e-02, 3.0022e-01, 3.0204e-01,\n",
      "         1.8520e-01, 1.4083e-01, 6.2681e-03, 1.4724e-04],\n",
      "        [3.0713e-06, 2.0888e-05, 2.8831e-04, 9.4822e-03, 7.5004e-02, 1.5467e-01,\n",
      "         2.9204e-01, 4.1283e-01, 5.1444e-02, 4.2134e-03],\n",
      "        [3.4039e-06, 1.8867e-05, 1.8439e-04, 4.4513e-03, 3.8107e-02, 9.3701e-02,\n",
      "         2.7255e-01, 4.9787e-01, 8.0767e-02, 1.2348e-02],\n",
      "        [4.1740e-06, 2.0890e-05, 1.7178e-04, 3.5021e-03, 2.9929e-02, 7.7465e-02,\n",
      "         2.5849e-01, 5.1884e-01, 9.3125e-02, 1.8445e-02],\n",
      "        [4.8852e-06, 2.3151e-05, 1.7344e-04, 3.1956e-03, 2.7034e-02, 7.1907e-02,\n",
      "         2.5076e-01, 5.2363e-01, 1.0064e-01, 2.2630e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.62, Train Loss: 0.00, Val Loss: 5.60, Train BLEU: 0.00, Val BLEU: 3.40, Minutes Elapsed: 32.59\n",
      "Sampling from val predictions...\n",
      "Source: và điều thứ_hai tôi muốn nói_là mọi người nghĩ rằng\n",
      "Reference: and my second message is that everyone thinks domestic\n",
      "Model: <SOS> and i i i i i i i to\n",
      "Attention Weights: tensor([[1.1037e-01, 7.0491e-01, 1.4750e-01, 3.7063e-02, 1.3919e-04, 7.7674e-06,\n",
      "         3.9060e-06, 1.6860e-06, 6.7571e-07, 5.0811e-07],\n",
      "        [6.8913e-02, 7.1355e-01, 1.9303e-01, 2.4452e-02, 5.5080e-05, 1.0403e-06,\n",
      "         2.9983e-07, 8.1419e-08, 2.1812e-08, 1.1737e-08],\n",
      "        [1.0286e-03, 1.0157e-01, 6.7504e-01, 2.2218e-01, 1.7347e-04, 9.6359e-07,\n",
      "         2.1646e-07, 4.4960e-08, 9.9329e-09, 5.3106e-09],\n",
      "        [1.7969e-06, 2.3861e-04, 8.3730e-02, 9.1400e-01, 2.0250e-03, 4.5371e-06,\n",
      "         6.9935e-07, 6.8191e-08, 6.1186e-09, 2.3409e-09],\n",
      "        [1.8520e-07, 7.2519e-06, 3.8203e-03, 9.6048e-01, 3.5281e-02, 3.5348e-04,\n",
      "         5.3350e-05, 2.1579e-06, 4.4610e-08, 7.2081e-09],\n",
      "        [2.7801e-07, 4.7711e-06, 1.3546e-03, 7.7163e-01, 2.0144e-01, 1.8706e-02,\n",
      "         6.5283e-03, 3.2637e-04, 3.8936e-06, 3.5263e-07],\n",
      "        [5.2375e-07, 4.3284e-06, 5.6810e-04, 2.8904e-01, 2.7517e-01, 1.9021e-01,\n",
      "         2.1595e-01, 2.8477e-02, 5.3066e-04, 4.3613e-05],\n",
      "        [4.2862e-07, 2.2018e-06, 1.7389e-04, 5.6921e-02, 8.9304e-02, 1.7334e-01,\n",
      "         5.0364e-01, 1.6789e-01, 7.7439e-03, 9.8396e-04],\n",
      "        [2.7061e-07, 9.0264e-07, 4.1960e-05, 8.3569e-03, 1.7214e-02, 7.2459e-02,\n",
      "         4.5637e-01, 3.7518e-01, 5.5147e-02, 1.5227e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.67, Train Loss: 0.00, Val Loss: 5.55, Train BLEU: 0.00, Val BLEU: 3.72, Minutes Elapsed: 35.08\n",
      "Sampling from val predictions...\n",
      "Source: câu_hỏi cuối_cùng mà người_ta hỏi tôi là : \" cảm_giác\n",
      "Reference: so the last question people ask me is ,\n",
      "Model: <SOS> so , the , i i : &quot; &quot;\n",
      "Attention Weights: tensor([[4.9055e-01, 3.8774e-01, 1.1988e-01, 1.7320e-03, 9.6985e-05, 4.6135e-06,\n",
      "         2.0300e-07, 4.6817e-08, 1.3970e-07, 3.0262e-08],\n",
      "        [3.2392e-02, 2.5114e-01, 6.9217e-01, 2.3467e-02, 8.2104e-04, 9.5871e-06,\n",
      "         8.2926e-08, 5.7011e-09, 2.1211e-08, 1.6315e-09],\n",
      "        [9.5558e-05, 1.9940e-03, 4.9421e-01, 4.0883e-01, 9.2676e-02, 2.1936e-03,\n",
      "         3.9060e-06, 6.6337e-08, 5.3904e-07, 1.0028e-08],\n",
      "        [6.7745e-06, 5.9751e-05, 1.3277e-02, 1.9278e-01, 5.5696e-01, 2.3520e-01,\n",
      "         1.4225e-03, 1.2737e-05, 2.8420e-04, 7.4746e-07],\n",
      "        [1.5426e-06, 1.0503e-05, 1.0207e-03, 2.6593e-02, 2.6478e-01, 6.5085e-01,\n",
      "         2.9244e-02, 9.3824e-04, 2.6473e-02, 8.7474e-05],\n",
      "        [4.6308e-07, 2.4861e-06, 1.1278e-04, 2.7088e-03, 4.6594e-02, 3.5166e-01,\n",
      "         1.0315e-01, 2.4372e-02, 4.6147e-01, 9.9318e-03],\n",
      "        [1.8522e-07, 8.4128e-07, 2.3128e-05, 4.1158e-04, 7.3191e-03, 8.6361e-02,\n",
      "         6.0666e-02, 5.4889e-02, 6.9680e-01, 9.3527e-02],\n",
      "        [1.4637e-07, 6.4075e-07, 1.5072e-05, 2.1276e-04, 3.3744e-03, 4.4623e-02,\n",
      "         3.9277e-02, 6.0482e-02, 6.6026e-01, 1.9176e-01],\n",
      "        [1.4260e-07, 5.7256e-07, 1.1298e-05, 1.3662e-04, 1.9225e-03, 2.6468e-02,\n",
      "         2.7217e-02, 6.0847e-02, 5.7656e-01, 3.0684e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.77, Train Loss: 0.00, Val Loss: 5.46, Train BLEU: 0.00, Val BLEU: 5.28, Minutes Elapsed: 40.02\n",
      "Sampling from val predictions...\n",
      "Source: nô_lệ ngày_nay phục_vụ cho thương_mại , hàng_hoá mà người nô_lệ\n",
      "Reference: today &apos;s slavery is about commerce , so the\n",
      "Model: <SOS> it &apos;s the to the , , , ,\n",
      "Attention Weights: tensor([[8.0050e-01, 1.4997e-01, 4.2372e-02, 3.2528e-03, 9.9424e-04, 1.2077e-03,\n",
      "         8.9237e-04, 4.5219e-04, 2.0877e-04, 1.4913e-04],\n",
      "        [5.4096e-02, 5.4102e-01, 3.9715e-01, 6.4573e-03, 6.4467e-04, 4.4545e-04,\n",
      "         1.2498e-04, 4.2546e-05, 1.4682e-05, 7.4359e-06],\n",
      "        [2.2727e-04, 4.3074e-02, 9.1096e-01, 4.0492e-02, 2.8058e-03, 2.0365e-03,\n",
      "         3.1641e-04, 6.9798e-05, 1.4399e-05, 4.7048e-06],\n",
      "        [1.3490e-05, 1.1751e-03, 4.8741e-01, 3.4837e-01, 6.4312e-02, 9.2415e-02,\n",
      "         5.4833e-03, 7.5666e-04, 6.0683e-05, 6.3060e-06],\n",
      "        [3.0373e-05, 5.8892e-04, 7.5368e-02, 2.6083e-01, 1.5556e-01, 4.5980e-01,\n",
      "         3.7788e-02, 8.9655e-03, 9.7642e-04, 9.5699e-05],\n",
      "        [1.9769e-06, 3.2867e-05, 2.1002e-03, 2.6387e-02, 8.0198e-02, 5.9611e-01,\n",
      "         1.8200e-01, 8.8538e-02, 2.0875e-02, 3.7518e-03],\n",
      "        [2.3604e-06, 2.9286e-05, 9.9526e-04, 1.1965e-02, 4.9395e-02, 4.8743e-01,\n",
      "         2.3584e-01, 1.5167e-01, 4.9578e-02, 1.3097e-02],\n",
      "        [1.0768e-06, 1.3202e-05, 3.9378e-04, 4.8354e-03, 2.6663e-02, 2.9997e-01,\n",
      "         2.8375e-01, 2.3149e-01, 1.0720e-01, 4.5692e-02],\n",
      "        [5.5223e-07, 7.1350e-06, 2.1390e-04, 2.6365e-03, 1.6644e-02, 2.1031e-01,\n",
      "         2.8654e-01, 2.6059e-01, 1.4231e-01, 8.0751e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.82, Train Loss: 0.00, Val Loss: 5.40, Train BLEU: 0.00, Val BLEU: 4.73, Minutes Elapsed: 42.48\n",
      "Sampling from val predictions...\n",
      "Source: vì không có kền_kền nên mới có sự tràn_lan về\n",
      "Reference: because there are no vultures , there &apos;s been\n",
      "Model: <SOS> so &apos;s not a a a a a a\n",
      "Attention Weights: tensor([[1.5270e-01, 8.1646e-01, 3.0560e-02, 2.5537e-04, 1.1453e-05, 4.1168e-06,\n",
      "         1.1056e-06, 3.1507e-07, 7.8508e-08, 8.3457e-08],\n",
      "        [5.1992e-03, 7.2852e-01, 2.6317e-01, 3.0718e-03, 3.3307e-05, 3.3300e-06,\n",
      "         2.7895e-07, 3.3756e-08, 5.9625e-09, 5.7102e-09],\n",
      "        [1.1721e-04, 3.8180e-02, 7.0668e-01, 2.3868e-01, 1.4455e-02, 1.8037e-03,\n",
      "         8.4603e-05, 3.8370e-06, 2.2505e-07, 1.9238e-07],\n",
      "        [3.4558e-06, 1.1101e-04, 4.9435e-03, 1.1941e-01, 4.0641e-01, 4.3052e-01,\n",
      "         3.7751e-02, 8.2623e-04, 9.2130e-06, 6.6647e-06],\n",
      "        [1.1008e-06, 1.2358e-05, 2.0781e-04, 4.6576e-03, 9.7083e-02, 6.4065e-01,\n",
      "         2.4246e-01, 1.4384e-02, 2.1824e-04, 3.1993e-04],\n",
      "        [1.6606e-06, 1.1111e-05, 9.1377e-05, 1.0508e-03, 2.4703e-02, 3.7506e-01,\n",
      "         4.6118e-01, 1.1330e-01, 6.9533e-03, 1.7654e-02],\n",
      "        [1.4857e-06, 8.4771e-06, 5.8924e-05, 5.0444e-04, 8.7738e-03, 1.6055e-01,\n",
      "         3.6606e-01, 2.3668e-01, 5.5625e-02, 1.7174e-01],\n",
      "        [3.1372e-06, 1.5130e-05, 8.8185e-05, 6.0942e-04, 7.1751e-03, 9.8845e-02,\n",
      "         2.5778e-01, 2.2579e-01, 9.5509e-02, 3.1419e-01],\n",
      "        [1.3348e-06, 7.3765e-06, 5.2723e-05, 4.1087e-04, 5.1369e-03, 8.6407e-02,\n",
      "         2.3957e-01, 2.3640e-01, 1.0623e-01, 3.2578e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.86, Train Loss: 0.00, Val Loss: 5.34, Train BLEU: 0.00, Val BLEU: 5.32, Minutes Elapsed: 44.97\n",
      "Sampling from val predictions...\n",
      "Source: ở ấn_độ và nepal , tôi được đưa tới 1\n",
      "Reference: in india and nepal , i was introduced to\n",
      "Model: <SOS> in , , , , i i to to\n",
      "Attention Weights: tensor([[8.7168e-01, 1.2829e-01, 2.7023e-05, 1.7518e-06, 2.4356e-07, 9.0148e-08,\n",
      "         1.5688e-08, 7.8030e-09, 3.7179e-09, 3.8579e-09],\n",
      "        [2.5066e-02, 9.6946e-01, 4.8557e-03, 5.9817e-04, 1.6818e-05, 1.2238e-06,\n",
      "         4.3601e-08, 8.1133e-09, 1.8570e-09, 1.4985e-09],\n",
      "        [1.6787e-03, 4.5425e-01, 2.1219e-01, 2.8655e-01, 4.2493e-02, 2.8243e-03,\n",
      "         1.5308e-05, 6.6289e-07, 4.5426e-08, 2.7738e-08],\n",
      "        [2.2424e-04, 3.0858e-02, 8.4996e-02, 2.6502e-01, 4.6017e-01, 1.5720e-01,\n",
      "         1.4835e-03, 5.1738e-05, 1.7593e-06, 7.6845e-07],\n",
      "        [2.1737e-05, 1.4740e-03, 8.9649e-03, 5.0626e-02, 3.7042e-01, 5.5375e-01,\n",
      "         1.4057e-02, 6.6243e-04, 2.0480e-05, 7.6438e-06],\n",
      "        [9.5537e-06, 6.1005e-04, 3.8010e-03, 2.8028e-02, 2.3095e-01, 6.9494e-01,\n",
      "         3.8994e-02, 2.5628e-03, 7.8044e-05, 2.6690e-05],\n",
      "        [1.3953e-06, 7.6015e-05, 6.3415e-04, 6.8815e-03, 1.2695e-01, 7.1028e-01,\n",
      "         1.3184e-01, 2.1492e-02, 1.3010e-03, 5.4673e-04],\n",
      "        [9.2386e-07, 4.3586e-05, 3.6745e-04, 4.4262e-03, 9.9918e-02, 6.3514e-01,\n",
      "         1.9590e-01, 5.4621e-02, 6.0792e-03, 3.5076e-03],\n",
      "        [4.5220e-07, 7.0208e-06, 3.6802e-05, 2.3207e-04, 2.9164e-02, 2.1568e-01,\n",
      "         2.6710e-01, 2.5993e-01, 1.0403e-01, 1.2382e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.91, Train Loss: 0.00, Val Loss: 5.29, Train BLEU: 0.00, Val BLEU: 5.18, Minutes Elapsed: 47.45\n",
      "Sampling from val predictions...\n",
      "Source: tôi luôn sống trong một nỗi sợ thường_trực rằng danh_tính\n",
      "Reference: so i was living in constant fear that my\n",
      "Model: <SOS> i i i in in in of the the\n",
      "Attention Weights: tensor([[9.9237e-01, 7.4847e-03, 1.3464e-04, 8.5499e-06, 1.1443e-06, 3.5057e-07,\n",
      "         2.1352e-07, 1.3668e-07, 6.8434e-08, 4.3427e-08],\n",
      "        [2.3590e-01, 7.0730e-01, 5.5778e-02, 1.0062e-03, 1.2297e-05, 7.9729e-07,\n",
      "         2.0646e-07, 6.0625e-08, 1.5731e-08, 6.6448e-09],\n",
      "        [1.1547e-03, 1.9728e-01, 7.0373e-01, 9.5871e-02, 1.8803e-03, 7.8705e-05,\n",
      "         1.0351e-05, 1.3845e-06, 1.2015e-07, 2.7268e-08],\n",
      "        [1.5006e-05, 4.8522e-03, 3.2986e-01, 6.3664e-01, 2.7471e-02, 1.0387e-03,\n",
      "         1.0914e-04, 9.1701e-06, 3.1106e-07, 2.8646e-08],\n",
      "        [5.3687e-06, 1.5807e-04, 2.3340e-02, 7.3547e-01, 2.1114e-01, 2.4184e-02,\n",
      "         4.8809e-03, 7.6467e-04, 5.3167e-05, 5.6115e-06],\n",
      "        [1.1058e-05, 1.7323e-04, 8.2525e-03, 3.7444e-01, 4.5423e-01, 1.1352e-01,\n",
      "         3.9345e-02, 8.8693e-03, 9.9615e-04, 1.6162e-04],\n",
      "        [1.2343e-06, 1.5759e-05, 4.5133e-04, 2.7461e-02, 2.7516e-01, 2.5119e-01,\n",
      "         2.4436e-01, 1.4758e-01, 4.0821e-02, 1.2966e-02],\n",
      "        [1.2307e-06, 9.1574e-06, 1.5358e-04, 7.3867e-03, 1.3386e-01, 1.7510e-01,\n",
      "         2.6667e-01, 2.5080e-01, 1.0981e-01, 5.6224e-02],\n",
      "        [4.3883e-07, 3.6420e-06, 5.5166e-05, 2.0601e-03, 5.1653e-02, 9.2239e-02,\n",
      "         1.8752e-01, 2.7378e-01, 2.0671e-01, 1.8597e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.96, Train Loss: 0.00, Val Loss: 5.26, Train BLEU: 0.00, Val BLEU: 5.66, Minutes Elapsed: 49.93\n",
      "Sampling from val predictions...\n",
      "Source: mà tôi đến đây để thú_nhận rằng chúng_tôi - toàn_bộ\n",
      "Reference: i &apos;m rather here today to confess that we\n",
      "Model: <SOS> i i to to to to to to the\n",
      "Attention Weights: tensor([[9.1450e-01, 8.5334e-02, 1.5667e-04, 8.7753e-06, 1.6031e-06, 3.2249e-07,\n",
      "         1.9355e-07, 1.3943e-07, 8.0760e-08, 5.3230e-08],\n",
      "        [2.2294e-01, 7.6179e-01, 1.4771e-02, 4.8101e-04, 1.5964e-05, 7.7097e-07,\n",
      "         2.3120e-07, 9.2500e-08, 4.4787e-08, 1.8298e-08],\n",
      "        [5.9071e-03, 2.9158e-01, 5.0441e-01, 1.8674e-01, 1.1199e-02, 1.4698e-04,\n",
      "         1.4277e-05, 1.9056e-06, 4.1599e-07, 5.3993e-08],\n",
      "        [9.1625e-06, 6.1850e-04, 2.2300e-02, 5.9031e-01, 3.7351e-01, 1.1277e-02,\n",
      "         1.7519e-03, 1.9744e-04, 2.7344e-05, 1.0596e-06],\n",
      "        [2.7786e-05, 4.9968e-04, 1.2259e-02, 4.4714e-01, 5.0067e-01, 2.8668e-02,\n",
      "         8.4528e-03, 1.8149e-03, 4.4015e-04, 2.4291e-05],\n",
      "        [2.7936e-06, 3.7697e-05, 9.9672e-04, 7.8742e-02, 5.7188e-01, 1.5231e-01,\n",
      "         1.2666e-01, 4.8510e-02, 1.9619e-02, 1.2408e-03],\n",
      "        [9.9663e-07, 8.0422e-06, 1.1072e-04, 5.5231e-03, 1.0867e-01, 9.8432e-02,\n",
      "         2.5504e-01, 2.7301e-01, 2.2085e-01, 3.8358e-02],\n",
      "        [3.8824e-07, 3.6139e-06, 4.0484e-05, 1.2164e-03, 2.3054e-02, 2.9581e-02,\n",
      "         1.3677e-01, 3.0452e-01, 3.3730e-01, 1.6751e-01],\n",
      "        [2.1483e-06, 1.2860e-05, 8.3017e-05, 1.2032e-03, 1.5377e-02, 2.1179e-02,\n",
      "         1.1073e-01, 2.8427e-01, 3.6254e-01, 2.0460e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.00, Train Loss: 0.00, Val Loss: 5.28, Train BLEU: 0.00, Val BLEU: 6.03, Minutes Elapsed: 52.02\n",
      "Sampling from val predictions...\n",
      "Source: trong đó có viết : khi chị đọc được những\n",
      "Reference: it read , &quot; when you read this ,\n",
      "Model: <SOS> in the a the : , can to the\n",
      "Attention Weights: tensor([[8.6388e-01, 1.3544e-01, 6.7670e-04, 1.5325e-06, 2.6926e-07, 2.9850e-07,\n",
      "         1.1137e-07, 4.2744e-08, 1.6382e-08, 9.4509e-09],\n",
      "        [5.4980e-02, 9.0591e-01, 3.9082e-02, 2.1801e-05, 8.7538e-07, 5.9822e-07,\n",
      "         6.7294e-08, 1.4676e-08, 3.8120e-09, 1.4178e-09],\n",
      "        [9.9959e-04, 2.4220e-01, 7.4983e-01, 6.5623e-03, 1.9802e-04, 2.0079e-04,\n",
      "         3.2182e-06, 1.3476e-07, 8.0545e-09, 1.3225e-09],\n",
      "        [1.6443e-04, 1.8972e-02, 5.2727e-01, 1.8219e-01, 4.7285e-02, 2.1023e-01,\n",
      "         1.3258e-02, 6.0851e-04, 1.4469e-05, 8.1195e-07],\n",
      "        [4.2891e-06, 2.0819e-04, 9.0535e-03, 3.0172e-02, 5.3070e-02, 6.6945e-01,\n",
      "         2.0445e-01, 3.1958e-02, 1.5249e-03, 1.0048e-04],\n",
      "        [1.0163e-06, 5.1926e-05, 1.7629e-03, 7.6161e-03, 2.3209e-02, 4.1851e-01,\n",
      "         4.1856e-01, 1.1915e-01, 1.0141e-02, 1.0007e-03],\n",
      "        [4.2472e-07, 1.6277e-05, 5.4736e-04, 2.9969e-03, 1.0972e-02, 2.5625e-01,\n",
      "         4.3460e-01, 2.4417e-01, 4.2811e-02, 7.6351e-03],\n",
      "        [2.2406e-07, 7.7572e-06, 2.4385e-04, 1.5990e-03, 7.2224e-03, 1.6289e-01,\n",
      "         3.9427e-01, 3.2228e-01, 8.8080e-02, 2.3409e-02],\n",
      "        [1.1009e-06, 2.3155e-05, 3.9091e-04, 1.7648e-03, 5.6246e-03, 9.6852e-02,\n",
      "         2.7246e-01, 3.8350e-01, 1.7110e-01, 6.8287e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.05, Train Loss: 0.00, Val Loss: 5.22, Train BLEU: 0.00, Val BLEU: 6.15, Minutes Elapsed: 54.59\n",
      "Sampling from val predictions...\n",
      "Source: tôi như bị tát vào mặt khi đọc cuốn sách\n",
      "Reference: i was given a slap in the face reading\n",
      "Model: <SOS> i i to to to to to , of\n",
      "Attention Weights: tensor([[9.9557e-01, 4.2010e-03, 2.2152e-04, 3.9503e-06, 3.4180e-07, 1.1063e-07,\n",
      "         5.1102e-08, 3.5063e-08, 2.5537e-08, 1.6048e-08],\n",
      "        [2.2293e-01, 5.5896e-01, 2.1616e-01, 1.9203e-03, 2.2978e-05, 1.3741e-06,\n",
      "         1.3642e-07, 3.8396e-08, 1.6925e-08, 6.1387e-09],\n",
      "        [3.8388e-03, 7.7396e-02, 8.0802e-01, 1.0390e-01, 6.2763e-03, 5.2751e-04,\n",
      "         3.4151e-05, 4.9985e-06, 1.0753e-06, 1.3858e-07],\n",
      "        [9.0357e-05, 5.8155e-04, 1.3947e-01, 5.8879e-01, 2.3839e-01, 3.0435e-02,\n",
      "         1.7948e-03, 3.5908e-04, 7.6750e-05, 6.2066e-06],\n",
      "        [2.3200e-05, 1.3225e-04, 2.2208e-02, 3.0809e-01, 4.7219e-01, 1.6860e-01,\n",
      "         2.1658e-02, 5.6029e-03, 1.3688e-03, 1.2778e-04],\n",
      "        [3.4960e-06, 1.7408e-05, 1.2814e-03, 3.0244e-02, 2.0703e-01, 3.4188e-01,\n",
      "         2.2315e-01, 1.3855e-01, 5.0541e-02, 7.3028e-03],\n",
      "        [1.9061e-06, 7.0401e-06, 2.7117e-04, 4.9275e-03, 4.6315e-02, 1.4669e-01,\n",
      "         2.5270e-01, 3.1905e-01, 1.8441e-01, 4.5630e-02],\n",
      "        [2.5480e-07, 1.1598e-06, 4.2179e-05, 8.0926e-04, 9.6543e-03, 4.0898e-02,\n",
      "         1.2068e-01, 3.0631e-01, 3.3290e-01, 1.8870e-01],\n",
      "        [1.6461e-07, 7.1965e-07, 2.0271e-05, 3.1541e-04, 3.6210e-03, 1.7804e-02,\n",
      "         7.6827e-02, 2.6316e-01, 3.5418e-01, 2.8407e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.10, Train Loss: 0.00, Val Loss: 5.17, Train BLEU: 0.00, Val BLEU: 6.85, Minutes Elapsed: 57.05\n",
      "Sampling from val predictions...\n",
      "Source: để tôi nói bạn biết một bí_mật . <EOS> <PAD>\n",
      "Reference: let me tell you a secret . <EOS> <PAD>\n",
      "Model: <SOS> i i want you you it . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9630, 0.0366, 0.0004, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1440, 0.6888, 0.1646, 0.0025, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0004, 0.0198, 0.6443, 0.3245, 0.0109, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0002, 0.0550, 0.6822, 0.2542, 0.0084, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0022, 0.2424, 0.5518, 0.1939, 0.0094, 0.0004, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0006, 0.0877, 0.3904, 0.3889, 0.1130, 0.0190, 0.0005,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0003, 0.0318, 0.2573, 0.4105, 0.2180, 0.0783, 0.0037,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0013, 0.0771, 0.3367, 0.3404, 0.1809, 0.0593, 0.0042,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0013, 0.0619, 0.2682, 0.3306, 0.2309, 0.0916, 0.0157,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.14, Train Loss: 0.00, Val Loss: 5.17, Train BLEU: 0.00, Val BLEU: 6.81, Minutes Elapsed: 59.52\n",
      "Sampling from val predictions...\n",
      "Source: và những điều đó là sự_thật , nhưng đó chỉ\n",
      "Reference: and those things are true , but they &apos;re\n",
      "Model: <SOS> and the the is is , , , ,\n",
      "Attention Weights: tensor([[2.6406e-02, 9.2069e-01, 5.2845e-02, 5.5866e-05, 6.5481e-07, 4.4988e-08,\n",
      "         2.2057e-08, 9.4809e-09, 5.5495e-09, 3.5182e-09],\n",
      "        [6.1228e-03, 6.3170e-01, 3.6169e-01, 4.8363e-04, 3.5651e-06, 1.3097e-07,\n",
      "         3.2065e-08, 5.9752e-09, 1.9119e-09, 9.8889e-10],\n",
      "        [1.7282e-04, 1.8992e-02, 9.3934e-01, 4.1325e-02, 1.7012e-04, 9.7777e-07,\n",
      "         1.1887e-07, 1.4576e-08, 3.6745e-09, 1.6208e-09],\n",
      "        [1.9404e-05, 4.6808e-04, 8.3256e-02, 6.3064e-01, 2.8207e-01, 3.3881e-03,\n",
      "         1.6424e-04, 1.2391e-06, 5.7881e-08, 7.2852e-09],\n",
      "        [7.2716e-06, 3.3912e-05, 9.1442e-04, 2.2498e-02, 3.7487e-01, 2.9048e-01,\n",
      "         3.0511e-01, 5.8946e-03, 1.8008e-04, 9.2102e-06],\n",
      "        [9.9575e-07, 1.8581e-06, 2.4074e-05, 4.2962e-04, 1.3462e-02, 8.9039e-02,\n",
      "         7.3539e-01, 1.4057e-01, 1.9043e-02, 2.0383e-03],\n",
      "        [4.0519e-05, 2.0775e-05, 1.2671e-04, 8.5378e-04, 1.0315e-02, 5.7761e-02,\n",
      "         7.6124e-01, 1.3903e-01, 2.6734e-02, 3.8824e-03],\n",
      "        [2.6792e-06, 1.7041e-06, 1.4970e-05, 2.1874e-04, 3.7462e-03, 3.0160e-02,\n",
      "         2.7700e-01, 3.9985e-01, 2.2942e-01, 5.9581e-02],\n",
      "        [3.1733e-07, 3.0418e-07, 3.3384e-06, 5.9921e-05, 1.2409e-03, 8.6390e-03,\n",
      "         8.4070e-02, 2.9966e-01, 3.9895e-01, 2.0737e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.19, Train Loss: 0.00, Val Loss: 5.15, Train BLEU: 0.00, Val BLEU: 6.29, Minutes Elapsed: 62.00\n",
      "Sampling from val predictions...\n",
      "Source: khi gạch tên đi thì sao ? gạch đi là\n",
      "Reference: what do bullets do ? bullets kill , and\n",
      "Model: <SOS> so what you you ? ? ? ? is\n",
      "Attention Weights: tensor([[8.7963e-01, 1.1877e-01, 1.5520e-03, 3.3997e-05, 6.0530e-06, 1.9424e-06,\n",
      "         1.5913e-06, 4.3164e-07, 1.6611e-07, 1.0339e-07],\n",
      "        [5.4343e-02, 8.5310e-01, 9.1796e-02, 7.1463e-04, 3.9544e-05, 4.6039e-06,\n",
      "         2.0760e-06, 1.8292e-07, 3.9377e-08, 1.9373e-08],\n",
      "        [2.6706e-04, 4.2659e-02, 8.4904e-01, 1.0063e-01, 7.0787e-03, 2.6138e-04,\n",
      "         6.0040e-05, 8.7167e-07, 4.0882e-08, 8.2205e-09],\n",
      "        [7.7218e-06, 3.3686e-04, 5.8845e-02, 4.5603e-01, 3.8314e-01, 6.7507e-02,\n",
      "         3.3602e-02, 5.2537e-04, 7.2803e-06, 4.4102e-07],\n",
      "        [7.3946e-07, 7.7990e-06, 7.0048e-04, 3.5383e-02, 1.6023e-01, 2.1940e-01,\n",
      "         5.3939e-01, 4.2761e-02, 1.9736e-03, 1.6025e-04],\n",
      "        [3.6797e-07, 3.2788e-06, 1.4332e-04, 5.5329e-03, 3.7116e-02, 1.0871e-01,\n",
      "         5.1806e-01, 2.6209e-01, 5.6622e-02, 1.1714e-02],\n",
      "        [2.2009e-07, 1.8713e-06, 5.8633e-05, 1.8870e-03, 1.3174e-02, 4.7748e-02,\n",
      "         2.7207e-01, 3.9149e-01, 1.9700e-01, 7.6567e-02],\n",
      "        [8.1024e-08, 8.4601e-07, 2.9532e-05, 1.0040e-03, 6.9774e-03, 2.5210e-02,\n",
      "         1.2938e-01, 3.5299e-01, 2.9369e-01, 1.9072e-01],\n",
      "        [8.3167e-08, 6.7724e-07, 1.8681e-05, 5.7488e-04, 3.9754e-03, 1.6554e-02,\n",
      "         1.0796e-01, 3.1456e-01, 3.1524e-01, 2.4112e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.24, Train Loss: 0.00, Val Loss: 5.11, Train BLEU: 0.00, Val BLEU: 7.25, Minutes Elapsed: 64.50\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi đã lấy nhiều mẫu_vật từ con đường này và\n",
      "Reference: so we took a lot of samples from this\n",
      "Model: <SOS> we &apos;ve have to to of the of and\n",
      "Attention Weights: tensor([[9.8282e-01, 1.7087e-02, 8.5048e-05, 3.2438e-06, 4.3947e-07, 1.4597e-07,\n",
      "         3.7056e-08, 2.2699e-08, 1.4213e-08, 9.4608e-09],\n",
      "        [1.8990e-01, 7.7372e-01, 3.5866e-02, 4.9285e-04, 1.7251e-05, 2.1562e-06,\n",
      "         1.7456e-07, 7.1227e-08, 3.1454e-08, 1.3938e-08],\n",
      "        [5.9480e-03, 1.4337e-01, 7.5869e-01, 8.5902e-02, 5.4042e-03, 6.6671e-04,\n",
      "         1.8267e-05, 2.8185e-06, 4.9360e-07, 9.1710e-08],\n",
      "        [1.3617e-04, 6.9275e-03, 5.8785e-01, 3.5184e-01, 4.5995e-02, 7.0143e-03,\n",
      "         2.0587e-04, 2.8314e-05, 3.3323e-06, 4.0453e-07],\n",
      "        [5.8089e-05, 4.7967e-04, 3.5067e-02, 3.2873e-01, 3.8957e-01, 2.2467e-01,\n",
      "         1.5702e-02, 4.8942e-03, 7.5125e-04, 7.5920e-05],\n",
      "        [3.6005e-04, 1.2259e-03, 1.9341e-02, 1.5711e-01, 3.5232e-01, 3.6951e-01,\n",
      "         5.2471e-02, 3.4884e-02, 1.0953e-02, 1.8263e-03],\n",
      "        [2.5742e-04, 4.9842e-04, 4.5957e-03, 3.4956e-02, 1.4896e-01, 3.4181e-01,\n",
      "         1.5029e-01, 1.9554e-01, 9.7536e-02, 2.5558e-02],\n",
      "        [2.0593e-05, 5.5404e-05, 5.4870e-04, 4.3199e-03, 2.5524e-02, 1.0046e-01,\n",
      "         1.2413e-01, 3.0756e-01, 2.8706e-01, 1.5032e-01],\n",
      "        [3.9484e-05, 4.9856e-05, 4.3638e-04, 3.0052e-03, 1.9803e-02, 8.9614e-02,\n",
      "         1.0957e-01, 2.9418e-01, 2.9467e-01, 1.8863e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.29, Train Loss: 0.00, Val Loss: 5.10, Train BLEU: 0.00, Val BLEU: 7.06, Minutes Elapsed: 66.98\n",
      "Sampling from val predictions...\n",
      "Source: bà ấy đây . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: there she is . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank you . . . <EOS> . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9218, 0.0777, 0.0006, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3792, 0.4351, 0.1813, 0.0043, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0195, 0.0523, 0.5112, 0.4097, 0.0073, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0050, 0.0110, 0.1297, 0.7864, 0.0679, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0046, 0.0094, 0.0874, 0.6836, 0.2150, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0144, 0.0244, 0.1196, 0.5756, 0.2660, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0026, 0.0070, 0.0597, 0.3955, 0.5353, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0251, 0.0353, 0.1140, 0.4955, 0.3301, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0030, 0.0068, 0.0535, 0.3793, 0.5574, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.34, Train Loss: 0.00, Val Loss: 5.07, Train BLEU: 0.00, Val BLEU: 7.15, Minutes Elapsed: 69.47\n",
      "Sampling from val predictions...\n",
      "Source: và cả hai đều chuẩn_xác . nhưng thêm một điều\n",
      "Reference: and they &apos;re both accurate . okay , but\n",
      "Model: <SOS> and the the the . . the &apos;s a\n",
      "Attention Weights: tensor([[5.7358e-02, 4.7153e-01, 4.6096e-01, 9.9356e-03, 1.1771e-04, 7.1352e-05,\n",
      "         1.0541e-05, 5.9851e-06, 3.2004e-06, 1.6655e-06],\n",
      "        [2.2745e-02, 2.1043e-01, 6.8875e-01, 7.6746e-02, 9.5279e-04, 3.5257e-04,\n",
      "         1.4900e-05, 4.1154e-06, 1.2977e-06, 4.5507e-07],\n",
      "        [1.6758e-03, 5.1665e-03, 6.2016e-02, 8.4451e-01, 6.3908e-02, 2.2557e-02,\n",
      "         1.4648e-04, 2.1003e-05, 3.0497e-06, 5.4261e-07],\n",
      "        [7.5378e-05, 8.7124e-05, 5.8073e-04, 2.0962e-02, 8.5476e-02, 8.7158e-01,\n",
      "         1.8366e-02, 2.6587e-03, 2.0034e-04, 9.6979e-06],\n",
      "        [4.3251e-05, 2.6487e-05, 8.0336e-05, 1.3575e-03, 1.0750e-02, 8.3938e-01,\n",
      "         1.0440e-01, 3.8060e-02, 5.4579e-03, 4.4455e-04],\n",
      "        [5.5354e-04, 2.6689e-04, 4.1558e-04, 2.8555e-03, 1.3881e-02, 7.4484e-01,\n",
      "         1.6181e-01, 6.3317e-02, 1.0586e-02, 1.4746e-03],\n",
      "        [4.8187e-05, 2.6637e-05, 5.9567e-05, 5.0666e-04, 2.6668e-03, 8.5545e-02,\n",
      "         3.6628e-01, 4.0039e-01, 1.2231e-01, 2.2169e-02],\n",
      "        [1.9081e-06, 1.1224e-06, 2.9037e-06, 3.9220e-05, 3.4991e-04, 1.1421e-02,\n",
      "         1.1909e-01, 3.5239e-01, 3.5198e-01, 1.6473e-01],\n",
      "        [2.6571e-07, 1.9600e-07, 8.0766e-07, 1.9330e-05, 2.7839e-04, 1.0659e-02,\n",
      "         7.1362e-02, 2.4888e-01, 4.0051e-01, 2.6829e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.38, Train Loss: 0.00, Val Loss: 5.05, Train BLEU: 0.00, Val BLEU: 7.26, Minutes Elapsed: 71.95\n",
      "Sampling from val predictions...\n",
      "Source: tôi hi_vọng những tấm hình sẽ đánh_thức một nguồn sức_mạnh\n",
      "Reference: i hope that these images awaken a force in\n",
      "Model: <SOS> i i the i i to a a a\n",
      "Attention Weights: tensor([[9.9904e-01, 9.3205e-04, 2.6411e-05, 2.8327e-06, 7.2863e-07, 3.0209e-07,\n",
      "         8.5623e-08, 4.0166e-08, 1.6494e-08, 1.1087e-08],\n",
      "        [2.2795e-01, 7.5241e-01, 1.9064e-02, 5.4066e-04, 3.8176e-05, 2.5823e-06,\n",
      "         2.6267e-07, 6.2765e-08, 1.3292e-08, 6.6274e-09],\n",
      "        [1.1963e-02, 5.2330e-01, 3.8439e-01, 6.0152e-02, 1.7946e-02, 2.0859e-03,\n",
      "         1.4018e-04, 1.6562e-05, 8.8603e-07, 2.1099e-07],\n",
      "        [7.8836e-05, 4.3388e-03, 1.7118e-01, 4.5190e-01, 2.9919e-01, 6.7594e-02,\n",
      "         5.3551e-03, 3.6027e-04, 7.2081e-06, 9.2502e-07],\n",
      "        [7.8522e-07, 8.4917e-05, 5.8724e-03, 7.5449e-02, 3.0069e-01, 4.3226e-01,\n",
      "         1.5032e-01, 3.3726e-02, 1.4363e-03, 1.6685e-04],\n",
      "        [1.9188e-07, 1.7664e-05, 8.3635e-04, 1.2046e-02, 9.5700e-02, 4.0880e-01,\n",
      "         2.9598e-01, 1.6035e-01, 2.0938e-02, 5.3427e-03],\n",
      "        [1.1129e-07, 5.1864e-06, 1.6486e-04, 2.6165e-03, 3.0394e-02, 2.3191e-01,\n",
      "         2.9427e-01, 3.3057e-01, 7.4725e-02, 3.5346e-02],\n",
      "        [4.1467e-07, 5.8517e-06, 7.2215e-05, 6.9456e-04, 7.0289e-03, 6.9705e-02,\n",
      "         1.8559e-01, 4.6747e-01, 1.4876e-01, 1.2067e-01],\n",
      "        [7.6907e-07, 1.0236e-05, 8.8726e-05, 6.1178e-04, 4.7428e-03, 4.7416e-02,\n",
      "         1.1454e-01, 3.7716e-01, 1.9247e-01, 2.6295e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.43, Train Loss: 0.00, Val Loss: 5.00, Train BLEU: 0.00, Val BLEU: 7.43, Minutes Elapsed: 74.43\n",
      "Sampling from val predictions...\n",
      "Source: tôi cũng đã từng tự_hỏi , thực_tế nó có hoạt_động\n",
      "Reference: and i also ask myself , does it really\n",
      "Model: <SOS> i i i to , , it it it\n",
      "Attention Weights: tensor([[9.9214e-01, 7.7866e-03, 6.7171e-05, 1.9064e-06, 1.1328e-07, 1.1504e-08,\n",
      "         3.6558e-09, 1.4864e-09, 7.7230e-10, 3.6251e-10],\n",
      "        [2.5560e-02, 9.4332e-01, 2.9930e-02, 1.1675e-03, 2.1739e-05, 3.3658e-07,\n",
      "         2.4628e-08, 2.9193e-09, 7.7567e-10, 2.3977e-10],\n",
      "        [1.9874e-03, 3.2337e-01, 4.1562e-01, 2.2081e-01, 3.6627e-02, 1.4872e-03,\n",
      "         9.5575e-05, 3.9221e-06, 2.8164e-07, 2.6917e-08],\n",
      "        [5.9307e-04, 1.6416e-01, 4.0209e-01, 3.4022e-01, 8.8180e-02, 4.3975e-03,\n",
      "         3.4147e-04, 1.2554e-05, 7.9350e-07, 5.2698e-08],\n",
      "        [1.3027e-05, 1.6961e-04, 6.5250e-03, 1.1709e-01, 7.2745e-01, 1.2017e-01,\n",
      "         2.7020e-02, 1.4393e-03, 1.1327e-04, 6.6914e-06],\n",
      "        [4.9876e-06, 8.3012e-05, 1.6100e-03, 2.1351e-02, 3.2928e-01, 4.0466e-01,\n",
      "         2.1312e-01, 2.5178e-02, 4.2275e-03, 4.8796e-04],\n",
      "        [7.3365e-07, 1.1562e-05, 2.4105e-04, 3.8887e-03, 5.8173e-02, 2.2081e-01,\n",
      "         5.0820e-01, 1.6038e-01, 4.1152e-02, 7.1386e-03],\n",
      "        [5.7458e-07, 5.3338e-06, 7.1116e-05, 9.1586e-04, 1.8189e-02, 1.1676e-01,\n",
      "         4.0823e-01, 3.0504e-01, 1.2280e-01, 2.7983e-02],\n",
      "        [2.8180e-08, 7.9689e-07, 1.4701e-05, 1.5498e-04, 1.3703e-03, 7.0209e-03,\n",
      "         1.0694e-01, 3.2469e-01, 3.8592e-01, 1.7389e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.48, Train Loss: 0.00, Val Loss: 4.99, Train BLEU: 0.00, Val BLEU: 7.86, Minutes Elapsed: 76.91\n",
      "Sampling from val predictions...\n",
      "Source: vâng , tôi làm toán_học ứng_dụng và đây là một\n",
      "Reference: so , well , i do applied math ,\n",
      "Model: <SOS> now , i i i i this and and\n",
      "Attention Weights: tensor([[8.2017e-01, 1.6697e-01, 1.2854e-02, 1.0832e-05, 2.1251e-07, 2.2537e-08,\n",
      "         3.4705e-09, 2.1153e-09, 1.1640e-09, 7.8178e-10],\n",
      "        [6.2991e-03, 9.2579e-02, 8.9630e-01, 4.7758e-03, 4.7127e-05, 1.6594e-06,\n",
      "         8.8967e-08, 4.1286e-08, 1.2429e-08, 4.8800e-09],\n",
      "        [6.1704e-05, 3.4421e-03, 5.6481e-01, 4.1660e-01, 1.4852e-02, 2.4057e-04,\n",
      "         1.9922e-06, 2.0581e-07, 1.1318e-08, 1.7941e-09],\n",
      "        [1.2386e-06, 5.0991e-05, 2.1406e-02, 6.8861e-01, 2.6785e-01, 2.1918e-02,\n",
      "         1.4479e-04, 1.0293e-05, 3.3092e-07, 3.2842e-08],\n",
      "        [2.1633e-06, 9.1122e-05, 2.1989e-02, 5.7853e-01, 3.4560e-01, 5.2612e-02,\n",
      "         1.0665e-03, 1.0689e-04, 6.1830e-06, 7.9547e-07],\n",
      "        [1.1502e-06, 5.0852e-05, 2.1879e-02, 5.2042e-01, 3.7859e-01, 7.7371e-02,\n",
      "         1.5189e-03, 1.5874e-04, 8.2712e-06, 9.3219e-07],\n",
      "        [6.9734e-07, 2.4050e-05, 6.1545e-03, 2.9690e-01, 4.4845e-01, 2.2984e-01,\n",
      "         1.4812e-02, 3.3427e-03, 4.0324e-04, 7.2566e-05],\n",
      "        [2.3854e-07, 1.2428e-06, 7.7914e-05, 4.4601e-03, 2.2115e-01, 6.4214e-01,\n",
      "         1.0295e-01, 2.4959e-02, 3.3830e-03, 8.8278e-04],\n",
      "        [5.9621e-07, 2.3335e-06, 1.1220e-04, 2.3247e-03, 6.2398e-02, 2.9278e-01,\n",
      "         2.6211e-01, 1.9531e-01, 1.0748e-01, 7.7473e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.53, Train Loss: 0.00, Val Loss: 4.98, Train BLEU: 0.00, Val BLEU: 7.74, Minutes Elapsed: 79.43\n",
      "Sampling from val predictions...\n",
      "Source: tôi hoàn_toàn tuyệt_vọng . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: i lost all hope . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> i &apos;m &apos;t . . <EOS> <EOS> . <EOS>\n",
      "Attention Weights: tensor([[0.9988, 0.0012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1857, 0.8131, 0.0013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0185, 0.9349, 0.0448, 0.0017, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0019, 0.1908, 0.7106, 0.0955, 0.0012, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0017, 0.0538, 0.3623, 0.5377, 0.0445, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0029, 0.0598, 0.2395, 0.5366, 0.1612, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.0471, 0.2118, 0.4884, 0.2517, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0013, 0.0936, 0.2929, 0.4381, 0.1741, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0402, 0.0843, 0.1975, 0.4661, 0.2120, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.58, Train Loss: 0.00, Val Loss: 4.94, Train BLEU: 0.00, Val BLEU: 8.42, Minutes Elapsed: 81.90\n",
      "Sampling from val predictions...\n",
      "Source: chúng_ta những nạn_nhân cần đến tất_cả mọi người . <EOS>\n",
      "Reference: we victims need everyone . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> we &apos;re to to to . . . <EOS>\n",
      "Attention Weights: tensor([[7.2615e-01, 2.6974e-01, 4.0456e-03, 5.6556e-05, 4.0203e-06, 8.1541e-07,\n",
      "         2.6263e-07, 7.0225e-08, 2.0005e-08, 5.6939e-09],\n",
      "        [3.6141e-03, 8.5823e-01, 1.3639e-01, 1.6929e-03, 6.2679e-05, 5.6954e-06,\n",
      "         9.8269e-07, 1.1686e-07, 1.6429e-08, 3.7402e-09],\n",
      "        [1.0230e-03, 1.1429e-01, 5.9419e-01, 2.3066e-01, 4.6777e-02, 1.0628e-02,\n",
      "         2.2226e-03, 1.9901e-04, 8.0080e-06, 2.4484e-07],\n",
      "        [9.4990e-06, 9.3833e-04, 1.1893e-01, 4.5354e-01, 2.9185e-01, 1.0468e-01,\n",
      "         2.7998e-02, 2.0189e-03, 4.1310e-05, 2.8501e-07],\n",
      "        [1.2167e-06, 4.6670e-05, 3.2895e-03, 6.9221e-02, 2.7250e-01, 3.2384e-01,\n",
      "         2.4775e-01, 7.6577e-02, 6.6900e-03, 8.4216e-05],\n",
      "        [1.5844e-06, 3.4525e-05, 9.0997e-04, 1.8092e-02, 1.1051e-01, 2.6027e-01,\n",
      "         3.2972e-01, 2.2847e-01, 5.0037e-02, 1.9593e-03],\n",
      "        [7.4503e-06, 1.0087e-04, 1.3268e-03, 1.3720e-02, 7.0033e-02, 1.8750e-01,\n",
      "         2.9152e-01, 3.0459e-01, 1.1869e-01, 1.2521e-02],\n",
      "        [1.7847e-05, 1.7818e-04, 2.5846e-03, 2.9719e-02, 8.3025e-02, 2.3102e-01,\n",
      "         2.8127e-01, 2.5624e-01, 1.0088e-01, 1.5065e-02],\n",
      "        [5.4114e-04, 9.1002e-04, 5.4434e-03, 3.2296e-02, 1.1616e-01, 2.0197e-01,\n",
      "         2.5437e-01, 2.5454e-01, 1.1704e-01, 1.6720e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.62, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 8.48, Minutes Elapsed: 84.39\n",
      "Sampling from val predictions...\n",
      "Source: nhưng hầu_hết mọi người không đồng_ý . <EOS> <PAD> <PAD>\n",
      "Reference: but most people don &apos;t agree . <EOS> <PAD>\n",
      "Model: <SOS> but the people don &apos;t &apos;t . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.2219, 0.7689, 0.0090, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0177, 0.8464, 0.1339, 0.0019, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0014, 0.0380, 0.4621, 0.3942, 0.1039, 0.0004, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0002, 0.0032, 0.0721, 0.3496, 0.5502, 0.0244, 0.0003, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0001, 0.0017, 0.0376, 0.2865, 0.6285, 0.0453, 0.0004, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0011, 0.0432, 0.4192, 0.4943, 0.0420, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0004, 0.0142, 0.2050, 0.5331, 0.2430, 0.0042, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0001, 0.0010, 0.0199, 0.1825, 0.5435, 0.2483, 0.0046, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0001, 0.0014, 0.0186, 0.1891, 0.5142, 0.2609, 0.0157, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.67, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 8.31, Minutes Elapsed: 86.86\n",
      "Sampling from val predictions...\n",
      "Source: hiện có khoảng 10,000 loài chim trên thế_giới . <EOS>\n",
      "Reference: there are 10,000 species of birds in the world\n",
      "Model: <SOS> the &apos;s a <UNK> <UNK> the . . .\n",
      "Attention Weights: tensor([[9.5661e-01, 4.3160e-02, 2.0226e-04, 3.0986e-05, 7.2221e-07, 7.6645e-08,\n",
      "         4.5135e-08, 1.1182e-08, 4.6009e-09, 1.7822e-09],\n",
      "        [7.9941e-02, 8.6104e-01, 4.7251e-02, 1.1732e-02, 3.1918e-05, 7.4061e-07,\n",
      "         1.9493e-07, 1.7956e-08, 4.0855e-09, 1.1754e-09],\n",
      "        [2.9508e-02, 4.8274e-01, 1.9895e-01, 2.8009e-01, 8.1401e-03, 3.7969e-04,\n",
      "         1.7916e-04, 8.0972e-06, 5.0936e-07, 3.1013e-08],\n",
      "        [2.9371e-04, 2.2198e-02, 1.1853e-01, 7.7534e-01, 8.0064e-02, 2.0437e-03,\n",
      "         1.4714e-03, 5.3059e-05, 2.4566e-06, 6.8521e-08],\n",
      "        [8.7501e-05, 1.5566e-03, 9.7932e-03, 6.9738e-01, 2.7425e-01, 7.4520e-03,\n",
      "         9.1223e-03, 3.4394e-04, 1.5559e-05, 2.4705e-07],\n",
      "        [1.8114e-04, 1.1305e-03, 2.7657e-03, 2.6148e-01, 6.2602e-01, 3.5320e-02,\n",
      "         6.9177e-02, 3.7153e-03, 2.0600e-04, 3.3174e-06],\n",
      "        [1.4402e-05, 1.2322e-04, 3.4834e-04, 2.3651e-02, 4.1678e-01, 1.1060e-01,\n",
      "         3.9180e-01, 5.2203e-02, 4.3881e-03, 9.1623e-05],\n",
      "        [3.7400e-05, 1.6040e-04, 1.2265e-04, 7.8900e-03, 1.5595e-01, 5.7579e-02,\n",
      "         5.3778e-01, 1.9326e-01, 4.5784e-02, 1.4390e-03],\n",
      "        [5.2596e-06, 3.4012e-05, 9.5411e-05, 2.6365e-03, 5.8161e-02, 6.6789e-02,\n",
      "         4.9075e-01, 2.8542e-01, 8.8893e-02, 7.2173e-03]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.72, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 8.41, Minutes Elapsed: 89.35\n",
      "Sampling from val predictions...\n",
      "Source: và mảnh vườn , nó rất đẹp . <EOS> <PAD>\n",
      "Reference: and the garden , it was beautiful . <EOS>\n",
      "Model: <SOS> and the , , , it it . .\n",
      "Attention Weights: tensor([[0.0185, 0.9779, 0.0036, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0059, 0.9785, 0.0154, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0028, 0.5406, 0.4423, 0.0130, 0.0012, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0009, 0.0390, 0.5348, 0.3246, 0.0875, 0.0127, 0.0006, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0007, 0.0085, 0.1956, 0.4293, 0.3299, 0.0348, 0.0011, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0001, 0.0006, 0.0224, 0.1688, 0.6113, 0.1877, 0.0090, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0005, 0.0061, 0.2049, 0.6075, 0.1734, 0.0076, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0041, 0.1039, 0.4555, 0.3704, 0.0646, 0.0014,\n",
      "         0.0000],\n",
      "        [0.0002, 0.0001, 0.0010, 0.0170, 0.1293, 0.3463, 0.3688, 0.1289, 0.0084,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.77, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 8.78, Minutes Elapsed: 91.82\n",
      "Sampling from val predictions...\n",
      "Source: gần hai năm đã trôi_qua từ khi cuộc khởi_nghĩa ở\n",
      "Reference: almost two years have passed since the libyan revolution\n",
      "Model: <SOS> in the years years years from the the of\n",
      "Attention Weights: tensor([[9.5761e-01, 4.1826e-02, 5.5401e-04, 5.0208e-06, 4.2771e-07, 3.6749e-07,\n",
      "         9.1712e-08, 3.2084e-08, 1.4192e-08, 9.2345e-09],\n",
      "        [9.6825e-03, 6.4403e-01, 3.4050e-01, 5.4651e-03, 1.9059e-04, 1.2281e-04,\n",
      "         6.5627e-06, 9.5632e-07, 1.9224e-07, 6.3751e-08],\n",
      "        [7.6104e-04, 1.2170e-01, 7.6418e-01, 9.6732e-02, 8.6934e-03, 7.4433e-03,\n",
      "         4.6250e-04, 2.4978e-05, 1.3831e-06, 1.8274e-07],\n",
      "        [3.0895e-05, 6.8709e-03, 6.2854e-01, 2.6003e-01, 3.8804e-02, 6.1001e-02,\n",
      "         4.5077e-03, 2.0541e-04, 7.9472e-06, 7.3218e-07],\n",
      "        [1.1409e-05, 8.0460e-04, 1.3901e-01, 2.8537e-01, 1.3278e-01, 3.6824e-01,\n",
      "         6.8351e-02, 5.1177e-03, 2.7927e-04, 3.3012e-05],\n",
      "        [3.3910e-07, 2.0695e-05, 5.5860e-03, 5.0792e-02, 1.2875e-01, 4.1622e-01,\n",
      "         2.9499e-01, 9.1429e-02, 1.0430e-02, 1.7810e-03],\n",
      "        [1.5112e-07, 4.1886e-06, 5.4660e-04, 6.6541e-03, 3.7964e-02, 2.4996e-01,\n",
      "         3.7436e-01, 2.4835e-01, 6.0369e-02, 2.1795e-02],\n",
      "        [9.3964e-08, 1.8463e-06, 1.3374e-04, 1.4882e-03, 1.0581e-02, 9.3293e-02,\n",
      "         2.4427e-01, 3.2297e-01, 1.9297e-01, 1.3430e-01],\n",
      "        [4.8562e-08, 9.4429e-07, 6.4076e-05, 7.3277e-04, 5.3055e-03, 5.0412e-02,\n",
      "         1.6470e-01, 2.9487e-01, 2.4930e-01, 2.3461e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.82, Train Loss: 0.00, Val Loss: 4.86, Train BLEU: 0.00, Val BLEU: 8.96, Minutes Elapsed: 94.27\n",
      "Sampling from val predictions...\n",
      "Source: rất nhiều người bị lừa bởi những lời_hứa <UNK> về\n",
      "Reference: many have been tricked by false promises of a\n",
      "Model: <SOS> there many people people the the <UNK> the the\n",
      "Attention Weights: tensor([[8.4313e-01, 1.5314e-01, 3.7003e-03, 3.2351e-05, 7.8410e-07, 1.8214e-07,\n",
      "         3.7866e-08, 1.7370e-08, 6.1126e-09, 4.1833e-09],\n",
      "        [2.8937e-02, 3.8460e-01, 5.4339e-01, 4.1440e-02, 1.3685e-03, 2.4116e-04,\n",
      "         1.6568e-05, 4.5355e-06, 5.4297e-07, 1.1094e-07],\n",
      "        [5.4036e-03, 6.0618e-02, 4.2004e-01, 3.5399e-01, 8.7940e-02, 6.2641e-02,\n",
      "         7.1475e-03, 2.0661e-03, 1.4130e-04, 8.7082e-06],\n",
      "        [4.9295e-04, 6.3296e-03, 7.4775e-02, 2.1681e-01, 1.5311e-01, 4.1907e-01,\n",
      "         8.3796e-02, 4.0998e-02, 4.2311e-03, 3.7912e-04],\n",
      "        [2.4644e-05, 2.1389e-04, 2.2247e-03, 1.2855e-02, 2.5399e-02, 3.1456e-01,\n",
      "         1.8980e-01, 3.4021e-01, 7.7997e-02, 3.6717e-02],\n",
      "        [3.4912e-05, 1.5600e-04, 7.4489e-04, 2.3373e-03, 4.7864e-03, 6.6947e-02,\n",
      "         8.5506e-02, 3.6870e-01, 1.9042e-01, 2.8037e-01],\n",
      "        [1.1547e-06, 6.4714e-06, 4.7582e-05, 2.2683e-04, 5.9778e-04, 8.4280e-03,\n",
      "         1.7501e-02, 9.2784e-02, 1.2373e-01, 7.5668e-01],\n",
      "        [7.0682e-06, 1.8017e-05, 8.1663e-05, 3.1050e-04, 5.5217e-04, 5.4216e-03,\n",
      "         9.9761e-03, 4.9888e-02, 6.9582e-02, 8.6416e-01],\n",
      "        [1.4325e-06, 7.1123e-06, 5.7667e-05, 3.0776e-04, 7.8787e-04, 7.2039e-03,\n",
      "         1.3580e-02, 6.0462e-02, 9.2907e-02, 8.2468e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.86, Train Loss: 0.00, Val Loss: 4.82, Train BLEU: 0.00, Val BLEU: 9.06, Minutes Elapsed: 96.73\n",
      "Sampling from val predictions...\n",
      "Source: trước_tiên , bạn phải mang đến cho họ sự bảo_mật\n",
      "Reference: first , you have to offer them <UNK> .\n",
      "Model: <SOS> so , , you to to to to they\n",
      "Attention Weights: tensor([[9.9772e-01, 1.8826e-03, 3.9229e-04, 3.9685e-06, 1.4737e-07, 4.9318e-08,\n",
      "         2.5052e-08, 1.4160e-08, 5.9412e-09, 3.9148e-09],\n",
      "        [3.3111e-01, 1.6352e-01, 4.9632e-01, 8.8233e-03, 1.7410e-04, 3.2846e-05,\n",
      "         1.2108e-05, 5.6810e-06, 1.6858e-06, 7.9894e-07],\n",
      "        [2.8410e-03, 2.3425e-02, 8.3171e-01, 1.3962e-01, 2.1906e-03, 1.8774e-04,\n",
      "         1.7973e-05, 2.1137e-06, 1.3792e-07, 2.6933e-08],\n",
      "        [1.3478e-04, 2.9765e-03, 7.2049e-01, 2.6567e-01, 9.4177e-03, 1.1817e-03,\n",
      "         1.2177e-04, 1.1732e-05, 4.0109e-07, 4.1637e-08],\n",
      "        [7.3337e-05, 2.4644e-03, 1.4516e-01, 7.1791e-01, 1.0524e-01, 2.3349e-02,\n",
      "         5.0025e-03, 7.7907e-04, 2.0732e-05, 1.4504e-06],\n",
      "        [4.0361e-06, 2.6978e-05, 1.1776e-03, 1.5768e-02, 1.4906e-01, 4.0503e-01,\n",
      "         2.7661e-01, 1.2009e-01, 2.8278e-02, 3.9454e-03],\n",
      "        [4.2511e-06, 2.2708e-05, 3.2130e-04, 1.9494e-03, 1.8816e-02, 1.6142e-01,\n",
      "         2.4030e-01, 3.2292e-01, 1.8851e-01, 6.5737e-02],\n",
      "        [1.8539e-06, 1.1429e-05, 1.7181e-04, 6.3404e-04, 4.2704e-03, 4.7531e-02,\n",
      "         1.1362e-01, 3.3675e-01, 3.0318e-01, 1.9382e-01],\n",
      "        [3.8050e-07, 2.8157e-06, 1.0756e-04, 2.9114e-04, 1.0695e-03, 1.2636e-02,\n",
      "         4.4423e-02, 2.5345e-01, 2.9875e-01, 3.8927e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.91, Train Loss: 0.00, Val Loss: 4.81, Train BLEU: 0.00, Val BLEU: 8.46, Minutes Elapsed: 99.26\n",
      "Sampling from val predictions...\n",
      "Source: ở kathmandu , tôi được hộ_tống bởi những người phụ_nữ\n",
      "Reference: in <UNK> , i was escorted by women who\n",
      "Model: <SOS> in , , i was the to the people\n",
      "Attention Weights: tensor([[9.8762e-01, 1.2077e-02, 2.8273e-04, 1.7028e-05, 7.2015e-07, 1.8834e-07,\n",
      "         1.1056e-07, 5.7100e-08, 4.2957e-08, 2.0413e-08],\n",
      "        [7.1267e-02, 8.3979e-01, 8.3610e-02, 5.2406e-03, 7.8665e-05, 7.1489e-06,\n",
      "         1.8767e-06, 4.3514e-07, 2.0084e-07, 6.2888e-08],\n",
      "        [4.0578e-03, 2.4063e-01, 4.3818e-01, 2.9611e-01, 1.8938e-02, 1.8185e-03,\n",
      "         2.5774e-04, 1.4167e-05, 2.3593e-06, 2.1876e-07],\n",
      "        [4.3439e-04, 1.2164e-02, 1.7104e-01, 7.6813e-01, 4.2238e-02, 4.8787e-03,\n",
      "         1.0162e-03, 7.4829e-05, 1.4720e-05, 1.0262e-06],\n",
      "        [1.7290e-05, 5.4171e-04, 5.3893e-03, 2.7622e-01, 4.6518e-01, 1.8879e-01,\n",
      "         5.6100e-02, 6.8014e-03, 9.1658e-04, 4.2203e-05],\n",
      "        [4.8210e-05, 1.8368e-03, 5.5304e-03, 2.9097e-02, 3.2828e-01, 4.0248e-01,\n",
      "         1.6896e-01, 4.9624e-02, 1.2828e-02, 1.3188e-03],\n",
      "        [5.0893e-06, 5.0384e-05, 3.4668e-04, 4.6009e-03, 3.8558e-02, 1.3381e-01,\n",
      "         4.0106e-01, 2.0663e-01, 1.8043e-01, 3.4504e-02],\n",
      "        [6.1334e-05, 2.3801e-04, 3.2784e-03, 2.0118e-02, 2.3305e-02, 6.8797e-02,\n",
      "         3.0269e-01, 1.8914e-01, 3.1596e-01, 7.6409e-02],\n",
      "        [1.3272e-06, 7.0456e-06, 8.9988e-05, 1.6540e-03, 5.1376e-03, 2.2200e-02,\n",
      "         1.3308e-01, 1.6778e-01, 4.3265e-01, 2.3740e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.96, Train Loss: 0.00, Val Loss: 4.79, Train BLEU: 0.00, Val BLEU: 8.76, Minutes Elapsed: 101.75\n",
      "Sampling from val predictions...\n",
      "Source: vì_thế chúng_ta được dạy rằng \" sự khoan_dung của ta\n",
      "Reference: thus we are told that &quot; my mercy takes\n",
      "Model: <SOS> so we we to to &quot; the &quot; is\n",
      "Attention Weights: tensor([[8.2594e-01, 1.7068e-01, 3.3300e-03, 4.5948e-05, 3.3174e-06, 5.8644e-07,\n",
      "         1.2150e-07, 3.6839e-08, 3.0507e-08, 2.4257e-08],\n",
      "        [7.6713e-02, 8.3395e-01, 8.8266e-02, 1.0277e-03, 3.8595e-05, 1.9208e-06,\n",
      "         1.8934e-07, 2.6500e-08, 9.8858e-09, 4.6258e-09],\n",
      "        [3.2678e-03, 1.1631e-01, 8.0626e-01, 7.0540e-02, 3.5692e-03, 5.0646e-05,\n",
      "         1.9675e-06, 5.0951e-08, 7.7132e-09, 9.3587e-10],\n",
      "        [3.0039e-04, 5.3957e-02, 5.0791e-01, 3.0692e-01, 1.2265e-01, 7.7907e-03,\n",
      "         4.6299e-04, 1.5081e-05, 1.8958e-06, 1.0574e-07],\n",
      "        [3.1156e-06, 2.0346e-04, 1.5285e-02, 3.3224e-01, 5.0304e-01, 1.2810e-01,\n",
      "         2.0243e-02, 7.9595e-04, 8.8377e-05, 3.0149e-06],\n",
      "        [1.3377e-05, 3.9717e-04, 4.8054e-03, 1.1520e-01, 3.4243e-01, 3.2766e-01,\n",
      "         1.9572e-01, 9.3836e-03, 4.1413e-03, 2.4682e-04],\n",
      "        [8.9543e-07, 3.8689e-05, 2.9022e-04, 5.2269e-03, 2.8120e-02, 7.3826e-02,\n",
      "         5.0181e-01, 1.3689e-01, 2.2482e-01, 2.8977e-02],\n",
      "        [5.9254e-07, 1.7200e-05, 6.6980e-05, 7.9979e-04, 4.5043e-03, 1.8192e-02,\n",
      "         1.7010e-01, 1.2076e-01, 4.5749e-01, 2.2808e-01],\n",
      "        [1.0671e-06, 1.7728e-05, 3.9098e-05, 2.9781e-04, 1.5038e-03, 8.3540e-03,\n",
      "         7.0034e-02, 7.1428e-02, 4.3786e-01, 4.1046e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.00, Train Loss: 0.00, Val Loss: 4.83, Train BLEU: 0.00, Val BLEU: 8.66, Minutes Elapsed: 103.82\n",
      "Sampling from val predictions...\n",
      "Source: cảm_ơn cháu rất nhiều . cảm_ơn . <EOS> <PAD> <PAD>\n",
      "Reference: thank you so much . thank you . <EOS>\n",
      "Model: <SOS> thank you very much . . you . thank\n",
      "Attention Weights: tensor([[0.0081, 0.7454, 0.2453, 0.0012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0059, 0.6445, 0.3467, 0.0029, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0069, 0.1989, 0.6151, 0.1701, 0.0089, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0012, 0.0175, 0.1463, 0.5090, 0.3184, 0.0071, 0.0005, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0006, 0.0033, 0.0295, 0.2830, 0.6428, 0.0376, 0.0032, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0007, 0.0055, 0.0367, 0.1660, 0.5708, 0.1940, 0.0255, 0.0008, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0007, 0.0046, 0.0273, 0.1053, 0.4069, 0.3556, 0.0946, 0.0049, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0009, 0.0067, 0.0434, 0.1439, 0.3639, 0.3167, 0.1155, 0.0091, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0002, 0.0010, 0.0064, 0.0422, 0.2877, 0.4362, 0.2041, 0.0223, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.05, Train Loss: 0.00, Val Loss: 4.79, Train BLEU: 0.00, Val BLEU: 9.53, Minutes Elapsed: 106.30\n",
      "Sampling from val predictions...\n",
      "Source: tôi đã làm việc cho tổ_chức phi chính_phủ của ý\n",
      "Reference: i worked for an italian ngo , and every\n",
      "Model: <SOS> i i to the of of the <EOS> the\n",
      "Attention Weights: tensor([[9.8557e-01, 1.4106e-02, 3.1369e-04, 5.8559e-06, 1.2775e-06, 4.0610e-07,\n",
      "         1.1400e-07, 4.2056e-08, 1.9205e-08, 1.4889e-08],\n",
      "        [2.4545e-02, 7.2661e-01, 2.4773e-01, 1.0487e-03, 6.0802e-05, 5.9800e-06,\n",
      "         4.0124e-07, 4.1971e-08, 1.1783e-08, 4.5012e-09],\n",
      "        [1.9012e-03, 4.0009e-02, 6.9614e-01, 1.9625e-01, 5.5312e-02, 9.8838e-03,\n",
      "         4.8426e-04, 1.4438e-05, 1.3403e-06, 1.3127e-07],\n",
      "        [6.7231e-06, 5.4828e-05, 4.4832e-03, 9.2615e-02, 4.4610e-01, 4.1765e-01,\n",
      "         3.8274e-02, 7.8411e-04, 3.6831e-05, 1.6337e-06],\n",
      "        [1.8654e-06, 1.1750e-05, 4.4901e-04, 1.2876e-02, 1.9959e-01, 5.4188e-01,\n",
      "         2.0775e-01, 3.3566e-02, 3.6631e-03, 2.1296e-04],\n",
      "        [1.3567e-05, 2.4923e-05, 1.7141e-04, 9.0123e-04, 1.4807e-02, 1.0270e-01,\n",
      "         1.9878e-01, 4.2261e-01, 2.1844e-01, 4.1546e-02],\n",
      "        [1.0194e-05, 1.7170e-05, 8.0279e-05, 2.6639e-04, 2.8325e-03, 1.9612e-02,\n",
      "         6.7069e-02, 3.9040e-01, 3.7422e-01, 1.4549e-01],\n",
      "        [9.5720e-07, 3.3402e-06, 2.9806e-05, 1.2699e-04, 1.1963e-03, 8.2446e-03,\n",
      "         3.0716e-02, 2.9660e-01, 3.8733e-01, 2.7575e-01],\n",
      "        [6.5444e-07, 2.9200e-06, 2.9889e-05, 1.4717e-04, 1.4841e-03, 1.0259e-02,\n",
      "         3.1781e-02, 2.8590e-01, 3.4828e-01, 3.2211e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.10, Train Loss: 0.00, Val Loss: 4.76, Train BLEU: 0.00, Val BLEU: 9.65, Minutes Elapsed: 108.79\n",
      "Sampling from val predictions...\n",
      "Source: tôi nghĩ , trời ạ , tôi cảm_thấy rất tệ\n",
      "Reference: so i &apos;m like , man , it made\n",
      "Model: <SOS> i think , , , , , i i\n",
      "Attention Weights: tensor([[9.9744e-01, 2.5396e-03, 1.8719e-05, 1.0402e-06, 4.0601e-08, 5.8282e-09,\n",
      "         3.0138e-09, 1.1230e-09, 5.9998e-10, 3.4148e-10],\n",
      "        [6.1613e-02, 9.1847e-01, 1.9305e-02, 6.1293e-04, 3.6635e-06, 8.8430e-08,\n",
      "         1.5620e-08, 4.5456e-09, 1.9572e-09, 7.2121e-10],\n",
      "        [5.5589e-03, 4.1873e-01, 4.5108e-01, 1.2108e-01, 3.4835e-03, 5.8954e-05,\n",
      "         9.6922e-06, 6.5823e-07, 1.0872e-07, 1.6214e-08],\n",
      "        [4.1206e-03, 3.2629e-01, 3.5629e-01, 3.0305e-01, 1.0033e-02, 1.7301e-04,\n",
      "         4.1983e-05, 2.9169e-06, 3.1758e-07, 3.7379e-08],\n",
      "        [1.2909e-04, 2.9895e-03, 4.4387e-02, 8.8127e-01, 6.6315e-02, 2.6218e-03,\n",
      "         2.1562e-03, 1.0714e-04, 1.6821e-05, 3.1374e-06],\n",
      "        [7.3831e-04, 4.6259e-03, 5.4431e-02, 8.2285e-01, 1.0002e-01, 1.0038e-02,\n",
      "         6.9037e-03, 3.1960e-04, 5.9452e-05, 1.3355e-05],\n",
      "        [8.1647e-05, 1.4638e-03, 2.5403e-02, 6.7474e-01, 2.5086e-01, 2.0851e-02,\n",
      "         2.5703e-02, 8.0949e-04, 7.5062e-05, 1.0125e-05],\n",
      "        [4.4245e-06, 1.7202e-04, 2.2488e-03, 4.0607e-02, 9.6892e-02, 6.3121e-02,\n",
      "         4.8501e-01, 2.7110e-01, 3.6785e-02, 4.0557e-03],\n",
      "        [3.1399e-07, 6.3571e-06, 1.0087e-04, 3.0579e-03, 1.0965e-02, 1.2391e-02,\n",
      "         3.2524e-01, 4.4981e-01, 1.5752e-01, 4.0905e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.14, Train Loss: 0.00, Val Loss: 4.78, Train BLEU: 0.00, Val BLEU: 9.30, Minutes Elapsed: 111.34\n",
      "Sampling from val predictions...\n",
      "Source: điều mà tôi đã làm là trồng một rừng thực_phẩm\n",
      "Reference: so what i did , i planted a food\n",
      "Model: <SOS> what what i did this is is a a\n",
      "Attention Weights: tensor([[8.8009e-01, 1.1950e-01, 4.0832e-04, 1.8700e-06, 1.0428e-07, 9.5418e-09,\n",
      "         2.2470e-09, 1.0837e-09, 4.2280e-10, 3.8107e-10],\n",
      "        [3.4963e-02, 6.6394e-01, 2.9929e-01, 1.7774e-03, 3.0833e-05, 4.4851e-07,\n",
      "         4.1018e-08, 7.7980e-09, 1.7881e-09, 9.8249e-10],\n",
      "        [2.0877e-03, 9.3438e-02, 6.5047e-01, 2.2954e-01, 2.4298e-02, 1.6098e-04,\n",
      "         2.2900e-06, 9.4541e-08, 3.4294e-09, 6.9768e-10],\n",
      "        [1.6623e-04, 4.5284e-03, 1.3865e-01, 4.2033e-01, 4.0283e-01, 3.2538e-02,\n",
      "         9.1508e-04, 3.7676e-05, 6.0592e-07, 6.9718e-08],\n",
      "        [4.8042e-05, 1.2879e-03, 4.2963e-02, 2.6335e-01, 5.6843e-01, 1.1816e-01,\n",
      "         5.4610e-03, 2.9073e-04, 5.1358e-06, 5.7486e-07],\n",
      "        [1.5088e-06, 1.0353e-05, 7.7827e-04, 8.0753e-03, 1.2957e-01, 3.6360e-01,\n",
      "         4.1739e-01, 7.7392e-02, 2.8499e-03, 3.2483e-04],\n",
      "        [2.6088e-07, 2.4044e-06, 2.1540e-04, 2.2414e-03, 2.8549e-02, 1.3041e-01,\n",
      "         4.7171e-01, 2.9016e-01, 5.7548e-02, 1.9159e-02],\n",
      "        [1.8719e-06, 1.8212e-05, 9.3633e-04, 8.9920e-03, 6.8972e-02, 1.6922e-01,\n",
      "         3.9414e-01, 2.3945e-01, 8.0672e-02, 3.7601e-02],\n",
      "        [5.4405e-06, 1.0326e-05, 8.5627e-05, 3.2023e-04, 2.8200e-03, 2.1665e-02,\n",
      "         2.1137e-01, 4.7013e-01, 1.9460e-01, 9.8987e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.19, Train Loss: 0.00, Val Loss: 4.77, Train BLEU: 0.00, Val BLEU: 9.16, Minutes Elapsed: 113.82\n",
      "Sampling from val predictions...\n",
      "Source: tôi đã có cơ_hội đi đến đây bằng máy_bay lần\n",
      "Reference: i got a chance to come by plane for\n",
      "Model: <SOS> i i a opportunity to and a a that\n",
      "Attention Weights: tensor([[9.7414e-01, 2.5597e-02, 2.5455e-04, 4.6676e-06, 7.8499e-08, 1.1478e-08,\n",
      "         7.1971e-09, 4.2632e-09, 1.7201e-09, 1.0121e-09],\n",
      "        [3.5950e-02, 8.4127e-01, 1.2151e-01, 1.2732e-03, 2.7490e-06, 1.0945e-07,\n",
      "         2.3477e-08, 4.6279e-09, 8.9365e-10, 5.0906e-10],\n",
      "        [2.0603e-03, 7.6637e-02, 7.3972e-01, 1.7935e-01, 2.1119e-03, 9.8843e-05,\n",
      "         1.6829e-05, 9.6265e-07, 3.1741e-08, 8.2338e-09],\n",
      "        [2.3128e-05, 4.4921e-04, 4.1571e-02, 8.8937e-01, 5.9727e-02, 6.8137e-03,\n",
      "         1.9109e-03, 1.3397e-04, 2.9138e-06, 3.9328e-07],\n",
      "        [1.4116e-04, 6.1370e-04, 1.5862e-02, 4.3594e-01, 3.7250e-01, 1.2760e-01,\n",
      "         4.3755e-02, 3.4686e-03, 9.9743e-05, 2.7460e-05],\n",
      "        [1.9394e-04, 4.4540e-04, 4.3566e-03, 9.0994e-02, 3.1073e-01, 3.0246e-01,\n",
      "         2.4470e-01, 4.2914e-02, 2.1367e-03, 1.0690e-03],\n",
      "        [1.6879e-07, 2.8418e-06, 1.5837e-04, 1.0235e-02, 6.9378e-02, 1.9834e-01,\n",
      "         4.2607e-01, 2.4255e-01, 3.6532e-02, 1.6731e-02],\n",
      "        [4.4501e-07, 3.2646e-06, 5.7818e-05, 1.9813e-03, 1.3092e-02, 6.3065e-02,\n",
      "         2.7205e-01, 3.2697e-01, 1.8357e-01, 1.3921e-01],\n",
      "        [1.4242e-06, 5.8396e-06, 6.6631e-05, 1.0557e-03, 5.6362e-03, 2.7731e-02,\n",
      "         1.6111e-01, 2.9137e-01, 2.2673e-01, 2.8630e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.24, Train Loss: 0.00, Val Loss: 4.73, Train BLEU: 0.00, Val BLEU: 9.86, Minutes Elapsed: 116.30\n",
      "Sampling from val predictions...\n",
      "Source: anh ta cần chúng để cảm_thấy được bảo_vệ . <EOS>\n",
      "Reference: he needed them to feel protected . <EOS> <PAD>\n",
      "Model: <SOS> he &apos;s to to to . . <EOS> <EOS>\n",
      "Attention Weights: tensor([[9.7446e-01, 2.5148e-02, 3.8118e-04, 1.4742e-05, 9.2797e-07, 2.4093e-07,\n",
      "         1.2098e-07, 3.3980e-08, 1.0508e-08, 2.6695e-09],\n",
      "        [2.2049e-03, 3.1471e-02, 9.4276e-01, 2.3450e-02, 1.0123e-04, 6.0596e-06,\n",
      "         1.3200e-06, 1.1106e-07, 1.2036e-08, 1.4551e-09],\n",
      "        [7.2716e-04, 1.2652e-02, 5.4758e-01, 4.2262e-01, 1.2205e-02, 3.3748e-03,\n",
      "         7.9761e-04, 3.7875e-05, 1.3204e-06, 3.8890e-08],\n",
      "        [1.4801e-05, 1.6070e-04, 6.2389e-02, 5.8386e-01, 2.5912e-01, 6.9335e-02,\n",
      "         2.3515e-02, 1.5696e-03, 3.0830e-05, 2.3955e-07],\n",
      "        [1.3571e-06, 1.1989e-05, 4.6110e-03, 9.5509e-02, 2.2386e-01, 3.4536e-01,\n",
      "         2.7826e-01, 5.0312e-02, 2.0603e-03, 1.4162e-05],\n",
      "        [1.5459e-06, 4.6051e-06, 2.3549e-04, 3.5892e-03, 2.6585e-02, 1.7473e-01,\n",
      "         4.3197e-01, 3.1752e-01, 4.4416e-02, 9.4981e-04],\n",
      "        [1.9662e-05, 3.1910e-05, 5.7445e-04, 4.7385e-03, 2.6877e-02, 1.4333e-01,\n",
      "         3.4701e-01, 3.7102e-01, 1.0061e-01, 5.7851e-03],\n",
      "        [1.4907e-05, 5.1570e-05, 9.7731e-04, 8.8408e-03, 4.5804e-02, 1.5018e-01,\n",
      "         3.0686e-01, 3.3918e-01, 1.3173e-01, 1.6361e-02],\n",
      "        [2.8397e-05, 1.6933e-04, 2.6376e-03, 2.1264e-02, 1.0867e-01, 2.1157e-01,\n",
      "         2.9951e-01, 2.3732e-01, 9.6943e-02, 2.1884e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.29, Train Loss: 0.00, Val Loss: 4.74, Train BLEU: 0.00, Val BLEU: 9.38, Minutes Elapsed: 118.78\n",
      "Sampling from val predictions...\n",
      "Source: thật khó_tin , và chúng_tôi nói với những người zambia\n",
      "Reference: and we could not believe , and we were\n",
      "Model: <SOS> the the , , and and we we we\n",
      "Attention Weights: tensor([[9.7950e-01, 2.0378e-02, 1.1768e-04, 8.4127e-07, 1.5633e-07, 1.5626e-08,\n",
      "         3.8288e-09, 1.5523e-09, 9.1800e-10, 3.2954e-10],\n",
      "        [5.4865e-02, 9.2634e-01, 1.8684e-02, 9.5958e-05, 1.1589e-05, 3.7103e-07,\n",
      "         3.9167e-08, 7.4464e-09, 2.2571e-09, 4.8285e-10],\n",
      "        [5.5305e-03, 1.6770e-01, 7.7781e-01, 4.0556e-02, 8.2211e-03, 1.7677e-04,\n",
      "         7.6022e-06, 1.4827e-07, 1.6442e-08, 9.6905e-10],\n",
      "        [2.0211e-03, 5.3988e-02, 4.2501e-01, 4.3181e-01, 8.3278e-02, 3.6807e-03,\n",
      "         2.1405e-04, 3.6210e-06, 3.1029e-07, 8.3739e-09],\n",
      "        [7.2112e-05, 2.1199e-02, 2.3432e-01, 5.4808e-01, 1.8940e-01, 6.5308e-03,\n",
      "         3.9349e-04, 7.8061e-06, 9.0837e-07, 3.9539e-08],\n",
      "        [4.7422e-05, 9.8600e-03, 1.1567e-01, 4.9249e-01, 3.6066e-01, 1.9896e-02,\n",
      "         1.3459e-03, 2.9399e-05, 3.2933e-06, 1.5362e-07],\n",
      "        [1.8888e-05, 4.8411e-03, 5.5168e-02, 3.3594e-01, 5.0803e-01, 8.6389e-02,\n",
      "         9.3726e-03, 2.2865e-04, 1.8548e-05, 5.5373e-07],\n",
      "        [2.2680e-06, 1.0655e-03, 7.0001e-03, 3.2110e-02, 3.3698e-01, 4.4827e-01,\n",
      "         1.6443e-01, 9.4181e-03, 7.0710e-04, 1.4088e-05],\n",
      "        [2.0205e-06, 5.1134e-04, 3.6495e-03, 7.6537e-03, 4.5350e-02, 2.6936e-01,\n",
      "         5.3823e-01, 1.1562e-01, 1.9117e-02, 5.1012e-04]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.34, Train Loss: 0.00, Val Loss: 4.72, Train BLEU: 0.00, Val BLEU: 9.42, Minutes Elapsed: 121.26\n",
      "Sampling from val predictions...\n",
      "Source: khi tôi lên 7 , tôi chứng_kiến cảnh người_ta <UNK>\n",
      "Reference: when i was seven years old , i saw\n",
      "Model: <SOS> when i was , , , i i i\n",
      "Attention Weights: tensor([[9.6158e-01, 3.8309e-02, 1.1245e-04, 8.9452e-07, 5.7896e-07, 2.0181e-07,\n",
      "         3.8096e-08, 1.8722e-08, 9.4097e-09, 4.0696e-09],\n",
      "        [6.3572e-02, 7.9982e-01, 1.3581e-01, 6.2919e-04, 1.4651e-04, 2.5164e-05,\n",
      "         2.9796e-06, 5.8958e-07, 1.3107e-07, 4.1190e-08],\n",
      "        [3.9371e-03, 7.8366e-02, 8.7037e-01, 3.6545e-02, 9.6343e-03, 1.0752e-03,\n",
      "         6.3096e-05, 4.5184e-06, 1.2968e-07, 2.6625e-08],\n",
      "        [6.0949e-04, 2.5705e-02, 5.8837e-01, 2.0215e-01, 1.3234e-01, 4.5053e-02,\n",
      "         5.1917e-03, 5.7055e-04, 1.2790e-05, 1.6511e-06],\n",
      "        [2.1219e-05, 6.2034e-04, 2.2236e-02, 8.5875e-02, 3.7028e-01, 4.6559e-01,\n",
      "         4.7198e-02, 7.7562e-03, 3.6035e-04, 5.7112e-05],\n",
      "        [4.8204e-05, 6.3693e-04, 1.6685e-03, 3.0431e-03, 6.9036e-02, 8.7542e-01,\n",
      "         4.3758e-02, 5.9333e-03, 3.9889e-04, 5.4230e-05],\n",
      "        [7.3490e-06, 1.6949e-04, 6.2310e-04, 1.3138e-03, 1.9805e-02, 7.9682e-01,\n",
      "         1.4715e-01, 3.0904e-02, 2.7935e-03, 4.1132e-04],\n",
      "        [1.4302e-07, 4.9109e-06, 1.2260e-04, 5.8941e-04, 3.1779e-03, 2.0365e-01,\n",
      "         4.9197e-01, 2.5090e-01, 4.3159e-02, 6.4317e-03],\n",
      "        [4.5499e-07, 1.6966e-05, 2.2703e-04, 8.4951e-04, 9.2354e-03, 2.0155e-01,\n",
      "         3.7705e-01, 3.2214e-01, 7.5389e-02, 1.3552e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.38, Train Loss: 0.00, Val Loss: 4.71, Train BLEU: 0.00, Val BLEU: 9.92, Minutes Elapsed: 123.74\n",
      "Sampling from val predictions...\n",
      "Source: tất_cả đều có_thể với những thông_tin này . <EOS> <PAD>\n",
      "Reference: all this is possible with this information . <EOS>\n",
      "Model: <SOS> all all can all of the . . <EOS>\n",
      "Attention Weights: tensor([[0.8540, 0.1444, 0.0016, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0415, 0.5595, 0.3891, 0.0097, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0059, 0.1048, 0.6248, 0.2344, 0.0259, 0.0041, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0004, 0.0084, 0.1650, 0.4837, 0.2285, 0.1050, 0.0088, 0.0003, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0001, 0.0007, 0.0266, 0.2042, 0.2733, 0.3941, 0.0932, 0.0078, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0009, 0.0029, 0.0165, 0.0789, 0.1741, 0.4713, 0.2271, 0.0281, 0.0002,\n",
      "         0.0000],\n",
      "        [0.0002, 0.0006, 0.0042, 0.0138, 0.0579, 0.2911, 0.4360, 0.1872, 0.0092,\n",
      "         0.0000],\n",
      "        [0.0008, 0.0017, 0.0049, 0.0101, 0.0366, 0.1770, 0.4435, 0.2909, 0.0345,\n",
      "         0.0000],\n",
      "        [0.0007, 0.0018, 0.0068, 0.0129, 0.0447, 0.1932, 0.3791, 0.2994, 0.0613,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.43, Train Loss: 0.00, Val Loss: 4.68, Train BLEU: 0.00, Val BLEU: 10.29, Minutes Elapsed: 126.24\n",
      "Sampling from val predictions...\n",
      "Source: vì chị không có quyền_lực nào cả , còn em\n",
      "Reference: because i &apos;m not in charge of anything ,\n",
      "Model: <SOS> because there &apos;s &apos;t a , , but ,\n",
      "Attention Weights: tensor([[9.4373e-01, 4.8655e-02, 7.5427e-03, 6.9060e-05, 2.0896e-06, 2.9249e-07,\n",
      "         8.0600e-08, 9.5342e-08, 5.3258e-08, 2.9421e-08],\n",
      "        [3.7684e-02, 1.2161e-01, 8.2695e-01, 1.3591e-02, 1.5974e-04, 6.2743e-06,\n",
      "         4.1117e-07, 2.1475e-07, 3.4314e-08, 1.1727e-08],\n",
      "        [2.8966e-03, 1.5300e-02, 6.1640e-01, 3.4360e-01, 2.1299e-02, 4.9426e-04,\n",
      "         9.6107e-06, 1.8942e-06, 1.4158e-07, 1.5585e-08],\n",
      "        [3.6991e-03, 2.1129e-02, 4.4701e-01, 3.1662e-01, 1.8071e-01, 2.9064e-02,\n",
      "         1.3760e-03, 3.5845e-04, 2.7309e-05, 1.7662e-06],\n",
      "        [1.1143e-04, 2.5307e-04, 4.7915e-03, 7.2309e-02, 4.2869e-01, 4.2034e-01,\n",
      "         5.2817e-02, 1.8058e-02, 2.4924e-03, 1.3817e-04],\n",
      "        [4.5325e-04, 5.0258e-04, 2.4949e-03, 1.3707e-02, 9.9535e-02, 3.6601e-01,\n",
      "         1.8602e-01, 2.4979e-01, 7.7126e-02, 4.3627e-03],\n",
      "        [9.7025e-06, 2.2197e-05, 2.8612e-04, 2.9045e-03, 3.7249e-02, 2.3628e-01,\n",
      "         1.9270e-01, 2.7837e-01, 2.3033e-01, 2.1850e-02],\n",
      "        [1.0429e-05, 2.4406e-05, 1.7085e-04, 6.3818e-04, 2.0905e-03, 1.8760e-02,\n",
      "         2.9368e-02, 1.3714e-01, 6.7883e-01, 1.3297e-01],\n",
      "        [2.3809e-07, 9.5508e-07, 4.1753e-05, 4.1746e-04, 2.0144e-03, 1.3158e-02,\n",
      "         1.8270e-02, 5.1738e-02, 5.2706e-01, 3.8730e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.48, Train Loss: 0.00, Val Loss: 4.67, Train BLEU: 0.00, Val BLEU: 10.44, Minutes Elapsed: 128.70\n",
      "Sampling from val predictions...\n",
      "Source: sau đó , tổ_chức tình_nguyện tôi tham_gia all <UNK> <UNK>\n",
      "Reference: soon after , an organization i volunteer with ,\n",
      "Model: <SOS> after , i , i , i to <UNK>\n",
      "Attention Weights: tensor([[4.8253e-01, 5.1461e-01, 2.5905e-03, 2.6975e-04, 1.5616e-06, 1.6715e-07,\n",
      "         1.2052e-08, 5.2928e-09, 3.7001e-09, 3.6396e-09],\n",
      "        [1.8842e-02, 1.8680e-01, 4.0443e-01, 3.8873e-01, 1.1655e-03, 3.1768e-05,\n",
      "         1.1093e-06, 2.6105e-07, 1.2740e-07, 7.4632e-08],\n",
      "        [1.1417e-04, 8.7777e-04, 2.3369e-02, 9.4916e-01, 2.6080e-02, 3.9111e-04,\n",
      "         5.4356e-06, 3.5986e-07, 1.0673e-07, 4.2934e-08],\n",
      "        [1.3314e-05, 1.0746e-04, 6.2913e-03, 6.1464e-01, 3.5610e-01, 2.2734e-02,\n",
      "         1.0848e-04, 3.5145e-06, 1.3198e-06, 3.1430e-07],\n",
      "        [4.5437e-05, 2.5235e-04, 8.0002e-03, 4.1069e-01, 4.1081e-01, 1.6768e-01,\n",
      "         2.1914e-03, 1.6676e-04, 1.2247e-04, 4.0738e-05],\n",
      "        [4.7615e-06, 2.9928e-05, 1.8905e-03, 1.0124e-01, 3.8633e-01, 4.8980e-01,\n",
      "         1.7188e-02, 1.7280e-03, 1.3372e-03, 4.5528e-04],\n",
      "        [2.8925e-05, 1.6075e-04, 5.9594e-03, 1.9583e-01, 3.1539e-01, 4.6772e-01,\n",
      "         1.0528e-02, 1.5096e-03, 1.8692e-03, 1.0003e-03],\n",
      "        [1.1126e-06, 5.0210e-06, 1.9320e-04, 2.0916e-02, 1.3999e-01, 7.0843e-01,\n",
      "         8.2150e-02, 1.6929e-02, 1.9984e-02, 1.1402e-02],\n",
      "        [1.9483e-07, 3.1462e-07, 4.8340e-06, 3.8361e-04, 6.7603e-03, 1.2464e-01,\n",
      "         1.6473e-01, 2.0507e-01, 2.5526e-01, 2.4315e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.53, Train Loss: 0.00, Val Loss: 4.68, Train BLEU: 0.00, Val BLEU: 9.64, Minutes Elapsed: 131.17\n",
      "Sampling from val predictions...\n",
      "Source: chúng tới ngày đầu_tiên và nhìn_thấy con bù_nhìn , và\n",
      "Reference: they will come the first day and they see\n",
      "Model: <SOS> they &apos;re the and and and and and and\n",
      "Attention Weights: tensor([[9.9759e-01, 2.3922e-03, 1.4181e-05, 1.5229e-07, 1.1490e-09, 6.4596e-10,\n",
      "         2.2822e-10, 1.1232e-10, 5.0969e-11, 2.4793e-11],\n",
      "        [1.0899e-01, 8.0581e-01, 8.4385e-02, 8.1480e-04, 3.1672e-06, 8.8447e-07,\n",
      "         1.4817e-07, 3.2558e-08, 3.2890e-09, 7.8658e-10],\n",
      "        [3.3878e-03, 3.9458e-02, 8.5809e-01, 9.5209e-02, 3.4058e-03, 3.2377e-04,\n",
      "         9.7017e-05, 3.0076e-05, 8.8146e-07, 8.3014e-08],\n",
      "        [9.7750e-05, 3.4340e-03, 4.3907e-01, 5.3959e-01, 1.3224e-02, 3.2581e-03,\n",
      "         9.8648e-04, 3.2336e-04, 1.8420e-05, 2.3712e-06],\n",
      "        [4.8131e-05, 5.4847e-04, 1.7249e-01, 7.5305e-01, 5.7714e-02, 1.0777e-02,\n",
      "         3.6128e-03, 1.5993e-03, 1.4054e-04, 2.0358e-05],\n",
      "        [4.0355e-05, 1.6039e-04, 4.4529e-02, 6.7927e-01, 1.8048e-01, 5.8547e-02,\n",
      "         2.3231e-02, 1.2132e-02, 1.3013e-03, 3.0269e-04],\n",
      "        [1.5096e-04, 1.0504e-04, 1.4824e-02, 4.2273e-01, 3.6298e-01, 9.2727e-02,\n",
      "         4.8495e-02, 4.3658e-02, 1.0638e-02, 3.6919e-03],\n",
      "        [3.3947e-05, 4.0219e-05, 4.9993e-03, 1.8068e-01, 2.3750e-01, 2.3398e-01,\n",
      "         1.6014e-01, 1.3655e-01, 3.0771e-02, 1.5299e-02],\n",
      "        [2.6505e-06, 8.1329e-06, 1.1073e-03, 4.1818e-02, 5.7005e-02, 2.6430e-01,\n",
      "         3.0114e-01, 2.6816e-01, 4.1338e-02, 2.5125e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.58, Train Loss: 0.00, Val Loss: 4.65, Train BLEU: 0.00, Val BLEU: 10.21, Minutes Elapsed: 133.68\n",
      "Sampling from val predictions...\n",
      "Source: hãy tìm đọc cuốn sách của bà ấy . <EOS>\n",
      "Reference: just go and read her book . <EOS> <PAD>\n",
      "Model: <SOS> let &apos;s to your &apos;s she . <EOS> .\n",
      "Attention Weights: tensor([[7.2833e-01, 2.6767e-01, 3.9218e-03, 6.9736e-05, 3.8939e-06, 4.2969e-07,\n",
      "         7.3970e-08, 2.0956e-08, 8.8854e-09, 2.4950e-09],\n",
      "        [8.3278e-03, 4.0069e-01, 5.1864e-01, 7.0549e-02, 1.7125e-03, 7.3899e-05,\n",
      "         1.9138e-06, 8.9171e-08, 1.5078e-08, 2.6514e-09],\n",
      "        [2.9163e-03, 4.3648e-02, 2.6044e-01, 4.8601e-01, 1.6047e-01, 4.1732e-02,\n",
      "         4.5146e-03, 2.5795e-04, 1.1314e-05, 3.8669e-07],\n",
      "        [4.0885e-05, 5.6205e-04, 1.7985e-02, 2.8027e-01, 4.5484e-01, 1.9286e-01,\n",
      "         4.8477e-02, 4.7815e-03, 1.8417e-04, 2.8134e-06],\n",
      "        [3.3038e-06, 5.0867e-05, 2.5419e-03, 8.6466e-02, 4.0536e-01, 3.2920e-01,\n",
      "         1.4896e-01, 2.6038e-02, 1.3560e-03, 2.2825e-05],\n",
      "        [1.1711e-05, 1.2309e-04, 1.6123e-03, 2.3236e-02, 2.2973e-01, 2.7066e-01,\n",
      "         2.8905e-01, 1.6501e-01, 2.0110e-02, 4.6378e-04],\n",
      "        [9.5951e-07, 1.0617e-05, 2.1123e-04, 3.8488e-03, 7.3334e-02, 1.4514e-01,\n",
      "         3.1830e-01, 3.7272e-01, 8.2473e-02, 3.9647e-03],\n",
      "        [1.5341e-05, 5.1136e-05, 4.9645e-04, 4.2890e-03, 3.5712e-02, 7.0838e-02,\n",
      "         2.3300e-01, 4.3304e-01, 1.9762e-01, 2.4940e-02],\n",
      "        [1.7772e-05, 6.8235e-05, 5.3341e-04, 3.5843e-03, 4.2921e-02, 6.2076e-02,\n",
      "         1.9115e-01, 3.9156e-01, 2.5364e-01, 5.4447e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.62, Train Loss: 0.00, Val Loss: 4.63, Train BLEU: 0.00, Val BLEU: 10.96, Minutes Elapsed: 136.17\n",
      "Sampling from val predictions...\n",
      "Source: ít_hơn một tháng rồi , ông và con_gái ông đang\n",
      "Reference: less than a month ago , he and his\n",
      "Model: <SOS> so a , , , and and and and\n",
      "Attention Weights: tensor([[9.9086e-01, 9.0829e-03, 6.0018e-05, 5.9402e-07, 4.2926e-08, 1.6110e-08,\n",
      "         5.2379e-10, 4.5076e-10, 7.8010e-10, 3.8926e-10],\n",
      "        [1.6991e-01, 5.5037e-01, 2.7360e-01, 5.9995e-03, 9.4653e-05, 2.7110e-05,\n",
      "         1.6489e-07, 6.8919e-08, 3.2701e-08, 6.7784e-09],\n",
      "        [2.0914e-02, 5.1660e-02, 4.4691e-01, 3.7776e-01, 5.9016e-02, 4.2411e-02,\n",
      "         7.1002e-04, 4.7042e-04, 1.4041e-04, 8.4639e-06],\n",
      "        [1.1009e-02, 1.7037e-02, 6.7525e-02, 2.8088e-01, 2.0079e-01, 4.2074e-01,\n",
      "         1.5652e-03, 2.0356e-04, 2.4170e-04, 9.5575e-06],\n",
      "        [8.7440e-03, 1.0044e-02, 3.0129e-02, 1.0857e-01, 1.1126e-01, 7.2725e-01,\n",
      "         3.1180e-03, 3.6240e-04, 5.0127e-04, 3.0087e-05],\n",
      "        [6.1952e-03, 5.4670e-03, 1.4435e-02, 7.8518e-02, 1.0153e-01, 7.8673e-01,\n",
      "         5.3221e-03, 5.1429e-04, 1.1849e-03, 1.0169e-04],\n",
      "        [1.1227e-04, 3.5832e-04, 3.0707e-03, 2.2569e-02, 4.7580e-02, 8.5627e-01,\n",
      "         4.2146e-02, 1.5190e-02, 1.1807e-02, 8.9773e-04],\n",
      "        [2.3843e-05, 6.8483e-05, 5.1393e-04, 5.4004e-03, 2.3170e-02, 6.5691e-01,\n",
      "         1.1255e-01, 8.7366e-02, 1.0209e-01, 1.1903e-02],\n",
      "        [1.5987e-06, 1.4688e-05, 1.5734e-04, 6.6270e-04, 1.7226e-03, 5.1502e-02,\n",
      "         2.8739e-02, 1.2962e-01, 5.4050e-01, 2.4708e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.67, Train Loss: 0.00, Val Loss: 4.63, Train BLEU: 0.00, Val BLEU: 10.47, Minutes Elapsed: 138.69\n",
      "Sampling from val predictions...\n",
      "Source: bởi_vì gia_đình tôi không biết tiếng trung , nên tôi\n",
      "Reference: since my family couldn &apos;t speak chinese , i\n",
      "Model: <SOS> because my i i i know know i ,\n",
      "Attention Weights: tensor([[9.9397e-01, 5.9459e-03, 8.2672e-05, 1.8242e-06, 7.4976e-08, 7.6526e-09,\n",
      "         2.9501e-09, 1.8789e-09, 8.9452e-10, 7.4802e-10],\n",
      "        [1.7131e-01, 7.7196e-01, 5.4776e-02, 1.8839e-03, 5.8384e-05, 3.2345e-06,\n",
      "         3.3981e-07, 2.6544e-08, 4.3833e-09, 2.7763e-09],\n",
      "        [6.7954e-03, 1.2171e-01, 5.3402e-01, 2.9865e-01, 3.6554e-02, 2.1792e-03,\n",
      "         8.1489e-05, 1.0103e-06, 3.2180e-08, 5.3653e-09],\n",
      "        [2.8525e-03, 4.2412e-02, 3.4059e-01, 5.1676e-01, 9.3154e-02, 4.1065e-03,\n",
      "         1.2695e-04, 9.2102e-07, 2.7367e-08, 4.5206e-09],\n",
      "        [7.5832e-04, 1.6031e-02, 3.1477e-01, 4.1107e-01, 2.2854e-01, 2.7006e-02,\n",
      "         1.7733e-03, 3.8725e-05, 2.1938e-06, 1.8200e-07],\n",
      "        [9.2653e-05, 4.8481e-03, 1.3557e-01, 3.1975e-01, 4.5515e-01, 7.8663e-02,\n",
      "         5.7841e-03, 1.2791e-04, 9.9385e-06, 5.4735e-07],\n",
      "        [2.7545e-06, 2.5186e-04, 1.3645e-02, 9.6688e-02, 5.1803e-01, 2.9434e-01,\n",
      "         7.0591e-02, 5.7418e-03, 6.5662e-04, 4.8880e-05],\n",
      "        [2.9607e-06, 1.8991e-04, 1.1260e-02, 7.2118e-02, 3.5152e-01, 3.5490e-01,\n",
      "         1.6457e-01, 3.3287e-02, 1.1666e-02, 4.8800e-04],\n",
      "        [1.4697e-06, 3.1256e-05, 1.6986e-03, 1.9678e-02, 7.7445e-02, 1.0349e-01,\n",
      "         1.4241e-01, 1.2963e-01, 4.1580e-01, 1.0982e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.72, Train Loss: 0.00, Val Loss: 4.62, Train BLEU: 0.00, Val BLEU: 10.50, Minutes Elapsed: 141.22\n",
      "Sampling from val predictions...\n",
      "Source: đó là những hình_ảnh được dựng lên , và nó\n",
      "Reference: they are constructions , and they are constructions by\n",
      "Model: <SOS> it &apos;s the the the , , , and\n",
      "Attention Weights: tensor([[9.8313e-01, 1.6762e-02, 1.0622e-04, 1.1916e-06, 2.5746e-08, 1.9917e-09,\n",
      "         3.6551e-10, 1.9450e-10, 6.4922e-11, 5.6179e-11],\n",
      "        [4.3510e-02, 8.1144e-01, 1.4062e-01, 4.3443e-03, 8.0250e-05, 2.7388e-06,\n",
      "         1.3304e-07, 8.5302e-09, 1.0847e-09, 4.9437e-10],\n",
      "        [1.4007e-02, 7.4425e-02, 6.1418e-01, 2.3942e-01, 4.7945e-02, 9.4435e-03,\n",
      "         5.6873e-04, 1.3205e-05, 9.0085e-07, 2.8694e-07],\n",
      "        [3.9267e-04, 3.7843e-03, 1.2839e-01, 3.8582e-01, 3.8149e-01, 9.1185e-02,\n",
      "         8.7613e-03, 1.6095e-04, 7.8778e-06, 1.6944e-06],\n",
      "        [5.1291e-04, 1.3942e-03, 4.2724e-03, 3.9750e-02, 3.5892e-01, 2.5294e-01,\n",
      "         3.0949e-01, 3.1337e-02, 1.2682e-03, 1.1800e-04],\n",
      "        [3.8253e-06, 1.7365e-05, 1.8649e-04, 3.1630e-03, 4.9843e-02, 1.3471e-01,\n",
      "         4.6814e-01, 2.6418e-01, 5.4691e-02, 2.5066e-02],\n",
      "        [6.1130e-06, 3.0562e-05, 2.7362e-04, 4.7508e-03, 6.1733e-02, 8.8527e-02,\n",
      "         1.8192e-01, 1.5211e-01, 1.4416e-01, 3.6649e-01],\n",
      "        [3.2237e-06, 1.1449e-05, 8.1336e-05, 1.4433e-03, 2.0211e-02, 3.3044e-02,\n",
      "         9.2940e-02, 1.6359e-01, 2.4090e-01, 4.4778e-01],\n",
      "        [7.3504e-07, 2.0679e-06, 3.6518e-05, 5.3920e-04, 7.2151e-03, 3.5563e-02,\n",
      "         1.6296e-01, 2.8126e-01, 2.2346e-01, 2.8898e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.77, Train Loss: 0.00, Val Loss: 4.63, Train BLEU: 0.00, Val BLEU: 10.51, Minutes Elapsed: 143.71\n",
      "Sampling from val predictions...\n",
      "Source: phát_minh sắp tới của cháu , cháu muốn làm một\n",
      "Reference: my next invention is , i want to make\n",
      "Model: <SOS> the the of , , because want to do\n",
      "Attention Weights: tensor([[9.1971e-01, 8.0182e-02, 1.0626e-04, 4.2878e-06, 6.2832e-07, 2.2735e-07,\n",
      "         2.2837e-07, 1.2000e-07, 3.7471e-08, 1.3482e-08],\n",
      "        [4.4665e-02, 8.6240e-01, 8.9029e-02, 3.6922e-03, 2.0987e-04, 6.0212e-06,\n",
      "         1.9429e-06, 5.4993e-07, 8.6283e-08, 2.2808e-08],\n",
      "        [7.1190e-03, 3.2090e-01, 4.4304e-01, 1.8102e-01, 4.6426e-02, 1.1478e-03,\n",
      "         2.9089e-04, 5.2909e-05, 2.5984e-06, 2.7335e-07],\n",
      "        [1.5921e-04, 5.8377e-03, 1.0159e-01, 2.7156e-01, 4.2601e-01, 1.0484e-01,\n",
      "         7.0588e-02, 1.6755e-02, 2.2197e-03, 4.3632e-04],\n",
      "        [8.0817e-05, 6.7085e-04, 1.2147e-02, 7.8273e-02, 1.7865e-01, 8.8917e-02,\n",
      "         5.1561e-01, 1.1466e-01, 9.4489e-03, 1.5474e-03],\n",
      "        [5.1485e-07, 1.5074e-05, 2.4440e-04, 1.6569e-03, 4.0614e-03, 5.4630e-03,\n",
      "         4.1264e-01, 5.1287e-01, 5.6355e-02, 6.6973e-03],\n",
      "        [1.2324e-06, 1.1894e-05, 9.8329e-05, 1.0000e-03, 3.8087e-03, 1.8218e-02,\n",
      "         7.0223e-01, 2.4516e-01, 2.5434e-02, 4.0378e-03],\n",
      "        [4.1464e-07, 4.2659e-05, 5.4823e-04, 2.2468e-03, 2.8564e-03, 3.0138e-03,\n",
      "         1.3012e-01, 6.4157e-01, 1.8942e-01, 3.0191e-02],\n",
      "        [1.5681e-06, 1.5582e-05, 3.2194e-04, 3.3950e-03, 6.7051e-03, 1.1455e-02,\n",
      "         4.0672e-02, 1.7033e-01, 3.0875e-01, 4.5835e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.82, Train Loss: 0.00, Val Loss: 4.60, Train BLEU: 0.00, Val BLEU: 10.57, Minutes Elapsed: 146.23\n",
      "Sampling from val predictions...\n",
      "Source: afghanistan nhìn rất khác khi nhìn từ mỹ . <EOS>\n",
      "Reference: afghanistan looks so different from here in america .\n",
      "Model: <SOS> you &apos;s have the the . . <EOS> .\n",
      "Attention Weights: tensor([[9.4127e-01, 5.8469e-02, 2.5554e-04, 7.7491e-06, 2.0025e-06, 5.5660e-07,\n",
      "         1.4487e-07, 6.2429e-08, 2.2585e-08, 4.9121e-09],\n",
      "        [3.3156e-02, 8.5675e-01, 1.0796e-01, 2.0044e-03, 1.1758e-04, 1.4242e-05,\n",
      "         1.5714e-06, 1.7195e-07, 1.3735e-08, 1.4514e-09],\n",
      "        [1.1405e-02, 3.0674e-01, 4.7826e-01, 1.4247e-01, 5.0475e-02, 9.3616e-03,\n",
      "         1.1329e-03, 1.4416e-04, 2.9725e-06, 7.7635e-08],\n",
      "        [1.2628e-04, 5.2097e-03, 7.0650e-02, 2.3248e-01, 5.2311e-01, 1.3659e-01,\n",
      "         2.6338e-02, 5.3737e-03, 1.1935e-04, 1.6758e-06],\n",
      "        [3.5176e-05, 4.6619e-04, 5.6093e-03, 4.0256e-02, 4.3264e-01, 3.2970e-01,\n",
      "         1.2984e-01, 5.6751e-02, 4.5770e-03, 1.1746e-04],\n",
      "        [1.2933e-05, 3.3289e-04, 4.5442e-03, 2.2923e-02, 2.0338e-01, 4.0307e-01,\n",
      "         1.7805e-01, 1.7189e-01, 1.5264e-02, 5.2898e-04],\n",
      "        [2.0410e-05, 1.6374e-04, 8.6483e-04, 4.1112e-03, 9.1495e-02, 2.5203e-01,\n",
      "         2.1022e-01, 3.4815e-01, 8.6272e-02, 6.6757e-03],\n",
      "        [8.6193e-05, 3.9547e-04, 1.0190e-03, 3.2290e-03, 1.1431e-01, 2.4474e-01,\n",
      "         1.6126e-01, 3.1582e-01, 1.4901e-01, 1.0127e-02],\n",
      "        [2.1311e-05, 2.6613e-04, 1.1832e-03, 5.0982e-03, 8.1481e-02, 2.8484e-01,\n",
      "         2.3074e-01, 3.0936e-01, 7.9326e-02, 7.6861e-03]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.86, Train Loss: 0.00, Val Loss: 4.57, Train BLEU: 0.00, Val BLEU: 10.66, Minutes Elapsed: 148.73\n",
      "Sampling from val predictions...\n",
      "Source: bởi_vì hoà_bình như thuật giả kim , nó cần đến\n",
      "Reference: for peace has an alchemy , and this alchemy\n",
      "Model: <SOS> because &apos;s is like , , , it it\n",
      "Attention Weights: tensor([[9.9648e-01, 3.3922e-03, 1.1083e-04, 1.0017e-05, 2.1334e-06, 6.5467e-07,\n",
      "         1.3370e-07, 4.8813e-08, 2.5693e-08, 1.1948e-08],\n",
      "        [1.3556e-01, 6.6320e-01, 1.8008e-01, 1.8353e-02, 2.6202e-03, 1.8006e-04,\n",
      "         3.3555e-06, 3.6492e-07, 1.0533e-07, 3.7885e-08],\n",
      "        [4.8717e-02, 1.4345e-01, 4.4109e-01, 2.1555e-01, 1.1141e-01, 3.8756e-02,\n",
      "         9.3832e-04, 7.4371e-05, 1.7396e-05, 2.2553e-06],\n",
      "        [6.1102e-03, 2.2101e-02, 2.3455e-01, 3.0476e-01, 2.6673e-01, 1.5818e-01,\n",
      "         6.7571e-03, 7.0031e-04, 9.8208e-05, 1.4159e-05],\n",
      "        [3.0459e-05, 5.2366e-04, 6.3618e-02, 2.8367e-01, 4.4623e-01, 1.9013e-01,\n",
      "         1.3055e-02, 2.2172e-03, 4.5744e-04, 7.1322e-05],\n",
      "        [3.1731e-05, 1.1856e-04, 2.1350e-02, 1.3865e-01, 3.4536e-01, 2.9991e-01,\n",
      "         1.2863e-01, 5.3044e-02, 1.0884e-02, 2.0285e-03],\n",
      "        [3.7750e-05, 5.8333e-05, 5.4683e-03, 3.0064e-02, 9.1546e-02, 1.6438e-01,\n",
      "         3.1338e-01, 3.4693e-01, 4.1972e-02, 6.1591e-03],\n",
      "        [3.8816e-06, 2.7622e-05, 1.8545e-03, 1.4954e-02, 3.3690e-02, 3.7765e-02,\n",
      "         1.0774e-01, 5.2253e-01, 2.4611e-01, 3.5330e-02],\n",
      "        [6.2726e-07, 1.3386e-05, 7.0219e-04, 4.5042e-03, 7.3559e-03, 8.2445e-03,\n",
      "         1.2528e-02, 1.2322e-01, 5.8786e-01, 2.5557e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.91, Train Loss: 0.00, Val Loss: 4.57, Train BLEU: 0.00, Val BLEU: 10.12, Minutes Elapsed: 151.29\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi có những cư_dân địa_phương có_thể tiếp_cận được với <UNK>\n",
      "Reference: we have local numbers accessible to three quarters of\n",
      "Model: <SOS> we have the the that can that that that\n",
      "Attention Weights: tensor([[9.9158e-01, 8.4044e-03, 1.6199e-05, 4.4682e-07, 3.2656e-08, 4.7649e-09,\n",
      "         4.1918e-10, 7.6390e-11, 1.8135e-11, 7.9058e-12],\n",
      "        [2.3578e-02, 9.4912e-01, 2.6463e-02, 7.9520e-04, 3.8687e-05, 1.3905e-06,\n",
      "         4.9957e-08, 3.0943e-09, 5.6559e-10, 1.3978e-10],\n",
      "        [1.2712e-02, 1.2066e-01, 6.1786e-01, 1.8979e-01, 5.3015e-02, 5.7126e-03,\n",
      "         2.4157e-04, 6.2966e-06, 3.5468e-07, 2.6815e-08],\n",
      "        [3.5518e-04, 3.1390e-03, 1.3654e-01, 4.3302e-01, 3.5144e-01, 6.2182e-02,\n",
      "         1.2063e-02, 1.1745e-03, 8.4753e-05, 4.6215e-06],\n",
      "        [8.7279e-04, 2.5179e-03, 7.5233e-03, 1.0067e-01, 5.3568e-01, 2.7077e-01,\n",
      "         6.2863e-02, 1.7450e-02, 1.5896e-03, 5.9282e-05],\n",
      "        [7.2211e-05, 2.1840e-04, 6.4910e-04, 8.9467e-03, 1.9531e-01, 5.9504e-01,\n",
      "         1.2207e-01, 6.7040e-02, 1.0065e-02, 5.8418e-04],\n",
      "        [3.3944e-06, 8.0899e-05, 7.3793e-04, 7.2221e-03, 1.1493e-01, 5.0113e-01,\n",
      "         1.8955e-01, 1.4807e-01, 3.4155e-02, 4.1203e-03],\n",
      "        [6.9905e-06, 3.6542e-05, 2.6108e-04, 3.1575e-03, 6.0423e-02, 2.4570e-01,\n",
      "         1.5481e-01, 2.9305e-01, 1.9748e-01, 4.5073e-02],\n",
      "        [1.3180e-05, 4.9410e-05, 2.4498e-04, 2.5355e-03, 3.5207e-02, 1.5755e-01,\n",
      "         1.2751e-01, 3.0768e-01, 2.8340e-01, 8.5800e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.96, Train Loss: 0.00, Val Loss: 4.56, Train BLEU: 0.00, Val BLEU: 10.42, Minutes Elapsed: 153.81\n",
      "Sampling from val predictions...\n",
      "Source: tôi chuyển đến thành_phố new_york khi làm công_việc đầu_tiên viết\n",
      "Reference: i had moved to new york city for my\n",
      "Model: <SOS> i i to to the the to i i\n",
      "Attention Weights: tensor([[9.8473e-01, 1.5139e-02, 1.2723e-04, 6.1881e-06, 3.7420e-07, 1.0742e-07,\n",
      "         3.2376e-08, 1.1183e-08, 5.4349e-09, 2.8595e-09],\n",
      "        [6.8433e-02, 9.1289e-01, 1.7824e-02, 8.4400e-04, 1.2310e-05, 4.7742e-07,\n",
      "         5.7707e-08, 1.6463e-08, 4.5163e-09, 1.3955e-09],\n",
      "        [2.4473e-02, 3.5913e-01, 5.0982e-01, 9.9965e-02, 5.5363e-03, 9.7127e-04,\n",
      "         9.0171e-05, 8.0712e-06, 1.3638e-06, 1.8696e-07],\n",
      "        [5.3514e-04, 2.2164e-02, 2.1583e-01, 6.4654e-01, 1.0529e-01, 8.9763e-03,\n",
      "         5.4464e-04, 1.0007e-04, 2.0416e-05, 1.8720e-06],\n",
      "        [2.6182e-04, 2.4244e-03, 7.2046e-02, 5.0574e-01, 2.7724e-01, 1.2523e-01,\n",
      "         1.4011e-02, 2.2560e-03, 6.9246e-04, 1.0346e-04],\n",
      "        [4.2092e-05, 1.7433e-04, 8.2767e-03, 1.3321e-01, 2.2974e-01, 4.8490e-01,\n",
      "         1.1756e-01, 1.7189e-02, 7.0486e-03, 1.8687e-03],\n",
      "        [1.3920e-05, 5.5294e-05, 2.7055e-03, 4.5637e-02, 1.1487e-01, 5.6985e-01,\n",
      "         2.2436e-01, 2.6652e-02, 1.1782e-02, 4.0731e-03],\n",
      "        [5.3409e-06, 1.2493e-05, 3.7368e-04, 4.4708e-03, 2.0538e-02, 4.3556e-01,\n",
      "         4.4619e-01, 5.0085e-02, 2.5602e-02, 1.7168e-02],\n",
      "        [2.3135e-07, 2.1585e-06, 5.1534e-05, 6.3653e-04, 3.0382e-03, 1.2574e-01,\n",
      "         5.3045e-01, 1.3477e-01, 8.9540e-02, 1.1577e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.00, Train Loss: 0.00, Val Loss: 4.60, Train BLEU: 0.00, Val BLEU: 10.42, Minutes Elapsed: 155.87\n",
      "Sampling from val predictions...\n",
      "Source: nhờ phát_minh này , tôi đã may_mắn nhận được học_bổng\n",
      "Reference: because of this invention , i was lucky to\n",
      "Model: <SOS> now this this , , i &apos;ve been to\n",
      "Attention Weights: tensor([[9.9144e-01, 8.4841e-03, 7.4864e-05, 2.2458e-06, 3.9751e-07, 3.1512e-08,\n",
      "         1.3791e-08, 4.3257e-09, 2.2129e-09, 1.0199e-09],\n",
      "        [3.8256e-01, 5.7824e-01, 3.6745e-02, 1.9182e-03, 4.7612e-04, 4.6858e-05,\n",
      "         6.5567e-06, 9.8826e-07, 1.2244e-07, 3.0979e-08],\n",
      "        [1.6030e-01, 3.5288e-01, 2.8954e-01, 1.0927e-01, 5.6461e-02, 2.6530e-02,\n",
      "         4.5029e-03, 4.8833e-04, 3.0472e-05, 2.9472e-06],\n",
      "        [6.0782e-02, 1.4751e-01, 2.0855e-01, 1.8846e-01, 2.4019e-01, 1.3207e-01,\n",
      "         1.9072e-02, 3.1417e-03, 2.0901e-04, 1.8989e-05],\n",
      "        [2.5127e-02, 2.7770e-02, 1.3362e-01, 2.9023e-01, 4.8581e-01, 3.4574e-02,\n",
      "         2.2910e-03, 5.4115e-04, 3.8176e-05, 4.4492e-06],\n",
      "        [8.9715e-03, 2.7909e-02, 5.5939e-02, 1.3926e-01, 6.5393e-01, 1.0465e-01,\n",
      "         7.3130e-03, 1.9135e-03, 1.0915e-04, 1.1778e-05],\n",
      "        [2.3134e-04, 3.0107e-03, 4.6888e-03, 8.3517e-03, 3.0585e-01, 5.8470e-01,\n",
      "         6.8410e-02, 2.3583e-02, 1.0855e-03, 8.7568e-05],\n",
      "        [2.3528e-04, 2.0708e-03, 3.1960e-03, 2.9779e-03, 2.6285e-02, 3.6331e-01,\n",
      "         2.9141e-01, 2.2275e-01, 7.2209e-02, 1.5569e-02],\n",
      "        [2.4133e-06, 3.3916e-05, 2.3607e-04, 2.4666e-03, 1.8782e-02, 5.1729e-02,\n",
      "         1.9281e-01, 4.3546e-01, 1.8145e-01, 1.1704e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.05, Train Loss: 0.00, Val Loss: 4.56, Train BLEU: 0.00, Val BLEU: 11.13, Minutes Elapsed: 158.39\n",
      "Sampling from val predictions...\n",
      "Source: thuật_ngữ ngăn_cản chúng_tôi hiểu được ý_tưởng của anh . <EOS>\n",
      "Reference: jargon is a barrier to our understanding of your\n",
      "Model: <SOS> the the we to that the the . <EOS>\n",
      "Attention Weights: tensor([[9.5056e-01, 4.9049e-02, 3.8578e-04, 3.3159e-06, 2.5169e-07, 4.4292e-08,\n",
      "         1.0504e-08, 5.8288e-09, 1.3588e-09, 2.9924e-10],\n",
      "        [2.0889e-01, 7.3174e-01, 5.6780e-02, 2.3888e-03, 1.7202e-04, 2.0142e-05,\n",
      "         2.6321e-06, 5.1442e-07, 3.9049e-08, 5.9917e-09],\n",
      "        [1.6025e-02, 1.7976e-01, 5.7983e-01, 1.7896e-01, 3.9940e-02, 4.9049e-03,\n",
      "         5.4057e-04, 4.3039e-05, 9.2335e-07, 1.5650e-08],\n",
      "        [5.9984e-05, 3.9034e-03, 7.6381e-02, 5.4486e-01, 3.1156e-01, 5.4652e-02,\n",
      "         7.8050e-03, 7.7115e-04, 1.1646e-05, 9.7936e-08],\n",
      "        [4.7113e-05, 3.3660e-03, 9.7629e-02, 2.2606e-01, 4.1253e-01, 1.6777e-01,\n",
      "         7.8586e-02, 1.3398e-02, 6.0222e-04, 5.9687e-06],\n",
      "        [1.6798e-04, 4.7787e-03, 1.0714e-01, 1.8551e-01, 3.7690e-01, 1.6874e-01,\n",
      "         1.1660e-01, 3.7926e-02, 2.2055e-03, 2.8889e-05],\n",
      "        [1.2405e-05, 4.2212e-04, 1.7344e-02, 6.9199e-02, 2.7932e-01, 2.7140e-01,\n",
      "         2.2122e-01, 1.2532e-01, 1.5326e-02, 4.4066e-04],\n",
      "        [2.4856e-05, 4.2293e-04, 7.1717e-03, 2.1183e-02, 1.0463e-01, 1.7597e-01,\n",
      "         3.0233e-01, 3.0438e-01, 7.7662e-02, 6.2164e-03],\n",
      "        [1.7487e-04, 1.3011e-03, 1.4681e-02, 2.2413e-02, 6.5371e-02, 9.2173e-02,\n",
      "         1.9616e-01, 4.1841e-01, 1.6302e-01, 2.6291e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.10, Train Loss: 0.00, Val Loss: 4.54, Train BLEU: 0.00, Val BLEU: 11.12, Minutes Elapsed: 160.92\n",
      "Sampling from val predictions...\n",
      "Source: cùng nhau , chúng_ta đã chỉ ra những không_gian công_cộng\n",
      "Reference: together , we &apos;ve shown how powerful our public\n",
      "Model: <SOS> in , , we going the the the .\n",
      "Attention Weights: tensor([[9.1138e-01, 8.8306e-02, 2.9193e-04, 1.9218e-05, 5.8623e-07, 7.7431e-08,\n",
      "         1.2803e-08, 4.0224e-09, 1.9117e-09, 7.4133e-10],\n",
      "        [9.3747e-02, 6.7459e-01, 2.1788e-01, 1.1635e-02, 1.8694e-03, 2.5083e-04,\n",
      "         2.3908e-05, 3.5833e-06, 7.0524e-07, 1.4107e-07],\n",
      "        [1.8461e-02, 1.3971e-01, 2.0702e-01, 4.1492e-01, 1.6339e-01, 4.9776e-02,\n",
      "         5.9308e-03, 7.2202e-04, 6.1240e-05, 4.0414e-06],\n",
      "        [9.9880e-05, 3.2174e-03, 8.3785e-03, 3.3960e-01, 4.2720e-01, 1.8773e-01,\n",
      "         2.7539e-02, 5.6238e-03, 5.7806e-04, 2.8073e-05],\n",
      "        [2.4941e-05, 4.0464e-04, 1.1390e-03, 5.3874e-03, 2.2125e-01, 5.5461e-01,\n",
      "         1.7019e-01, 4.1991e-02, 4.7182e-03, 2.8098e-04],\n",
      "        [2.1639e-06, 1.5136e-05, 5.6371e-05, 2.1420e-04, 5.8057e-04, 1.4552e-02,\n",
      "         1.3071e-01, 4.2615e-01, 3.4768e-01, 8.0043e-02],\n",
      "        [4.6834e-05, 1.3549e-04, 3.8005e-04, 1.1152e-03, 1.6851e-03, 1.9508e-02,\n",
      "         1.3305e-01, 3.7922e-01, 3.7463e-01, 9.0228e-02],\n",
      "        [2.7084e-05, 5.4585e-05, 2.0241e-04, 5.5579e-03, 2.1718e-03, 7.6755e-03,\n",
      "         3.3229e-02, 1.7426e-01, 4.3266e-01, 3.4417e-01],\n",
      "        [2.3664e-06, 8.1472e-06, 1.7282e-05, 5.5105e-04, 1.8062e-03, 9.8484e-03,\n",
      "         2.8572e-02, 1.2832e-01, 3.8060e-01, 4.5027e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.14, Train Loss: 0.00, Val Loss: 4.55, Train BLEU: 0.00, Val BLEU: 11.22, Minutes Elapsed: 163.42\n",
      "Sampling from val predictions...\n",
      "Source: và chúng_tôi tiếp_tục đi cho tới biên_giới lào , nhung\n",
      "Reference: we made it all the way to the border\n",
      "Model: <SOS> and we to to to the , the ,\n",
      "Attention Weights: tensor([[1.6326e-02, 9.8318e-01, 4.9380e-04, 8.7716e-07, 1.4111e-08, 9.2145e-10,\n",
      "         2.7674e-10, 1.0402e-10, 8.8216e-11, 6.9092e-11],\n",
      "        [4.5696e-03, 9.1209e-01, 8.3135e-02, 1.9596e-04, 3.9079e-06, 2.0402e-07,\n",
      "         4.7637e-08, 9.6399e-09, 3.0321e-09, 7.1113e-10],\n",
      "        [1.4775e-05, 6.8794e-03, 9.8988e-01, 3.1966e-03, 2.5832e-05, 5.6949e-07,\n",
      "         5.3907e-08, 4.2613e-09, 5.3926e-10, 1.2072e-10],\n",
      "        [1.2674e-05, 2.4559e-04, 1.2315e-02, 6.1369e-01, 3.2969e-01, 3.1210e-02,\n",
      "         1.1305e-02, 1.4290e-03, 9.4821e-05, 2.9701e-06],\n",
      "        [1.5012e-04, 4.5979e-04, 2.3033e-03, 6.3786e-02, 3.4576e-01, 2.6901e-01,\n",
      "         2.0957e-01, 8.8712e-02, 1.8204e-02, 2.0511e-03],\n",
      "        [1.3590e-06, 7.9138e-06, 9.3953e-05, 3.0237e-03, 3.4332e-02, 1.3027e-01,\n",
      "         3.5963e-01, 3.2140e-01, 1.3682e-01, 1.4429e-02],\n",
      "        [5.7910e-07, 3.4033e-06, 2.6001e-05, 4.5235e-04, 4.2263e-03, 2.7630e-02,\n",
      "         1.4668e-01, 3.1778e-01, 3.6197e-01, 1.4123e-01],\n",
      "        [5.9675e-05, 1.8486e-04, 8.6882e-05, 8.4331e-04, 3.5428e-03, 9.2379e-03,\n",
      "         3.2997e-02, 1.0902e-01, 3.1156e-01, 5.3247e-01],\n",
      "        [1.0592e-06, 5.6668e-06, 2.8904e-05, 3.7227e-04, 4.4424e-03, 2.8523e-02,\n",
      "         1.1147e-01, 1.9754e-01, 2.0728e-01, 4.5033e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.19, Train Loss: 0.00, Val Loss: 4.54, Train BLEU: 0.00, Val BLEU: 11.41, Minutes Elapsed: 165.95\n",
      "Sampling from val predictions...\n",
      "Source: tất_nhiên những người dân bản_địa <UNK> hứng_thú làm công_việc này\n",
      "Reference: and of course the local people had absolutely no\n",
      "Model: <SOS> of many people of people of that &apos;t to\n",
      "Attention Weights: tensor([[9.9737e-01, 2.5759e-03, 5.2270e-05, 2.7708e-06, 2.2983e-07, 1.3159e-08,\n",
      "         1.5308e-08, 5.7492e-09, 1.5541e-09, 9.7116e-10],\n",
      "        [7.9062e-01, 1.7006e-01, 3.3808e-02, 4.9936e-03, 4.9782e-04, 8.9846e-06,\n",
      "         4.8144e-06, 4.4467e-07, 8.5891e-08, 2.4456e-08],\n",
      "        [2.8174e-01, 3.4229e-01, 2.3673e-01, 1.0434e-01, 3.3414e-02, 1.0587e-03,\n",
      "         4.0180e-04, 1.7426e-05, 4.6679e-07, 4.7755e-08],\n",
      "        [7.4508e-02, 1.2668e-01, 2.4407e-01, 2.6331e-01, 2.5949e-01, 2.0808e-02,\n",
      "         1.0640e-02, 4.8230e-04, 1.2745e-05, 8.5187e-07],\n",
      "        [2.6800e-02, 5.2480e-02, 1.8427e-01, 2.5992e-01, 3.8992e-01, 3.8306e-02,\n",
      "         4.6407e-02, 1.8497e-03, 3.8446e-05, 3.1896e-06],\n",
      "        [3.0820e-03, 1.2537e-02, 6.0880e-02, 1.4040e-01, 3.9846e-01, 1.1294e-01,\n",
      "         2.5025e-01, 2.0465e-02, 9.3625e-04, 5.3521e-05],\n",
      "        [2.0959e-03, 6.3261e-03, 3.8398e-02, 8.0737e-02, 2.6570e-01, 7.5929e-02,\n",
      "         4.5780e-01, 6.9889e-02, 2.7616e-03, 3.5599e-04],\n",
      "        [3.2038e-04, 1.6384e-03, 1.3334e-02, 3.5205e-02, 1.4761e-01, 5.4437e-02,\n",
      "         5.2808e-01, 2.0094e-01, 1.5513e-02, 2.9247e-03],\n",
      "        [5.5950e-06, 6.7575e-05, 1.6825e-03, 5.7525e-03, 5.6745e-02, 4.1548e-02,\n",
      "         4.8918e-01, 2.7549e-01, 8.6890e-02, 4.2645e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.24, Train Loss: 0.00, Val Loss: 4.53, Train BLEU: 0.00, Val BLEU: 11.33, Minutes Elapsed: 168.47\n",
      "Sampling from val predictions...\n",
      "Source: \" trước_khi tôi chết , tôi muốn hoàn_toàn là chính\n",
      "Reference: &quot; before i die , i want to be\n",
      "Model: <SOS> &quot; i i i , i i to do\n",
      "Attention Weights: tensor([[9.7628e-01, 2.3669e-02, 5.0264e-05, 1.1509e-06, 1.8944e-07, 1.1456e-07,\n",
      "         5.6437e-08, 1.5350e-08, 5.2392e-09, 2.8104e-09],\n",
      "        [8.3770e-03, 9.5946e-01, 3.1512e-02, 6.3595e-04, 6.6347e-06, 3.8679e-06,\n",
      "         9.6280e-07, 1.5444e-07, 3.1655e-08, 1.2030e-08],\n",
      "        [1.6402e-03, 2.8465e-01, 5.9691e-01, 1.1574e-01, 8.1016e-04, 2.0736e-04,\n",
      "         2.9092e-05, 7.0102e-07, 4.6227e-08, 9.0387e-09],\n",
      "        [3.7222e-04, 5.3995e-02, 3.0600e-01, 6.1359e-01, 1.5966e-02, 5.0285e-03,\n",
      "         4.8684e-03, 1.7579e-04, 5.2996e-06, 4.4808e-07],\n",
      "        [6.0449e-04, 3.0990e-02, 1.9455e-01, 6.7499e-01, 5.2990e-02, 2.3923e-02,\n",
      "         2.0898e-02, 1.0002e-03, 5.7133e-05, 5.3007e-06],\n",
      "        [1.6306e-05, 2.8735e-03, 4.2517e-02, 3.9427e-01, 1.1332e-01, 3.6288e-01,\n",
      "         7.6887e-02, 6.5483e-03, 5.5134e-04, 1.3420e-04],\n",
      "        [2.2449e-07, 4.7657e-05, 1.3815e-03, 6.5386e-03, 3.1422e-03, 7.2610e-02,\n",
      "         8.1432e-01, 9.4709e-02, 5.3841e-03, 1.8667e-03],\n",
      "        [4.4130e-07, 2.4881e-05, 6.0488e-04, 4.4992e-03, 2.0794e-03, 7.3942e-03,\n",
      "         4.9944e-01, 4.4107e-01, 3.6070e-02, 8.8164e-03],\n",
      "        [5.1496e-07, 1.5007e-05, 1.6162e-04, 2.7304e-03, 1.2903e-02, 4.1432e-02,\n",
      "         1.3684e-01, 3.2338e-01, 2.1072e-01, 2.7182e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.29, Train Loss: 0.00, Val Loss: 4.54, Train BLEU: 0.00, Val BLEU: 11.00, Minutes Elapsed: 170.99\n",
      "Sampling from val predictions...\n",
      "Source: ông làm thế_nào - - ? \" và tôi đáp\n",
      "Reference: how can you do — ? &quot; and i\n",
      "Model: <SOS> he do do -- ? ? &quot; and i\n",
      "Attention Weights: tensor([[9.8944e-01, 1.0490e-02, 6.0344e-05, 7.9326e-06, 7.2038e-07, 7.8079e-08,\n",
      "         3.5476e-08, 4.2190e-09, 3.9162e-09, 2.8961e-09],\n",
      "        [3.4438e-01, 5.9372e-01, 5.3949e-02, 7.7061e-03, 2.3359e-04, 1.5713e-05,\n",
      "         1.9858e-06, 5.3958e-08, 1.2620e-08, 1.0851e-08],\n",
      "        [1.5633e-01, 1.9742e-01, 2.9526e-01, 3.1056e-01, 3.6123e-02, 3.8218e-03,\n",
      "         4.7341e-04, 1.5636e-05, 1.7681e-06, 5.5211e-07],\n",
      "        [1.3763e-02, 8.7010e-02, 2.5491e-01, 5.3475e-01, 9.6233e-02, 1.0333e-02,\n",
      "         2.8519e-03, 1.3458e-04, 1.9223e-05, 3.6459e-06],\n",
      "        [4.0824e-05, 5.5888e-04, 1.2590e-02, 3.1811e-01, 5.0344e-01, 1.1943e-01,\n",
      "         4.2752e-02, 2.9254e-03, 1.3417e-04, 1.5746e-05],\n",
      "        [6.4010e-05, 3.2672e-04, 2.5230e-03, 5.3470e-02, 3.5058e-01, 2.9045e-01,\n",
      "         2.5128e-01, 4.7827e-02, 3.2337e-03, 2.5240e-04],\n",
      "        [4.7098e-05, 2.2438e-04, 8.7716e-04, 1.2942e-02, 1.1401e-01, 1.8874e-01,\n",
      "         4.0104e-01, 2.5008e-01, 3.0025e-02, 2.0105e-03],\n",
      "        [1.4891e-05, 9.8034e-05, 2.4234e-04, 1.7463e-03, 1.3764e-02, 4.6718e-02,\n",
      "         2.1291e-01, 5.0401e-01, 1.9736e-01, 2.3132e-02],\n",
      "        [1.4301e-06, 2.0895e-05, 1.0443e-04, 8.2612e-04, 4.0843e-03, 1.9425e-02,\n",
      "         7.2987e-02, 1.8094e-01, 5.5080e-01, 1.7080e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.34, Train Loss: 0.00, Val Loss: 4.52, Train BLEU: 0.00, Val BLEU: 10.73, Minutes Elapsed: 173.51\n",
      "Sampling from val predictions...\n",
      "Source: thực ra , hầu_hết chúng đều được chụp bởi những\n",
      "Reference: in fact , most of them were taken by\n",
      "Model: <SOS> in fact , the they are are are by\n",
      "Attention Weights: tensor([[9.5678e-01, 3.8812e-02, 3.7967e-03, 6.0010e-04, 7.3871e-06, 8.1656e-07,\n",
      "         3.6055e-07, 1.1097e-07, 5.5202e-08, 2.4728e-08],\n",
      "        [5.6001e-02, 2.0746e-01, 6.2270e-01, 1.0779e-01, 5.5654e-03, 3.8511e-04,\n",
      "         7.2140e-05, 1.3811e-05, 3.3229e-06, 1.0147e-06],\n",
      "        [2.5101e-03, 1.3133e-02, 6.3285e-02, 6.9668e-01, 1.9392e-01, 2.5167e-02,\n",
      "         4.6727e-03, 5.5285e-04, 7.0990e-05, 5.8391e-06],\n",
      "        [2.8201e-04, 1.7462e-03, 1.6588e-02, 8.8255e-01, 8.6846e-02, 8.2798e-03,\n",
      "         2.9460e-03, 5.9504e-04, 1.5517e-04, 1.6943e-05],\n",
      "        [2.2365e-05, 3.4744e-04, 5.2681e-03, 1.8657e-01, 6.4793e-01, 1.2305e-01,\n",
      "         3.2241e-02, 4.1072e-03, 4.3320e-04, 2.5272e-05],\n",
      "        [1.8177e-05, 2.1815e-04, 3.2645e-03, 9.5563e-03, 9.9207e-02, 2.8596e-01,\n",
      "         4.2200e-01, 1.4340e-01, 3.3375e-02, 3.0048e-03],\n",
      "        [2.1388e-06, 2.2769e-05, 6.3121e-04, 4.8406e-03, 6.5625e-02, 1.4583e-01,\n",
      "         3.2754e-01, 2.5915e-01, 1.6265e-01, 3.3704e-02],\n",
      "        [3.8360e-05, 1.6380e-04, 3.3220e-03, 2.0294e-02, 1.0334e-01, 1.1163e-01,\n",
      "         2.6945e-01, 2.2987e-01, 2.0756e-01, 5.4327e-02],\n",
      "        [3.0414e-07, 1.3669e-06, 2.4857e-05, 5.7375e-04, 6.9617e-03, 2.9555e-02,\n",
      "         1.2192e-01, 2.2745e-01, 4.3572e-01, 1.7779e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.38, Train Loss: 0.00, Val Loss: 4.52, Train BLEU: 0.00, Val BLEU: 11.70, Minutes Elapsed: 176.01\n",
      "Sampling from val predictions...\n",
      "Source: bảo_hộ , là tôi đối_xử với bất_cứ người nào đến\n",
      "Reference: <UNK> , i treat anybody from a different culture\n",
      "Model: <SOS> so , i &apos;m to lot some of of\n",
      "Attention Weights: tensor([[9.7373e-01, 2.5134e-02, 1.1259e-03, 1.4308e-05, 6.1769e-07, 8.4839e-08,\n",
      "         2.5247e-08, 1.2786e-08, 6.5012e-09, 3.7595e-09],\n",
      "        [1.7804e-01, 2.2277e-01, 5.4269e-01, 5.4652e-02, 1.7234e-03, 1.0961e-04,\n",
      "         1.2029e-05, 2.9053e-06, 6.8866e-07, 2.1985e-07],\n",
      "        [4.6420e-03, 1.4970e-02, 4.6447e-01, 4.5063e-01, 6.0352e-02, 4.6444e-03,\n",
      "         2.6941e-04, 2.0248e-05, 1.8996e-06, 2.5830e-07],\n",
      "        [1.5011e-05, 7.3504e-05, 8.7216e-03, 3.0920e-01, 4.7647e-01, 1.7675e-01,\n",
      "         2.6417e-02, 2.2278e-03, 1.1371e-04, 7.7841e-06],\n",
      "        [7.3093e-05, 7.3180e-04, 4.0006e-03, 5.2120e-02, 3.3406e-01, 4.5874e-01,\n",
      "         1.3686e-01, 1.1577e-02, 1.5685e-03, 2.6807e-04],\n",
      "        [1.7511e-05, 2.6411e-05, 2.3834e-04, 2.1031e-03, 1.8235e-02, 2.9259e-01,\n",
      "         5.3233e-01, 1.2580e-01, 2.3734e-02, 4.9294e-03],\n",
      "        [3.5003e-06, 5.8845e-06, 7.7523e-05, 5.4872e-04, 1.7331e-03, 3.0583e-02,\n",
      "         2.5926e-01, 3.0493e-01, 2.5506e-01, 1.4780e-01],\n",
      "        [8.2331e-07, 1.1591e-06, 1.0801e-05, 1.2862e-04, 7.0050e-04, 1.2274e-02,\n",
      "         1.3405e-01, 2.4673e-01, 3.1437e-01, 2.9173e-01],\n",
      "        [5.0746e-06, 9.7766e-06, 1.6678e-04, 5.5220e-04, 5.9494e-04, 3.9679e-03,\n",
      "         3.9358e-02, 1.4862e-01, 2.9804e-01, 5.0868e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.43, Train Loss: 0.00, Val Loss: 4.48, Train BLEU: 0.00, Val BLEU: 11.65, Minutes Elapsed: 178.52\n",
      "Sampling from val predictions...\n",
      "Source: và làm thế_nào chúng_ta chia_sẻ nhiều hơn những hy_vọng của\n",
      "Reference: and how can we share more of our hopes\n",
      "Model: <SOS> and how we we take to to of of\n",
      "Attention Weights: tensor([[2.4548e-03, 9.9710e-01, 4.3710e-04, 4.4645e-06, 3.5071e-08, 4.8981e-09,\n",
      "         1.4689e-09, 5.0133e-10, 3.5316e-10, 2.3301e-10],\n",
      "        [1.6388e-03, 9.9198e-01, 6.0227e-03, 3.5124e-04, 3.7225e-06, 2.9367e-07,\n",
      "         3.0749e-08, 5.3743e-09, 1.6973e-09, 3.9392e-10],\n",
      "        [3.8881e-04, 2.6756e-01, 3.7170e-01, 3.5191e-01, 7.8251e-03, 5.8734e-04,\n",
      "         2.1195e-05, 2.0991e-06, 3.0951e-07, 3.5797e-08],\n",
      "        [1.2307e-04, 1.7027e-02, 7.3640e-02, 5.2795e-01, 2.2723e-01, 1.4620e-01,\n",
      "         7.4310e-03, 3.6553e-04, 3.2777e-05, 2.0890e-06],\n",
      "        [8.1729e-07, 2.7721e-05, 5.1500e-04, 1.8125e-02, 1.8388e-01, 6.8985e-01,\n",
      "         9.2460e-02, 1.2835e-02, 2.1468e-03, 1.5127e-04],\n",
      "        [6.1360e-06, 1.0010e-05, 7.4247e-05, 1.0294e-03, 1.0994e-02, 4.5787e-01,\n",
      "         3.4450e-01, 1.1827e-01, 5.8510e-02, 8.7365e-03],\n",
      "        [1.3824e-05, 1.3740e-05, 9.1366e-05, 5.2957e-04, 4.4387e-03, 1.8079e-01,\n",
      "         3.7085e-01, 2.1843e-01, 1.7710e-01, 4.7755e-02],\n",
      "        [1.8176e-05, 1.6934e-05, 4.2719e-05, 1.7918e-04, 3.4118e-04, 1.5458e-02,\n",
      "         1.1461e-01, 1.7169e-01, 3.9452e-01, 3.0312e-01],\n",
      "        [1.2648e-05, 2.1698e-05, 2.5376e-05, 1.8125e-04, 1.2984e-04, 4.4726e-03,\n",
      "         5.9510e-02, 1.0680e-01, 3.3521e-01, 4.9364e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.48, Train Loss: 0.00, Val Loss: 4.48, Train BLEU: 0.00, Val BLEU: 11.95, Minutes Elapsed: 181.01\n",
      "Sampling from val predictions...\n",
      "Source: với_lại tôi cũng cảm_thấy mệt_mỏi với việc phải lái_xe 45\n",
      "Reference: plus i got tired of driving 45 minutes round\n",
      "Model: <SOS> so i i also with with to to to\n",
      "Attention Weights: tensor([[9.6228e-01, 3.6958e-02, 7.3957e-04, 2.1424e-05, 2.2604e-06, 3.8445e-07,\n",
      "         1.2242e-07, 1.0246e-07, 2.4890e-08, 2.1009e-08],\n",
      "        [1.2203e-01, 7.1402e-01, 1.6000e-01, 3.7221e-03, 2.1231e-04, 1.2817e-05,\n",
      "         1.5545e-06, 7.0336e-07, 1.4877e-07, 6.8846e-08],\n",
      "        [1.7511e-03, 1.0668e-02, 8.6978e-01, 1.0852e-01, 9.0910e-03, 1.8237e-04,\n",
      "         6.3478e-06, 1.2806e-06, 5.8977e-08, 7.7704e-09],\n",
      "        [2.8842e-04, 1.5560e-02, 4.2481e-01, 3.4370e-01, 1.8952e-01, 2.4303e-02,\n",
      "         1.3112e-03, 4.9861e-04, 9.7657e-06, 1.0459e-06],\n",
      "        [1.8021e-05, 7.6331e-04, 1.4305e-02, 2.3759e-01, 5.5447e-01, 1.6767e-01,\n",
      "         1.8818e-02, 6.0862e-03, 2.3044e-04, 4.9836e-05],\n",
      "        [1.6352e-05, 1.8811e-04, 1.8024e-03, 3.8489e-02, 2.6851e-01, 3.9145e-01,\n",
      "         1.7455e-01, 1.0900e-01, 1.0834e-02, 5.1665e-03],\n",
      "        [1.7356e-07, 2.8690e-06, 6.1753e-05, 1.4781e-03, 2.0132e-02, 8.1251e-02,\n",
      "         2.0173e-01, 4.3134e-01, 1.2072e-01, 1.4328e-01],\n",
      "        [2.7821e-06, 3.6576e-05, 7.3853e-05, 4.2511e-04, 3.4907e-03, 2.1899e-02,\n",
      "         1.2388e-01, 4.7592e-01, 8.7015e-02, 2.8725e-01],\n",
      "        [3.8943e-07, 9.7225e-06, 1.1385e-04, 8.4809e-04, 5.8455e-03, 2.5082e-02,\n",
      "         1.0009e-01, 4.3966e-01, 1.0491e-01, 3.2343e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.53, Train Loss: 0.00, Val Loss: 4.50, Train BLEU: 0.00, Val BLEU: 11.29, Minutes Elapsed: 183.61\n",
      "Sampling from val predictions...\n",
      "Source: suy_nghĩ về cái chết làm cuộc_sống của bạn rõ_ràng .\n",
      "Reference: thinking about death <UNK> your life . <EOS> <PAD>\n",
      "Model: <SOS> the &apos;s the of the <EOS> . <EOS> you\n",
      "Attention Weights: tensor([[9.5105e-01, 4.6801e-02, 1.9741e-03, 1.3405e-04, 2.8312e-05, 5.8945e-06,\n",
      "         2.0325e-06, 1.9847e-06, 9.0619e-07, 6.0409e-07],\n",
      "        [2.3738e-01, 6.5553e-01, 9.8655e-02, 7.7302e-03, 6.0612e-04, 9.5104e-05,\n",
      "         8.5928e-06, 2.5192e-06, 1.0717e-06, 1.7563e-07],\n",
      "        [2.1012e-02, 1.8603e-01, 3.0132e-01, 3.4154e-01, 1.0591e-01, 3.8182e-02,\n",
      "         4.9524e-03, 9.0846e-04, 1.3810e-04, 5.1069e-06],\n",
      "        [4.9393e-04, 1.5133e-02, 8.3033e-02, 3.1109e-01, 2.2166e-01, 2.9746e-01,\n",
      "         4.2683e-02, 2.0293e-02, 7.9495e-03, 2.1627e-04],\n",
      "        [3.5898e-05, 8.3786e-04, 1.1095e-02, 7.3999e-02, 1.6972e-01, 3.5237e-01,\n",
      "         1.6385e-01, 1.5375e-01, 6.9485e-02, 4.8509e-03],\n",
      "        [3.9263e-06, 6.4422e-05, 1.3769e-03, 1.3856e-02, 5.4582e-02, 2.3459e-01,\n",
      "         1.7146e-01, 2.6719e-01, 2.2820e-01, 2.8677e-02],\n",
      "        [1.7359e-05, 8.6438e-05, 1.0881e-03, 7.3300e-03, 2.4932e-02, 6.1923e-02,\n",
      "         1.0228e-01, 3.2180e-01, 3.5186e-01, 1.2868e-01],\n",
      "        [1.6372e-04, 4.5209e-04, 1.8041e-03, 5.7149e-03, 1.8330e-02, 3.9388e-02,\n",
      "         1.0688e-01, 3.0285e-01, 2.8986e-01, 2.3456e-01],\n",
      "        [2.0638e-05, 1.0409e-04, 1.2749e-03, 8.6362e-03, 2.3444e-02, 5.3569e-02,\n",
      "         9.6122e-02, 3.0541e-01, 3.6014e-01, 1.5128e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.58, Train Loss: 0.00, Val Loss: 4.46, Train BLEU: 0.00, Val BLEU: 11.36, Minutes Elapsed: 186.10\n",
      "Sampling from val predictions...\n",
      "Source: tại nhật , tới tháng_bảy , chúng_tôi đã mở_rộng ra\n",
      "Reference: within japan , by july , we &apos;d <UNK>\n",
      "Model: <SOS> in , , the , , we &apos;ve to\n",
      "Attention Weights: tensor([[9.5880e-01, 4.0591e-02, 5.9834e-04, 6.2801e-06, 1.0668e-07, 3.2619e-08,\n",
      "         2.4040e-08, 6.2299e-09, 2.0980e-09, 8.7038e-10],\n",
      "        [3.0167e-01, 4.3394e-01, 2.4606e-01, 1.6689e-02, 1.5380e-03, 4.5946e-05,\n",
      "         4.5626e-05, 5.7140e-06, 1.2778e-06, 2.9079e-07],\n",
      "        [4.8782e-02, 8.2060e-02, 8.2235e-02, 7.0840e-01, 7.2926e-02, 1.3214e-03,\n",
      "         3.8584e-03, 3.8776e-04, 3.0110e-05, 2.9959e-06],\n",
      "        [5.5998e-03, 1.1618e-02, 5.0758e-02, 8.4926e-01, 4.8359e-02, 4.8548e-03,\n",
      "         2.8529e-02, 9.5900e-04, 5.6038e-05, 7.8877e-06],\n",
      "        [1.7799e-05, 1.4022e-04, 1.8507e-03, 1.2022e-01, 6.3097e-01, 4.7077e-02,\n",
      "         1.7718e-01, 1.9715e-02, 2.5975e-03, 2.3735e-04],\n",
      "        [7.6007e-05, 3.2157e-04, 1.1891e-03, 2.5823e-02, 1.1440e-01, 3.5144e-02,\n",
      "         2.9149e-01, 4.0797e-01, 1.0820e-01, 1.5392e-02],\n",
      "        [4.1606e-06, 1.2856e-05, 1.8913e-04, 4.2875e-03, 9.2224e-03, 3.4081e-02,\n",
      "         4.9048e-01, 3.4072e-01, 9.3783e-02, 2.7218e-02],\n",
      "        [3.8171e-07, 2.3492e-06, 3.1831e-05, 1.0557e-03, 5.8076e-03, 6.6191e-03,\n",
      "         1.0346e-01, 5.1103e-01, 3.0452e-01, 6.7473e-02],\n",
      "        [8.2400e-06, 5.3445e-05, 5.6716e-04, 1.6127e-03, 1.7323e-02, 1.2002e-02,\n",
      "         2.4435e-02, 1.6156e-01, 6.3024e-01, 1.5219e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.62, Train Loss: 0.00, Val Loss: 4.45, Train BLEU: 0.00, Val BLEU: 12.08, Minutes Elapsed: 188.61\n",
      "Sampling from val predictions...\n",
      "Source: khi lượng đậu_xanh có_giá 1 đô_la sẽ cho bạn lượng\n",
      "Reference: when one dollar &apos;s worth of green beans will\n",
      "Model: <SOS> when the &apos;s to a a a to you\n",
      "Attention Weights: tensor([[9.9152e-01, 8.3438e-03, 1.2498e-04, 5.2453e-06, 1.9559e-06, 6.9753e-07,\n",
      "         2.0667e-06, 5.9901e-07, 2.8756e-07, 1.0779e-07],\n",
      "        [1.2249e-01, 7.8071e-01, 9.5149e-02, 1.5394e-03, 9.2858e-05, 6.4259e-06,\n",
      "         1.5587e-05, 2.4950e-06, 6.2406e-07, 1.6704e-07],\n",
      "        [1.0030e-01, 4.5453e-01, 4.0123e-01, 4.1124e-02, 2.4273e-03, 6.2058e-05,\n",
      "         2.9685e-04, 2.0089e-05, 2.6671e-06, 5.2291e-07],\n",
      "        [4.8869e-02, 2.3613e-01, 3.0925e-01, 3.4649e-01, 3.5809e-02, 1.4204e-03,\n",
      "         1.9585e-02, 2.0742e-03, 3.2392e-04, 5.8181e-05],\n",
      "        [8.1686e-03, 4.0997e-02, 1.8778e-01, 5.1889e-01, 1.5207e-01, 7.6449e-03,\n",
      "         7.2788e-02, 1.0290e-02, 1.1587e-03, 2.1694e-04],\n",
      "        [3.6296e-05, 8.4753e-04, 1.2337e-02, 1.0312e-01, 1.0529e-01, 4.9918e-02,\n",
      "         5.1695e-01, 1.7292e-01, 3.0205e-02, 8.3695e-03],\n",
      "        [4.9317e-05, 2.4245e-04, 1.9531e-03, 4.4805e-02, 7.7112e-02, 2.9587e-02,\n",
      "         5.3343e-01, 2.6296e-01, 4.0760e-02, 9.0951e-03],\n",
      "        [6.9804e-07, 1.2785e-05, 1.7306e-04, 3.5791e-03, 1.8663e-02, 2.8995e-02,\n",
      "         3.8261e-01, 3.5139e-01, 1.5212e-01, 6.2457e-02],\n",
      "        [4.7110e-06, 2.7446e-05, 8.4474e-05, 1.4129e-03, 1.0443e-02, 1.3966e-02,\n",
      "         4.4594e-01, 3.2188e-01, 1.6755e-01, 3.8698e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.67, Train Loss: 0.00, Val Loss: 4.46, Train BLEU: 0.00, Val BLEU: 11.71, Minutes Elapsed: 191.12\n",
      "Sampling from val predictions...\n",
      "Source: đó là lí_do tại_sao người da_trắng tại châu phi được\n",
      "Reference: that &apos;s why the white people in africa are\n",
      "Model: <SOS> that &apos;s why why people is are the ,\n",
      "Attention Weights: tensor([[9.5147e-01, 4.7656e-02, 8.5873e-04, 1.5045e-05, 2.0685e-07, 1.3155e-08,\n",
      "         3.8956e-09, 1.5890e-09, 9.4307e-10, 6.1860e-10],\n",
      "        [5.4132e-02, 8.8877e-01, 5.2408e-02, 4.5540e-03, 1.2604e-04, 5.5482e-06,\n",
      "         7.1835e-07, 9.7261e-08, 2.7423e-08, 6.2484e-09],\n",
      "        [3.2839e-02, 1.8989e-01, 5.2173e-01, 2.2110e-01, 3.1210e-02, 2.6379e-03,\n",
      "         5.3119e-04, 4.7743e-05, 6.9045e-06, 8.7662e-07],\n",
      "        [1.0193e-03, 7.7732e-03, 1.0710e-01, 3.5891e-01, 4.6637e-01, 4.5757e-02,\n",
      "         1.1972e-02, 9.7698e-04, 9.6589e-05, 1.4998e-05],\n",
      "        [4.8419e-05, 3.6542e-04, 3.6560e-03, 1.5283e-01, 6.6223e-01, 1.1861e-01,\n",
      "         5.6949e-02, 4.8442e-03, 3.9389e-04, 7.1678e-05],\n",
      "        [2.8082e-06, 6.5514e-05, 1.2712e-03, 4.0245e-02, 2.1390e-01, 2.2259e-01,\n",
      "         4.2824e-01, 7.5297e-02, 1.1922e-02, 6.4652e-03],\n",
      "        [8.8622e-05, 2.3804e-04, 4.0312e-04, 3.8372e-02, 1.4469e-01, 8.4275e-02,\n",
      "         4.5642e-01, 1.7885e-01, 4.6198e-02, 5.0466e-02],\n",
      "        [9.8237e-06, 6.7040e-05, 3.7331e-04, 1.6214e-02, 9.1762e-02, 1.2132e-01,\n",
      "         4.0510e-01, 2.1896e-01, 6.8371e-02, 7.7819e-02],\n",
      "        [4.1538e-07, 1.5856e-06, 4.8351e-05, 9.2586e-04, 1.7685e-02, 9.4490e-02,\n",
      "         3.8780e-01, 2.4046e-01, 1.1985e-01, 1.3874e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.72, Train Loss: 0.00, Val Loss: 4.44, Train BLEU: 0.00, Val BLEU: 11.69, Minutes Elapsed: 193.62\n",
      "Sampling from val predictions...\n",
      "Source: cảm_ứng từ có_thể tạo nhiệt , đặc_biệt thép lại dẫn_nhiệt\n",
      "Reference: induction can heat , especially steel ; it &apos;s\n",
      "Model: <SOS> but you can the , , , , &apos;s\n",
      "Attention Weights: tensor([[2.5280e-01, 7.4172e-01, 5.4561e-03, 2.5592e-05, 3.6791e-07, 1.1815e-08,\n",
      "         6.1857e-09, 9.2796e-10, 1.8683e-10, 1.4910e-10],\n",
      "        [8.4835e-02, 6.0284e-01, 3.0579e-01, 6.4300e-03, 1.0783e-04, 1.3717e-06,\n",
      "         4.8153e-07, 2.8584e-08, 2.6226e-09, 1.1598e-09],\n",
      "        [1.5280e-02, 1.2323e-01, 5.8054e-01, 2.5624e-01, 2.4133e-02, 5.2201e-04,\n",
      "         5.6403e-05, 3.1750e-06, 1.6711e-07, 3.8763e-08],\n",
      "        [3.0451e-05, 2.4511e-03, 4.9619e-02, 3.3302e-01, 4.9070e-01, 9.5553e-02,\n",
      "         2.5682e-02, 2.6143e-03, 2.3177e-04, 9.7455e-05],\n",
      "        [7.2576e-05, 1.0478e-03, 6.0738e-03, 9.8546e-02, 4.5733e-01, 2.7393e-01,\n",
      "         1.4202e-01, 1.3946e-02, 3.8665e-03, 3.1723e-03],\n",
      "        [7.6771e-06, 6.8060e-05, 4.3180e-04, 4.2076e-04, 1.9937e-03, 1.1466e-02,\n",
      "         6.1479e-01, 3.0954e-01, 3.7562e-02, 2.3723e-02],\n",
      "        [2.3852e-06, 2.7587e-05, 3.4090e-04, 3.0692e-04, 6.7504e-04, 2.4044e-03,\n",
      "         9.3327e-02, 3.3501e-01, 2.4558e-01, 3.2233e-01],\n",
      "        [5.4655e-06, 3.5466e-05, 1.2161e-04, 1.3268e-04, 5.2046e-04, 6.0801e-03,\n",
      "         2.1198e-01, 2.1013e-01, 1.4659e-01, 4.2440e-01],\n",
      "        [4.7391e-06, 2.9779e-05, 9.8044e-05, 9.0839e-05, 3.4236e-04, 3.2177e-03,\n",
      "         1.1806e-01, 1.9916e-01, 1.8022e-01, 4.9878e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.77, Train Loss: 0.00, Val Loss: 4.48, Train BLEU: 0.00, Val BLEU: 11.97, Minutes Elapsed: 196.19\n",
      "Sampling from val predictions...\n",
      "Source: khá là ấm_áp trong mùa_đông nhưng mùa_hè thì cực_kì nóng\n",
      "Reference: it was cozy in winter but extremely hot in\n",
      "Model: <SOS> it &apos;s the in the , but the the\n",
      "Attention Weights: tensor([[9.9450e-01, 4.9758e-03, 5.1616e-04, 4.4820e-06, 2.1109e-07, 1.6939e-08,\n",
      "         6.2696e-09, 3.6715e-09, 1.4626e-09, 1.0625e-09],\n",
      "        [4.5150e-01, 2.0068e-01, 3.3253e-01, 1.4451e-02, 7.9906e-04, 2.9890e-05,\n",
      "         6.3080e-06, 1.6376e-06, 4.4717e-07, 1.4519e-07],\n",
      "        [2.2578e-01, 1.2902e-01, 5.4771e-01, 6.9965e-02, 2.3624e-02, 2.6194e-03,\n",
      "         9.8650e-04, 2.1637e-04, 5.6683e-05, 1.7207e-05],\n",
      "        [5.1575e-03, 7.0388e-03, 6.3128e-01, 2.5862e-01, 8.9436e-02, 6.9642e-03,\n",
      "         1.2343e-03, 2.2843e-04, 3.1039e-05, 8.2901e-06],\n",
      "        [9.3183e-04, 1.1530e-03, 1.4389e-01, 3.7848e-01, 3.8778e-01, 7.1837e-02,\n",
      "         1.3418e-02, 2.1684e-03, 2.8079e-04, 6.2682e-05],\n",
      "        [2.4195e-05, 5.0650e-05, 4.1257e-03, 5.4208e-02, 4.8825e-01, 3.0058e-01,\n",
      "         1.0885e-01, 3.2498e-02, 8.7703e-03, 2.6485e-03],\n",
      "        [2.6277e-05, 3.5385e-05, 6.6968e-04, 5.3137e-03, 1.7284e-01, 2.7158e-01,\n",
      "         2.1750e-01, 1.6899e-01, 1.1035e-01, 5.2693e-02],\n",
      "        [3.7759e-06, 8.0973e-06, 2.9348e-04, 3.1417e-03, 4.9210e-02, 8.7657e-02,\n",
      "         2.0538e-01, 2.4515e-01, 2.3626e-01, 1.7290e-01],\n",
      "        [3.2609e-06, 1.3655e-05, 6.2549e-04, 5.5840e-03, 4.9974e-02, 5.7681e-02,\n",
      "         1.5199e-01, 2.8615e-01, 2.1808e-01, 2.2990e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.82, Train Loss: 0.00, Val Loss: 4.42, Train BLEU: 0.00, Val BLEU: 12.03, Minutes Elapsed: 198.68\n",
      "Sampling from val predictions...\n",
      "Source: thật ra , hy_vọng là nó không đến_nỗi kì_cục như\n",
      "Reference: well , hopefully not as awkward as that picture\n",
      "Model: <SOS> so , , &apos;s it it , , ,\n",
      "Attention Weights: tensor([[8.6329e-01, 1.2099e-01, 1.3857e-02, 1.8474e-03, 6.4633e-06, 7.1417e-07,\n",
      "         2.7652e-07, 2.5163e-08, 1.1430e-08, 7.9094e-09],\n",
      "        [9.7348e-03, 3.3617e-02, 4.6466e-01, 4.8972e-01, 2.0264e-03, 2.1071e-04,\n",
      "         3.1083e-05, 9.8227e-07, 1.9955e-07, 4.1241e-08],\n",
      "        [9.4577e-04, 2.9370e-03, 2.1208e-02, 8.8597e-01, 5.0220e-02, 3.0102e-02,\n",
      "         7.9235e-03, 5.9584e-04, 8.3439e-05, 1.2708e-05],\n",
      "        [2.0957e-04, 6.1148e-04, 8.1570e-03, 7.2646e-01, 1.2784e-01, 1.0451e-01,\n",
      "         3.0286e-02, 1.7203e-03, 1.7912e-04, 2.1484e-05],\n",
      "        [7.6500e-05, 1.8627e-04, 2.2446e-03, 7.0364e-02, 1.0719e-01, 4.6579e-01,\n",
      "         2.9744e-01, 4.7954e-02, 7.7360e-03, 1.0277e-03],\n",
      "        [3.6414e-06, 1.2145e-05, 1.8778e-04, 1.8532e-02, 3.1899e-02, 3.3773e-01,\n",
      "         2.9666e-01, 2.4079e-01, 6.3855e-02, 1.0327e-02],\n",
      "        [2.2404e-06, 5.5363e-06, 5.4806e-05, 2.9650e-03, 5.1824e-03, 8.9397e-02,\n",
      "         1.9564e-01, 4.0806e-01, 2.2758e-01, 7.1116e-02],\n",
      "        [4.8595e-06, 5.8260e-06, 5.8730e-05, 4.6099e-03, 1.1477e-02, 1.2054e-01,\n",
      "         2.7615e-01, 2.3806e-01, 2.0842e-01, 1.4067e-01],\n",
      "        [1.3004e-06, 1.6456e-06, 8.1807e-06, 7.8480e-04, 2.4507e-03, 2.8288e-02,\n",
      "         1.1349e-01, 3.0015e-01, 2.9368e-01, 2.6114e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.86, Train Loss: 0.00, Val Loss: 4.41, Train BLEU: 0.00, Val BLEU: 12.19, Minutes Elapsed: 201.27\n",
      "Sampling from val predictions...\n",
      "Source: vì quân_cảnh không giữ cho môi_trường trong_sạch . <EOS> <PAD>\n",
      "Reference: because mps do not keep the environment clean .\n",
      "Model: <SOS> because it &apos;s &apos;t just the . . <EOS>\n",
      "Attention Weights: tensor([[0.9127, 0.0810, 0.0063, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0155, 0.3470, 0.6297, 0.0074, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0174, 0.1827, 0.6153, 0.1425, 0.0353, 0.0064, 0.0005, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0133, 0.0580, 0.6447, 0.1118, 0.1145, 0.0435, 0.0124, 0.0017, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0001, 0.0014, 0.0118, 0.1556, 0.3856, 0.3324, 0.0998, 0.0131, 0.0002,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0001, 0.0017, 0.0170, 0.1106, 0.4150, 0.2714, 0.1799, 0.0042,\n",
      "         0.0000],\n",
      "        [0.0001, 0.0002, 0.0012, 0.0059, 0.0382, 0.3281, 0.3689, 0.2329, 0.0247,\n",
      "         0.0000],\n",
      "        [0.0002, 0.0003, 0.0014, 0.0042, 0.0286, 0.2200, 0.3691, 0.3158, 0.0604,\n",
      "         0.0000],\n",
      "        [0.0026, 0.0009, 0.0028, 0.0026, 0.0141, 0.0527, 0.1237, 0.6230, 0.1777,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.91, Train Loss: 0.00, Val Loss: 4.42, Train BLEU: 0.00, Val BLEU: 11.39, Minutes Elapsed: 203.79\n",
      "Sampling from val predictions...\n",
      "Source: người có ý_tưởng chưa chắc đã có sự hiểu_biết ,\n",
      "Reference: the person with the idea may not have the\n",
      "Model: <SOS> people are are &apos;t who who not had ,\n",
      "Attention Weights: tensor([[9.8783e-01, 1.2118e-02, 5.5632e-05, 6.2058e-07, 2.3958e-08, 1.3102e-09,\n",
      "         1.4068e-10, 5.8727e-11, 3.1719e-11, 2.3479e-11],\n",
      "        [1.5517e-01, 3.1474e-01, 4.7243e-01, 5.4569e-02, 3.0190e-03, 7.1909e-05,\n",
      "         3.6143e-06, 7.7113e-07, 1.1330e-07, 9.3136e-09],\n",
      "        [5.0296e-02, 9.2185e-02, 3.7805e-01, 3.8118e-01, 9.1052e-02, 6.1834e-03,\n",
      "         7.0494e-04, 3.0355e-04, 4.6771e-05, 1.2145e-06],\n",
      "        [8.8952e-02, 1.2250e-01, 1.8825e-01, 3.7273e-01, 2.0812e-01, 1.7871e-02,\n",
      "         1.1463e-03, 3.6826e-04, 5.2000e-05, 2.8297e-06],\n",
      "        [1.6973e-03, 4.2369e-03, 4.1606e-02, 2.5826e-01, 5.5864e-01, 1.0508e-01,\n",
      "         1.5062e-02, 1.2541e-02, 2.8032e-03, 7.1554e-05],\n",
      "        [1.5764e-03, 4.3264e-03, 1.5691e-02, 1.0660e-01, 4.0118e-01, 2.3473e-01,\n",
      "         7.8069e-02, 9.8713e-02, 5.2543e-02, 6.5707e-03],\n",
      "        [2.0252e-03, 3.8943e-03, 9.3506e-03, 7.7819e-02, 3.9774e-01, 3.0757e-01,\n",
      "         8.8110e-02, 7.0244e-02, 3.5923e-02, 7.3292e-03],\n",
      "        [4.6525e-05, 2.6934e-04, 4.7856e-03, 4.4937e-02, 2.2612e-01, 1.7189e-01,\n",
      "         1.3558e-01, 2.3796e-01, 1.5768e-01, 2.0734e-02],\n",
      "        [8.5049e-06, 4.7784e-05, 1.4138e-03, 1.6949e-02, 8.7934e-02, 9.7946e-02,\n",
      "         1.4564e-01, 3.0443e-01, 2.7547e-01, 7.0172e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.96, Train Loss: 0.00, Val Loss: 4.40, Train BLEU: 0.00, Val BLEU: 11.57, Minutes Elapsed: 206.31\n",
      "Sampling from val predictions...\n",
      "Source: điều đó không tồn_tại <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: it doesn &apos;t exist . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> that &apos;s not . . <EOS> <EOS> . <EOS>\n",
      "Attention Weights: tensor([[0.5130, 0.4130, 0.0740, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0499, 0.1414, 0.7881, 0.0206, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0406, 0.0966, 0.6625, 0.1925, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0072, 0.0107, 0.0725, 0.7530, 0.1566, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0035, 0.0045, 0.0366, 0.3750, 0.5805, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0113, 0.0119, 0.0476, 0.2474, 0.6818, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0191, 0.0206, 0.1191, 0.3417, 0.4994, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0061, 0.0143, 0.3111, 0.5026, 0.1658, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2369, 0.1544, 0.1827, 0.1534, 0.2727, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.00, Train Loss: 0.00, Val Loss: 4.45, Train BLEU: 0.00, Val BLEU: 11.49, Minutes Elapsed: 208.44\n",
      "Sampling from val predictions...\n",
      "Source: cảm_ơn các bạn . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank you . <EOS> you . <EOS> you <EOS>\n",
      "Attention Weights: tensor([[0.2028, 0.1538, 0.6361, 0.0072, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0772, 0.1368, 0.7056, 0.0798, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0283, 0.0290, 0.2362, 0.6356, 0.0710, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0150, 0.0161, 0.0742, 0.4438, 0.4508, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0065, 0.0070, 0.0267, 0.2742, 0.6856, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0144, 0.0150, 0.0698, 0.3194, 0.5814, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1107, 0.1572, 0.2247, 0.2302, 0.2772, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0171, 0.0293, 0.1240, 0.3226, 0.5070, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0076, 0.0136, 0.0696, 0.2834, 0.6257, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.05, Train Loss: 0.00, Val Loss: 4.41, Train BLEU: 0.00, Val BLEU: 12.18, Minutes Elapsed: 211.04\n",
      "Sampling from val predictions...\n",
      "Source: thay vào đó , slide ví_dụ này của <UNK> brown\n",
      "Reference: instead , this example slide by <UNK> brown is\n",
      "Model: <SOS> instead , the example of of the of of\n",
      "Attention Weights: tensor([[9.5286e-01, 3.9203e-02, 7.8304e-03, 1.0273e-04, 6.7340e-07, 3.8434e-07,\n",
      "         1.1890e-08, 1.7199e-09, 3.9079e-10, 4.6941e-10],\n",
      "        [1.0235e-02, 2.4966e-02, 2.8899e-01, 6.5511e-01, 7.0630e-03, 1.3471e-02,\n",
      "         1.6215e-04, 4.8174e-06, 1.2968e-07, 9.9153e-08],\n",
      "        [4.6379e-03, 1.0723e-02, 4.6903e-02, 2.3631e-01, 3.6870e-01, 3.1163e-01,\n",
      "         2.0069e-02, 1.0079e-03, 1.6095e-05, 4.4117e-06],\n",
      "        [1.4874e-04, 6.5039e-04, 3.9525e-03, 3.8751e-02, 5.3841e-02, 8.2888e-01,\n",
      "         6.8293e-02, 5.4107e-03, 5.8154e-05, 1.3027e-05],\n",
      "        [1.0414e-04, 2.8310e-04, 2.5974e-03, 3.2229e-02, 9.7653e-03, 7.2678e-01,\n",
      "         1.8147e-01, 4.5533e-02, 8.7237e-04, 3.6058e-04],\n",
      "        [1.2473e-05, 2.3868e-05, 5.1795e-05, 1.1345e-03, 5.1343e-03, 2.7290e-01,\n",
      "         1.2820e-01, 5.0000e-01, 5.1355e-02, 4.1185e-02],\n",
      "        [1.3640e-05, 1.4648e-05, 2.3952e-05, 2.5208e-04, 1.9961e-03, 9.3360e-02,\n",
      "         6.2856e-02, 3.3730e-01, 1.3901e-01, 3.6517e-01],\n",
      "        [2.9639e-06, 2.9460e-06, 5.9738e-06, 6.5783e-05, 6.3075e-04, 3.6190e-02,\n",
      "         2.1971e-02, 1.2177e-01, 8.3100e-02, 7.3626e-01],\n",
      "        [1.6846e-04, 6.1615e-05, 6.8217e-05, 2.4863e-04, 5.1587e-04, 2.1914e-02,\n",
      "         1.7015e-02, 1.0071e-01, 5.6142e-02, 8.0316e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.10, Train Loss: 0.00, Val Loss: 4.40, Train BLEU: 0.00, Val BLEU: 12.45, Minutes Elapsed: 213.68\n",
      "Sampling from val predictions...\n",
      "Source: sự sụp_đổ của bức tường berlin có_thể sẽ không tồn_tại\n",
      "Reference: the fall of the berlin wall would maybe not\n",
      "Model: <SOS> the the of of of of is that be\n",
      "Attention Weights: tensor([[6.3794e-01, 3.5861e-01, 3.4140e-03, 3.7999e-05, 1.3962e-06, 3.1377e-07,\n",
      "         8.8664e-08, 5.8500e-08, 1.8339e-08, 4.4118e-09],\n",
      "        [9.2424e-02, 7.3086e-01, 1.3763e-01, 3.3931e-02, 4.6963e-03, 4.1258e-04,\n",
      "         3.1280e-05, 8.9617e-06, 2.5382e-06, 4.3398e-07],\n",
      "        [7.7969e-02, 2.8933e-01, 2.0811e-01, 2.0963e-01, 1.6403e-01, 4.1850e-02,\n",
      "         6.8626e-03, 1.9147e-03, 2.8086e-04, 2.5749e-05],\n",
      "        [2.3139e-02, 6.4206e-02, 1.0501e-01, 3.0509e-01, 3.8687e-01, 9.7801e-02,\n",
      "         1.3537e-02, 3.6869e-03, 6.2507e-04, 3.1965e-05],\n",
      "        [8.3620e-03, 6.4643e-02, 1.0267e-01, 2.6807e-01, 3.7259e-01, 1.1893e-01,\n",
      "         4.8287e-02, 1.4197e-02, 2.1831e-03, 8.1194e-05],\n",
      "        [5.3323e-03, 1.6955e-02, 4.3214e-02, 1.3121e-01, 4.4927e-01, 2.0914e-01,\n",
      "         1.1783e-01, 2.3499e-02, 3.4247e-03, 1.3054e-04],\n",
      "        [1.6359e-03, 6.2314e-03, 1.8132e-02, 7.5398e-02, 3.8884e-01, 2.7611e-01,\n",
      "         1.9561e-01, 3.2711e-02, 5.1098e-03, 2.2412e-04],\n",
      "        [1.9877e-05, 4.5533e-04, 1.5422e-03, 1.2815e-02, 7.9754e-02, 7.6292e-02,\n",
      "         4.5128e-01, 2.9082e-01, 7.9919e-02, 7.1050e-03],\n",
      "        [1.0138e-06, 4.6378e-05, 1.6365e-04, 2.0058e-03, 1.6407e-02, 3.0905e-02,\n",
      "         2.2219e-01, 3.5629e-01, 2.8666e-01, 8.5340e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.14, Train Loss: 0.00, Val Loss: 4.40, Train BLEU: 0.00, Val BLEU: 12.37, Minutes Elapsed: 216.30\n",
      "Sampling from val predictions...\n",
      "Source: 150 năm trước , 1 nông_nô có_giá gấp 3 lần\n",
      "Reference: a hundred and fifty years ago , an agricultural\n",
      "Model: <SOS> 150 years years ago , , a a a\n",
      "Attention Weights: tensor([[9.1920e-01, 7.9738e-02, 1.0292e-03, 2.5437e-05, 9.9366e-06, 1.0496e-06,\n",
      "         3.7544e-07, 1.9516e-07, 7.9137e-08, 1.3778e-07],\n",
      "        [2.3184e-01, 7.1021e-01, 5.6557e-02, 1.0636e-03, 2.5247e-04, 7.1204e-05,\n",
      "         1.2102e-05, 1.7042e-06, 2.0423e-07, 2.0175e-07],\n",
      "        [2.6887e-02, 4.7439e-01, 4.5014e-01, 3.8618e-02, 7.4716e-03, 2.1181e-03,\n",
      "         3.6030e-04, 1.8544e-05, 6.0108e-07, 4.9440e-07],\n",
      "        [7.0404e-03, 1.5794e-01, 4.3173e-01, 2.3840e-01, 1.4035e-01, 1.9944e-02,\n",
      "         4.0499e-03, 5.1216e-04, 2.4575e-05, 1.3789e-05],\n",
      "        [8.9895e-04, 1.5839e-02, 8.3509e-02, 1.7137e-01, 6.4040e-01, 7.6722e-02,\n",
      "         9.4271e-03, 1.6688e-03, 1.0241e-04, 5.7899e-05],\n",
      "        [7.1557e-04, 1.0046e-02, 4.1902e-02, 1.0038e-01, 5.0521e-01, 2.7263e-01,\n",
      "         5.6436e-02, 1.1537e-02, 6.4881e-04, 4.8511e-04],\n",
      "        [7.2984e-05, 6.4685e-04, 2.9438e-03, 3.5548e-02, 6.0302e-01, 2.4533e-01,\n",
      "         7.2461e-02, 3.1036e-02, 3.7877e-03, 5.1494e-03],\n",
      "        [3.3788e-05, 2.8006e-04, 8.7596e-04, 4.4892e-03, 1.6757e-01, 4.2572e-01,\n",
      "         2.2530e-01, 1.1528e-01, 2.4202e-02, 3.6262e-02],\n",
      "        [3.6034e-05, 3.2304e-04, 8.7319e-04, 3.7947e-03, 1.0226e-01, 2.5704e-01,\n",
      "         2.4748e-01, 2.0679e-01, 5.8525e-02, 1.2288e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.19, Train Loss: 0.00, Val Loss: 4.40, Train BLEU: 0.00, Val BLEU: 12.37, Minutes Elapsed: 218.90\n",
      "Sampling from val predictions...\n",
      "Source: với một cộng_đồng 10,000 người , chúng_ta có_thể có khoảng\n",
      "Reference: in a community of 10,000 people , we get\n",
      "Model: <SOS> in a , , , , , we can\n",
      "Attention Weights: tensor([[9.8742e-01, 1.2254e-02, 2.8464e-04, 3.6559e-05, 1.3613e-06, 5.2841e-08,\n",
      "         3.5269e-08, 8.0767e-09, 7.9574e-10, 2.5478e-10],\n",
      "        [6.1913e-02, 5.2460e-01, 2.9742e-01, 1.0716e-01, 8.5695e-03, 1.9527e-04,\n",
      "         1.3434e-04, 8.8777e-06, 6.3179e-07, 3.0768e-07],\n",
      "        [1.4722e-02, 1.0836e-01, 4.4864e-01, 2.7112e-01, 1.5057e-01, 4.1743e-03,\n",
      "         2.0138e-03, 3.6828e-04, 3.1224e-05, 5.9687e-06],\n",
      "        [1.1279e-03, 1.2389e-02, 3.5989e-02, 8.9899e-02, 8.0186e-01, 3.9612e-02,\n",
      "         1.8579e-02, 5.0434e-04, 3.7716e-05, 4.3629e-06],\n",
      "        [2.5161e-04, 1.8865e-03, 5.6850e-03, 2.3490e-02, 7.3676e-01, 1.0342e-01,\n",
      "         1.2571e-01, 2.6686e-03, 1.1774e-04, 8.3178e-06],\n",
      "        [1.1977e-05, 2.3393e-04, 1.2548e-03, 2.7052e-03, 1.1475e-01, 7.9131e-02,\n",
      "         7.4957e-01, 5.1448e-02, 8.5096e-04, 4.3552e-05],\n",
      "        [1.0881e-05, 1.5915e-04, 3.3595e-04, 9.0576e-04, 4.1788e-02, 7.3219e-02,\n",
      "         8.2657e-01, 5.5039e-02, 1.8437e-03, 1.2533e-04],\n",
      "        [3.3833e-05, 5.1119e-04, 2.1615e-03, 3.8906e-03, 1.4104e-01, 1.2162e-01,\n",
      "         6.7722e-01, 5.2238e-02, 1.1782e-03, 1.0405e-04],\n",
      "        [3.6161e-07, 4.1252e-05, 6.6273e-04, 1.3870e-03, 5.8544e-03, 5.1137e-03,\n",
      "         1.2390e-01, 7.8446e-01, 7.3848e-02, 4.7394e-03]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.24, Train Loss: 0.00, Val Loss: 4.39, Train BLEU: 0.00, Val BLEU: 12.52, Minutes Elapsed: 221.39\n",
      "Sampling from val predictions...\n",
      "Source: ổ_gà , đương_nhiên , có_thể trở_thành một vấn_đề , nhưng\n",
      "Reference: potholes , of course , that can become a\n",
      "Model: <SOS> so , the course , , can be a\n",
      "Attention Weights: tensor([[9.7777e-01, 1.4577e-02, 7.6480e-03, 2.4063e-06, 6.6184e-07, 1.9581e-08,\n",
      "         2.2115e-09, 3.5008e-10, 1.8378e-10, 1.2379e-10],\n",
      "        [1.1808e-01, 1.3716e-01, 7.4204e-01, 1.9272e-03, 7.2406e-04, 6.5179e-05,\n",
      "         3.4055e-06, 5.8421e-07, 5.3593e-08, 1.5624e-08],\n",
      "        [1.5990e-02, 2.1919e-02, 9.2611e-01, 9.2604e-03, 2.4312e-02, 2.2270e-03,\n",
      "         1.7038e-04, 9.0436e-06, 1.5468e-07, 9.4559e-08],\n",
      "        [3.4735e-03, 1.2460e-02, 7.4103e-01, 4.1571e-02, 1.7268e-01, 2.6080e-02,\n",
      "         2.5521e-03, 1.5656e-04, 1.6845e-06, 1.6293e-06],\n",
      "        [1.7073e-04, 4.6096e-04, 1.7956e-02, 2.2648e-02, 6.3173e-01, 2.5794e-01,\n",
      "         6.3028e-02, 5.9149e-03, 7.5566e-05, 8.0661e-05],\n",
      "        [3.0863e-05, 1.4747e-04, 7.3571e-03, 3.5656e-02, 7.0962e-01, 1.6422e-01,\n",
      "         7.3455e-02, 8.6829e-03, 3.1923e-04, 5.1002e-04],\n",
      "        [3.2896e-07, 3.0049e-06, 4.3790e-04, 2.9156e-03, 2.0175e-01, 5.1082e-01,\n",
      "         2.4645e-01, 3.5844e-02, 1.0898e-03, 6.8653e-04],\n",
      "        [4.4391e-06, 4.8239e-05, 5.4930e-04, 2.1222e-03, 4.6320e-02, 3.5955e-01,\n",
      "         4.9623e-01, 9.1789e-02, 2.5465e-03, 8.4315e-04],\n",
      "        [9.2805e-07, 5.4018e-06, 1.5602e-04, 4.1462e-04, 5.8609e-03, 5.6427e-02,\n",
      "         2.7285e-01, 5.9052e-01, 5.7753e-02, 1.6010e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.29, Train Loss: 0.00, Val Loss: 4.40, Train BLEU: 0.00, Val BLEU: 12.31, Minutes Elapsed: 223.88\n",
      "Sampling from val predictions...\n",
      "Source: phụ_nữ thắng <UNK> phần_trăm trong quốc_hội nhà_nước ở những cuộc\n",
      "Reference: women won <UNK> percent of the national congress in\n",
      "Model: <SOS> women women the the in the in in in\n",
      "Attention Weights: tensor([[9.6961e-01, 3.0354e-02, 2.2570e-05, 7.6112e-06, 9.0507e-07, 9.8105e-08,\n",
      "         2.3852e-08, 2.6220e-08, 7.2962e-09, 6.4652e-09],\n",
      "        [1.7338e-01, 7.6750e-01, 2.2227e-02, 3.5034e-02, 1.6234e-03, 2.1210e-04,\n",
      "         2.2617e-05, 5.5133e-06, 8.7608e-07, 3.4467e-07],\n",
      "        [6.6884e-02, 4.2430e-01, 1.0740e-01, 3.1442e-01, 5.9846e-02, 2.3446e-02,\n",
      "         3.1218e-03, 5.4040e-04, 3.7307e-05, 1.0027e-05],\n",
      "        [5.2675e-03, 6.7323e-02, 7.4555e-02, 6.4879e-01, 1.2657e-01, 7.0093e-02,\n",
      "         6.8254e-03, 5.2641e-04, 3.5003e-05, 1.0821e-05],\n",
      "        [1.6494e-03, 2.1933e-02, 3.2784e-02, 4.9623e-01, 2.2031e-01, 1.9849e-01,\n",
      "         2.5320e-02, 2.9489e-03, 2.4752e-04, 9.1492e-05],\n",
      "        [2.7809e-05, 1.4500e-04, 6.4811e-04, 3.6137e-02, 1.0039e-01, 5.6807e-01,\n",
      "         1.9270e-01, 7.4291e-02, 1.8541e-02, 9.0451e-03],\n",
      "        [8.2208e-06, 4.7793e-05, 2.8052e-04, 1.5868e-02, 5.5277e-02, 4.2612e-01,\n",
      "         2.1888e-01, 1.5284e-01, 7.5398e-02, 5.5274e-02],\n",
      "        [5.8792e-06, 2.0223e-05, 5.1356e-05, 1.5744e-03, 2.4915e-02, 1.1524e-01,\n",
      "         1.2401e-01, 3.7379e-01, 1.6494e-01, 1.9545e-01],\n",
      "        [7.4425e-06, 1.8655e-05, 4.3317e-05, 1.3702e-03, 2.1015e-02, 1.0774e-01,\n",
      "         1.2478e-01, 3.5517e-01, 1.7729e-01, 2.1257e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.34, Train Loss: 0.00, Val Loss: 4.39, Train BLEU: 0.00, Val BLEU: 11.91, Minutes Elapsed: 226.39\n",
      "Sampling from val predictions...\n",
      "Source: nóng và bụi lan_toả khắp_nơi đến_nỗi camera của tôi bị\n",
      "Reference: so pervasive was the heat and the dust that\n",
      "Model: <SOS> and and and and and of the of of\n",
      "Attention Weights: tensor([[9.9602e-01, 3.0906e-03, 8.7391e-04, 1.2276e-05, 1.5866e-06, 2.2955e-07,\n",
      "         7.5288e-08, 2.7129e-08, 3.4330e-08, 2.3218e-08],\n",
      "        [7.6773e-01, 1.5886e-01, 7.0869e-02, 2.0744e-03, 4.2830e-04, 3.1785e-05,\n",
      "         6.2611e-06, 5.4275e-07, 1.9118e-07, 1.1111e-07],\n",
      "        [2.9497e-01, 1.5863e-01, 4.6288e-01, 7.0461e-02, 1.2206e-02, 7.8296e-04,\n",
      "         6.4605e-05, 2.8286e-06, 4.9679e-07, 1.5860e-07],\n",
      "        [2.0085e-01, 1.3222e-01, 4.0506e-01, 1.6343e-01, 8.7055e-02, 1.0127e-02,\n",
      "         1.2090e-03, 3.8984e-05, 4.8589e-06, 1.2148e-06],\n",
      "        [6.6512e-03, 1.3685e-02, 1.2604e-01, 2.7783e-01, 4.1088e-01, 1.3803e-01,\n",
      "         2.5251e-02, 1.3022e-03, 2.9340e-04, 3.9104e-05],\n",
      "        [2.0030e-03, 1.6072e-02, 9.0446e-02, 8.3118e-02, 2.8931e-01, 3.5865e-01,\n",
      "         1.4334e-01, 1.4289e-02, 2.3405e-03, 4.3884e-04],\n",
      "        [1.5181e-03, 8.6423e-03, 7.5600e-02, 5.1504e-02, 1.8105e-01, 4.0113e-01,\n",
      "         2.3541e-01, 3.8593e-02, 5.5949e-03, 9.5770e-04],\n",
      "        [3.9609e-05, 1.7635e-04, 4.0491e-03, 2.4728e-02, 8.0054e-02, 2.8774e-01,\n",
      "         4.3095e-01, 1.1103e-01, 4.9486e-02, 1.1743e-02],\n",
      "        [2.0036e-05, 8.3825e-05, 1.0238e-03, 3.9209e-03, 2.6274e-02, 1.5017e-01,\n",
      "         3.1329e-01, 2.1127e-01, 2.0679e-01, 8.7154e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.38, Train Loss: 0.00, Val Loss: 4.39, Train BLEU: 0.00, Val BLEU: 12.67, Minutes Elapsed: 228.89\n",
      "Sampling from val predictions...\n",
      "Source: hai điều quan_trọng nhất chúng_ta có là thời_gian và mối\n",
      "Reference: two of the most valuable things we have are\n",
      "Model: <SOS> two second the thing that we we have is\n",
      "Attention Weights: tensor([[7.4137e-01, 2.5701e-01, 1.5637e-03, 5.5898e-05, 2.1890e-06, 4.9949e-07,\n",
      "         4.4045e-08, 1.0104e-08, 5.0635e-09, 2.9228e-09],\n",
      "        [1.3565e-02, 6.5823e-01, 2.9015e-01, 3.7075e-02, 8.7596e-04, 9.3585e-05,\n",
      "         1.0601e-05, 2.1982e-06, 2.1768e-07, 7.6152e-08],\n",
      "        [7.2853e-03, 2.9568e-01, 4.4813e-01, 2.1258e-01, 2.2002e-02, 1.2473e-02,\n",
      "         1.7193e-03, 1.1757e-04, 7.7916e-06, 8.3519e-07],\n",
      "        [3.1064e-04, 2.0254e-02, 1.2138e-01, 3.1526e-01, 3.6854e-01, 1.5175e-01,\n",
      "         2.0686e-02, 1.6923e-03, 1.1461e-04, 1.4872e-05],\n",
      "        [7.8978e-04, 3.4610e-02, 4.1731e-02, 2.3763e-01, 3.0825e-01, 3.4658e-01,\n",
      "         2.8298e-02, 1.9370e-03, 1.5329e-04, 1.3893e-05],\n",
      "        [1.4019e-03, 5.0053e-02, 7.3337e-02, 2.9495e-01, 2.3511e-01, 3.2045e-01,\n",
      "         2.2301e-02, 2.2266e-03, 1.5936e-04, 1.8655e-05],\n",
      "        [4.6453e-04, 1.8413e-02, 5.9303e-02, 1.5806e-01, 1.3874e-01, 5.2489e-01,\n",
      "         8.9094e-02, 1.0479e-02, 4.7366e-04, 9.3154e-05],\n",
      "        [1.5512e-04, 5.5864e-03, 2.9446e-02, 7.3243e-02, 8.2315e-02, 5.6721e-01,\n",
      "         1.9162e-01, 4.6728e-02, 2.5265e-03, 1.1730e-03],\n",
      "        [4.1493e-05, 1.1829e-03, 1.5739e-02, 2.1054e-02, 3.1121e-02, 3.0689e-01,\n",
      "         4.2156e-01, 1.9602e-01, 4.5437e-03, 1.8465e-03]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.43, Train Loss: 0.00, Val Loss: 4.35, Train BLEU: 0.00, Val BLEU: 12.94, Minutes Elapsed: 231.38\n",
      "Sampling from val predictions...\n",
      "Source: nhưng thay_vì hỏi sao họ lại chẳng trồng bất_cứ thứ\n",
      "Reference: but instead of asking them how come they were\n",
      "Model: <SOS> but instead how do they they they they ,\n",
      "Attention Weights: tensor([[2.4892e-02, 9.7273e-01, 2.3654e-03, 1.5113e-05, 6.0515e-07, 2.0502e-08,\n",
      "         1.1146e-08, 3.4377e-09, 3.3617e-09, 2.3195e-09],\n",
      "        [1.8343e-03, 9.4953e-01, 4.7782e-02, 8.1943e-04, 2.7419e-05, 1.2951e-06,\n",
      "         7.3737e-07, 1.5846e-07, 5.7236e-08, 2.7103e-08],\n",
      "        [5.0550e-04, 7.7824e-02, 8.0348e-01, 1.0712e-01, 1.0697e-02, 2.9474e-04,\n",
      "         7.2002e-05, 4.7444e-06, 7.0360e-07, 1.8204e-07],\n",
      "        [2.5506e-04, 4.2052e-02, 4.6836e-01, 3.3499e-01, 1.4320e-01, 8.8803e-03,\n",
      "         2.1196e-03, 1.3934e-04, 8.2057e-06, 1.0951e-06],\n",
      "        [5.2855e-05, 4.9704e-03, 6.4966e-02, 1.8969e-01, 5.9466e-01, 1.0054e-01,\n",
      "         4.1772e-02, 3.1888e-03, 1.3890e-04, 1.2360e-05],\n",
      "        [1.9235e-04, 1.5252e-02, 1.8347e-01, 3.4968e-01, 3.3750e-01, 8.7395e-02,\n",
      "         2.4610e-02, 1.7742e-03, 1.1308e-04, 1.4135e-05],\n",
      "        [1.6329e-06, 1.1519e-04, 6.3222e-03, 6.4512e-02, 3.7547e-01, 3.3535e-01,\n",
      "         1.8065e-01, 3.3813e-02, 3.3489e-03, 4.2095e-04],\n",
      "        [8.0854e-08, 1.5413e-06, 1.9611e-04, 5.6653e-03, 7.7699e-02, 1.4232e-01,\n",
      "         4.3732e-01, 2.3392e-01, 8.4836e-02, 1.8043e-02],\n",
      "        [4.7015e-08, 9.2091e-07, 3.6133e-05, 4.2751e-04, 1.1312e-02, 4.4604e-02,\n",
      "         4.3247e-01, 2.1419e-01, 1.7438e-01, 1.2258e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.48, Train Loss: 0.00, Val Loss: 4.35, Train BLEU: 0.00, Val BLEU: 12.94, Minutes Elapsed: 233.86\n",
      "Sampling from val predictions...\n",
      "Source: nhưng sư_tử rất thông_minh . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: but lions are very clever . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> but , &apos;s very . . <EOS> <EOS> .\n",
      "Attention Weights: tensor([[0.6406, 0.0165, 0.3423, 0.0006, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1491, 0.0283, 0.8110, 0.0113, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0314, 0.0093, 0.8499, 0.1037, 0.0055, 0.0002, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0137, 0.0030, 0.3658, 0.4580, 0.1462, 0.0133, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0172, 0.0030, 0.1125, 0.4776, 0.2865, 0.1031, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1142, 0.0236, 0.1605, 0.1596, 0.2931, 0.2490, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0981, 0.0157, 0.1236, 0.1179, 0.2441, 0.4005, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0139, 0.0038, 0.0914, 0.2779, 0.2674, 0.3455, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0102, 0.0037, 0.1139, 0.3258, 0.2530, 0.2934, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.53, Train Loss: 0.00, Val Loss: 4.37, Train BLEU: 0.00, Val BLEU: 12.39, Minutes Elapsed: 236.36\n",
      "Sampling from val predictions...\n",
      "Source: bạn sẽ bất_ngờ với những gì mà mảnh_đất có_thể làm\n",
      "Reference: you &apos;d be surprised what the soil could do\n",
      "Model: <SOS> you will going to to to what of that\n",
      "Attention Weights: tensor([[9.7696e-01, 2.2943e-02, 9.6363e-05, 1.0983e-06, 4.8852e-08, 5.0262e-08,\n",
      "         1.7661e-08, 1.0437e-08, 3.9299e-09, 2.0487e-09],\n",
      "        [5.4831e-02, 9.3197e-01, 1.3134e-02, 6.4935e-05, 1.1309e-06, 5.2221e-07,\n",
      "         5.1622e-08, 1.6202e-08, 1.5214e-09, 7.6531e-10],\n",
      "        [3.8985e-02, 4.8408e-01, 4.5281e-01, 2.3119e-02, 5.1202e-04, 4.2530e-04,\n",
      "         6.1999e-05, 1.1980e-05, 4.5223e-07, 1.2647e-07],\n",
      "        [2.4563e-03, 1.5151e-02, 6.2583e-01, 3.0331e-01, 2.5851e-02, 2.3678e-02,\n",
      "         3.2906e-03, 4.0143e-04, 2.5239e-05, 7.1499e-06],\n",
      "        [3.6515e-03, 1.9075e-02, 5.4079e-01, 2.9905e-01, 7.3158e-02, 4.7356e-02,\n",
      "         1.3221e-02, 3.2390e-03, 3.4028e-04, 1.2090e-04],\n",
      "        [5.2542e-03, 1.0884e-02, 1.6142e-01, 3.9887e-01, 1.2998e-01, 2.1622e-01,\n",
      "         6.4516e-02, 1.1477e-02, 9.6373e-04, 4.2086e-04],\n",
      "        [1.7032e-05, 2.5068e-04, 1.7785e-02, 1.4505e-01, 2.0572e-01, 3.6496e-01,\n",
      "         1.9057e-01, 6.8575e-02, 5.2232e-03, 1.8409e-03],\n",
      "        [3.8362e-07, 1.7562e-05, 1.0793e-03, 5.3594e-03, 3.1066e-02, 6.8933e-02,\n",
      "         2.0443e-01, 5.2747e-01, 1.0409e-01, 5.7555e-02],\n",
      "        [5.9623e-06, 2.2537e-05, 4.9170e-05, 5.8661e-04, 4.0842e-03, 6.6445e-02,\n",
      "         2.7992e-01, 2.9759e-01, 1.7691e-01, 1.7439e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.58, Train Loss: 0.00, Val Loss: 4.33, Train BLEU: 0.00, Val BLEU: 12.47, Minutes Elapsed: 238.85\n",
      "Sampling from val predictions...\n",
      "Source: ở trường , chúng_tôi dành rất nhiều thời_gian để học\n",
      "Reference: in school , we spent a lot of time\n",
      "Model: <SOS> in the , we we a lot of to\n",
      "Attention Weights: tensor([[8.8540e-01, 1.1386e-01, 5.9631e-04, 1.3690e-04, 1.4545e-06, 1.5564e-07,\n",
      "         3.6850e-08, 1.9086e-08, 1.4902e-08, 7.0772e-09],\n",
      "        [1.5737e-01, 7.4043e-01, 4.8497e-02, 5.1723e-02, 1.8363e-03, 1.2866e-04,\n",
      "         9.7611e-06, 2.0285e-06, 3.1578e-07, 7.7748e-08],\n",
      "        [4.4795e-02, 2.1952e-01, 6.7149e-02, 5.9941e-01, 6.3780e-02, 4.9459e-03,\n",
      "         3.5585e-04, 3.7754e-05, 3.6451e-06, 3.4640e-07],\n",
      "        [4.7190e-03, 1.7428e-02, 2.8828e-02, 9.2389e-01, 2.1550e-02, 2.9849e-03,\n",
      "         4.7155e-04, 1.0581e-04, 2.2084e-05, 1.5493e-06],\n",
      "        [2.6915e-04, 1.4225e-03, 1.8058e-03, 1.3711e-01, 8.0058e-01, 5.5078e-02,\n",
      "         3.4176e-03, 2.9912e-04, 1.7589e-05, 2.2853e-06],\n",
      "        [1.0269e-03, 2.6994e-03, 4.0241e-03, 1.2903e-02, 1.4767e-01, 6.9815e-01,\n",
      "         1.2363e-01, 9.3444e-03, 4.1749e-04, 1.3209e-04],\n",
      "        [1.1533e-05, 6.3382e-05, 1.0807e-04, 1.0380e-03, 4.7269e-03, 9.3917e-02,\n",
      "         6.0326e-01, 2.6956e-01, 2.2742e-02, 4.5736e-03],\n",
      "        [2.8572e-05, 9.1106e-05, 2.5225e-04, 1.1202e-03, 6.1720e-04, 4.0463e-03,\n",
      "         6.7035e-02, 2.5672e-01, 4.9769e-01, 1.7240e-01],\n",
      "        [1.2646e-05, 2.0460e-05, 1.5338e-04, 6.8687e-04, 2.7778e-04, 1.7879e-03,\n",
      "         2.3352e-02, 1.3303e-01, 5.1339e-01, 3.2729e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.62, Train Loss: 0.00, Val Loss: 4.33, Train BLEU: 0.00, Val BLEU: 13.16, Minutes Elapsed: 241.38\n",
      "Sampling from val predictions...\n",
      "Source: đây là cuộc_sống trong sáu tháng của tôi , đã\n",
      "Reference: this is six months of my life , into\n",
      "Model: <SOS> this is the in my my , , ,\n",
      "Attention Weights: tensor([[9.5454e-01, 4.4116e-02, 1.3241e-03, 1.6484e-05, 5.8697e-07, 6.0658e-08,\n",
      "         4.7286e-09, 1.7585e-09, 7.1654e-10, 6.2534e-10],\n",
      "        [9.0895e-02, 6.1984e-01, 2.8092e-01, 7.9765e-03, 3.3108e-04, 3.3498e-05,\n",
      "         1.2741e-06, 1.9155e-07, 2.1444e-08, 8.9168e-09],\n",
      "        [5.5682e-03, 1.9896e-02, 8.0131e-01, 1.2762e-01, 3.7609e-02, 7.3782e-03,\n",
      "         5.7506e-04, 4.4128e-05, 2.2804e-06, 8.9386e-07],\n",
      "        [1.3307e-03, 3.3207e-03, 3.9399e-01, 3.9377e-01, 1.6317e-01, 4.2648e-02,\n",
      "         1.5371e-03, 2.1177e-04, 9.3478e-06, 2.4306e-06],\n",
      "        [3.3150e-04, 6.5081e-04, 2.2977e-02, 2.2280e-01, 5.4235e-01, 1.8808e-01,\n",
      "         1.8344e-02, 4.3666e-03, 9.6375e-05, 9.1249e-06],\n",
      "        [1.3105e-05, 2.2803e-05, 6.0810e-04, 1.3870e-02, 2.8864e-01, 5.2587e-01,\n",
      "         1.2224e-01, 4.4794e-02, 2.7570e-03, 1.1904e-03],\n",
      "        [9.4385e-07, 3.5447e-06, 1.0243e-04, 2.3378e-03, 5.8138e-02, 2.9110e-01,\n",
      "         1.6954e-01, 2.0314e-01, 1.0021e-01, 1.7543e-01],\n",
      "        [6.5256e-06, 8.0471e-06, 1.6717e-05, 3.5573e-04, 6.0211e-03, 2.6277e-02,\n",
      "         3.3917e-02, 1.1543e-01, 2.4616e-01, 5.7181e-01],\n",
      "        [1.7000e-07, 3.5155e-07, 2.2608e-06, 3.3669e-05, 4.6064e-04, 1.9354e-03,\n",
      "         3.5790e-03, 1.1723e-02, 4.4080e-02, 9.3819e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.67, Train Loss: 0.00, Val Loss: 4.34, Train BLEU: 0.00, Val BLEU: 12.93, Minutes Elapsed: 243.88\n",
      "Sampling from val predictions...\n",
      "Source: tôi gặp những bé trai này lúc 5 giờ sáng\n",
      "Reference: i met these boys at five in the morning\n",
      "Model: <SOS> i &apos;ve my i had five five ago years\n",
      "Attention Weights: tensor([[9.3891e-01, 6.0568e-02, 5.1769e-04, 7.2957e-06, 2.5711e-07, 3.4264e-08,\n",
      "         1.5215e-08, 6.1697e-09, 1.3320e-09, 6.1399e-10],\n",
      "        [4.6501e-02, 8.6086e-01, 9.1847e-02, 7.5312e-04, 3.8282e-05, 2.2229e-06,\n",
      "         6.2081e-07, 1.2591e-07, 8.9986e-09, 1.2936e-09],\n",
      "        [1.1910e-02, 2.7908e-01, 4.4711e-01, 1.9216e-01, 5.2914e-02, 1.0553e-02,\n",
      "         5.9979e-03, 2.4951e-04, 1.9030e-05, 1.0638e-06],\n",
      "        [9.0696e-04, 1.2670e-02, 1.9866e-01, 2.1242e-01, 4.0978e-01, 6.5532e-02,\n",
      "         9.3173e-02, 6.1736e-03, 6.1436e-04, 6.8417e-05],\n",
      "        [2.8522e-05, 2.2317e-04, 1.9967e-03, 7.7013e-03, 6.1444e-02, 7.1967e-02,\n",
      "         6.6837e-01, 1.4230e-01, 3.5624e-02, 1.0345e-02],\n",
      "        [4.5750e-06, 3.3301e-05, 2.8549e-04, 1.8811e-03, 1.1514e-02, 1.9824e-02,\n",
      "         4.1025e-01, 2.4806e-01, 1.9509e-01, 1.1306e-01],\n",
      "        [3.8342e-07, 3.8194e-06, 5.6864e-05, 2.0625e-04, 2.4666e-03, 2.5850e-03,\n",
      "         7.7566e-02, 1.7142e-01, 3.0126e-01, 4.4443e-01],\n",
      "        [7.5622e-06, 1.5451e-05, 5.3137e-05, 2.6430e-04, 1.5323e-03, 2.3236e-03,\n",
      "         4.1013e-02, 6.2112e-02, 2.3504e-01, 6.5764e-01],\n",
      "        [4.6512e-06, 1.2219e-05, 5.0366e-05, 2.2828e-04, 1.7569e-03, 2.4466e-03,\n",
      "         2.6490e-02, 4.6055e-02, 2.0308e-01, 7.1987e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.72, Train Loss: 0.00, Val Loss: 4.31, Train BLEU: 0.00, Val BLEU: 12.94, Minutes Elapsed: 246.39\n",
      "Sampling from val predictions...\n",
      "Source: không lâu sau đó , khi tôi đi qua một\n",
      "Reference: soon after , when i was walking past a\n",
      "Model: <SOS> not , , , i went to a a\n",
      "Attention Weights: tensor([[9.1142e-01, 8.7940e-02, 5.5042e-04, 8.4566e-05, 2.4134e-06, 7.9110e-07,\n",
      "         1.4780e-08, 2.2348e-09, 6.0496e-10, 2.1802e-10],\n",
      "        [4.1549e-02, 4.5716e-01, 3.4986e-01, 1.3388e-01, 1.2424e-02, 4.9923e-03,\n",
      "         1.1872e-04, 1.5011e-05, 1.9408e-06, 1.5587e-07],\n",
      "        [5.8534e-02, 1.3029e-01, 3.6770e-01, 2.7253e-01, 1.0650e-01, 6.0112e-02,\n",
      "         3.1586e-03, 9.5087e-04, 2.0238e-04, 2.2271e-05],\n",
      "        [8.0425e-03, 1.1824e-02, 3.8645e-02, 1.0654e-01, 1.3035e-01, 4.6583e-01,\n",
      "         2.1369e-01, 2.1922e-02, 2.7429e-03, 4.0581e-04],\n",
      "        [4.2246e-04, 2.8430e-04, 2.7793e-03, 2.1041e-02, 6.5358e-02, 7.6857e-01,\n",
      "         1.1652e-01, 2.1175e-02, 3.0154e-03, 8.2892e-04],\n",
      "        [8.9409e-05, 1.1645e-04, 8.5135e-04, 2.1269e-03, 5.0178e-03, 6.2194e-02,\n",
      "         3.1813e-01, 4.9809e-01, 9.7210e-02, 1.6169e-02],\n",
      "        [1.2227e-04, 2.4590e-04, 2.2331e-03, 3.0249e-03, 4.1269e-03, 1.8931e-02,\n",
      "         3.5900e-02, 3.9187e-01, 4.0474e-01, 1.3881e-01],\n",
      "        [2.8442e-05, 5.4779e-05, 9.9906e-04, 8.2745e-04, 1.7162e-03, 9.9229e-03,\n",
      "         4.3929e-03, 1.9457e-02, 3.2183e-01, 6.4077e-01],\n",
      "        [3.3461e-04, 3.7585e-04, 2.8059e-03, 3.1099e-03, 1.1015e-02, 9.1756e-02,\n",
      "         2.1418e-02, 3.7125e-02, 2.7198e-01, 5.6008e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.77, Train Loss: 0.00, Val Loss: 4.38, Train BLEU: 0.00, Val BLEU: 12.57, Minutes Elapsed: 248.92\n",
      "Sampling from val predictions...\n",
      "Source: đó là về hiểu những người xung_quanh chúng_ta theo những\n",
      "Reference: it &apos;s about understanding our neighbors in new and\n",
      "Model: <SOS> it &apos;s about about people people in us that\n",
      "Attention Weights: tensor([[9.4041e-01, 5.6010e-02, 3.4896e-03, 8.6894e-05, 1.3316e-06, 1.1865e-07,\n",
      "         2.9440e-08, 7.5280e-09, 2.3160e-09, 8.1862e-10],\n",
      "        [5.3352e-02, 5.7687e-01, 3.4335e-01, 2.5651e-02, 7.0158e-04, 7.1167e-05,\n",
      "         5.1984e-06, 5.9415e-07, 1.3391e-07, 2.1961e-08],\n",
      "        [3.4455e-02, 8.4769e-02, 7.6395e-01, 9.7352e-02, 1.4367e-02, 4.4334e-03,\n",
      "         5.9431e-04, 5.6724e-05, 2.3877e-05, 4.1905e-06],\n",
      "        [1.1236e-03, 2.6421e-03, 1.3464e-01, 5.3287e-01, 2.2527e-01, 9.5115e-02,\n",
      "         7.2815e-03, 9.1549e-04, 1.2557e-04, 1.8792e-05],\n",
      "        [1.6500e-04, 7.4901e-04, 3.9313e-02, 5.2240e-01, 2.8944e-01, 1.3169e-01,\n",
      "         1.3715e-02, 2.2185e-03, 2.9295e-04, 2.8073e-05],\n",
      "        [1.0169e-04, 3.7482e-04, 6.6893e-03, 1.2520e-01, 2.9275e-01, 4.3005e-01,\n",
      "         1.0534e-01, 3.5508e-02, 3.6945e-03, 2.9808e-04],\n",
      "        [8.2024e-07, 6.4000e-06, 1.7776e-04, 5.5313e-03, 3.7430e-02, 2.0716e-01,\n",
      "         4.4957e-01, 1.9843e-01, 8.9238e-02, 1.2459e-02],\n",
      "        [8.4272e-07, 2.5540e-06, 2.1925e-05, 2.4984e-04, 2.2370e-03, 4.6883e-02,\n",
      "         1.0711e-01, 2.5491e-01, 3.7950e-01, 2.0908e-01],\n",
      "        [3.5241e-08, 2.0812e-07, 7.5375e-06, 1.4219e-04, 1.7330e-03, 3.0373e-02,\n",
      "         9.4016e-02, 1.0965e-01, 3.5112e-01, 4.1295e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.82, Train Loss: 0.00, Val Loss: 4.31, Train BLEU: 0.00, Val BLEU: 12.93, Minutes Elapsed: 251.44\n",
      "Sampling from val predictions...\n",
      "Source: vậy , điều đó có nghĩa_là mọi người sẽ có_thể\n",
      "Reference: so what that means is that people will be\n",
      "Model: <SOS> so what that means is the the would be\n",
      "Attention Weights: tensor([[9.8725e-01, 9.8917e-03, 2.8292e-03, 2.4599e-05, 7.2809e-07, 1.2406e-07,\n",
      "         1.1846e-08, 3.6147e-09, 2.4092e-09, 7.6161e-10],\n",
      "        [9.6155e-03, 1.0276e-02, 9.4225e-01, 3.5252e-02, 2.1031e-03, 4.8578e-04,\n",
      "         1.4648e-05, 1.5594e-06, 2.8425e-07, 7.9542e-08],\n",
      "        [2.9575e-03, 5.1483e-03, 5.8901e-01, 2.6905e-01, 9.2465e-02, 3.9806e-02,\n",
      "         1.3794e-03, 1.5754e-04, 1.9658e-05, 1.4346e-06],\n",
      "        [8.9724e-04, 2.7572e-03, 6.7145e-02, 1.1167e-01, 2.3764e-01, 5.1507e-01,\n",
      "         4.9440e-02, 1.3149e-02, 2.0646e-03, 1.6879e-04],\n",
      "        [2.2346e-05, 5.2982e-05, 5.0932e-03, 1.7044e-02, 6.8340e-02, 2.3623e-01,\n",
      "         3.9738e-01, 2.2490e-01, 4.4336e-02, 6.5946e-03],\n",
      "        [6.1590e-06, 3.0519e-05, 1.9748e-03, 9.5777e-03, 6.6717e-02, 1.2082e-01,\n",
      "         2.5084e-01, 2.8212e-01, 1.9019e-01, 7.7719e-02],\n",
      "        [2.1670e-07, 1.6439e-06, 1.3040e-04, 9.7405e-04, 1.0946e-02, 2.9790e-02,\n",
      "         1.2034e-01, 2.4248e-01, 3.6338e-01, 2.3195e-01],\n",
      "        [3.8110e-07, 2.0322e-06, 7.9891e-05, 2.5120e-04, 1.4915e-03, 8.5321e-03,\n",
      "         9.8501e-02, 3.2534e-01, 3.1168e-01, 2.5412e-01],\n",
      "        [7.5536e-08, 5.1518e-07, 2.2694e-05, 2.4140e-04, 2.3412e-03, 4.9923e-03,\n",
      "         3.9311e-02, 1.3489e-01, 3.9415e-01, 4.2405e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.86, Train Loss: 0.00, Val Loss: 4.29, Train BLEU: 0.00, Val BLEU: 13.14, Minutes Elapsed: 254.00\n",
      "Sampling from val predictions...\n",
      "Source: nếu người đó không muốn làm , vậy_thì bạn phải\n",
      "Reference: if that person doesn &apos;t want to do it\n",
      "Model: <SOS> if you don &apos;t &apos;t want to , ,\n",
      "Attention Weights: tensor([[9.7382e-01, 2.5455e-02, 6.6781e-04, 5.6298e-05, 1.3217e-06, 4.0501e-08,\n",
      "         4.7551e-09, 2.2742e-09, 1.5844e-09, 7.9697e-10],\n",
      "        [1.8344e-02, 7.2128e-01, 2.2405e-01, 3.5190e-02, 1.1080e-03, 2.8418e-05,\n",
      "         6.2737e-07, 1.1579e-07, 2.8707e-08, 1.7316e-08],\n",
      "        [7.6799e-04, 5.1429e-02, 2.2865e-01, 6.0106e-01, 1.1213e-01, 5.9164e-03,\n",
      "         4.7134e-05, 7.1880e-06, 1.0472e-06, 3.5718e-07],\n",
      "        [7.3029e-04, 1.0218e-02, 7.3622e-02, 6.2458e-01, 2.5055e-01, 3.8828e-02,\n",
      "         1.2571e-03, 1.4923e-04, 3.8820e-05, 2.9441e-05],\n",
      "        [4.4636e-04, 1.5366e-02, 4.3460e-02, 3.6434e-01, 4.7848e-01, 9.3921e-02,\n",
      "         2.8380e-03, 8.6023e-04, 1.7425e-04, 1.1490e-04],\n",
      "        [2.0943e-05, 9.9276e-04, 3.4306e-03, 3.5772e-02, 5.7451e-01, 3.2439e-01,\n",
      "         2.6224e-02, 2.7390e-02, 4.4131e-03, 2.8575e-03],\n",
      "        [3.6719e-06, 8.9861e-05, 5.8956e-04, 6.5782e-03, 6.9583e-02, 2.8803e-01,\n",
      "         2.7569e-01, 1.4099e-01, 7.7869e-02, 1.4058e-01],\n",
      "        [2.1427e-06, 4.3540e-05, 1.1945e-04, 2.0185e-03, 2.0980e-02, 9.5050e-02,\n",
      "         5.2942e-02, 7.0702e-01, 8.6705e-02, 3.5117e-02],\n",
      "        [1.1280e-06, 1.9925e-05, 1.0010e-04, 1.5665e-03, 1.8884e-02, 9.6043e-02,\n",
      "         8.5159e-02, 4.2913e-01, 2.2124e-01, 1.4786e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.91, Train Loss: 0.00, Val Loss: 4.31, Train BLEU: 0.00, Val BLEU: 12.64, Minutes Elapsed: 256.50\n",
      "Sampling from val predictions...\n",
      "Source: tôi chẳng cho họ tiền được , không gì cả\n",
      "Reference: i couldn &apos;t give them money , nothing .\n",
      "Model: <SOS> i i trying them them , , , i\n",
      "Attention Weights: tensor([[8.9774e-01, 1.0093e-01, 1.3220e-03, 4.4527e-06, 3.1409e-07, 3.0540e-08,\n",
      "         1.3878e-08, 1.2720e-08, 3.4598e-09, 2.3532e-09],\n",
      "        [4.6474e-03, 9.7415e-01, 2.0949e-02, 2.4353e-04, 1.1260e-05, 2.1282e-07,\n",
      "         1.9402e-08, 4.0552e-09, 9.3441e-10, 6.8465e-10],\n",
      "        [1.9424e-02, 6.0350e-01, 3.0196e-01, 5.7587e-02, 1.6975e-02, 5.1804e-04,\n",
      "         2.9658e-05, 5.7016e-06, 1.1990e-06, 3.0947e-07],\n",
      "        [2.7081e-03, 2.5956e-02, 4.0648e-01, 2.5226e-01, 2.8329e-01, 2.7820e-02,\n",
      "         9.5977e-04, 4.7575e-04, 3.7532e-05, 7.3713e-06],\n",
      "        [3.4985e-05, 9.1787e-04, 3.8793e-02, 2.7719e-01, 5.1105e-01, 1.5987e-01,\n",
      "         7.7593e-03, 3.8093e-03, 4.7928e-04, 9.5217e-05],\n",
      "        [2.1422e-05, 5.2554e-04, 1.4019e-02, 7.9315e-02, 6.3391e-01, 2.4151e-01,\n",
      "         1.0041e-02, 1.7704e-02, 2.6295e-03, 3.3197e-04],\n",
      "        [2.1971e-04, 7.3108e-04, 8.0660e-03, 3.6767e-02, 3.4280e-01, 3.7135e-01,\n",
      "         4.9673e-02, 1.6916e-01, 1.8269e-02, 2.9671e-03],\n",
      "        [1.1051e-05, 2.0258e-04, 2.2281e-03, 2.2547e-02, 1.8089e-01, 1.4733e-01,\n",
      "         2.6903e-02, 4.5910e-01, 1.4506e-01, 1.5727e-02],\n",
      "        [4.9378e-06, 6.6475e-05, 5.7583e-04, 5.5331e-03, 2.3246e-02, 2.5684e-02,\n",
      "         1.4752e-02, 4.2736e-01, 4.0004e-01, 1.0274e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.96, Train Loss: 0.00, Val Loss: 4.29, Train BLEU: 0.00, Val BLEU: 12.88, Minutes Elapsed: 259.11\n",
      "Sampling from val predictions...\n",
      "Source: l. a. dẫn đầu nước mỹ về diện_tích <UNK> mà\n",
      "Reference: l.a. leads the united states in vacant lots that\n",
      "Model: <SOS> and the of the of in the <UNK> ,\n",
      "Attention Weights: tensor([[1.1633e-03, 5.0332e-04, 9.6803e-01, 3.0246e-02, 5.7753e-05, 3.4865e-06,\n",
      "         2.5195e-07, 5.0653e-08, 2.0072e-08, 6.7359e-08],\n",
      "        [1.2117e-03, 5.9578e-04, 2.9574e-01, 6.9615e-01, 5.9636e-03, 3.2246e-04,\n",
      "         1.4191e-05, 2.2195e-06, 7.3383e-07, 2.5904e-07],\n",
      "        [5.5419e-04, 7.1654e-05, 6.5331e-03, 7.2082e-01, 2.5438e-01, 1.7064e-02,\n",
      "         5.1991e-04, 5.0358e-05, 2.4217e-06, 4.3603e-07],\n",
      "        [5.3714e-04, 9.3086e-05, 6.1858e-03, 1.5774e-01, 3.1850e-01, 4.3774e-01,\n",
      "         7.0844e-02, 8.0718e-03, 2.6298e-04, 2.5718e-05],\n",
      "        [5.1021e-05, 7.0477e-06, 1.3862e-04, 2.3139e-02, 1.5952e-01, 4.8417e-01,\n",
      "         2.5079e-01, 7.0131e-02, 1.1213e-02, 8.3834e-04],\n",
      "        [5.4299e-05, 1.3531e-05, 2.6268e-04, 3.9187e-03, 3.9646e-02, 4.2711e-01,\n",
      "         3.8937e-01, 1.1928e-01, 1.4677e-02, 5.6728e-03],\n",
      "        [3.6676e-06, 1.7375e-06, 5.2038e-06, 6.3748e-05, 1.3138e-03, 3.4648e-02,\n",
      "         1.4893e-01, 3.7096e-01, 1.8674e-01, 2.5734e-01],\n",
      "        [4.0485e-07, 3.3681e-07, 4.5070e-07, 6.0970e-06, 1.6419e-04, 3.7315e-03,\n",
      "         1.8403e-02, 1.2436e-01, 1.3571e-01, 7.1762e-01],\n",
      "        [3.0235e-06, 2.5336e-06, 1.5847e-05, 1.8932e-05, 1.0450e-04, 2.7657e-03,\n",
      "         1.1413e-02, 4.8138e-02, 4.3461e-02, 8.9408e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.00, Train Loss: 0.00, Val Loss: 4.35, Train BLEU: 0.00, Val BLEU: 12.74, Minutes Elapsed: 261.20\n",
      "Sampling from val predictions...\n",
      "Source: tôi gặp cậu_bé ở khu cứu_trợ mà tổ_chức giải_phóng nô_lệ\n",
      "Reference: i met him at a shelter where free the\n",
      "Model: <SOS> i &apos;ve the in the global of the the\n",
      "Attention Weights: tensor([[9.1180e-01, 8.5753e-02, 2.3864e-03, 5.7657e-05, 1.5391e-06, 5.6394e-08,\n",
      "         4.4682e-09, 1.9428e-09, 7.2456e-10, 1.9710e-10],\n",
      "        [1.0281e-02, 8.2202e-01, 1.6188e-01, 5.7080e-03, 1.1525e-04, 1.2036e-06,\n",
      "         2.8737e-08, 3.9945e-09, 8.3003e-10, 1.8086e-10],\n",
      "        [1.0496e-02, 3.4952e-01, 2.6672e-01, 3.4185e-01, 2.8094e-02, 3.0839e-03,\n",
      "         2.2894e-04, 1.3622e-05, 2.2606e-06, 1.4236e-07],\n",
      "        [1.1394e-03, 8.5994e-03, 5.5495e-02, 4.6465e-01, 3.7833e-01, 8.5857e-02,\n",
      "         5.5321e-03, 3.5075e-04, 4.7441e-05, 2.1236e-06],\n",
      "        [6.6004e-04, 2.1071e-03, 1.3644e-02, 9.3719e-02, 4.2808e-01, 3.5398e-01,\n",
      "         9.5470e-02, 9.9748e-03, 2.1802e-03, 1.8515e-04],\n",
      "        [1.3832e-04, 4.9773e-04, 4.8827e-03, 5.5784e-02, 4.2076e-01, 4.1309e-01,\n",
      "         8.4054e-02, 1.6009e-02, 4.2121e-03, 5.7272e-04],\n",
      "        [6.0098e-04, 3.7840e-04, 1.8753e-03, 8.4104e-03, 7.7697e-02, 1.8983e-01,\n",
      "         5.7608e-01, 1.0376e-01, 3.5660e-02, 5.7016e-03],\n",
      "        [1.0579e-03, 2.2510e-04, 5.9560e-04, 2.2432e-03, 2.9454e-02, 1.0228e-01,\n",
      "         6.8001e-01, 1.3328e-01, 4.2790e-02, 8.0599e-03],\n",
      "        [8.5135e-06, 1.9605e-05, 1.2587e-04, 3.0512e-04, 5.8661e-03, 4.7579e-02,\n",
      "         2.6234e-01, 4.0562e-01, 2.1080e-01, 6.7329e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.05, Train Loss: 0.00, Val Loss: 4.30, Train BLEU: 0.00, Val BLEU: 13.09, Minutes Elapsed: 263.70\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi tình_cờ quay_lại new_york đúng một năm sau đó ,\n",
      "Reference: we happened to be back in new york exactly\n",
      "Model: <SOS> we we to the back to the francisco ,\n",
      "Attention Weights: tensor([[9.6491e-01, 3.4966e-02, 1.2707e-04, 2.0862e-06, 1.5295e-08, 1.7973e-09,\n",
      "         7.1051e-10, 1.4974e-10, 5.0128e-11, 4.4373e-11],\n",
      "        [9.8690e-03, 9.7684e-01, 1.2998e-02, 2.9281e-04, 1.3109e-06, 3.1706e-08,\n",
      "         1.0176e-08, 8.7197e-10, 2.0288e-10, 8.2375e-11],\n",
      "        [1.4883e-02, 4.0522e-01, 5.0790e-01, 6.5963e-02, 5.4648e-03, 3.3643e-04,\n",
      "         2.1333e-04, 1.1856e-05, 1.0569e-06, 8.6568e-08],\n",
      "        [4.1815e-04, 1.1091e-02, 3.6501e-01, 4.8786e-01, 1.2049e-01, 9.4023e-03,\n",
      "         4.9575e-03, 6.5326e-04, 1.1012e-04, 1.0089e-05],\n",
      "        [7.9794e-05, 8.6407e-04, 5.7279e-02, 3.6395e-01, 4.1799e-01, 1.0199e-01,\n",
      "         5.0260e-02, 6.1782e-03, 1.2911e-03, 1.1744e-04],\n",
      "        [5.6816e-05, 2.3648e-04, 1.1093e-02, 2.3213e-01, 3.9395e-01, 2.1547e-01,\n",
      "         1.2110e-01, 1.9860e-02, 5.5811e-03, 5.2715e-04],\n",
      "        [8.3183e-05, 1.6072e-04, 4.3029e-03, 1.8213e-01, 4.0653e-01, 2.2901e-01,\n",
      "         1.4139e-01, 2.7894e-02, 7.6334e-03, 8.6693e-04],\n",
      "        [7.6431e-06, 3.2180e-05, 2.3276e-03, 1.5096e-01, 2.9973e-01, 2.6018e-01,\n",
      "         2.3354e-01, 4.0089e-02, 1.1774e-02, 1.3677e-03],\n",
      "        [1.3946e-05, 1.0252e-05, 1.7176e-04, 2.6916e-02, 1.6827e-01, 2.4205e-01,\n",
      "         3.1996e-01, 1.6325e-01, 6.8056e-02, 1.1302e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.10, Train Loss: 0.00, Val Loss: 4.29, Train BLEU: 0.00, Val BLEU: 13.48, Minutes Elapsed: 266.19\n",
      "Sampling from val predictions...\n",
      "Source: mỗi đứa con_trai từ 6 đến 9 tuổi , trong\n",
      "Reference: so a boy , from six to nine years\n",
      "Model: <SOS> the &apos;s &apos;s of from a , , ,\n",
      "Attention Weights: tensor([[9.8775e-01, 1.2128e-02, 9.9590e-05, 2.0863e-05, 3.1163e-07, 5.0101e-08,\n",
      "         1.0471e-08, 9.0763e-09, 5.5003e-09, 3.0511e-09],\n",
      "        [2.5080e-01, 4.9459e-01, 2.1397e-01, 3.9195e-02, 1.0818e-03, 3.1832e-04,\n",
      "         1.7756e-05, 2.8237e-05, 1.7781e-06, 5.7852e-07],\n",
      "        [4.0584e-02, 1.5205e-01, 2.4513e-01, 4.4927e-01, 6.9929e-02, 3.0174e-02,\n",
      "         1.9766e-03, 1.0435e-02, 4.0753e-04, 3.9619e-05],\n",
      "        [2.7262e-02, 1.4237e-01, 2.7332e-01, 4.7715e-01, 5.5573e-02, 1.7941e-02,\n",
      "         1.1393e-03, 5.0939e-03, 1.4252e-04, 9.3496e-06],\n",
      "        [6.2242e-03, 3.5828e-02, 1.5652e-01, 6.9318e-01, 7.4349e-02, 2.8143e-02,\n",
      "         8.5723e-04, 4.7045e-03, 1.8068e-04, 1.1814e-05],\n",
      "        [2.3982e-05, 2.3553e-04, 9.9391e-03, 8.6946e-02, 1.8517e-01, 4.2223e-01,\n",
      "         4.3838e-02, 2.3531e-01, 1.4650e-02, 1.6506e-03],\n",
      "        [3.1182e-04, 2.9454e-03, 2.5939e-02, 1.2573e-01, 1.0383e-01, 2.3194e-01,\n",
      "         4.1234e-02, 4.3192e-01, 3.1896e-02, 4.2513e-03],\n",
      "        [3.6470e-05, 9.7954e-05, 5.7603e-04, 1.1225e-02, 2.3270e-02, 2.2494e-01,\n",
      "         3.1763e-02, 4.0260e-01, 2.1264e-01, 9.2855e-02],\n",
      "        [5.1333e-06, 1.1220e-05, 4.8731e-05, 6.5307e-04, 2.6212e-03, 5.7799e-02,\n",
      "         2.3219e-02, 1.7070e-01, 1.7321e-01, 5.7173e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.14, Train Loss: 0.00, Val Loss: 4.29, Train BLEU: 0.00, Val BLEU: 13.29, Minutes Elapsed: 268.68\n",
      "Sampling from val predictions...\n",
      "Source: nhưng sư_tử rất thông_minh . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: but lions are very clever . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> but it &apos;s very . . <EOS> <EOS> .\n",
      "Attention Weights: tensor([[0.5348, 0.0053, 0.4591, 0.0008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1524, 0.0152, 0.8253, 0.0069, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0303, 0.0038, 0.8676, 0.0961, 0.0022, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0201, 0.0019, 0.5485, 0.3520, 0.0760, 0.0014, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0038, 0.0004, 0.0771, 0.6736, 0.2276, 0.0175, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0604, 0.0107, 0.1821, 0.2853, 0.3424, 0.1191, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0764, 0.0105, 0.0924, 0.1413, 0.3310, 0.3483, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0091, 0.0022, 0.0705, 0.2877, 0.3078, 0.3228, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0081, 0.0017, 0.1157, 0.5072, 0.2301, 0.1373, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.19, Train Loss: 0.00, Val Loss: 4.30, Train BLEU: 0.00, Val BLEU: 13.33, Minutes Elapsed: 271.16\n",
      "Sampling from val predictions...\n",
      "Source: à . anh ta đang sẵn_sàng . <EOS> <PAD> <PAD>\n",
      "Reference: ha . he &apos;s ready . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> okay . he &apos;s a . <EOS> <EOS> .\n",
      "Attention Weights: tensor([[0.9472, 0.0489, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0386, 0.6582, 0.2927, 0.0079, 0.0025, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0023, 0.0744, 0.8736, 0.0240, 0.0230, 0.0026, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0002, 0.0092, 0.0361, 0.0502, 0.8558, 0.0484, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0008, 0.0074, 0.0048, 0.0127, 0.5932, 0.3753, 0.0057, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0001, 0.0026, 0.0054, 0.0018, 0.0326, 0.8456, 0.1092, 0.0027, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0020, 0.0160, 0.0744, 0.0088, 0.0383, 0.6438, 0.1848, 0.0319, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0002, 0.0013, 0.0145, 0.0304, 0.1417, 0.5768, 0.1895, 0.0457, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0001, 0.0002, 0.0029, 0.0140, 0.6689, 0.3096, 0.0040, 0.0004, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.24, Train Loss: 0.00, Val Loss: 4.29, Train BLEU: 0.00, Val BLEU: 13.22, Minutes Elapsed: 273.65\n",
      "Sampling from val predictions...\n",
      "Source: vậy_nên vào năm 1860 , họ nhìn_thấy cái công_nghệ <UNK>\n",
      "Reference: so <UNK> , they are seeing this dirty technology\n",
      "Model: <SOS> so said said , they the the , ,\n",
      "Attention Weights: tensor([[9.8553e-01, 1.4432e-02, 4.2571e-05, 1.6092e-07, 1.5849e-08, 4.3242e-09,\n",
      "         1.0832e-09, 2.7161e-10, 8.0916e-11, 4.1352e-11],\n",
      "        [2.9514e-01, 5.7967e-01, 1.2367e-01, 1.2513e-03, 2.1125e-04, 4.0270e-05,\n",
      "         1.0646e-05, 1.7036e-06, 4.3468e-07, 1.4829e-07],\n",
      "        [3.7596e-02, 1.9989e-01, 6.1616e-01, 7.0895e-02, 5.3986e-02, 1.7886e-02,\n",
      "         3.2297e-03, 3.2150e-04, 3.4137e-05, 2.4800e-06],\n",
      "        [4.9774e-03, 3.2182e-02, 1.6026e-01, 8.8800e-02, 4.1738e-01, 2.3404e-01,\n",
      "         5.4145e-02, 7.2902e-03, 8.8223e-04, 4.4061e-05],\n",
      "        [1.4485e-05, 1.1564e-04, 4.8870e-03, 2.1064e-02, 1.2697e-01, 6.7264e-01,\n",
      "         1.6622e-01, 6.6943e-03, 1.2888e-03, 1.1388e-04],\n",
      "        [3.6121e-07, 2.5085e-06, 2.4812e-05, 5.5632e-05, 6.3369e-03, 6.5375e-02,\n",
      "         8.2980e-01, 9.1207e-02, 6.8854e-03, 3.1142e-04],\n",
      "        [1.5643e-07, 1.2839e-06, 7.1344e-05, 6.9728e-04, 1.4145e-02, 9.7704e-02,\n",
      "         1.9447e-01, 4.1227e-01, 2.4312e-01, 3.7526e-02],\n",
      "        [1.3444e-06, 6.5133e-06, 1.5966e-04, 9.6624e-04, 1.9395e-02, 2.9212e-01,\n",
      "         3.8340e-01, 1.5421e-01, 1.2221e-01, 2.7522e-02],\n",
      "        [4.7130e-08, 3.8253e-07, 1.0510e-05, 4.9710e-05, 2.8755e-03, 1.6687e-01,\n",
      "         4.6562e-01, 2.3153e-01, 1.0789e-01, 2.5148e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.29, Train Loss: 0.00, Val Loss: 4.29, Train BLEU: 0.00, Val BLEU: 13.31, Minutes Elapsed: 276.12\n",
      "Sampling from val predictions...\n",
      "Source: mỗi bức ảnh gợi nhắc về ai đó hoặc một\n",
      "Reference: a photo is a reminder of someone or something\n",
      "Model: <SOS> the is of the in of people who a\n",
      "Attention Weights: tensor([[9.7459e-01, 2.4798e-02, 5.6390e-04, 3.7912e-05, 4.9500e-06, 1.1727e-06,\n",
      "         4.2624e-07, 1.6915e-07, 1.0535e-07, 5.8208e-08],\n",
      "        [1.9537e-01, 4.6537e-01, 3.0366e-01, 2.9809e-02, 5.0714e-03, 6.1060e-04,\n",
      "         8.6709e-05, 1.3218e-05, 4.3838e-06, 6.8984e-07],\n",
      "        [5.2787e-02, 1.0938e-01, 2.3945e-01, 2.7658e-01, 1.4399e-01, 1.2875e-01,\n",
      "         3.5651e-02, 1.1623e-02, 1.6068e-03, 1.8102e-04],\n",
      "        [1.1824e-02, 3.6782e-02, 1.2744e-01, 3.2262e-01, 2.4991e-01, 1.8797e-01,\n",
      "         5.4765e-02, 7.9185e-03, 7.0036e-04, 6.4143e-05],\n",
      "        [4.0252e-04, 3.8724e-03, 3.0080e-02, 8.6760e-02, 2.0568e-01, 2.7380e-01,\n",
      "         2.4370e-01, 1.3003e-01, 2.1026e-02, 4.6433e-03],\n",
      "        [2.0498e-04, 1.9208e-03, 1.2201e-02, 5.4296e-02, 1.6851e-01, 2.8807e-01,\n",
      "         2.5159e-01, 1.7451e-01, 3.7756e-02, 1.0943e-02],\n",
      "        [1.2584e-04, 6.3685e-04, 3.5871e-03, 2.4132e-02, 9.1496e-02, 2.8430e-01,\n",
      "         3.2361e-01, 2.1850e-01, 4.1824e-02, 1.1784e-02],\n",
      "        [8.0879e-06, 1.0155e-04, 1.0421e-03, 6.8423e-03, 3.5534e-02, 1.6282e-01,\n",
      "         2.5180e-01, 3.4155e-01, 1.3867e-01, 6.1627e-02],\n",
      "        [8.3383e-06, 4.1357e-05, 3.0468e-04, 2.7766e-03, 1.3363e-02, 1.0130e-01,\n",
      "         2.6694e-01, 3.7925e-01, 1.6312e-01, 7.2899e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.34, Train Loss: 0.00, Val Loss: 4.30, Train BLEU: 0.00, Val BLEU: 12.92, Minutes Elapsed: 278.61\n",
      "Sampling from val predictions...\n",
      "Source: <UNK> tin_tưởng tôi , như một nhà_văn và một phụ_nữ\n",
      "Reference: <UNK> believed in me , as a writer and\n",
      "Model: <SOS> <UNK> <UNK> me like as like a and and\n",
      "Attention Weights: tensor([[7.1335e-01, 2.8125e-01, 5.2888e-03, 8.0600e-05, 3.1310e-05, 4.9877e-06,\n",
      "         1.1409e-06, 2.8670e-07, 2.3939e-07, 1.5209e-07],\n",
      "        [4.4053e-02, 8.8482e-01, 6.6360e-02, 4.1719e-03, 5.0492e-04, 6.8169e-05,\n",
      "         1.8254e-05, 1.3527e-06, 3.4441e-07, 3.0520e-07],\n",
      "        [1.2261e-02, 3.9613e-01, 4.0721e-01, 1.4140e-01, 3.6311e-02, 5.7020e-03,\n",
      "         9.1473e-04, 6.4493e-05, 6.7203e-06, 1.3074e-06],\n",
      "        [4.4712e-04, 4.8909e-02, 1.0417e-01, 1.0150e-01, 6.6397e-01, 6.0897e-02,\n",
      "         1.8190e-02, 1.6467e-03, 2.3602e-04, 2.6560e-05],\n",
      "        [2.4337e-04, 2.5300e-02, 6.7041e-02, 3.0061e-02, 6.7201e-01, 1.7409e-01,\n",
      "         2.7493e-02, 2.1408e-03, 1.4995e-03, 1.2150e-04],\n",
      "        [9.5776e-05, 3.3571e-03, 1.9245e-02, 3.5544e-02, 7.5267e-01, 1.0605e-01,\n",
      "         4.6438e-02, 1.3256e-02, 2.0940e-02, 2.4040e-03],\n",
      "        [1.8145e-05, 1.2647e-03, 3.7457e-03, 5.5942e-03, 2.3439e-01, 4.9110e-01,\n",
      "         2.0022e-01, 2.1362e-02, 3.5214e-02, 7.0997e-03],\n",
      "        [4.3068e-06, 5.9859e-04, 3.2921e-04, 7.0715e-04, 4.6740e-02, 2.5145e-01,\n",
      "         5.9217e-01, 1.4428e-02, 3.0309e-02, 6.3267e-02],\n",
      "        [9.9127e-06, 2.2085e-04, 1.8393e-03, 1.5063e-02, 2.4787e-01, 1.0613e-01,\n",
      "         2.2530e-01, 2.0033e-01, 1.3413e-01, 6.9099e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.38, Train Loss: 0.00, Val Loss: 4.29, Train BLEU: 0.00, Val BLEU: 13.36, Minutes Elapsed: 281.09\n",
      "Sampling from val predictions...\n",
      "Source: bức này được chụp vài tuần sau sự_kiện 11/9 ,\n",
      "Reference: this one was taken just weeks after 9 /\n",
      "Model: <SOS> this this was a years a years the 9\n",
      "Attention Weights: tensor([[9.1390e-01, 7.4322e-02, 1.1164e-02, 5.7150e-04, 3.7756e-05, 4.9785e-07,\n",
      "         4.7515e-08, 6.1627e-09, 1.5651e-09, 8.4159e-10],\n",
      "        [4.6700e-01, 1.1867e-01, 3.2441e-01, 8.2772e-02, 6.7822e-03, 3.2836e-04,\n",
      "         3.1547e-05, 5.1277e-06, 7.7101e-07, 3.9223e-08],\n",
      "        [6.5855e-02, 3.1198e-02, 4.8559e-01, 2.8721e-01, 1.2000e-01, 8.7079e-03,\n",
      "         1.2344e-03, 1.8176e-04, 2.7168e-05, 1.3570e-06],\n",
      "        [4.5943e-03, 2.1148e-03, 9.1001e-02, 2.6813e-01, 5.5518e-01, 6.8126e-02,\n",
      "         9.5234e-03, 1.1347e-03, 1.8872e-04, 9.1821e-06],\n",
      "        [6.1100e-04, 3.4315e-04, 1.1505e-02, 6.3685e-02, 5.0224e-01, 3.4429e-01,\n",
      "         5.9378e-02, 1.5777e-02, 2.0253e-03, 1.4876e-04],\n",
      "        [4.5849e-04, 3.2203e-04, 4.0185e-03, 2.4258e-02, 4.1330e-01, 4.2599e-01,\n",
      "         9.6259e-02, 3.1452e-02, 3.7325e-03, 2.0982e-04],\n",
      "        [7.5940e-05, 6.6457e-05, 7.4573e-04, 3.6635e-03, 1.2039e-01, 3.8693e-01,\n",
      "         2.3063e-01, 2.2691e-01, 2.8423e-02, 2.1633e-03],\n",
      "        [1.2982e-05, 1.2212e-05, 1.8506e-04, 9.0615e-04, 3.4296e-02, 1.9638e-01,\n",
      "         2.1247e-01, 4.7542e-01, 7.1018e-02, 9.2988e-03],\n",
      "        [9.9492e-07, 1.2924e-06, 3.0506e-05, 1.3698e-04, 4.0849e-03, 2.8327e-02,\n",
      "         8.4914e-02, 6.0455e-01, 1.8346e-01, 9.4499e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.43, Train Loss: 0.00, Val Loss: 4.26, Train BLEU: 0.00, Val BLEU: 13.82, Minutes Elapsed: 283.61\n",
      "Sampling from val predictions...\n",
      "Source: và các bạn có bao_giờ tự_hỏi chính mình \" nếu\n",
      "Reference: and if you ever are wondering , &quot; if\n",
      "Model: <SOS> and you you know the what &quot; &quot; if\n",
      "Attention Weights: tensor([[1.7627e-02, 8.2347e-01, 1.5594e-01, 2.8937e-03, 6.9489e-05, 1.3703e-06,\n",
      "         3.2714e-08, 6.1678e-09, 2.4494e-09, 1.3114e-09],\n",
      "        [4.1257e-03, 3.5796e-01, 5.6504e-01, 6.7658e-02, 5.0298e-03, 1.8136e-04,\n",
      "         2.3647e-06, 1.8239e-07, 3.5993e-08, 3.5322e-09],\n",
      "        [3.5956e-05, 1.5986e-03, 5.5932e-02, 4.1835e-01, 5.0985e-01, 1.4132e-02,\n",
      "         9.1097e-05, 6.2220e-06, 2.4681e-07, 1.6163e-08],\n",
      "        [3.1365e-05, 7.1221e-04, 9.6771e-03, 8.8877e-02, 5.3494e-01, 3.5518e-01,\n",
      "         9.4383e-03, 1.0865e-03, 5.6291e-05, 4.2160e-06],\n",
      "        [7.5729e-05, 8.3919e-04, 7.6695e-03, 7.5843e-02, 3.5083e-01, 5.0649e-01,\n",
      "         4.5034e-02, 1.1264e-02, 1.8772e-03, 8.4814e-05],\n",
      "        [1.3962e-04, 1.9971e-04, 3.1538e-04, 7.4066e-04, 4.6078e-02, 5.5138e-01,\n",
      "         2.8321e-01, 9.6703e-02, 1.6316e-02, 4.9127e-03],\n",
      "        [8.2410e-06, 3.1360e-05, 6.7724e-05, 1.7475e-04, 2.1821e-03, 6.0886e-02,\n",
      "         2.0802e-01, 3.6195e-01, 2.2051e-01, 1.4617e-01],\n",
      "        [2.9315e-05, 6.1758e-05, 8.9409e-05, 9.4668e-05, 8.1233e-04, 1.2052e-02,\n",
      "         6.2305e-02, 1.9548e-01, 2.7571e-01, 4.5336e-01],\n",
      "        [2.2766e-07, 1.4564e-06, 8.3599e-06, 2.3175e-05, 1.3638e-04, 1.9609e-03,\n",
      "         2.1319e-02, 1.0409e-01, 2.0108e-01, 6.7138e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.48, Train Loss: 0.00, Val Loss: 4.25, Train BLEU: 0.00, Val BLEU: 13.55, Minutes Elapsed: 286.08\n",
      "Sampling from val predictions...\n",
      "Source: hai điều quan_trọng nhất chúng_ta có là thời_gian và mối\n",
      "Reference: two of the most valuable things we have are\n",
      "Model: <SOS> two most most most that we we have is\n",
      "Attention Weights: tensor([[7.3259e-01, 2.6308e-01, 4.2634e-03, 6.5064e-05, 2.5130e-06, 4.1652e-07,\n",
      "         6.4716e-08, 2.3006e-08, 5.7342e-09, 3.2298e-09],\n",
      "        [1.3741e-02, 5.4536e-01, 4.1109e-01, 2.9240e-02, 5.1533e-04, 4.0961e-05,\n",
      "         1.1509e-05, 6.4583e-06, 4.1888e-07, 1.2392e-07],\n",
      "        [1.3778e-02, 3.1500e-01, 4.6309e-01, 1.7652e-01, 1.8744e-02, 9.3885e-03,\n",
      "         2.7317e-03, 7.0269e-04, 3.3032e-05, 3.6007e-06],\n",
      "        [2.7586e-04, 1.8256e-02, 1.6578e-01, 4.8406e-01, 2.1476e-01, 7.9888e-02,\n",
      "         2.4739e-02, 1.1608e-02, 5.5003e-04, 8.3663e-05],\n",
      "        [4.8786e-04, 2.3796e-02, 5.6241e-02, 3.7643e-01, 2.8254e-01, 2.1001e-01,\n",
      "         3.7822e-02, 1.1976e-02, 6.3745e-04, 6.1090e-05],\n",
      "        [6.0066e-04, 2.4661e-02, 5.4286e-02, 4.0600e-01, 2.2288e-01, 2.4145e-01,\n",
      "         3.6283e-02, 1.3045e-02, 7.0370e-04, 8.7036e-05],\n",
      "        [1.6844e-04, 7.7226e-03, 3.8651e-02, 2.1210e-01, 1.6224e-01, 4.5476e-01,\n",
      "         8.1493e-02, 4.0170e-02, 2.2740e-03, 4.2534e-04],\n",
      "        [6.1200e-05, 2.2093e-03, 1.8062e-02, 9.1589e-02, 8.6958e-02, 4.6523e-01,\n",
      "         1.4649e-01, 1.6376e-01, 1.5452e-02, 1.0180e-02],\n",
      "        [1.0204e-05, 3.0729e-04, 7.1166e-03, 2.5369e-02, 2.7929e-02, 1.7204e-01,\n",
      "         2.0833e-01, 5.2302e-01, 2.4284e-02, 1.1606e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.53, Train Loss: 0.00, Val Loss: 4.27, Train BLEU: 0.00, Val BLEU: 13.29, Minutes Elapsed: 288.54\n",
      "Sampling from val predictions...\n",
      "Source: em không nói được , nhưng em truyền_đạt niềm_vui theo\n",
      "Reference: he &apos;s <UNK> , but he communicates joy in\n",
      "Model: <SOS> he don &apos;t saying , but the to .\n",
      "Attention Weights: tensor([[9.5321e-01, 4.6575e-02, 2.0316e-04, 1.3195e-05, 2.9349e-07, 3.6517e-08,\n",
      "         6.6555e-09, 1.0601e-08, 2.4377e-09, 7.0583e-10],\n",
      "        [1.2465e-02, 9.0829e-01, 7.4904e-02, 4.2423e-03, 9.9313e-05, 1.8091e-06,\n",
      "         3.0432e-07, 2.2013e-07, 1.9070e-08, 3.4222e-09],\n",
      "        [1.8304e-02, 7.2320e-01, 1.9153e-01, 6.1535e-02, 5.1218e-03, 2.3747e-04,\n",
      "         3.0816e-05, 4.0332e-05, 2.1435e-06, 3.7987e-07],\n",
      "        [5.1441e-02, 1.7014e-01, 3.6884e-01, 3.7150e-01, 2.7961e-02, 8.2541e-03,\n",
      "         1.0404e-03, 7.2364e-04, 8.0293e-05, 1.3577e-05],\n",
      "        [2.0863e-02, 8.5155e-02, 1.3034e-01, 5.0857e-01, 1.2516e-01, 1.1723e-01,\n",
      "         8.7700e-03, 3.3993e-03, 4.0381e-04, 1.0464e-04],\n",
      "        [3.9137e-05, 4.6581e-04, 1.0132e-03, 5.9058e-03, 1.1551e-02, 8.1936e-01,\n",
      "         1.0629e-01, 3.6394e-02, 9.4383e-03, 9.5456e-03],\n",
      "        [3.4353e-06, 2.7786e-04, 4.9050e-04, 1.4148e-03, 1.1217e-03, 9.5861e-03,\n",
      "         1.0819e-01, 5.4085e-01, 2.0895e-01, 1.2912e-01],\n",
      "        [4.4143e-07, 4.8284e-05, 3.5008e-04, 2.1916e-03, 4.6327e-03, 8.4191e-03,\n",
      "         4.3068e-02, 2.1409e-01, 3.1186e-01, 4.1534e-01],\n",
      "        [5.3907e-07, 7.6370e-06, 4.3451e-05, 8.1653e-04, 5.9715e-03, 3.6289e-02,\n",
      "         2.1046e-02, 7.6583e-02, 2.8117e-01, 5.7807e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.58, Train Loss: 0.00, Val Loss: 4.24, Train BLEU: 0.00, Val BLEU: 13.37, Minutes Elapsed: 291.02\n",
      "Sampling from val predictions...\n",
      "Source: nó là nhựa xốp , như tôi đã nói ,\n",
      "Reference: it &apos;s a porous asphalt , like i said\n",
      "Model: <SOS> it &apos;s a , , but i i said\n",
      "Attention Weights: tensor([[9.1883e-01, 7.4521e-02, 5.8769e-03, 7.4529e-04, 2.0742e-05, 4.6568e-06,\n",
      "         1.5029e-06, 4.5711e-07, 1.1703e-07, 4.6865e-08],\n",
      "        [4.7247e-02, 8.6124e-01, 8.5180e-02, 6.3040e-03, 2.2297e-05, 4.2974e-06,\n",
      "         9.0730e-07, 3.4296e-07, 5.2972e-08, 6.1196e-09],\n",
      "        [9.3792e-03, 3.9573e-02, 6.4511e-01, 2.9835e-01, 5.8816e-03, 1.1847e-03,\n",
      "         2.2513e-04, 2.3012e-04, 5.9591e-05, 4.7371e-06],\n",
      "        [7.0504e-04, 2.4206e-03, 1.5258e-01, 7.9682e-01, 4.1483e-02, 5.0310e-03,\n",
      "         5.8449e-04, 2.5707e-04, 9.2591e-05, 2.1827e-05],\n",
      "        [1.4272e-03, 4.2949e-03, 3.0613e-02, 1.5279e-01, 2.8480e-01, 5.0272e-01,\n",
      "         1.7243e-02, 3.9308e-03, 1.8997e-03, 2.7784e-04],\n",
      "        [8.5187e-04, 3.2427e-03, 8.8100e-03, 5.3601e-02, 7.6773e-02, 8.3882e-01,\n",
      "         1.5240e-02, 2.3347e-03, 2.7486e-04, 5.0421e-05],\n",
      "        [3.0824e-05, 3.8276e-04, 2.9331e-03, 9.5226e-03, 5.2094e-03, 6.0762e-01,\n",
      "         3.2345e-01, 4.9364e-02, 1.4402e-03, 4.7815e-05],\n",
      "        [8.1160e-06, 1.9794e-04, 4.3018e-03, 1.3572e-02, 4.5679e-03, 4.3221e-01,\n",
      "         4.2473e-01, 1.1500e-01, 5.1828e-03, 2.2842e-04],\n",
      "        [7.6194e-05, 4.9557e-03, 3.9062e-02, 1.8863e-02, 2.8728e-03, 2.0968e-02,\n",
      "         8.3997e-02, 7.5857e-01, 6.9482e-02, 1.1570e-03]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.62, Train Loss: 0.00, Val Loss: 4.25, Train BLEU: 0.00, Val BLEU: 13.81, Minutes Elapsed: 293.52\n",
      "Sampling from val predictions...\n",
      "Source: đọc nó từ một người phụ_nữ châu phi , những\n",
      "Reference: read it from an african woman , the damage\n",
      "Model: <SOS> now it from a woman , , , the\n",
      "Attention Weights: tensor([[9.6350e-01, 3.5716e-02, 7.7319e-04, 6.0623e-06, 8.1880e-08, 2.1072e-08,\n",
      "         5.1887e-09, 4.2386e-09, 7.6857e-10, 3.8609e-10],\n",
      "        [1.7406e-01, 5.5660e-01, 2.6327e-01, 5.9624e-03, 9.2237e-05, 1.7286e-05,\n",
      "         1.2465e-06, 1.2590e-07, 2.1837e-09, 5.0639e-10],\n",
      "        [5.1527e-02, 8.0515e-02, 6.1391e-01, 2.1004e-01, 3.4764e-02, 8.0128e-03,\n",
      "         1.1571e-03, 6.6584e-05, 3.8546e-07, 5.7777e-08],\n",
      "        [1.2008e-04, 2.5617e-04, 1.9664e-02, 2.2101e-01, 4.7469e-01, 1.9660e-01,\n",
      "         8.3106e-02, 4.5129e-03, 4.0033e-05, 3.7961e-06],\n",
      "        [6.8195e-05, 1.2446e-04, 9.1695e-03, 8.1123e-02, 2.3135e-01, 2.9986e-01,\n",
      "         3.3623e-01, 4.0223e-02, 1.7374e-03, 1.1246e-04],\n",
      "        [1.7619e-04, 1.2716e-04, 1.4372e-03, 6.6198e-03, 2.6567e-02, 8.8793e-02,\n",
      "         3.7750e-01, 3.8301e-01, 1.0563e-01, 1.0140e-02],\n",
      "        [4.3328e-04, 1.3730e-04, 8.6471e-04, 3.6533e-03, 9.3614e-03, 3.8528e-02,\n",
      "         1.9081e-01, 4.5720e-01, 2.2874e-01, 7.0262e-02],\n",
      "        [1.1309e-04, 4.2029e-05, 3.4820e-04, 1.2328e-03, 2.4401e-03, 1.6665e-02,\n",
      "         2.6684e-02, 5.8366e-02, 2.1919e-01, 6.7492e-01],\n",
      "        [7.4633e-06, 2.8871e-06, 1.1817e-04, 5.8972e-04, 1.7450e-03, 1.1288e-02,\n",
      "         1.6703e-02, 3.0511e-02, 6.3248e-02, 8.7579e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.67, Train Loss: 0.00, Val Loss: 4.25, Train BLEU: 0.00, Val BLEU: 13.52, Minutes Elapsed: 296.04\n",
      "Sampling from val predictions...\n",
      "Source: cộng_đồng của tôi , người maasai , tin rằng chúng_tôi\n",
      "Reference: my community , the maasai , we believe that\n",
      "Model: <SOS> my my , , , , , believe that\n",
      "Attention Weights: tensor([[9.1548e-01, 7.9990e-02, 4.5197e-03, 7.3401e-06, 1.3278e-06, 2.4497e-07,\n",
      "         2.7941e-08, 9.0947e-09, 7.2969e-09, 2.7765e-09],\n",
      "        [5.0389e-01, 3.7047e-01, 1.2034e-01, 4.0400e-03, 9.0819e-04, 3.3526e-04,\n",
      "         1.1337e-05, 1.5724e-06, 6.7872e-07, 1.9393e-07],\n",
      "        [4.4981e-02, 1.0631e-01, 4.2840e-01, 2.5555e-01, 1.2866e-01, 3.3424e-02,\n",
      "         2.5446e-03, 8.5172e-05, 4.5497e-05, 3.1053e-06],\n",
      "        [8.6478e-04, 2.1439e-03, 8.1465e-03, 3.5293e-02, 8.5410e-01, 9.0845e-02,\n",
      "         6.8660e-03, 1.4686e-03, 2.3591e-04, 4.0005e-05],\n",
      "        [1.6035e-03, 6.9032e-03, 1.8706e-02, 3.4794e-02, 5.2284e-01, 3.8285e-01,\n",
      "         2.5380e-02, 5.7279e-03, 1.0976e-03, 9.5784e-05],\n",
      "        [3.4758e-04, 1.4912e-03, 4.2020e-03, 1.1585e-02, 1.8829e-01, 1.9181e-01,\n",
      "         3.8415e-01, 1.1094e-01, 8.9962e-02, 1.7217e-02],\n",
      "        [6.2922e-05, 2.6641e-04, 1.0558e-03, 4.4701e-03, 1.5665e-01, 2.4904e-02,\n",
      "         1.9956e-01, 4.4596e-01, 1.3624e-01, 3.0835e-02],\n",
      "        [3.5462e-06, 1.5792e-05, 7.3825e-05, 6.5321e-04, 6.6951e-02, 6.7488e-02,\n",
      "         1.0034e-01, 2.8489e-01, 3.7179e-01, 1.0779e-01],\n",
      "        [2.2712e-06, 1.1357e-05, 4.8600e-05, 4.8274e-04, 8.2881e-03, 1.4035e-02,\n",
      "         7.5378e-02, 6.3082e-02, 3.7346e-01, 4.6522e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.72, Train Loss: 0.00, Val Loss: 4.22, Train BLEU: 0.00, Val BLEU: 13.66, Minutes Elapsed: 298.55\n",
      "Sampling from val predictions...\n",
      "Source: trong_suốt những năm_qua đã có sự tiến_bộ to_lớn trong quá_trình\n",
      "Reference: so great progress and treatment has been made over\n",
      "Model: <SOS> while the have have have are a a in\n",
      "Attention Weights: tensor([[9.2538e-01, 7.3918e-02, 6.5911e-04, 4.3386e-05, 1.6783e-06, 7.5514e-08,\n",
      "         2.0123e-08, 5.4420e-09, 3.2313e-09, 1.1211e-09],\n",
      "        [7.8446e-02, 7.9355e-01, 1.1568e-01, 1.1813e-02, 4.5259e-04, 4.9327e-05,\n",
      "         7.2179e-06, 1.3396e-06, 2.7955e-07, 6.1073e-08],\n",
      "        [7.0423e-03, 2.1553e-01, 4.8287e-01, 2.2814e-01, 4.0583e-02, 1.8884e-02,\n",
      "         6.4439e-03, 4.9113e-04, 1.6472e-05, 2.9586e-06],\n",
      "        [8.0316e-04, 4.3971e-02, 1.5783e-01, 4.9009e-01, 1.8865e-01, 9.1543e-02,\n",
      "         2.3221e-02, 3.6599e-03, 1.8535e-04, 4.7784e-05],\n",
      "        [4.3411e-05, 2.8524e-03, 4.6404e-02, 2.2984e-01, 2.6235e-01, 2.7231e-01,\n",
      "         1.4084e-01, 4.1293e-02, 2.9171e-03, 1.1462e-03],\n",
      "        [4.9148e-06, 3.4340e-04, 1.2211e-02, 2.8676e-02, 9.4001e-02, 2.3055e-01,\n",
      "         4.0298e-01, 1.6836e-01, 3.3878e-02, 2.8992e-02],\n",
      "        [2.8739e-05, 8.3836e-04, 1.4715e-02, 6.3873e-02, 2.0174e-01, 3.2806e-01,\n",
      "         2.6015e-01, 8.7506e-02, 2.0256e-02, 2.2820e-02],\n",
      "        [5.3389e-06, 9.4724e-05, 1.1094e-03, 5.9197e-03, 2.7616e-02, 8.6184e-02,\n",
      "         1.7871e-01, 1.7315e-01, 1.2293e-01, 4.0428e-01],\n",
      "        [3.5249e-05, 1.5432e-04, 7.1947e-04, 2.2614e-03, 6.5255e-03, 2.7752e-02,\n",
      "         1.0487e-01, 1.7255e-01, 1.7512e-01, 5.1001e-01]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=100, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=True, print_attn=True, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_learning_curve(experiment_results[0]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(experiment_results)[['best_val_loss', 'best_val_bleu', 'runtime', \n",
    "                                       'total_params', 'trainable_params', 'dt_created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model and test \n",
    "checkpoint = torch.load('model_checkpoints/{}.pth.tar'.format(MODEL_NAME), map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
