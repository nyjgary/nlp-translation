{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import evaluate, train_and_eval, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params required for generating data loaders \n",
    "\n",
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture params \n",
    "NETWORK_TYPE = 'rnn'\n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0 \n",
    "DEC_DROPOUT = 0  \n",
    "ATTENTION_TYPE = 'additive'\n",
    "\n",
    "# training params  \n",
    "NUM_EPOCHS = 10 \n",
    "LR = 0.00015 \n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = False\n",
    "\n",
    "# name the model and experiment \n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    EXPERIMENT_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    EXPERIMENT_NAME = '{}-cnn'.format(SRC_LANG)\n",
    "MODEL_NAME = '{}-{}'.format(EXPERIMENT_NAME, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n",
    "          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n",
    "          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n",
    "          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n",
    "          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n",
    "          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "if ATTENTION_TYPE == 'without': \n",
    "    # without attention \n",
    "    decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                         targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)\n",
    "    \n",
    "else: \n",
    "    # with attention \n",
    "    decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                             num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, \n",
    "                             src_max_sentence_len=SRC_MAX_SENTENCE_LEN, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                             dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                    vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.19, Train BLEU: 0.00, Val BLEU: 0.02, Minutes Elapsed: 0.12\n",
      "Sampling from val predictions...\n",
      "Source: 当 她们 走出 矿井 时 全身 <UNK> <EOS> <PAD> <PAD>\n",
      "Reference: when they came out of the shaft , they\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.1055, 0.1135, 0.1191, 0.1235, 0.1271, 0.1309, 0.1390, 0.1413, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1054, 0.1136, 0.1192, 0.1237, 0.1272, 0.1308, 0.1388, 0.1412, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1055, 0.1137, 0.1193, 0.1238, 0.1273, 0.1308, 0.1387, 0.1410, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1055, 0.1137, 0.1194, 0.1238, 0.1273, 0.1308, 0.1386, 0.1409, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1054, 0.1137, 0.1194, 0.1238, 0.1274, 0.1308, 0.1386, 0.1408, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1054, 0.1137, 0.1194, 0.1239, 0.1274, 0.1309, 0.1386, 0.1408, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1054, 0.1137, 0.1194, 0.1239, 0.1274, 0.1309, 0.1386, 0.1408, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1054, 0.1137, 0.1194, 0.1239, 0.1274, 0.1309, 0.1386, 0.1408, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1054, 0.1137, 0.1194, 0.1239, 0.1274, 0.1309, 0.1386, 0.1408, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 尽管 他用 <UNK> 的 枪 指着 我 的 头 把\n",
      "Reference: even though he held those loaded guns to my\n",
      "Model: <SOS> censorship the the the the the the the the\n",
      "Attention Weights: tensor([[0.0813, 0.0870, 0.0954, 0.0969, 0.0998, 0.1020, 0.1053, 0.1082, 0.1114,\n",
      "         0.1127],\n",
      "        [0.0811, 0.0869, 0.0952, 0.0969, 0.0999, 0.1021, 0.1054, 0.1083, 0.1115,\n",
      "         0.1127],\n",
      "        [0.0811, 0.0869, 0.0952, 0.0969, 0.0999, 0.1021, 0.1054, 0.1083, 0.1115,\n",
      "         0.1127],\n",
      "        [0.0810, 0.0869, 0.0952, 0.0969, 0.0999, 0.1021, 0.1055, 0.1084, 0.1115,\n",
      "         0.1126],\n",
      "        [0.0810, 0.0868, 0.0952, 0.0969, 0.0999, 0.1021, 0.1055, 0.1084, 0.1115,\n",
      "         0.1126],\n",
      "        [0.0810, 0.0868, 0.0952, 0.0969, 0.0999, 0.1021, 0.1055, 0.1084, 0.1115,\n",
      "         0.1126],\n",
      "        [0.0810, 0.0868, 0.0952, 0.0969, 0.0999, 0.1021, 0.1055, 0.1084, 0.1115,\n",
      "         0.1126],\n",
      "        [0.0810, 0.0868, 0.0952, 0.0969, 0.0999, 0.1021, 0.1055, 0.1084, 0.1115,\n",
      "         0.1127],\n",
      "        [0.0810, 0.0868, 0.0952, 0.0969, 0.1000, 0.1022, 0.1055, 0.1084, 0.1115,\n",
      "         0.1126]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.15, Train Loss: 0.00, Val Loss: 6.09, Train BLEU: 0.00, Val BLEU: 3.09, Minutes Elapsed: 11.59\n",
      "Sampling from val predictions...\n",
      "Source: 所以 我们 做 的 是 把 这些 光子 送进 <UNK>\n",
      "Reference: so , what we do is we send that\n",
      "Model: <SOS> and &apos;s , , , , , , <EOS>\n",
      "Attention Weights: tensor([[2.7160e-04, 9.5680e-03, 1.2979e-01, 3.6054e-01, 3.5372e-01, 1.3080e-01,\n",
      "         1.4057e-02, 1.1522e-03, 9.6778e-05, 3.4403e-06],\n",
      "        [5.2004e-03, 4.1515e-02, 1.9378e-01, 3.3366e-01, 2.8401e-01, 1.1798e-01,\n",
      "         2.0304e-02, 3.0078e-03, 4.8398e-04, 5.1066e-05],\n",
      "        [9.1988e-03, 5.3768e-02, 1.9588e-01, 3.1063e-01, 2.7033e-01, 1.2683e-01,\n",
      "         2.7439e-02, 4.9392e-03, 8.9036e-04, 9.7347e-05],\n",
      "        [3.5973e-03, 3.1496e-02, 1.5299e-01, 3.0042e-01, 3.0320e-01, 1.6220e-01,\n",
      "         3.8657e-02, 6.5667e-03, 8.3838e-04, 3.6333e-05],\n",
      "        [1.0762e-03, 1.7100e-02, 1.2479e-01, 2.9882e-01, 3.2750e-01, 1.7960e-01,\n",
      "         4.2663e-02, 7.4533e-03, 9.6000e-04, 2.8844e-05],\n",
      "        [5.5979e-04, 1.2427e-02, 1.1312e-01, 2.9737e-01, 3.3724e-01, 1.8627e-01,\n",
      "         4.3972e-02, 7.8869e-03, 1.1238e-03, 3.8959e-05],\n",
      "        [4.7375e-04, 1.1220e-02, 1.0859e-01, 2.9502e-01, 3.3979e-01, 1.8952e-01,\n",
      "         4.5472e-02, 8.5129e-03, 1.3479e-03, 6.0376e-05],\n",
      "        [4.7886e-04, 1.1025e-02, 1.0680e-01, 2.9282e-01, 3.3977e-01, 1.9128e-01,\n",
      "         4.6947e-02, 9.2011e-03, 1.5907e-03, 8.7340e-05],\n",
      "        [5.0000e-04, 1.1072e-02, 1.0599e-01, 2.9113e-01, 3.3914e-01, 1.9234e-01,\n",
      "         4.8139e-02, 9.7842e-03, 1.8001e-03, 1.1310e-04]])\n",
      "\n",
      "Source: 微波 微波炉 跟 感应 <UNK> 电机 的 原理 类似 <EOS>\n",
      "Reference: so a microwave is a similar system . <EOS>\n",
      "Model: <SOS> and &apos;s , the the , , , <EOS>\n",
      "Attention Weights: tensor([[2.3512e-05, 9.0799e-05, 7.0783e-04, 2.2591e-04, 1.3468e-05, 6.5143e-01,\n",
      "         3.3139e-01, 1.5424e-02, 6.8969e-04, 9.7050e-06],\n",
      "        [1.0841e-03, 2.0589e-03, 6.5959e-03, 2.2830e-03, 3.1687e-04, 6.3251e-01,\n",
      "         3.2417e-01, 2.8221e-02, 2.6213e-03, 1.3823e-04],\n",
      "        [2.2386e-03, 3.9975e-03, 1.1084e-02, 4.0691e-03, 5.8954e-04, 6.0583e-01,\n",
      "         3.3125e-01, 3.6836e-02, 3.8897e-03, 2.0960e-04],\n",
      "        [4.8970e-04, 1.2697e-03, 5.0477e-03, 2.2371e-03, 2.2757e-04, 5.8934e-01,\n",
      "         3.5744e-01, 4.0963e-02, 2.9430e-03, 4.3050e-05],\n",
      "        [9.8545e-05, 3.6835e-04, 2.1645e-03, 1.3019e-03, 1.4875e-04, 5.7580e-01,\n",
      "         3.7078e-01, 4.5625e-02, 3.6724e-03, 3.4920e-05],\n",
      "        [6.0335e-05, 2.5164e-04, 1.6664e-03, 1.1693e-03, 1.6465e-04, 5.6871e-01,\n",
      "         3.7478e-01, 4.8576e-02, 4.5590e-03, 5.8079e-05],\n",
      "        [5.9666e-05, 2.5022e-04, 1.6434e-03, 1.2484e-03, 2.0706e-04, 5.6289e-01,\n",
      "         3.7664e-01, 5.1461e-02, 5.5016e-03, 9.8756e-05],\n",
      "        [6.7599e-05, 2.7673e-04, 1.7433e-03, 1.3839e-03, 2.5683e-04, 5.5888e-01,\n",
      "         3.7709e-01, 5.3809e-02, 6.3512e-03, 1.4696e-04],\n",
      "        [7.7598e-05, 3.0920e-04, 1.8686e-03, 1.5239e-03, 3.0568e-04, 5.5604e-01,\n",
      "         3.7694e-01, 5.5671e-02, 7.0711e-03, 1.9536e-04]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.30, Train Loss: 0.00, Val Loss: 5.96, Train BLEU: 0.00, Val BLEU: 4.08, Minutes Elapsed: 22.90\n",
      "Sampling from val predictions...\n",
      "Source: 因为 我 的 家人 不 懂 中文 所以 我 得\n",
      "Reference: since my family couldn &apos;t speak chinese , i\n",
      "Model: <SOS> and , , , , the to you .\n",
      "Attention Weights: tensor([[4.4054e-07, 2.8611e-07, 4.6611e-07, 1.9134e-06, 8.0812e-07, 1.1240e-06,\n",
      "         4.1567e-06, 2.9040e-05, 8.4442e-03, 9.9152e-01],\n",
      "        [3.4582e-06, 2.0243e-06, 2.8321e-06, 8.2715e-06, 4.1686e-06, 5.4247e-06,\n",
      "         1.4821e-05, 6.5417e-05, 7.8933e-03, 9.9200e-01],\n",
      "        [1.0757e-06, 7.3076e-07, 9.7131e-07, 2.4252e-06, 1.4228e-06, 1.9047e-06,\n",
      "         5.1795e-06, 3.0132e-05, 5.7557e-03, 9.9420e-01],\n",
      "        [6.2391e-07, 3.8020e-07, 5.2454e-07, 1.4449e-06, 7.9848e-07, 1.1010e-06,\n",
      "         3.2980e-06, 2.1118e-05, 4.9165e-03, 9.9505e-01],\n",
      "        [4.3418e-06, 3.2874e-06, 4.1031e-06, 7.7241e-06, 5.6632e-06, 7.2806e-06,\n",
      "         1.6164e-05, 7.2017e-05, 6.8402e-03, 9.9304e-01],\n",
      "        [1.4373e-06, 1.3929e-06, 1.6814e-06, 2.6030e-06, 2.3982e-06, 3.2393e-06,\n",
      "         7.3228e-06, 4.6972e-05, 7.1810e-03, 9.9275e-01],\n",
      "        [1.0838e-05, 1.0640e-05, 1.2400e-05, 1.7545e-05, 1.6304e-05, 2.0371e-05,\n",
      "         3.7631e-05, 1.5383e-04, 9.8227e-03, 9.8990e-01],\n",
      "        [2.6841e-05, 2.6685e-05, 3.0483e-05, 4.1078e-05, 3.8731e-05, 4.7084e-05,\n",
      "         8.0699e-05, 2.8368e-04, 1.2599e-02, 9.8683e-01],\n",
      "        [5.6115e-05, 5.6453e-05, 6.3875e-05, 8.3655e-05, 7.9446e-05, 9.4538e-05,\n",
      "         1.5256e-04, 4.6861e-04, 1.5209e-02, 9.8374e-01]])\n",
      "\n",
      "Source: 还有 一次 当 我 在 洛杉矶 <UNK> 市中心 中心 的\n",
      "Reference: there &apos;s another time when i put a garden\n",
      "Model: <SOS> and , , to of , , to .\n",
      "Attention Weights: tensor([[1.5803e-08, 1.5674e-08, 4.8186e-09, 3.6806e-09, 2.2295e-09, 1.7556e-09,\n",
      "         4.7627e-10, 3.1204e-05, 4.7795e-03, 9.9519e-01],\n",
      "        [3.5375e-07, 2.9733e-07, 1.1649e-07, 1.0827e-07, 9.9811e-08, 1.4000e-07,\n",
      "         8.0021e-08, 5.6996e-05, 3.9886e-03, 9.9595e-01],\n",
      "        [1.0871e-07, 9.6378e-08, 5.1780e-08, 5.2270e-08, 5.7401e-08, 1.0102e-07,\n",
      "         7.9288e-08, 1.8889e-05, 2.7323e-03, 9.9725e-01],\n",
      "        [7.2246e-08, 5.9450e-08, 2.7409e-08, 2.9236e-08, 3.6076e-08, 8.6288e-08,\n",
      "         7.9711e-08, 1.4231e-05, 2.3891e-03, 9.9760e-01],\n",
      "        [8.0680e-08, 8.0204e-08, 6.4032e-08, 7.2343e-08, 9.2821e-08, 1.6755e-07,\n",
      "         1.5730e-07, 1.1590e-05, 3.0015e-03, 9.9699e-01],\n",
      "        [1.8691e-07, 1.9775e-07, 1.8093e-07, 2.0648e-07, 2.6445e-07, 4.3756e-07,\n",
      "         4.1998e-07, 1.7913e-05, 4.0462e-03, 9.9593e-01],\n",
      "        [7.3236e-07, 7.7217e-07, 7.1333e-07, 8.0064e-07, 9.8937e-07, 1.5178e-06,\n",
      "         1.4602e-06, 3.5650e-05, 4.7315e-03, 9.9523e-01],\n",
      "        [1.0454e-06, 1.1017e-06, 1.0183e-06, 1.1375e-06, 1.3910e-06, 2.0848e-06,\n",
      "         2.0034e-06, 4.3642e-05, 4.9059e-03, 9.9504e-01],\n",
      "        [8.3981e-06, 8.7916e-06, 8.2280e-06, 8.9617e-06, 1.0417e-05, 1.4112e-05,\n",
      "         1.3566e-05, 1.5458e-04, 6.9806e-03, 9.9279e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.45, Train Loss: 0.00, Val Loss: 5.82, Train BLEU: 0.00, Val BLEU: 3.55, Minutes Elapsed: 34.24\n",
      "Sampling from val predictions...\n",
      "Source: 当时 我 在 一家 意大利 大利 非政府 政府 组织 织工\n",
      "Reference: i worked for an italian ngo , and every\n",
      "Model: <SOS> and , to the , of , and ,\n",
      "Attention Weights: tensor([[1.3582e-08, 4.1250e-08, 3.1295e-08, 1.5284e-08, 3.0145e-08, 1.0146e-07,\n",
      "         1.2612e-06, 3.2946e-05, 1.0124e-02, 9.8984e-01],\n",
      "        [1.6702e-07, 2.0980e-07, 1.9607e-07, 1.4239e-07, 2.1631e-07, 4.2088e-07,\n",
      "         2.9444e-06, 3.5646e-05, 8.2489e-03, 9.9171e-01],\n",
      "        [2.3452e-08, 1.4754e-08, 1.9653e-08, 4.3815e-08, 1.3707e-07, 1.7796e-07,\n",
      "         2.0933e-06, 1.5247e-05, 5.4209e-03, 9.9456e-01],\n",
      "        [2.0645e-05, 1.3555e-05, 1.6976e-05, 2.9749e-05, 5.4757e-05, 5.9961e-05,\n",
      "         1.8616e-04, 4.1865e-04, 1.2067e-02, 9.8713e-01],\n",
      "        [1.5076e-08, 1.3106e-08, 1.6473e-08, 2.8020e-08, 7.7820e-08, 1.0810e-07,\n",
      "         1.2114e-06, 1.0618e-05, 4.2355e-03, 9.9575e-01],\n",
      "        [8.3683e-10, 6.7247e-10, 9.4597e-10, 2.1426e-09, 1.0287e-08, 1.6711e-08,\n",
      "         5.8491e-07, 1.0742e-05, 8.0049e-03, 9.9198e-01],\n",
      "        [4.7975e-09, 3.9264e-09, 5.2124e-09, 1.0297e-08, 3.6791e-08, 5.3949e-08,\n",
      "         1.0874e-06, 1.3919e-05, 7.2981e-03, 9.9269e-01],\n",
      "        [5.3482e-09, 4.2198e-09, 5.5810e-09, 1.1265e-08, 4.1871e-08, 5.9861e-08,\n",
      "         1.2833e-06, 1.5965e-05, 8.1013e-03, 9.9188e-01],\n",
      "        [3.7102e-07, 3.3271e-07, 3.9053e-07, 5.6332e-07, 1.1108e-06, 1.3744e-06,\n",
      "         7.7884e-06, 4.1174e-05, 6.3248e-03, 9.9362e-01]])\n",
      "\n",
      "Source: 他们 捕杀 了 我们 的 家畜 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: they kill our livestock . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> and , a the . <EOS> . . .\n",
      "Attention Weights: tensor([[0.4533, 0.2346, 0.1755, 0.0790, 0.0181, 0.0114, 0.0283, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4143, 0.2321, 0.1708, 0.0889, 0.0340, 0.0229, 0.0370, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1721, 0.1775, 0.1315, 0.1774, 0.0983, 0.0932, 0.1500, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1625, 0.1673, 0.1287, 0.1682, 0.1050, 0.1076, 0.1608, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1238, 0.1317, 0.1320, 0.1562, 0.1340, 0.1395, 0.1828, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1271, 0.1316, 0.1358, 0.1484, 0.1370, 0.1409, 0.1792, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1290, 0.1325, 0.1386, 0.1480, 0.1391, 0.1407, 0.1721, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1244, 0.1314, 0.1273, 0.1549, 0.1332, 0.1436, 0.1853, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1286, 0.1312, 0.1411, 0.1436, 0.1398, 0.1407, 0.1751, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.60, Train Loss: 0.00, Val Loss: 5.70, Train BLEU: 0.00, Val BLEU: 2.41, Minutes Elapsed: 45.69\n",
      "Sampling from val predictions...\n",
      "Source: 我们 必须 学会 如何 让 这些 人 给出 他们 的\n",
      "Reference: you have to learn how to get these people\n",
      "Model: <SOS> we we to to , , , , ,\n",
      "Attention Weights: tensor([[7.8018e-08, 2.9161e-08, 3.6094e-08, 7.9847e-08, 7.2599e-08, 1.5760e-06,\n",
      "         5.8680e-06, 9.4273e-04, 3.9506e-02, 9.5954e-01],\n",
      "        [1.6530e-09, 1.1693e-09, 1.5521e-09, 3.3913e-09, 3.2982e-09, 5.7271e-08,\n",
      "         6.1783e-07, 1.1071e-04, 1.4204e-02, 9.8568e-01],\n",
      "        [1.0833e-07, 9.9222e-08, 1.1643e-07, 1.7351e-07, 1.6870e-07, 7.1629e-07,\n",
      "         3.0248e-06, 9.1374e-05, 7.3216e-03, 9.9258e-01],\n",
      "        [5.1667e-05, 5.0147e-05, 5.3548e-05, 6.2498e-05, 6.4144e-05, 1.1539e-04,\n",
      "         2.3064e-04, 1.2757e-03, 1.9889e-02, 9.7821e-01],\n",
      "        [1.2911e-07, 1.2924e-07, 1.4885e-07, 2.0272e-07, 2.5869e-07, 8.4571e-07,\n",
      "         4.2604e-06, 1.1209e-04, 8.6903e-03, 9.9119e-01],\n",
      "        [1.2819e-07, 1.2837e-07, 1.4734e-07, 2.0042e-07, 2.5853e-07, 8.5705e-07,\n",
      "         4.5047e-06, 1.2265e-04, 9.4569e-03, 9.9041e-01],\n",
      "        [5.4704e-08, 5.4484e-08, 6.3410e-08, 8.9829e-08, 1.2034e-07, 4.6504e-07,\n",
      "         2.9606e-06, 1.0466e-04, 9.5288e-03, 9.9036e-01],\n",
      "        [3.8687e-08, 3.8179e-08, 4.5095e-08, 6.6026e-08, 8.7235e-08, 3.5803e-07,\n",
      "         2.3011e-06, 8.5582e-05, 8.3401e-03, 9.9157e-01],\n",
      "        [8.7255e-06, 8.7991e-06, 9.6233e-06, 1.1515e-05, 1.3004e-05, 2.6077e-05,\n",
      "         6.8072e-05, 5.6659e-04, 1.4639e-02, 9.8465e-01]])\n",
      "\n",
      "Source: 她 不是 朝鲜 <UNK> <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: she &apos;s not north korean . &quot; <EOS> <PAD>\n",
      "Model: <SOS> i &apos;s a . . <EOS> <EOS> <EOS> .\n",
      "Attention Weights: tensor([[0.8375, 0.1289, 0.0139, 0.0064, 0.0132, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2212, 0.2495, 0.1301, 0.1631, 0.2361, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1316, 0.1761, 0.1563, 0.2437, 0.2923, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1868, 0.2106, 0.1857, 0.1952, 0.2218, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1704, 0.1897, 0.1806, 0.2102, 0.2491, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1687, 0.1788, 0.1860, 0.2232, 0.2434, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1849, 0.1870, 0.1845, 0.2084, 0.2352, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1858, 0.1878, 0.1850, 0.2082, 0.2332, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1645, 0.1888, 0.1985, 0.2173, 0.2310, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.75, Train Loss: 0.00, Val Loss: 5.61, Train BLEU: 0.00, Val BLEU: 4.71, Minutes Elapsed: 57.32\n",
      "Sampling from val predictions...\n",
      "Source: 他 只是 压力 太大 了 婚礼 的 筹备 和 我家\n",
      "Reference: he had just been really stressed out by the\n",
      "Model: <SOS> he he , to , , , the the\n",
      "Attention Weights: tensor([[1.0155e-06, 6.8017e-08, 9.8219e-09, 1.7654e-07, 7.7478e-07, 2.8095e-07,\n",
      "         1.2127e-05, 5.3829e-05, 8.4906e-03, 9.9144e-01],\n",
      "        [2.6017e-05, 1.9954e-05, 7.6429e-06, 6.8833e-05, 7.2266e-05, 6.9320e-05,\n",
      "         5.2356e-04, 1.7828e-03, 3.5026e-02, 9.6240e-01],\n",
      "        [2.5754e-06, 5.2077e-06, 6.6491e-06, 3.7527e-05, 3.3509e-05, 9.6709e-05,\n",
      "         4.3305e-04, 2.4746e-03, 4.1193e-02, 9.5572e-01],\n",
      "        [7.2178e-06, 1.1811e-05, 1.4734e-05, 4.9187e-05, 3.5536e-05, 8.9986e-05,\n",
      "         2.6905e-04, 1.5497e-03, 2.7827e-02, 9.7015e-01],\n",
      "        [2.0789e-06, 4.2658e-06, 5.7409e-06, 1.5129e-05, 1.3002e-05, 3.0318e-05,\n",
      "         9.5399e-05, 5.2374e-04, 1.5100e-02, 9.8421e-01],\n",
      "        [1.7485e-06, 3.5553e-06, 4.9020e-06, 1.3969e-05, 1.1956e-05, 3.0329e-05,\n",
      "         1.0636e-04, 6.4488e-04, 1.8607e-02, 9.8058e-01],\n",
      "        [3.5404e-05, 5.7892e-05, 6.6099e-05, 1.2268e-04, 1.1485e-04, 1.9053e-04,\n",
      "         4.5240e-04, 1.5602e-03, 2.3984e-02, 9.7342e-01],\n",
      "        [2.1258e-06, 4.0400e-06, 5.6138e-06, 1.6716e-05, 1.3065e-05, 3.5365e-05,\n",
      "         1.1910e-04, 7.6537e-04, 2.0479e-02, 9.7856e-01],\n",
      "        [1.5278e-05, 2.7710e-05, 3.4569e-05, 8.8530e-05, 6.6051e-05, 1.5054e-04,\n",
      "         4.0117e-04, 1.8893e-03, 3.0524e-02, 9.6680e-01]])\n",
      "\n",
      "Source: 你们 为什么 什么 <UNK> 说 你 又 没 问 过\n",
      "Reference: &quot; why didn &apos;t you tell us ? &quot;\n",
      "Model: <SOS> and , , &quot; do to , &quot; &quot;\n",
      "Attention Weights: tensor([[2.2249e-02, 5.9699e-06, 2.1310e-05, 2.6328e-06, 7.1199e-04, 2.7139e-02,\n",
      "         1.0714e-03, 3.3176e-03, 3.1953e-02, 9.1353e-01],\n",
      "        [7.9354e-01, 7.3571e-04, 3.2467e-03, 6.3163e-04, 1.7724e-02, 1.1057e-01,\n",
      "         8.3035e-03, 8.9899e-03, 1.4921e-02, 4.1335e-02],\n",
      "        [8.6330e-03, 2.8682e-03, 3.3077e-03, 2.5689e-03, 4.3500e-03, 1.7687e-02,\n",
      "         1.1534e-02, 2.7305e-02, 1.1320e-01, 8.0855e-01],\n",
      "        [3.0996e-02, 8.1690e-03, 1.0796e-02, 7.4786e-03, 1.4165e-02, 4.0334e-02,\n",
      "         2.6540e-02, 4.7530e-02, 1.3468e-01, 6.7931e-01],\n",
      "        [1.4842e-02, 4.5552e-03, 6.2328e-03, 4.4924e-03, 7.9358e-03, 2.5316e-02,\n",
      "         1.7261e-02, 3.5254e-02, 1.2119e-01, 7.6292e-01],\n",
      "        [3.6426e-03, 1.8022e-03, 1.8694e-03, 1.6822e-03, 2.2119e-03, 9.6587e-03,\n",
      "         7.2146e-03, 1.8971e-02, 9.1499e-02, 8.6145e-01],\n",
      "        [2.3009e-02, 1.5416e-02, 1.5562e-02, 1.5510e-02, 1.6507e-02, 3.9733e-02,\n",
      "         3.3938e-02, 5.7772e-02, 1.4315e-01, 6.3940e-01],\n",
      "        [2.8587e-02, 2.3733e-02, 2.3114e-02, 2.4526e-02, 2.5013e-02, 4.9270e-02,\n",
      "         4.5242e-02, 7.0080e-02, 1.4991e-01, 5.6052e-01],\n",
      "        [1.1328e-02, 6.1810e-03, 6.3345e-03, 6.0921e-03, 6.9906e-03, 2.2875e-02,\n",
      "         1.8342e-02, 3.7948e-02, 1.2395e-01, 7.5995e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.90, Train Loss: 0.00, Val Loss: 5.50, Train BLEU: 0.00, Val BLEU: 3.42, Minutes Elapsed: 68.81\n",
      "Sampling from val predictions...\n",
      "Source: 我 以为 我 这辈子 辈子 完 了 尽管 尽管如此 如此\n",
      "Reference: i thought my life was over , but i\n",
      "Model: <SOS> i you to you , , to , ,\n",
      "Attention Weights: tensor([[8.9659e-01, 1.4466e-08, 1.0303e-01, 9.5535e-09, 1.8258e-07, 1.2429e-06,\n",
      "         3.7815e-07, 4.1452e-07, 1.5702e-06, 3.7796e-04],\n",
      "        [7.7430e-01, 1.8426e-05, 2.1986e-01, 1.0759e-06, 1.2329e-05, 5.5734e-05,\n",
      "         2.2860e-05, 3.0927e-05, 1.2426e-04, 5.5714e-03],\n",
      "        [1.0220e-01, 6.7249e-03, 3.7458e-01, 8.2413e-03, 3.8505e-02, 6.6947e-02,\n",
      "         4.0325e-02, 4.6725e-02, 6.7465e-02, 2.4829e-01],\n",
      "        [5.5426e-03, 5.0637e-04, 3.4657e-01, 1.6419e-03, 1.5198e-02, 4.0256e-02,\n",
      "         1.7617e-02, 2.1832e-02, 3.7730e-02, 5.1310e-01],\n",
      "        [6.3685e-03, 4.3698e-04, 2.6056e-01, 8.4741e-04, 1.3391e-02, 4.5219e-02,\n",
      "         1.9062e-02, 2.4173e-02, 4.8134e-02, 5.8181e-01],\n",
      "        [8.9782e-04, 1.0600e-04, 1.9548e-01, 4.2153e-04, 7.6072e-03, 3.2877e-02,\n",
      "         1.2187e-02, 1.3632e-02, 2.5909e-02, 7.1089e-01],\n",
      "        [2.2626e-04, 2.9696e-05, 1.3352e-01, 5.6883e-05, 2.8382e-03, 2.1502e-02,\n",
      "         6.2196e-03, 7.5168e-03, 1.8975e-02, 8.0912e-01],\n",
      "        [3.7161e-04, 6.4601e-05, 2.4993e-01, 1.2616e-04, 2.6844e-03, 1.8413e-02,\n",
      "         5.4612e-03, 5.4175e-03, 1.1073e-02, 7.0646e-01],\n",
      "        [1.1634e-04, 1.4766e-05, 1.9003e-01, 7.0051e-05, 2.4704e-03, 2.0305e-02,\n",
      "         5.7085e-03, 5.1190e-03, 1.0309e-02, 7.6586e-01]])\n",
      "\n",
      "Source: 任何 健康 或者 患有 帕金森 疾病 的 人 都 能\n",
      "Reference: anyone healthy or with parkinson &apos;s can call in\n",
      "Model: <SOS> and , to to , , the , the\n",
      "Attention Weights: tensor([[3.0516e-06, 1.1036e-05, 1.2928e-05, 1.0280e-04, 2.2156e-05, 3.4757e-04,\n",
      "         1.3866e-03, 2.1806e-04, 6.0255e-02, 9.3764e-01],\n",
      "        [1.4424e-04, 2.0689e-04, 8.4121e-05, 2.7238e-04, 6.5575e-05, 7.5066e-04,\n",
      "         2.9970e-03, 9.2037e-04, 1.1580e-01, 8.7875e-01],\n",
      "        [1.5379e-03, 5.5608e-03, 2.8483e-03, 8.7208e-03, 2.3599e-03, 1.8377e-02,\n",
      "         4.3515e-02, 1.3123e-02, 2.7626e-01, 6.2769e-01],\n",
      "        [1.5294e-03, 2.2725e-03, 1.3674e-03, 3.7354e-03, 1.1546e-03, 9.5488e-03,\n",
      "         2.3043e-02, 6.1378e-03, 1.9331e-01, 7.5791e-01],\n",
      "        [1.8000e-03, 2.3254e-03, 2.8685e-03, 7.3482e-03, 3.0528e-03, 1.7647e-02,\n",
      "         3.5785e-02, 1.1728e-02, 1.8186e-01, 7.3558e-01],\n",
      "        [1.1492e-03, 1.8161e-03, 2.5783e-03, 7.3827e-03, 3.2297e-03, 1.9018e-02,\n",
      "         3.7810e-02, 1.1543e-02, 1.7858e-01, 7.3689e-01],\n",
      "        [6.8642e-04, 1.1280e-03, 1.7338e-03, 5.4723e-03, 2.3038e-03, 1.5771e-02,\n",
      "         3.2274e-02, 8.9052e-03, 1.5980e-01, 7.7193e-01],\n",
      "        [1.7871e-03, 1.9121e-03, 1.7087e-03, 4.0981e-03, 1.4804e-03, 1.0361e-02,\n",
      "         2.3327e-02, 7.4497e-03, 1.5967e-01, 7.8820e-01],\n",
      "        [2.8232e-03, 3.7183e-03, 3.7731e-03, 8.0338e-03, 3.5980e-03, 1.7009e-02,\n",
      "         3.2167e-02, 1.1204e-02, 1.5882e-01, 7.5885e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.00, Train Loss: 0.00, Val Loss: 5.43, Train BLEU: 0.00, Val BLEU: 5.31, Minutes Elapsed: 76.58\n",
      "Sampling from val predictions...\n",
      "Source: 在 学校 我们 用 很多 时间 来 学习 <UNK> <UNK>\n",
      "Reference: in school , we spent a lot of time\n",
      "Model: <SOS> and &apos;s , we we to to of .\n",
      "Attention Weights: tensor([[7.5371e-08, 1.3289e-07, 9.9176e-01, 4.8311e-07, 1.0595e-07, 2.6158e-07,\n",
      "         2.8969e-04, 1.9300e-03, 3.2203e-03, 2.8008e-03],\n",
      "        [6.9118e-06, 1.4951e-05, 9.3952e-01, 3.8328e-05, 2.1801e-06, 8.8145e-06,\n",
      "         3.7943e-03, 1.2993e-02, 2.2975e-02, 2.0649e-02],\n",
      "        [8.9688e-03, 1.3426e-02, 2.8846e-01, 4.4219e-02, 3.2606e-02, 4.0176e-02,\n",
      "         1.7746e-01, 1.7160e-01, 1.1490e-01, 1.0819e-01],\n",
      "        [1.4783e-03, 2.4828e-03, 5.6738e-01, 1.6287e-02, 1.2129e-02, 1.5678e-02,\n",
      "         1.3559e-01, 1.2606e-01, 6.5093e-02, 5.7821e-02],\n",
      "        [1.7543e-03, 2.7977e-03, 9.5578e-02, 2.9050e-02, 2.1478e-02, 3.6413e-02,\n",
      "         2.8094e-01, 2.5075e-01, 1.4395e-01, 1.3729e-01],\n",
      "        [2.2484e-03, 3.3961e-03, 9.8807e-02, 2.7130e-02, 1.9906e-02, 3.2117e-02,\n",
      "         2.9356e-01, 2.4596e-01, 1.4127e-01, 1.3561e-01],\n",
      "        [5.2301e-03, 8.4695e-03, 6.2000e-02, 4.0411e-02, 3.8750e-02, 6.1509e-02,\n",
      "         3.7195e-01, 2.5774e-01, 8.1887e-02, 7.2048e-02],\n",
      "        [3.3590e-03, 4.6534e-03, 3.1494e-02, 3.8085e-02, 2.6953e-02, 5.1962e-02,\n",
      "         4.6658e-01, 2.6664e-01, 6.2616e-02, 4.7660e-02],\n",
      "        [2.4126e-03, 3.1175e-03, 3.2557e-02, 2.7882e-02, 2.1088e-02, 4.4909e-02,\n",
      "         5.0924e-01, 2.7374e-01, 4.9400e-02, 3.5648e-02]])\n",
      "\n",
      "Source: 这 是 拍照 拍照片 照片 当天 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: here &apos;s me on the same day as this\n",
      "Model: <SOS> this is a . . . . . <EOS>\n",
      "Attention Weights: tensor([[0.0002, 0.0000, 0.0000, 0.0000, 0.0002, 0.9600, 0.0397, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0049, 0.0002, 0.0002, 0.0001, 0.0062, 0.8196, 0.1688, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0499, 0.0401, 0.0494, 0.0721, 0.1846, 0.4405, 0.1634, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1112, 0.0628, 0.0515, 0.0494, 0.1793, 0.4092, 0.1367, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1641, 0.0939, 0.0742, 0.0659, 0.1731, 0.3122, 0.1165, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0789, 0.0969, 0.1262, 0.1214, 0.1926, 0.2339, 0.1501, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0717, 0.0960, 0.1244, 0.1112, 0.2064, 0.2714, 0.1189, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1050, 0.1172, 0.1306, 0.1153, 0.1800, 0.2242, 0.1277, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1145, 0.1299, 0.1381, 0.1189, 0.1714, 0.1987, 0.1286, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.15, Train Loss: 0.00, Val Loss: 5.39, Train BLEU: 0.00, Val BLEU: 4.71, Minutes Elapsed: 88.09\n",
      "Sampling from val predictions...\n",
      "Source: 但是 很多 尝试 <UNK> 的 人 死 了 <EOS> <PAD>\n",
      "Reference: but many die . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> but , &apos;s , , &apos;s . . .\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0020, 0.9755, 0.0223,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0003, 0.0001, 0.0001, 0.0001, 0.0092, 0.9284, 0.0618,\n",
      "         0.0000],\n",
      "        [0.0033, 0.0266, 0.0556, 0.0254, 0.0261, 0.0245, 0.1694, 0.4601, 0.2088,\n",
      "         0.0000],\n",
      "        [0.0127, 0.0526, 0.0842, 0.0358, 0.0454, 0.0433, 0.1742, 0.3600, 0.1918,\n",
      "         0.0000],\n",
      "        [0.0160, 0.0495, 0.0761, 0.0485, 0.0572, 0.0539, 0.1760, 0.3438, 0.1790,\n",
      "         0.0000],\n",
      "        [0.0224, 0.0608, 0.0925, 0.0679, 0.0794, 0.0771, 0.1724, 0.2781, 0.1493,\n",
      "         0.0000],\n",
      "        [0.0398, 0.0722, 0.0925, 0.0690, 0.0887, 0.0910, 0.1594, 0.2251, 0.1623,\n",
      "         0.0000],\n",
      "        [0.0158, 0.0383, 0.0645, 0.0431, 0.0743, 0.0817, 0.1964, 0.3592, 0.1267,\n",
      "         0.0000],\n",
      "        [0.0309, 0.0454, 0.0605, 0.0507, 0.0907, 0.1109, 0.1803, 0.2767, 0.1539,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 我 不是 那个 国家 的 公民 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: i wasn &apos;t a citizen of that country .\n",
      "Model: <SOS> i i &apos;t to to . . . .\n",
      "Attention Weights: tensor([[0.9994, 0.0000, 0.0000, 0.0000, 0.0000, 0.0005, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9857, 0.0000, 0.0000, 0.0000, 0.0006, 0.0083, 0.0054, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2969, 0.0022, 0.0132, 0.0266, 0.1059, 0.2698, 0.2854, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0687, 0.0058, 0.0304, 0.0690, 0.1211, 0.3737, 0.3312, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0494, 0.0227, 0.0757, 0.1293, 0.1415, 0.2878, 0.2937, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0811, 0.0181, 0.0629, 0.1034, 0.1637, 0.3017, 0.2691, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0826, 0.0350, 0.0784, 0.1360, 0.1448, 0.2656, 0.2577, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0582, 0.0413, 0.0844, 0.1310, 0.1689, 0.2642, 0.2520, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0602, 0.0453, 0.0822, 0.1321, 0.1465, 0.2533, 0.2804, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.30, Train Loss: 0.00, Val Loss: 5.36, Train BLEU: 0.00, Val BLEU: 5.04, Minutes Elapsed: 99.69\n",
      "Sampling from val predictions...\n",
      "Source: 我 本 以为 只有 我们 意大利 意大利人 大利 利人 在\n",
      "Reference: i thought it was only us italians <UNK> around\n",
      "Model: <SOS> i i , &quot; us to of we the\n",
      "Attention Weights: tensor([[9.9995e-01, 1.5231e-12, 4.4855e-09, 1.2084e-08, 4.4111e-05, 6.9583e-10,\n",
      "         5.1692e-10, 2.0121e-09, 9.7176e-10, 2.8546e-06],\n",
      "        [9.9727e-01, 5.7783e-08, 2.9343e-05, 3.5427e-05, 2.0729e-03, 6.2201e-06,\n",
      "         4.9176e-06, 9.9952e-06, 8.4019e-06, 5.6425e-04],\n",
      "        [5.7734e-02, 3.2463e-04, 9.0769e-02, 6.7556e-02, 6.3662e-01, 7.7935e-03,\n",
      "         6.3610e-03, 1.3137e-02, 8.0745e-03, 1.1163e-01],\n",
      "        [1.4660e-02, 5.0055e-05, 3.7889e-02, 4.7404e-02, 7.7907e-01, 5.1163e-03,\n",
      "         4.1942e-03, 1.0120e-02, 4.7430e-03, 9.6757e-02],\n",
      "        [2.4541e-02, 4.8402e-05, 2.4100e-02, 5.3274e-02, 5.4549e-01, 7.2307e-03,\n",
      "         5.8467e-03, 2.0565e-02, 1.1407e-02, 3.0750e-01],\n",
      "        [5.8612e-02, 7.3333e-05, 2.6739e-02, 5.8402e-02, 5.3089e-01, 8.9639e-03,\n",
      "         7.5121e-03, 2.5841e-02, 1.3642e-02, 2.6933e-01],\n",
      "        [3.4243e-02, 2.2332e-04, 1.9508e-02, 4.6076e-02, 4.7964e-01, 2.4202e-02,\n",
      "         2.4665e-02, 5.4546e-02, 3.0019e-02, 2.8688e-01],\n",
      "        [2.3109e-02, 3.3458e-04, 1.5539e-02, 4.2638e-02, 3.1670e-01, 3.1859e-02,\n",
      "         3.4854e-02, 7.6541e-02, 4.4328e-02, 4.1410e-01],\n",
      "        [4.5096e-03, 2.4133e-05, 4.3638e-03, 2.1688e-02, 2.2530e-01, 1.4357e-02,\n",
      "         1.6930e-02, 7.2950e-02, 4.1828e-02, 5.9805e-01]])\n",
      "\n",
      "Source: 他 到 家 时 电话 <UNK> 了 一个 声音 警告\n",
      "Reference: as he arrived home , the phone rang ,\n",
      "Model: <SOS> he he said , the , he , ,\n",
      "Attention Weights: tensor([[0.0049, 0.0000, 0.0001, 0.2451, 0.0001, 0.0004, 0.0007, 0.0002, 0.0134,\n",
      "         0.7352],\n",
      "        [0.0294, 0.0002, 0.0010, 0.4730, 0.0004, 0.0043, 0.0013, 0.0005, 0.0254,\n",
      "         0.4644],\n",
      "        [0.0154, 0.0069, 0.0289, 0.4782, 0.0096, 0.0244, 0.0201, 0.0094, 0.0761,\n",
      "         0.3309],\n",
      "        [0.0149, 0.0079, 0.0381, 0.4466, 0.0129, 0.0309, 0.0220, 0.0103, 0.0820,\n",
      "         0.3344],\n",
      "        [0.0036, 0.0022, 0.0116, 0.2722, 0.0229, 0.0213, 0.0433, 0.0256, 0.1194,\n",
      "         0.4779],\n",
      "        [0.0013, 0.0007, 0.0023, 0.1114, 0.0079, 0.0110, 0.0392, 0.0169, 0.1500,\n",
      "         0.6594],\n",
      "        [0.0008, 0.0004, 0.0014, 0.0718, 0.0053, 0.0072, 0.0338, 0.0142, 0.1470,\n",
      "         0.7181],\n",
      "        [0.0019, 0.0012, 0.0053, 0.1530, 0.0110, 0.0156, 0.0371, 0.0176, 0.1207,\n",
      "         0.6365],\n",
      "        [0.0019, 0.0009, 0.0042, 0.1909, 0.0097, 0.0107, 0.0415, 0.0198, 0.1289,\n",
      "         0.5915]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.45, Train Loss: 0.00, Val Loss: 5.31, Train BLEU: 0.00, Val BLEU: 5.24, Minutes Elapsed: 111.25\n",
      "Sampling from val predictions...\n",
      "Source: 这些 迹象 表明 如果 我们 能 有 一个 表达 自己\n",
      "Reference: together , we &apos;ve shown how powerful our public\n",
      "Model: <SOS> and &apos;s the , to to that we ,\n",
      "Attention Weights: tensor([[2.0491e-05, 1.0613e-07, 1.1198e-05, 3.4705e-05, 9.8453e-01, 1.8271e-06,\n",
      "         6.1058e-06, 2.7894e-06, 2.0471e-04, 1.5185e-02],\n",
      "        [8.6773e-03, 5.1405e-04, 7.8968e-03, 1.2758e-02, 6.8429e-01, 2.1533e-03,\n",
      "         4.8214e-03, 3.0143e-03, 2.8777e-02, 2.4710e-01],\n",
      "        [5.1677e-02, 2.5833e-02, 8.8497e-02, 7.9895e-02, 2.4242e-01, 2.9187e-02,\n",
      "         4.3621e-02, 2.8329e-02, 1.0272e-01, 3.0782e-01],\n",
      "        [5.9387e-03, 2.0172e-03, 3.3737e-02, 4.8688e-02, 6.2109e-01, 4.5652e-03,\n",
      "         8.5845e-03, 5.0759e-03, 4.0734e-02, 2.2957e-01],\n",
      "        [1.3331e-02, 1.0575e-02, 7.6570e-02, 9.4447e-02, 2.5589e-01, 4.0740e-02,\n",
      "         5.3660e-02, 3.2431e-02, 1.0053e-01, 3.2183e-01],\n",
      "        [1.0745e-03, 1.9582e-03, 2.9549e-02, 5.5138e-02, 2.9160e-01, 4.2796e-02,\n",
      "         5.6333e-02, 2.6558e-02, 1.0637e-01, 3.8863e-01],\n",
      "        [6.9063e-04, 1.2016e-03, 2.8663e-02, 5.5872e-02, 2.7002e-01, 3.8902e-02,\n",
      "         5.2108e-02, 2.6146e-02, 1.0369e-01, 4.2271e-01],\n",
      "        [6.8700e-04, 1.1669e-03, 1.9518e-02, 5.2663e-02, 3.0673e-01, 4.8343e-02,\n",
      "         6.0632e-02, 3.4929e-02, 1.2212e-01, 3.5321e-01],\n",
      "        [2.5822e-04, 4.1679e-04, 1.3977e-02, 4.2570e-02, 1.9567e-01, 4.5057e-02,\n",
      "         6.4674e-02, 3.7934e-02, 1.5703e-01, 4.4241e-01]])\n",
      "\n",
      "Source: 这样 的 能人 尚未 未出 出生 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: this person has never been born . <EOS> <PAD>\n",
      "Model: <SOS> and &apos;s a a . . . <EOS> was\n",
      "Attention Weights: tensor([[0.0000, 0.0024, 0.0000, 0.0003, 0.0013, 0.9616, 0.0344, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.1027, 0.0018, 0.0288, 0.0422, 0.7329, 0.0907, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0282, 0.2211, 0.0227, 0.1288, 0.1124, 0.3286, 0.1583, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0155, 0.1440, 0.0284, 0.1513, 0.1454, 0.4145, 0.1009, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0221, 0.0747, 0.0459, 0.1671, 0.1855, 0.3738, 0.1308, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0282, 0.0671, 0.0579, 0.1434, 0.1838, 0.3320, 0.1876, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0252, 0.0549, 0.0739, 0.1330, 0.1918, 0.3205, 0.2006, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0245, 0.0515, 0.0574, 0.1176, 0.1932, 0.3257, 0.2302, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0135, 0.0391, 0.0364, 0.1204, 0.2197, 0.3683, 0.2025, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.60, Train Loss: 0.00, Val Loss: 5.25, Train BLEU: 0.00, Val BLEU: 4.29, Minutes Elapsed: 122.36\n",
      "Sampling from val predictions...\n",
      "Source: 现在 我们 重建 建基 基督 <UNK> 新西兰 西兰 却 不知\n",
      "Reference: so now you &apos;re rebuilding christchurch without knowing what\n",
      "Model: <SOS> now we we we to to , , ,\n",
      "Attention Weights: tensor([[2.2459e-07, 9.5433e-01, 3.1192e-07, 1.2685e-06, 1.0553e-05, 3.7570e-05,\n",
      "         6.0188e-06, 6.5718e-05, 6.9777e-04, 4.4846e-02],\n",
      "        [5.9297e-06, 9.9765e-01, 7.5348e-07, 2.5335e-06, 2.9389e-05, 6.5956e-05,\n",
      "         8.6488e-06, 6.1178e-05, 2.9384e-04, 1.8833e-03],\n",
      "        [8.4376e-03, 2.8113e-01, 1.7634e-02, 1.7513e-02, 4.9444e-02, 5.2958e-02,\n",
      "         1.6654e-02, 5.1721e-02, 1.2956e-01, 3.7495e-01],\n",
      "        [7.3122e-03, 1.5012e-01, 3.0620e-02, 2.7908e-02, 6.7448e-02, 6.1167e-02,\n",
      "         2.0554e-02, 5.6224e-02, 1.4105e-01, 4.3760e-01],\n",
      "        [3.4595e-03, 1.2857e-01, 1.4622e-02, 1.6855e-02, 6.2094e-02, 5.1620e-02,\n",
      "         1.4254e-02, 5.2358e-02, 1.5219e-01, 5.0397e-01],\n",
      "        [7.9618e-04, 8.7528e-03, 1.3843e-02, 2.7987e-02, 6.1367e-02, 2.0665e-02,\n",
      "         2.7817e-02, 7.6531e-02, 2.0869e-01, 5.5355e-01],\n",
      "        [2.8417e-04, 4.1070e-03, 3.3846e-03, 1.1300e-02, 3.8802e-02, 1.5997e-02,\n",
      "         2.5579e-02, 9.4931e-02, 2.5255e-01, 5.5307e-01],\n",
      "        [3.7574e-04, 3.5960e-03, 1.3972e-03, 4.8745e-03, 2.4343e-02, 1.3445e-02,\n",
      "         2.0733e-02, 9.0339e-02, 2.6319e-01, 5.7771e-01],\n",
      "        [8.2375e-04, 8.2286e-03, 1.1788e-03, 3.2660e-03, 2.2221e-02, 1.2229e-02,\n",
      "         1.6390e-02, 9.3460e-02, 2.7186e-01, 5.7034e-01]])\n",
      "\n",
      "Source: 我们 所 做 的 是 跟 他们 交朋友 朋友 并发\n",
      "Reference: and what we do , we become friends ,\n",
      "Model: <SOS> we we we we to we we the to\n",
      "Attention Weights: tensor([[9.9821e-01, 2.0903e-08, 1.2785e-08, 2.2248e-07, 7.4879e-08, 1.5292e-07,\n",
      "         3.0916e-06, 1.0187e-06, 6.1187e-06, 1.7802e-03],\n",
      "        [9.9777e-01, 1.5683e-05, 2.9276e-06, 1.5381e-05, 4.8146e-06, 1.1121e-05,\n",
      "         8.2841e-05, 3.0989e-05, 9.8824e-05, 1.9648e-03],\n",
      "        [1.3690e-01, 5.8398e-02, 1.5831e-02, 6.8620e-02, 1.6930e-02, 2.5472e-02,\n",
      "         6.6393e-02, 2.4866e-02, 6.2254e-02, 5.2433e-01],\n",
      "        [1.5303e-01, 5.2657e-02, 1.5683e-02, 7.0658e-02, 1.5581e-02, 2.3618e-02,\n",
      "         6.4566e-02, 2.1812e-02, 5.7913e-02, 5.2448e-01],\n",
      "        [9.4828e-02, 2.2792e-02, 5.7800e-03, 4.6659e-02, 8.4283e-03, 1.5504e-02,\n",
      "         6.6807e-02, 1.6802e-02, 6.0078e-02, 6.6232e-01],\n",
      "        [1.0564e-02, 6.8394e-03, 1.7277e-02, 1.4339e-01, 4.7456e-02, 5.0935e-02,\n",
      "         1.0909e-01, 3.2850e-02, 8.4661e-02, 4.9693e-01],\n",
      "        [1.0805e-02, 5.4250e-03, 6.4006e-03, 8.3158e-02, 2.1915e-02, 3.3039e-02,\n",
      "         1.1683e-01, 2.7278e-02, 1.0049e-01, 5.9466e-01],\n",
      "        [1.9937e-02, 9.0244e-03, 7.7604e-03, 8.2074e-02, 2.3355e-02, 3.2296e-02,\n",
      "         1.0424e-01, 2.5553e-02, 9.0958e-02, 6.0480e-01],\n",
      "        [7.5927e-04, 1.9597e-03, 7.2758e-03, 9.9314e-02, 4.7685e-02, 4.7970e-02,\n",
      "         1.1521e-01, 3.8015e-02, 1.1074e-01, 5.3107e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.75, Train Loss: 0.00, Val Loss: 5.24, Train BLEU: 0.00, Val BLEU: 5.95, Minutes Elapsed: 133.66\n",
      "Sampling from val predictions...\n",
      "Source: 在 50 <UNK> 摄氏 摄氏度 的 高温 中 男人 女人\n",
      "Reference: enveloped in temperatures of 130 degrees , men ,\n",
      "Model: <SOS> in , <UNK> , , , the the &apos;s\n",
      "Attention Weights: tensor([[6.8866e-08, 1.1563e-03, 1.6898e-05, 1.4592e-05, 9.9833e-04, 8.9488e-05,\n",
      "         4.7320e-05, 7.6496e-03, 5.1152e-04, 9.8952e-01],\n",
      "        [6.6058e-05, 1.0262e-01, 2.3977e-03, 2.3465e-03, 3.1472e-02, 3.1150e-03,\n",
      "         1.9614e-03, 3.7325e-02, 5.9935e-03, 8.1270e-01],\n",
      "        [1.6583e-03, 5.5056e-02, 1.0848e-02, 3.0023e-02, 1.0498e-01, 4.9428e-02,\n",
      "         2.6654e-02, 1.2270e-01, 4.7901e-02, 5.5075e-01],\n",
      "        [2.3374e-05, 4.6200e-03, 3.3065e-04, 2.2965e-03, 2.8598e-02, 1.8654e-02,\n",
      "         7.4795e-03, 1.5681e-01, 1.8220e-02, 7.6297e-01],\n",
      "        [1.0395e-04, 3.5113e-03, 5.1684e-04, 3.9682e-03, 4.1806e-02, 5.1388e-02,\n",
      "         2.1240e-02, 2.0124e-01, 2.9737e-02, 6.4649e-01],\n",
      "        [7.8177e-05, 3.2200e-03, 5.3315e-04, 2.8683e-03, 2.1470e-02, 4.5074e-02,\n",
      "         2.2426e-02, 2.9200e-01, 4.4053e-02, 5.6827e-01],\n",
      "        [1.7936e-05, 6.7727e-04, 1.2925e-04, 1.0494e-03, 1.4009e-02, 3.6839e-02,\n",
      "         2.1435e-02, 2.5497e-01, 3.7963e-02, 6.3291e-01],\n",
      "        [4.4230e-05, 5.7441e-04, 2.3778e-04, 1.1723e-03, 1.6606e-02, 4.2256e-02,\n",
      "         2.6272e-02, 2.1954e-01, 4.2927e-02, 6.5037e-01],\n",
      "        [4.6681e-05, 8.5852e-04, 2.5848e-04, 1.5874e-03, 1.4507e-02, 5.3825e-02,\n",
      "         3.5968e-02, 3.1635e-01, 5.8882e-02, 5.1772e-01]])\n",
      "\n",
      "Source: 我 确实 实相 相信 当 我们 把 他们 当做 同胞\n",
      "Reference: i truly believe , if we can see one\n",
      "Model: <SOS> i i &apos;t that we we &apos;re to ,\n",
      "Attention Weights: tensor([[9.9971e-01, 1.6857e-11, 5.6640e-10, 6.0931e-10, 1.0047e-05, 2.6778e-04,\n",
      "         4.6800e-09, 3.3308e-08, 7.9287e-08, 8.7328e-06],\n",
      "        [9.9537e-01, 4.1879e-06, 2.2060e-05, 1.1904e-05, 7.2482e-04, 3.1046e-03,\n",
      "         2.4061e-05, 5.4493e-05, 6.2835e-05, 6.1613e-04],\n",
      "        [6.5335e-02, 1.1886e-02, 5.3488e-02, 1.2243e-02, 2.2913e-01, 3.5669e-01,\n",
      "         1.2166e-02, 2.9094e-02, 2.1330e-02, 2.0864e-01],\n",
      "        [5.9057e-03, 1.7804e-03, 1.8893e-02, 1.1528e-02, 2.6363e-01, 4.2287e-01,\n",
      "         1.4136e-02, 3.8832e-02, 2.2451e-02, 1.9997e-01],\n",
      "        [3.2544e-03, 2.6989e-04, 1.0745e-02, 2.1132e-02, 3.1017e-01, 3.5670e-01,\n",
      "         2.6267e-02, 4.6904e-02, 3.7267e-02, 1.8729e-01],\n",
      "        [3.4079e-03, 1.9685e-04, 7.8455e-03, 1.5425e-02, 2.8451e-01, 2.3314e-01,\n",
      "         4.1476e-02, 6.1440e-02, 7.4747e-02, 2.7781e-01],\n",
      "        [1.1256e-02, 3.3051e-04, 7.7031e-03, 8.5732e-03, 2.8594e-01, 2.4414e-01,\n",
      "         2.7188e-02, 4.9135e-02, 5.8916e-02, 3.0682e-01],\n",
      "        [1.1747e-03, 2.9444e-04, 1.3203e-02, 3.3459e-02, 2.4371e-01, 2.0968e-01,\n",
      "         5.9591e-02, 9.2465e-02, 8.9734e-02, 2.5669e-01],\n",
      "        [2.3787e-04, 1.3427e-04, 3.6950e-03, 2.0412e-02, 3.0583e-01, 2.3316e-01,\n",
      "         4.9056e-02, 8.9264e-02, 7.5536e-02, 2.2268e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.90, Train Loss: 0.00, Val Loss: 5.17, Train BLEU: 0.00, Val BLEU: 5.35, Minutes Elapsed: 145.05\n",
      "Sampling from val predictions...\n",
      "Source: 在 加德 <UNK> 我 由 一些 从前 是 性 奴\n",
      "Reference: in kathmandu , i was escorted by women who\n",
      "Model: <SOS> in the , i was a to the ,\n",
      "Attention Weights: tensor([[1.1067e-09, 3.7363e-08, 2.6466e-07, 9.9280e-01, 2.7671e-07, 2.9366e-07,\n",
      "         8.0772e-07, 1.1544e-06, 8.1725e-06, 7.1852e-03],\n",
      "        [1.6210e-05, 4.2082e-04, 7.0285e-04, 9.5823e-01, 3.5047e-04, 2.0813e-04,\n",
      "         3.8725e-04, 5.0079e-04, 1.8378e-03, 3.7344e-02],\n",
      "        [1.7276e-03, 1.2257e-02, 1.4741e-02, 3.9285e-01, 3.7639e-02, 1.4026e-02,\n",
      "         1.8072e-02, 2.4218e-02, 5.9835e-02, 4.2464e-01],\n",
      "        [1.7413e-04, 9.4521e-04, 1.5754e-03, 8.3058e-01, 1.4808e-02, 5.7334e-03,\n",
      "         6.2077e-03, 5.9489e-03, 1.4361e-02, 1.1967e-01],\n",
      "        [1.1243e-04, 9.3563e-04, 1.6332e-03, 7.7670e-01, 5.2184e-03, 1.9755e-03,\n",
      "         3.6967e-03, 4.4319e-03, 1.6064e-02, 1.8923e-01],\n",
      "        [3.4439e-04, 3.4079e-03, 3.3692e-03, 3.0914e-01, 6.5569e-02, 2.7593e-02,\n",
      "         3.0352e-02, 2.4830e-02, 6.7507e-02, 4.6788e-01],\n",
      "        [3.9379e-05, 3.3835e-04, 3.8367e-04, 6.6423e-02, 4.4016e-02, 5.0381e-02,\n",
      "         7.1218e-02, 6.9051e-02, 1.1254e-01, 5.8561e-01],\n",
      "        [3.7808e-05, 1.9610e-04, 2.6038e-04, 6.8635e-02, 3.8906e-02, 5.3370e-02,\n",
      "         7.9579e-02, 7.6513e-02, 1.1969e-01, 5.6281e-01],\n",
      "        [2.1203e-05, 1.5444e-04, 2.4112e-04, 7.5787e-02, 8.1381e-03, 1.1039e-02,\n",
      "         2.9461e-02, 3.7203e-02, 9.2152e-02, 7.4580e-01]])\n",
      "\n",
      "Source: 第一 第一次 一次 我 驾车 去 寻找 秘密 的 海滩\n",
      "Reference: the first time , i was driving to find\n",
      "Model: <SOS> the first thing i i was to to the\n",
      "Attention Weights: tensor([[3.4052e-09, 6.1650e-08, 2.1229e-07, 9.9556e-01, 1.3177e-07, 1.0117e-07,\n",
      "         5.2148e-06, 7.6692e-08, 1.0950e-06, 4.4338e-03],\n",
      "        [6.2499e-05, 3.7350e-04, 1.5876e-03, 9.6559e-01, 1.4250e-04, 1.6827e-04,\n",
      "         1.3030e-03, 7.8219e-05, 5.1777e-04, 3.0180e-02],\n",
      "        [8.1878e-03, 1.5901e-02, 7.8144e-02, 3.7739e-01, 2.0508e-02, 2.4486e-02,\n",
      "         5.8055e-02, 7.7225e-03, 3.4551e-02, 3.7505e-01],\n",
      "        [2.2838e-03, 5.2701e-03, 5.0665e-02, 5.7653e-01, 1.6557e-02, 2.0470e-02,\n",
      "         4.8054e-02, 6.0917e-03, 2.3077e-02, 2.5100e-01],\n",
      "        [1.6510e-03, 3.9956e-03, 3.6724e-02, 5.5140e-01, 1.8437e-02, 2.3151e-02,\n",
      "         5.5847e-02, 6.3270e-03, 2.1327e-02, 2.8114e-01],\n",
      "        [2.3444e-04, 8.2545e-04, 1.2518e-02, 2.6054e-01, 2.7227e-02, 2.5573e-02,\n",
      "         1.1616e-01, 5.5711e-03, 2.5117e-02, 5.2623e-01],\n",
      "        [3.4215e-04, 1.1319e-03, 1.4049e-02, 2.8224e-01, 1.8954e-02, 2.1373e-02,\n",
      "         1.1394e-01, 4.0170e-03, 2.3889e-02, 5.2006e-01],\n",
      "        [7.8337e-05, 2.5096e-04, 1.6725e-03, 7.4015e-02, 5.3577e-02, 4.9544e-02,\n",
      "         1.6731e-01, 2.4395e-02, 5.4597e-02, 5.7456e-01],\n",
      "        [6.3219e-05, 1.6952e-04, 1.2950e-03, 7.4756e-02, 4.6841e-02, 4.7864e-02,\n",
      "         1.6246e-01, 2.6034e-02, 5.7083e-02, 5.8343e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.00, Train Loss: 0.00, Val Loss: 5.17, Train BLEU: 0.00, Val BLEU: 5.67, Minutes Elapsed: 152.78\n",
      "Sampling from val predictions...\n",
      "Source: 但是 狮子 十分 聪明 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: but lions are very clever . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> but it &apos;s &apos;t . . <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0008, 0.0770, 0.9222, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0004, 0.0073, 0.2009, 0.4403, 0.3511, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0190, 0.0678, 0.2385, 0.3367, 0.3379, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0294, 0.0734, 0.2456, 0.3088, 0.3429, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0391, 0.0893, 0.2866, 0.3006, 0.2843, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0436, 0.0987, 0.2611, 0.2885, 0.3082, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0132, 0.0512, 0.2380, 0.3287, 0.3688, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0085, 0.0341, 0.2031, 0.3662, 0.3882, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0299, 0.0772, 0.2151, 0.3248, 0.3530, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 她 的 名字 叫 琼 对 我 来讲 她 就\n",
      "Reference: her name was joan , and she was a\n",
      "Model: <SOS> and , , &quot; , and i said ,\n",
      "Attention Weights: tensor([[1.6398e-06, 4.2305e-09, 4.5928e-05, 7.8573e-08, 8.4388e-06, 4.6200e-06,\n",
      "         9.8261e-01, 2.7322e-06, 2.6038e-03, 1.4725e-02],\n",
      "        [1.2054e-02, 5.3855e-05, 1.1797e-01, 2.8047e-04, 1.3137e-02, 5.3965e-03,\n",
      "         6.6777e-01, 1.3836e-03, 5.2268e-02, 1.2969e-01],\n",
      "        [1.5872e-02, 2.0343e-03, 3.0611e-01, 5.6282e-03, 1.0825e-01, 3.2531e-02,\n",
      "         1.6250e-01, 3.2699e-03, 8.6975e-02, 2.7683e-01],\n",
      "        [1.3941e-03, 3.1797e-04, 5.6162e-02, 3.6495e-03, 1.0388e-01, 2.9811e-02,\n",
      "         4.0888e-01, 6.0946e-03, 1.0858e-01, 2.8123e-01],\n",
      "        [2.5602e-04, 1.8743e-05, 2.2171e-02, 3.7571e-04, 5.2466e-02, 1.5537e-02,\n",
      "         5.4728e-01, 2.1327e-03, 1.3106e-01, 2.2871e-01],\n",
      "        [4.7643e-04, 3.7837e-05, 1.7525e-02, 6.6349e-04, 6.4720e-02, 2.0335e-02,\n",
      "         4.5850e-01, 3.8608e-03, 1.4764e-01, 2.8624e-01],\n",
      "        [2.0097e-04, 1.7781e-05, 5.3337e-03, 2.2222e-04, 3.1610e-02, 1.2509e-02,\n",
      "         4.7418e-01, 2.5496e-03, 2.1116e-01, 2.6221e-01],\n",
      "        [4.6534e-04, 2.3522e-05, 2.0600e-02, 1.5382e-04, 3.5640e-02, 9.4163e-03,\n",
      "         4.6986e-01, 8.5560e-04, 1.5512e-01, 3.0786e-01],\n",
      "        [4.5467e-04, 1.2802e-04, 4.3838e-02, 3.1401e-03, 1.2517e-01, 4.1833e-02,\n",
      "         2.9102e-01, 6.2178e-03, 1.6356e-01, 3.2465e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.15, Train Loss: 0.00, Val Loss: 5.17, Train BLEU: 0.00, Val BLEU: 5.41, Minutes Elapsed: 164.28\n",
      "Sampling from val predictions...\n",
      "Source: 我 当时 也 不知 知道 第二 第二步 二步 是 孤立\n",
      "Reference: i also didn &apos;t know that the second step\n",
      "Model: <SOS> i i that to know to the was is\n",
      "Attention Weights: tensor([[9.9885e-01, 1.7672e-09, 3.2112e-07, 1.5083e-08, 1.5470e-06, 3.9712e-07,\n",
      "         1.4399e-06, 5.9101e-06, 2.1339e-06, 1.1381e-03],\n",
      "        [9.8631e-01, 1.7709e-04, 1.5834e-03, 1.4885e-04, 7.7589e-04, 2.7866e-04,\n",
      "         4.3470e-04, 8.4570e-04, 6.5842e-04, 8.7886e-03],\n",
      "        [9.3935e-02, 1.5041e-02, 2.2175e-01, 5.4701e-03, 3.7882e-02, 9.5533e-03,\n",
      "         1.6323e-02, 2.7633e-02, 2.4203e-02, 5.4821e-01],\n",
      "        [9.0686e-03, 6.6330e-03, 1.8853e-01, 1.2342e-02, 1.1505e-01, 2.9408e-02,\n",
      "         4.3598e-02, 6.3389e-02, 4.7344e-02, 4.8464e-01],\n",
      "        [3.9329e-03, 1.9489e-03, 1.1563e-01, 1.5606e-02, 1.6909e-01, 3.9402e-02,\n",
      "         5.6267e-02, 9.4569e-02, 5.5416e-02, 4.4814e-01],\n",
      "        [8.7586e-04, 1.9485e-03, 9.0677e-02, 5.9719e-03, 1.2387e-01, 2.3102e-02,\n",
      "         4.1322e-02, 8.1690e-02, 3.3881e-02, 5.9666e-01],\n",
      "        [5.5165e-03, 8.9396e-04, 5.3960e-02, 5.7668e-03, 1.5445e-01, 2.8438e-02,\n",
      "         5.2760e-02, 1.2438e-01, 6.0220e-02, 5.1362e-01],\n",
      "        [8.6844e-04, 1.8230e-04, 1.1163e-02, 1.9255e-03, 7.6145e-02, 2.5637e-02,\n",
      "         4.8183e-02, 2.2624e-01, 1.0443e-01, 5.0522e-01],\n",
      "        [1.6294e-04, 3.7539e-05, 2.7885e-03, 1.4781e-03, 5.0035e-02, 2.6116e-02,\n",
      "         4.5275e-02, 2.2761e-01, 1.2569e-01, 5.2080e-01]])\n",
      "\n",
      "Source: 但 我 转而 又 想到 他们 教 我 的 那些\n",
      "Reference: but i cast my mind back to the things\n",
      "Model: <SOS> but i i &apos;t , , they , ,\n",
      "Attention Weights: tensor([[1.7771e-11, 9.9685e-01, 6.8530e-11, 3.5090e-09, 5.7840e-08, 4.4206e-05,\n",
      "         7.5773e-09, 3.0111e-03, 2.6443e-08, 9.0668e-05],\n",
      "        [7.9535e-08, 9.9909e-01, 1.3735e-06, 1.3808e-05, 2.3158e-05, 1.6975e-04,\n",
      "         5.4137e-06, 4.1044e-04, 1.2635e-05, 2.6907e-04],\n",
      "        [1.7606e-04, 4.7454e-01, 6.4741e-03, 3.3705e-02, 3.0165e-02, 1.1502e-01,\n",
      "         5.2148e-03, 6.9624e-02, 1.3313e-02, 2.5177e-01],\n",
      "        [5.5028e-05, 1.0654e-01, 1.0252e-02, 8.4727e-02, 7.1625e-02, 3.0843e-01,\n",
      "         6.4204e-03, 6.7911e-02, 1.4808e-02, 3.2924e-01],\n",
      "        [2.0192e-05, 8.2726e-03, 2.1714e-03, 3.9380e-02, 6.7155e-02, 4.5149e-01,\n",
      "         6.1690e-03, 1.0022e-01, 1.3218e-02, 3.1191e-01],\n",
      "        [7.5111e-06, 1.6998e-03, 1.6145e-04, 3.2092e-03, 2.1922e-02, 4.8615e-01,\n",
      "         5.2643e-03, 2.8872e-01, 1.1313e-02, 1.8155e-01],\n",
      "        [1.5778e-06, 1.5321e-04, 1.0962e-04, 3.0341e-03, 2.0436e-02, 5.0465e-01,\n",
      "         9.2066e-03, 2.7686e-01, 1.7287e-02, 1.6826e-01],\n",
      "        [5.2202e-07, 3.9678e-04, 1.3099e-05, 6.6087e-04, 7.1033e-03, 3.2752e-01,\n",
      "         1.4666e-03, 4.0689e-01, 5.6764e-03, 2.5027e-01],\n",
      "        [1.2211e-06, 4.9914e-04, 3.0465e-05, 1.2032e-03, 1.0441e-02, 3.5285e-01,\n",
      "         5.8190e-03, 3.4299e-01, 1.7396e-02, 2.6878e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.30, Train Loss: 0.00, Val Loss: 5.15, Train BLEU: 0.00, Val BLEU: 6.51, Minutes Elapsed: 175.77\n",
      "Sampling from val predictions...\n",
      "Source: 这些 女孩 是 幸运 的 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: these girls were so lucky . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> these is are . . . <EOS> <EOS> .\n",
      "Attention Weights: tensor([[0.2140, 0.0001, 0.0042, 0.0042, 0.6445, 0.1331, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4846, 0.0270, 0.1181, 0.0483, 0.2151, 0.1070, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2264, 0.0626, 0.2228, 0.0860, 0.2276, 0.1747, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0595, 0.0592, 0.2591, 0.1862, 0.2969, 0.1391, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0918, 0.0766, 0.1877, 0.1905, 0.2747, 0.1786, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0767, 0.0568, 0.1502, 0.2017, 0.3060, 0.2086, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0783, 0.0714, 0.1246, 0.2162, 0.2689, 0.2405, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0612, 0.0474, 0.1054, 0.2271, 0.2854, 0.2735, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0727, 0.0663, 0.1334, 0.2498, 0.2413, 0.2365, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 议员 甚至 前来 拜访 表达 了 他 有 多么 赞赏\n",
      "Reference: my <UNK> even called in and said how they\n",
      "Model: <SOS> and , is who he his , he &quot;\n",
      "Attention Weights: tensor([[6.9830e-05, 1.3079e-06, 3.1731e-05, 7.1257e-06, 3.0352e-04, 4.8291e-05,\n",
      "         9.5174e-01, 9.7764e-05, 6.5887e-04, 4.7040e-02],\n",
      "        [5.4511e-02, 4.1787e-03, 4.9817e-03, 8.7854e-04, 8.8123e-03, 3.1356e-03,\n",
      "         7.7977e-01, 4.5744e-03, 1.0651e-02, 1.2850e-01],\n",
      "        [6.3876e-02, 4.7236e-02, 5.9956e-02, 1.2723e-02, 3.3281e-02, 1.6263e-02,\n",
      "         3.8748e-01, 2.1075e-02, 3.3593e-02, 3.2452e-01],\n",
      "        [2.1403e-03, 3.0641e-03, 2.5283e-02, 1.0130e-02, 6.9060e-02, 1.8216e-02,\n",
      "         6.7372e-01, 1.6389e-02, 2.2136e-02, 1.5986e-01],\n",
      "        [3.6163e-04, 2.8768e-04, 3.1914e-03, 1.7513e-03, 3.4238e-02, 9.6929e-03,\n",
      "         8.1580e-01, 1.3379e-02, 2.2816e-02, 9.8478e-02],\n",
      "        [3.4596e-04, 3.3897e-04, 4.8748e-03, 4.9788e-03, 7.5734e-02, 2.9106e-02,\n",
      "         4.6862e-01, 5.1750e-02, 8.1283e-02, 2.8297e-01],\n",
      "        [6.1367e-05, 4.1558e-05, 9.0055e-04, 1.4271e-03, 4.3414e-02, 1.5968e-02,\n",
      "         4.6600e-01, 4.2588e-02, 9.3094e-02, 3.3650e-01],\n",
      "        [3.9185e-05, 3.3113e-05, 5.7564e-04, 1.3200e-03, 3.3450e-02, 1.4999e-02,\n",
      "         4.4415e-01, 4.4182e-02, 8.3748e-02, 3.7751e-01],\n",
      "        [1.0830e-04, 9.1424e-05, 9.7194e-04, 1.6813e-03, 3.9205e-02, 1.7349e-02,\n",
      "         4.3018e-01, 5.0065e-02, 8.5460e-02, 3.7489e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.45, Train Loss: 0.00, Val Loss: 5.12, Train BLEU: 0.00, Val BLEU: 6.18, Minutes Elapsed: 187.38\n",
      "Sampling from val predictions...\n",
      "Source: 有人 指控 我 是 朝鲜 <UNK> 所以 警察 测试 了\n",
      "Reference: someone had accused me of being north korean ,\n",
      "Model: <SOS> and , , , i but , , ,\n",
      "Attention Weights: tensor([[2.0840e-10, 1.7978e-10, 9.9996e-01, 3.3520e-09, 2.6003e-07, 4.8832e-07,\n",
      "         4.1267e-06, 2.2833e-07, 8.1823e-08, 3.7690e-05],\n",
      "        [7.4979e-06, 7.5890e-06, 9.9398e-01, 7.2398e-05, 4.8550e-04, 6.3708e-04,\n",
      "         1.5934e-03, 2.6681e-04, 1.5204e-04, 2.7947e-03],\n",
      "        [1.2147e-03, 2.0177e-03, 4.4089e-01, 2.6015e-02, 5.2793e-02, 7.8326e-02,\n",
      "         5.9194e-02, 2.8474e-02, 2.8344e-02, 2.8273e-01],\n",
      "        [6.0677e-04, 9.7078e-04, 5.4208e-01, 1.3828e-02, 4.7861e-02, 6.0479e-02,\n",
      "         6.7729e-02, 2.7246e-02, 2.3404e-02, 2.1580e-01],\n",
      "        [2.2748e-05, 5.0590e-05, 1.1036e-01, 8.8570e-03, 1.1724e-01, 5.8824e-02,\n",
      "         1.9209e-01, 5.0979e-02, 4.2466e-02, 4.1910e-01],\n",
      "        [3.8584e-06, 7.5298e-06, 9.2520e-02, 6.3455e-04, 3.6262e-02, 1.9642e-02,\n",
      "         2.3225e-01, 8.0268e-02, 5.6982e-02, 4.8143e-01],\n",
      "        [2.3471e-06, 3.8680e-06, 9.7242e-03, 9.7373e-04, 3.2450e-02, 1.2466e-02,\n",
      "         1.8009e-01, 1.0680e-01, 8.5770e-02, 5.7172e-01],\n",
      "        [6.6763e-07, 1.3278e-06, 7.7087e-03, 9.3410e-04, 2.5009e-02, 9.6026e-03,\n",
      "         1.2451e-01, 9.5112e-02, 9.6152e-02, 6.4097e-01],\n",
      "        [8.2338e-07, 1.5510e-06, 6.3454e-03, 3.3868e-04, 1.9172e-02, 7.2710e-03,\n",
      "         1.2385e-01, 1.0420e-01, 1.0335e-01, 6.3547e-01]])\n",
      "\n",
      "Source: 而且 和 芭蕾 一样 这 也 需要 经过 特殊 地\n",
      "Reference: and like ballet , it takes an extraordinary level\n",
      "Model: <SOS> and , &apos;s , , &apos;s a the of\n",
      "Attention Weights: tensor([[5.8731e-05, 5.6930e-06, 2.4823e-04, 3.5014e-05, 2.2377e-01, 6.8305e-04,\n",
      "         1.5446e-03, 6.6270e-04, 1.2823e-02, 7.6017e-01],\n",
      "        [6.7737e-03, 5.0855e-03, 3.1633e-02, 3.6126e-03, 6.7435e-01, 9.9286e-03,\n",
      "         1.0563e-02, 4.5220e-03, 2.2225e-02, 2.3131e-01],\n",
      "        [1.0113e-02, 2.5756e-02, 7.6583e-02, 1.2506e-02, 2.4906e-01, 2.8261e-02,\n",
      "         2.2669e-02, 1.1790e-02, 4.5990e-02, 5.1727e-01],\n",
      "        [1.2624e-02, 3.2916e-02, 1.0216e-01, 2.0323e-02, 2.0596e-01, 3.7794e-02,\n",
      "         3.1431e-02, 1.7180e-02, 4.9073e-02, 4.9054e-01],\n",
      "        [2.4622e-03, 8.0832e-03, 5.9212e-02, 1.5859e-02, 3.2957e-01, 3.1086e-02,\n",
      "         2.8271e-02, 1.2591e-02, 3.8855e-02, 4.7402e-01],\n",
      "        [3.0295e-04, 4.1605e-04, 5.2607e-03, 2.6816e-03, 7.0256e-01, 1.6984e-02,\n",
      "         2.4059e-02, 9.1840e-03, 3.6263e-02, 2.0229e-01],\n",
      "        [8.5694e-04, 1.9135e-03, 1.7429e-02, 1.8360e-02, 4.3815e-01, 5.6845e-02,\n",
      "         5.9371e-02, 2.4810e-02, 5.0037e-02, 3.3222e-01],\n",
      "        [5.3360e-04, 5.2544e-04, 5.0088e-03, 6.7706e-03, 1.7010e-01, 4.9222e-02,\n",
      "         9.4684e-02, 5.9223e-02, 1.3370e-01, 4.8023e-01],\n",
      "        [2.8242e-04, 4.0352e-04, 2.9058e-03, 5.4838e-03, 2.4095e-01, 6.3046e-02,\n",
      "         1.0186e-01, 7.5935e-02, 1.5239e-01, 3.5674e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.60, Train Loss: 0.00, Val Loss: 5.07, Train BLEU: 0.00, Val BLEU: 5.59, Minutes Elapsed: 199.03\n",
      "Sampling from val predictions...\n",
      "Source: 我们 看 这个 样品 现在 还有 有点 烫 <EOS> <PAD>\n",
      "Reference: so we still have the specimen here . it\n",
      "Model: <SOS> we we we this this . . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9946, 0.0000, 0.0002, 0.0000, 0.0000, 0.0001, 0.0001, 0.0029, 0.0022,\n",
      "         0.0000],\n",
      "        [0.9482, 0.0016, 0.0323, 0.0006, 0.0012, 0.0018, 0.0013, 0.0070, 0.0061,\n",
      "         0.0000],\n",
      "        [0.0660, 0.0314, 0.7050, 0.0104, 0.0164, 0.0257, 0.0130, 0.0745, 0.0576,\n",
      "         0.0000],\n",
      "        [0.1501, 0.0407, 0.4677, 0.0235, 0.0311, 0.0430, 0.0235, 0.1228, 0.0976,\n",
      "         0.0000],\n",
      "        [0.0095, 0.0150, 0.4620, 0.0592, 0.1228, 0.0972, 0.0526, 0.1128, 0.0689,\n",
      "         0.0000],\n",
      "        [0.0164, 0.0112, 0.1556, 0.0628, 0.1431, 0.1761, 0.1123, 0.1963, 0.1261,\n",
      "         0.0000],\n",
      "        [0.0126, 0.0071, 0.1586, 0.0661, 0.1210, 0.1679, 0.1705, 0.1999, 0.0963,\n",
      "         0.0000],\n",
      "        [0.0739, 0.0185, 0.1403, 0.0800, 0.1298, 0.1573, 0.1125, 0.1645, 0.1234,\n",
      "         0.0000],\n",
      "        [0.0606, 0.0119, 0.0997, 0.0412, 0.0853, 0.1662, 0.1381, 0.2242, 0.1727,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 就 如 你 所见 由 太阳 太阳能 接收 板 来\n",
      "Reference: as you can see , the solar panel charges\n",
      "Model: <SOS> and , you , , the &apos;s is ,\n",
      "Attention Weights: tensor([[2.3414e-08, 9.8206e-07, 9.8365e-01, 6.6275e-07, 1.0501e-06, 2.8971e-06,\n",
      "         1.1937e-05, 2.7695e-06, 6.5328e-06, 1.6319e-02],\n",
      "        [1.2449e-04, 5.3803e-03, 9.0626e-01, 5.0032e-04, 1.1727e-03, 1.2235e-03,\n",
      "         1.4829e-03, 7.1453e-04, 1.4645e-03, 8.1678e-02],\n",
      "        [2.2137e-03, 5.7243e-02, 5.7042e-01, 5.4739e-03, 1.1931e-02, 9.5111e-03,\n",
      "         9.3018e-03, 4.5053e-03, 1.1131e-02, 3.1827e-01],\n",
      "        [8.0860e-04, 3.0463e-02, 7.4630e-01, 1.4619e-03, 4.2134e-03, 3.4725e-03,\n",
      "         3.9816e-03, 1.5747e-03, 3.9233e-03, 2.0380e-01],\n",
      "        [2.6327e-04, 9.0968e-03, 2.6512e-01, 5.0670e-02, 6.8643e-02, 6.9599e-02,\n",
      "         6.9605e-02, 2.7075e-02, 4.0686e-02, 3.9924e-01],\n",
      "        [1.6417e-04, 2.9765e-03, 1.0340e-01, 3.3534e-02, 9.0107e-02, 9.9841e-02,\n",
      "         1.0804e-01, 5.0645e-02, 6.7413e-02, 4.4388e-01],\n",
      "        [2.6912e-04, 2.8201e-03, 1.3580e-01, 6.3283e-03, 5.1931e-02, 5.1459e-02,\n",
      "         6.0345e-02, 4.6928e-02, 6.7524e-02, 5.7659e-01],\n",
      "        [2.6211e-04, 2.2809e-03, 1.9126e-01, 7.9339e-03, 5.2407e-02, 5.3396e-02,\n",
      "         5.8911e-02, 4.8942e-02, 6.7095e-02, 5.1752e-01],\n",
      "        [2.3946e-04, 2.6075e-03, 4.3211e-01, 9.5912e-03, 3.7560e-02, 3.5147e-02,\n",
      "         4.6605e-02, 4.2516e-02, 5.0539e-02, 3.4308e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.75, Train Loss: 0.00, Val Loss: 5.06, Train BLEU: 0.00, Val BLEU: 6.48, Minutes Elapsed: 210.59\n",
      "Sampling from val predictions...\n",
      "Source: 所以 你 已经 拿到 了 这个 奖学 奖学金 richard <UNK>\n",
      "Reference: so you got this scholarship . yep . <EOS>\n",
      "Model: <SOS> so , you to to , the was the\n",
      "Attention Weights: tensor([[5.0746e-06, 9.9989e-01, 1.8141e-08, 2.9299e-08, 9.2911e-08, 7.9278e-06,\n",
      "         8.7557e-07, 7.5238e-06, 2.0822e-05, 7.0360e-05],\n",
      "        [1.7183e-03, 9.9666e-01, 1.0721e-05, 8.3148e-06, 2.4029e-05, 2.6372e-04,\n",
      "         4.0344e-05, 1.2637e-04, 2.4025e-04, 9.0509e-04],\n",
      "        [1.2816e-02, 4.3388e-01, 2.3089e-02, 1.4380e-02, 2.0588e-02, 7.6625e-02,\n",
      "         2.4127e-02, 5.4299e-02, 1.0540e-01, 2.3479e-01],\n",
      "        [6.8778e-03, 3.2706e-01, 2.6113e-02, 2.7880e-02, 2.8179e-02, 1.6818e-01,\n",
      "         2.5335e-02, 5.6185e-02, 1.0569e-01, 2.2850e-01],\n",
      "        [3.9495e-03, 9.9792e-03, 2.2134e-02, 8.5250e-02, 7.5325e-02, 5.2210e-01,\n",
      "         6.1681e-02, 6.3915e-02, 7.0209e-02, 8.5454e-02],\n",
      "        [2.5084e-03, 5.3150e-03, 1.2313e-02, 4.2164e-02, 6.8133e-02, 5.1478e-01,\n",
      "         7.6958e-02, 9.0071e-02, 9.0834e-02, 9.6927e-02],\n",
      "        [2.9764e-03, 7.5644e-03, 3.7721e-03, 1.0879e-02, 3.0518e-02, 5.3020e-01,\n",
      "         8.0640e-02, 9.8207e-02, 1.0386e-01, 1.3139e-01],\n",
      "        [9.9978e-04, 8.3366e-03, 5.3001e-04, 1.5497e-03, 6.4232e-03, 5.9814e-01,\n",
      "         6.1203e-02, 1.0263e-01, 1.0195e-01, 1.1824e-01],\n",
      "        [1.6415e-03, 2.1258e-02, 4.9149e-03, 1.3763e-02, 3.4831e-02, 4.1835e-01,\n",
      "         7.2754e-02, 1.1057e-01, 1.3165e-01, 1.9027e-01]])\n",
      "\n",
      "Source: 康 纳 并 不是 回到 家 向 我 宣布 嘿\n",
      "Reference: now , <UNK> did not come home one day\n",
      "Model: <SOS> and , &apos;s &apos;s , , me , i\n",
      "Attention Weights: tensor([[1.8949e-06, 1.6651e-04, 8.0118e-06, 1.4571e-06, 1.4298e-05, 2.5691e-05,\n",
      "         9.5637e-05, 9.3047e-01, 2.1470e-05, 6.9196e-02],\n",
      "        [3.3544e-03, 4.6980e-01, 2.9983e-02, 3.4924e-03, 6.4672e-03, 6.7511e-03,\n",
      "         1.0773e-02, 2.6591e-01, 3.1965e-03, 2.0027e-01],\n",
      "        [9.3034e-03, 2.6662e-01, 1.6728e-01, 2.9049e-02, 2.0288e-02, 2.1038e-02,\n",
      "         2.5209e-02, 1.1056e-01, 9.9681e-03, 3.4069e-01],\n",
      "        [6.9773e-03, 2.6281e-01, 9.5478e-02, 1.9024e-02, 1.3096e-02, 9.9081e-03,\n",
      "         1.7603e-02, 1.6916e-01, 4.2929e-03, 4.0165e-01],\n",
      "        [4.7322e-03, 9.1044e-02, 1.5305e-01, 4.8501e-02, 4.4878e-02, 4.5977e-02,\n",
      "         4.4467e-02, 1.8655e-01, 1.6495e-02, 3.6431e-01],\n",
      "        [2.8558e-04, 2.3654e-03, 1.3154e-03, 1.2238e-03, 4.7843e-03, 8.1851e-03,\n",
      "         1.4438e-02, 7.4812e-01, 4.7161e-03, 2.1457e-01],\n",
      "        [8.1207e-05, 9.8080e-04, 1.2601e-03, 2.1473e-03, 1.0865e-02, 3.4766e-02,\n",
      "         3.5090e-02, 4.4389e-01, 2.2110e-02, 4.4881e-01],\n",
      "        [2.5600e-05, 1.9662e-04, 1.5185e-04, 5.3437e-04, 6.9498e-03, 2.2858e-02,\n",
      "         3.1390e-02, 5.3600e-01, 2.6200e-02, 3.7570e-01],\n",
      "        [1.3760e-05, 1.2141e-04, 1.3151e-04, 4.5691e-04, 7.6682e-03, 2.6868e-02,\n",
      "         4.3992e-02, 4.6807e-01, 4.9980e-02, 4.0270e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.90, Train Loss: 0.00, Val Loss: 5.03, Train BLEU: 0.00, Val BLEU: 6.51, Minutes Elapsed: 222.24\n",
      "Sampling from val predictions...\n",
      "Source: 这 是 初次 尝试 的 一张 照片 <EOS> <PAD> <PAD>\n",
      "Reference: this was the very first . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> it &apos;s a of of of . was .\n",
      "Attention Weights: tensor([[0.0044, 0.0000, 0.0000, 0.0002, 0.0000, 0.0014, 0.5528, 0.4411, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3947, 0.0403, 0.0496, 0.0696, 0.0171, 0.0584, 0.2383, 0.1321, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2562, 0.0630, 0.1209, 0.1136, 0.0204, 0.0779, 0.2106, 0.1375, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0172, 0.0115, 0.1446, 0.3632, 0.0529, 0.1459, 0.1876, 0.0772, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0209, 0.0105, 0.0949, 0.2871, 0.0479, 0.1729, 0.2316, 0.1343, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0269, 0.0113, 0.0816, 0.1944, 0.0379, 0.1409, 0.2981, 0.2090, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0054, 0.0020, 0.0263, 0.1029, 0.0247, 0.1985, 0.4162, 0.2241, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0171, 0.0069, 0.0476, 0.1336, 0.0558, 0.2056, 0.2891, 0.2442, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0170, 0.0077, 0.0355, 0.1475, 0.0639, 0.2622, 0.2717, 0.1944, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 现在 就 像 爱 <UNK> 顿 博士 一个 科学 科学家\n",
      "Reference: now , like doc <UNK> , a scientist himself\n",
      "Model: <SOS> now , now &apos;s , , is , of\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.0002, 0.0004, 0.0023, 0.0007, 0.0009, 0.0244,\n",
      "         0.9711],\n",
      "        [0.0449, 0.0688, 0.0291, 0.0357, 0.0571, 0.0468, 0.0100, 0.0128, 0.0664,\n",
      "         0.6284],\n",
      "        [0.0225, 0.0657, 0.0488, 0.0651, 0.0838, 0.0335, 0.0061, 0.0068, 0.0454,\n",
      "         0.6223],\n",
      "        [0.0338, 0.0784, 0.0669, 0.0807, 0.0554, 0.0343, 0.0053, 0.0053, 0.0453,\n",
      "         0.5947],\n",
      "        [0.0358, 0.0602, 0.0825, 0.1234, 0.0676, 0.0588, 0.0140, 0.0109, 0.0495,\n",
      "         0.4971],\n",
      "        [0.0083, 0.0110, 0.0249, 0.0914, 0.0581, 0.1249, 0.0284, 0.0179, 0.0731,\n",
      "         0.5620],\n",
      "        [0.0031, 0.0064, 0.0098, 0.0432, 0.0273, 0.1220, 0.0167, 0.0122, 0.0891,\n",
      "         0.6702],\n",
      "        [0.0001, 0.0002, 0.0004, 0.0047, 0.0056, 0.0909, 0.0213, 0.0208, 0.1682,\n",
      "         0.6878],\n",
      "        [0.0002, 0.0007, 0.0016, 0.0125, 0.0101, 0.1029, 0.0225, 0.0207, 0.1359,\n",
      "         0.6929]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.00, Train Loss: 0.00, Val Loss: 5.03, Train BLEU: 0.00, Val BLEU: 6.25, Minutes Elapsed: 230.13\n",
      "Sampling from val predictions...\n",
      "Source: 他 非常 害怕 主人 不敢 逃跑 由于 他 从小 就\n",
      "Reference: terrified of his master , he will not run\n",
      "Model: <SOS> he he , , , he he &apos;t to\n",
      "Attention Weights: tensor([[1.7356e-01, 5.8037e-07, 7.9498e-01, 4.9765e-06, 4.1660e-06, 7.2250e-04,\n",
      "         3.1336e-04, 1.9876e-02, 8.0335e-05, 1.0454e-02],\n",
      "        [8.4380e-01, 1.3231e-04, 1.3956e-01, 2.5660e-05, 1.9120e-05, 1.0566e-03,\n",
      "         4.3871e-04, 5.1905e-03, 3.9156e-04, 9.3828e-03],\n",
      "        [8.3022e-02, 2.9557e-03, 7.5363e-01, 5.5268e-04, 1.7779e-04, 9.7160e-03,\n",
      "         3.6815e-03, 1.9591e-02, 3.2040e-03, 1.2346e-01],\n",
      "        [9.9638e-02, 3.0234e-04, 6.7257e-01, 6.4302e-04, 2.2247e-04, 2.3606e-02,\n",
      "         1.0680e-02, 7.7265e-02, 4.8718e-03, 1.1021e-01],\n",
      "        [5.1863e-02, 3.4733e-04, 4.5382e-01, 5.3628e-03, 1.8395e-03, 6.0783e-02,\n",
      "         3.3519e-02, 1.3676e-01, 1.3276e-02, 2.4243e-01],\n",
      "        [3.9939e-03, 2.9169e-05, 1.6717e-01, 2.3080e-03, 1.4673e-03, 1.4496e-01,\n",
      "         1.0629e-01, 3.6351e-01, 2.8394e-02, 1.8188e-01],\n",
      "        [7.1735e-04, 1.9883e-06, 1.4094e-01, 5.7904e-05, 5.4573e-05, 1.0220e-01,\n",
      "         4.9475e-02, 4.3641e-01, 2.4123e-02, 2.4603e-01],\n",
      "        [2.4263e-03, 4.6993e-05, 3.0677e-01, 1.7871e-03, 6.7765e-04, 1.0318e-01,\n",
      "         4.1594e-02, 2.0432e-01, 2.2708e-02, 3.1649e-01],\n",
      "        [1.2410e-03, 9.4277e-05, 1.2771e-01, 1.1012e-02, 7.9564e-03, 1.8978e-01,\n",
      "         1.3132e-01, 2.0698e-01, 5.4220e-02, 2.6968e-01]])\n",
      "\n",
      "Source: 我 经常 思考 为什么 什么 他们 有 <UNK> 而 我们\n",
      "Reference: i always wondered why they had lights but we\n",
      "Model: <SOS> i i &apos;t why why they going , and\n",
      "Attention Weights: tensor([[9.9898e-01, 1.4831e-10, 1.4504e-07, 1.9910e-06, 4.5316e-08, 7.1209e-05,\n",
      "         1.5076e-06, 7.7944e-06, 4.1644e-05, 8.9209e-04],\n",
      "        [9.6030e-01, 4.3031e-05, 4.9521e-03, 9.0967e-03, 5.5076e-04, 1.1225e-02,\n",
      "         8.3389e-04, 1.1747e-03, 1.5490e-03, 1.0278e-02],\n",
      "        [1.5939e-01, 5.7914e-03, 2.4379e-01, 2.2082e-01, 1.8595e-02, 1.2697e-01,\n",
      "         2.0635e-02, 3.5996e-02, 2.6548e-02, 1.4147e-01],\n",
      "        [8.7188e-03, 2.5475e-03, 2.0226e-01, 4.9933e-01, 4.5810e-02, 1.4636e-01,\n",
      "         1.8053e-02, 1.3633e-02, 1.1036e-02, 5.2258e-02],\n",
      "        [4.5885e-03, 2.0711e-04, 4.6947e-02, 2.3272e-01, 4.8251e-02, 3.9035e-01,\n",
      "         5.5458e-02, 1.5884e-02, 5.2730e-02, 1.5286e-01],\n",
      "        [2.9295e-03, 2.2308e-05, 8.6587e-03, 3.3219e-02, 1.0021e-02, 4.1182e-01,\n",
      "         8.5040e-02, 1.2105e-02, 1.4950e-01, 2.8668e-01],\n",
      "        [7.3253e-04, 1.3129e-06, 1.7994e-03, 1.2122e-02, 2.5945e-03, 2.5797e-01,\n",
      "         7.2666e-02, 8.2398e-03, 1.7885e-01, 4.6503e-01],\n",
      "        [1.3268e-03, 9.5582e-06, 2.0531e-03, 1.7083e-02, 6.9086e-03, 2.2090e-01,\n",
      "         9.6807e-02, 1.0012e-02, 1.9999e-01, 4.4491e-01],\n",
      "        [1.0477e-03, 5.9199e-06, 1.9427e-03, 1.3124e-02, 7.5921e-03, 2.5118e-01,\n",
      "         8.7813e-02, 1.0761e-02, 1.8758e-01, 4.3895e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.15, Train Loss: 0.00, Val Loss: 5.06, Train BLEU: 0.00, Val BLEU: 6.46, Minutes Elapsed: 242.04\n",
      "Sampling from val predictions...\n",
      "Source: 绿地 组织 又 开始 安置 也许 是 20 个 花园\n",
      "Reference: so green grounds has gone on to plant maybe\n",
      "Model: <SOS> and , &apos;s , the be the , ,\n",
      "Attention Weights: tensor([[7.8504e-05, 4.9563e-06, 4.5322e-05, 8.7844e-04, 2.8999e-04, 6.6299e-02,\n",
      "         1.3383e-03, 1.0571e-02, 3.1420e-02, 8.8908e-01],\n",
      "        [1.0353e-01, 1.7524e-02, 3.8338e-02, 4.3572e-02, 1.5140e-02, 1.6735e-01,\n",
      "         2.4801e-02, 3.4180e-02, 6.4981e-02, 4.9058e-01],\n",
      "        [2.3479e-02, 2.2080e-02, 1.0626e-01, 4.1231e-02, 1.6065e-02, 6.0654e-02,\n",
      "         3.3099e-02, 4.9381e-02, 7.1417e-02, 5.7633e-01],\n",
      "        [2.0891e-02, 1.7666e-02, 1.1451e-01, 5.5842e-02, 1.9384e-02, 6.6569e-02,\n",
      "         3.1786e-02, 5.4706e-02, 7.1721e-02, 5.4693e-01],\n",
      "        [3.5540e-03, 2.6831e-03, 2.9882e-02, 5.2659e-02, 2.5206e-02, 1.6827e-01,\n",
      "         5.3543e-02, 9.5168e-02, 1.0680e-01, 4.6224e-01],\n",
      "        [4.5344e-04, 2.5548e-04, 4.0416e-03, 2.1197e-02, 1.0055e-02, 1.2398e-01,\n",
      "         2.8435e-02, 8.0441e-02, 1.3818e-01, 5.9296e-01],\n",
      "        [2.6849e-04, 1.2592e-04, 1.4814e-03, 1.5840e-02, 1.2446e-02, 1.3528e-01,\n",
      "         2.5687e-02, 1.1457e-01, 1.7369e-01, 5.2061e-01],\n",
      "        [2.1744e-04, 8.9727e-05, 6.0181e-04, 2.7654e-03, 2.5435e-03, 8.5813e-02,\n",
      "         2.6004e-02, 7.4250e-02, 2.1825e-01, 5.8947e-01],\n",
      "        [1.3311e-04, 6.4631e-05, 4.1387e-04, 1.8131e-03, 2.2691e-03, 1.1435e-01,\n",
      "         2.9255e-02, 7.5682e-02, 1.7829e-01, 5.9773e-01]])\n",
      "\n",
      "Source: 我们 说 能到 到处 旅行 和 有创造力 创造 创造力 的\n",
      "Reference: we say , &quot; it &apos;s really amazing to\n",
      "Model: <SOS> we we to &quot; , , a , to\n",
      "Attention Weights: tensor([[9.9635e-01, 1.0500e-07, 2.1893e-07, 1.3235e-06, 2.6020e-06, 8.5091e-06,\n",
      "         2.5199e-05, 2.4145e-06, 3.2959e-05, 3.5771e-03],\n",
      "        [9.8401e-01, 1.2974e-03, 3.7509e-04, 4.6031e-04, 2.6841e-04, 3.4700e-04,\n",
      "         6.9623e-04, 1.4705e-04, 6.4570e-04, 1.1750e-02],\n",
      "        [1.4179e-01, 1.6091e-01, 6.7073e-02, 4.5063e-02, 1.4816e-02, 1.9834e-02,\n",
      "         3.3782e-02, 5.9779e-03, 2.4899e-02, 4.8585e-01],\n",
      "        [1.1863e-02, 2.9402e-02, 1.0588e-01, 1.2874e-01, 8.5423e-02, 9.1959e-02,\n",
      "         1.0974e-01, 1.7539e-02, 4.4501e-02, 3.7496e-01],\n",
      "        [4.4981e-02, 1.2053e-02, 2.2324e-02, 9.0681e-02, 4.5086e-02, 5.4488e-02,\n",
      "         9.2897e-02, 7.3263e-03, 5.1398e-02, 5.7877e-01],\n",
      "        [5.3901e-02, 1.4726e-02, 2.0097e-02, 9.6055e-02, 6.3426e-02, 8.0586e-02,\n",
      "         1.0243e-01, 1.6285e-02, 6.9685e-02, 4.8280e-01],\n",
      "        [3.6580e-02, 1.2396e-02, 3.5453e-02, 1.0667e-01, 9.7431e-02, 9.7166e-02,\n",
      "         1.2408e-01, 3.3320e-02, 7.2948e-02, 3.8395e-01],\n",
      "        [6.9442e-02, 1.4510e-03, 1.3280e-02, 6.1711e-02, 1.2617e-01, 1.1259e-01,\n",
      "         2.1309e-01, 2.7432e-02, 9.0055e-02, 2.8478e-01],\n",
      "        [7.0244e-02, 2.9778e-03, 1.4525e-02, 5.7245e-02, 1.1339e-01, 1.1292e-01,\n",
      "         1.8338e-01, 3.1681e-02, 9.6048e-02, 3.1758e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.30, Train Loss: 0.00, Val Loss: 5.04, Train BLEU: 0.00, Val BLEU: 7.02, Minutes Elapsed: 253.73\n",
      "Sampling from val predictions...\n",
      "Source: 我 要 回答 下人 人们 常常 问 我 的 那些\n",
      "Reference: and i &apos;m going to answer the questions that\n",
      "Model: <SOS> i &apos;m &apos;m going to show you a of\n",
      "Attention Weights: tensor([[9.9989e-01, 2.5184e-09, 5.9866e-09, 4.1645e-09, 5.8046e-07, 5.2001e-09,\n",
      "         5.4514e-08, 8.8860e-05, 1.6903e-08, 2.1858e-05],\n",
      "        [9.8488e-01, 4.0518e-03, 1.0305e-03, 5.3814e-04, 2.3481e-03, 9.9662e-05,\n",
      "         2.2577e-04, 3.3203e-03, 1.4095e-04, 3.3640e-03],\n",
      "        [1.9741e-01, 1.2439e-01, 1.4792e-01, 6.8468e-02, 1.3263e-01, 9.7050e-03,\n",
      "         2.5932e-02, 6.8955e-02, 1.1941e-02, 2.1264e-01],\n",
      "        [1.9775e-01, 1.5348e-01, 1.5612e-01, 6.1867e-02, 1.1248e-01, 7.7827e-03,\n",
      "         2.1747e-02, 5.5192e-02, 9.1315e-03, 2.2444e-01],\n",
      "        [1.8617e-02, 2.3590e-02, 1.2517e-01, 1.3711e-01, 3.6553e-01, 2.7798e-02,\n",
      "         2.8238e-02, 8.9818e-02, 1.1964e-02, 1.7216e-01],\n",
      "        [1.4019e-02, 1.0545e-02, 7.8287e-02, 1.0660e-01, 4.0118e-01, 2.6376e-02,\n",
      "         3.8239e-02, 1.1427e-01, 1.4915e-02, 1.9557e-01],\n",
      "        [4.7303e-03, 5.6918e-03, 4.4853e-02, 5.8108e-02, 3.3208e-01, 2.1622e-02,\n",
      "         4.0918e-02, 2.6113e-01, 1.2590e-02, 2.1828e-01],\n",
      "        [4.5274e-03, 2.5559e-03, 2.2509e-02, 5.2217e-02, 2.8331e-01, 4.2204e-02,\n",
      "         6.1961e-02, 3.1275e-01, 2.3498e-02, 1.9447e-01],\n",
      "        [1.3732e-03, 8.4062e-04, 4.8863e-03, 1.2237e-02, 1.3321e-01, 4.6038e-02,\n",
      "         1.2338e-01, 4.9096e-01, 3.6347e-02, 1.5073e-01]])\n",
      "\n",
      "Source: 当然 它 也 有 缺点 这种 路 的 缺点 是\n",
      "Reference: it also has disadvantages , of course , and\n",
      "Model: <SOS> and course not it , it it , it\n",
      "Attention Weights: tensor([[0.0003, 0.4981, 0.0001, 0.0001, 0.0004, 0.2527, 0.0003, 0.0003, 0.0023,\n",
      "         0.2455],\n",
      "        [0.0038, 0.9785, 0.0015, 0.0005, 0.0002, 0.0107, 0.0001, 0.0001, 0.0002,\n",
      "         0.0043],\n",
      "        [0.0512, 0.5394, 0.0707, 0.0321, 0.0151, 0.0843, 0.0100, 0.0068, 0.0188,\n",
      "         0.1715],\n",
      "        [0.0115, 0.1058, 0.0391, 0.0379, 0.1077, 0.2878, 0.0253, 0.0182, 0.0435,\n",
      "         0.3233],\n",
      "        [0.0061, 0.0582, 0.0283, 0.0340, 0.1010, 0.3946, 0.0300, 0.0239, 0.0427,\n",
      "         0.2812],\n",
      "        [0.0014, 0.0090, 0.0039, 0.0055, 0.0628, 0.3789, 0.0423, 0.0316, 0.1347,\n",
      "         0.3300],\n",
      "        [0.0003, 0.0059, 0.0007, 0.0006, 0.0055, 0.2606, 0.0147, 0.0121, 0.1449,\n",
      "         0.5548],\n",
      "        [0.0003, 0.0038, 0.0021, 0.0016, 0.0091, 0.2836, 0.0314, 0.0199, 0.1044,\n",
      "         0.5437],\n",
      "        [0.0003, 0.0039, 0.0021, 0.0032, 0.0234, 0.2294, 0.0701, 0.0477, 0.1481,\n",
      "         0.4718]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.45, Train Loss: 0.00, Val Loss: 5.00, Train BLEU: 0.00, Val BLEU: 6.85, Minutes Elapsed: 265.52\n",
      "Sampling from val predictions...\n",
      "Source: 如果 有 任何 <UNK> 我 就 会 入狱 并 被\n",
      "Reference: if anything seemed unnatural , i could be imprisoned\n",
      "Model: <SOS> if if &apos;s it , i i to that\n",
      "Attention Weights: tensor([[6.1375e-09, 3.2762e-08, 4.6339e-05, 4.9321e-05, 9.9931e-01, 1.2998e-06,\n",
      "         2.4662e-04, 1.6889e-05, 2.1509e-06, 3.2353e-04],\n",
      "        [4.2583e-04, 9.6084e-03, 2.3595e-01, 2.8730e-02, 6.6389e-01, 3.2710e-03,\n",
      "         1.6041e-02, 6.3043e-03, 2.3264e-03, 3.3460e-02],\n",
      "        [3.6958e-03, 3.2822e-02, 2.7238e-01, 9.5685e-02, 1.3697e-01, 2.3874e-02,\n",
      "         8.0064e-02, 3.8902e-02, 2.3993e-02, 2.9162e-01],\n",
      "        [9.1360e-04, 7.6595e-03, 2.5959e-01, 5.0415e-02, 2.6325e-01, 2.2408e-02,\n",
      "         1.0837e-01, 3.7077e-02, 1.8418e-02, 2.3190e-01],\n",
      "        [1.4601e-04, 1.3184e-03, 7.7152e-02, 1.8577e-02, 7.0177e-01, 1.1572e-02,\n",
      "         7.5992e-02, 2.2229e-02, 7.4503e-03, 8.3791e-02],\n",
      "        [2.9769e-05, 1.8139e-04, 1.7660e-02, 1.4278e-02, 5.7641e-01, 1.2615e-02,\n",
      "         1.2153e-01, 4.4231e-02, 1.6038e-02, 1.9703e-01],\n",
      "        [1.3470e-05, 5.9909e-05, 4.4140e-03, 8.6806e-03, 6.7979e-01, 7.9096e-03,\n",
      "         1.2516e-01, 3.5637e-02, 1.0336e-02, 1.2799e-01],\n",
      "        [3.6503e-05, 2.5623e-04, 1.1453e-02, 1.1847e-02, 3.1945e-01, 1.8248e-02,\n",
      "         2.6222e-01, 6.0458e-02, 2.4496e-02, 2.9154e-01],\n",
      "        [1.3620e-05, 1.6181e-04, 1.7809e-02, 5.2486e-03, 2.4298e-01, 2.7126e-02,\n",
      "         2.1197e-01, 1.2601e-01, 4.7087e-02, 3.2160e-01]])\n",
      "\n",
      "Source: 英语 在 韩国 太重 重要 了 所以 我 必须 须要\n",
      "Reference: english was so important in south korea , so\n",
      "Model: <SOS> and in in in , the , , i\n",
      "Attention Weights: tensor([[1.0273e-07, 2.3994e-07, 6.9633e-07, 1.6994e-06, 8.4830e-06, 7.7415e-06,\n",
      "         4.3503e-03, 9.9301e-01, 1.6209e-05, 2.6018e-03],\n",
      "        [1.5292e-02, 2.9823e-02, 2.6451e-02, 1.0141e-02, 1.0129e-02, 9.5330e-03,\n",
      "         1.8798e-01, 5.8038e-01, 8.1109e-03, 1.2216e-01],\n",
      "        [4.0216e-02, 6.7035e-02, 8.1314e-02, 4.2673e-02, 3.4220e-02, 3.6584e-02,\n",
      "         1.2829e-01, 1.8570e-01, 4.0644e-02, 3.4333e-01],\n",
      "        [8.4480e-03, 4.0415e-02, 7.3987e-02, 5.8476e-02, 6.4604e-02, 3.5807e-02,\n",
      "         1.8736e-01, 2.4247e-01, 2.2293e-02, 2.6615e-01],\n",
      "        [1.6183e-03, 6.0070e-03, 2.5714e-02, 5.3082e-02, 6.8891e-02, 3.3843e-02,\n",
      "         2.8856e-01, 2.3587e-01, 2.5701e-02, 2.6072e-01],\n",
      "        [1.0300e-03, 5.0235e-03, 2.7345e-02, 5.8650e-02, 6.0443e-02, 4.3635e-02,\n",
      "         2.0394e-01, 1.4465e-01, 3.5886e-02, 4.1940e-01],\n",
      "        [8.1475e-05, 3.6765e-04, 1.6751e-03, 3.3535e-03, 3.9632e-03, 1.0340e-02,\n",
      "         1.0564e-01, 5.6790e-01, 2.2420e-02, 2.8426e-01],\n",
      "        [2.8273e-05, 2.0143e-04, 9.7550e-04, 6.8247e-03, 1.9357e-02, 3.4740e-02,\n",
      "         2.3819e-01, 2.8197e-01, 5.0218e-02, 3.6749e-01],\n",
      "        [3.0646e-05, 1.4166e-04, 4.2592e-04, 1.2669e-03, 2.7776e-03, 6.4559e-03,\n",
      "         1.2759e-01, 6.6140e-01, 1.7383e-02, 1.8253e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.60, Train Loss: 0.00, Val Loss: 4.97, Train BLEU: 0.00, Val BLEU: 6.32, Minutes Elapsed: 277.29\n",
      "Sampling from val predictions...\n",
      "Source: 但是 言语 语词 词汇 对于 政治 中心 之 作用 是非\n",
      "Reference: but it &apos;s very important that words are at\n",
      "Model: <SOS> but it &apos;s not , , it &apos;s and\n",
      "Attention Weights: tensor([[8.2588e-08, 1.6385e-05, 8.9901e-03, 4.9630e-05, 1.1689e-04, 2.3564e-01,\n",
      "         1.1927e-01, 3.5277e-02, 2.0094e-03, 5.9863e-01],\n",
      "        [2.0823e-03, 8.2653e-02, 5.5294e-01, 7.0903e-03, 3.8384e-03, 7.2555e-02,\n",
      "         4.5575e-02, 2.7867e-02, 5.9547e-03, 1.9945e-01],\n",
      "        [7.6308e-03, 1.0796e-01, 3.9658e-01, 2.6000e-02, 1.1793e-02, 4.4502e-02,\n",
      "         3.7975e-02, 3.0369e-02, 1.2087e-02, 3.2511e-01],\n",
      "        [5.3833e-03, 7.0447e-02, 3.9469e-01, 8.6990e-02, 3.7638e-02, 6.7844e-02,\n",
      "         4.4302e-02, 3.2073e-02, 1.9621e-02, 2.4101e-01],\n",
      "        [7.3671e-04, 1.6122e-02, 2.0322e-01, 8.9450e-02, 7.5248e-02, 1.8243e-01,\n",
      "         9.9842e-02, 5.8131e-02, 4.0662e-02, 2.3416e-01],\n",
      "        [9.7362e-04, 2.3937e-02, 2.2416e-01, 6.8163e-02, 5.4867e-02, 1.6313e-01,\n",
      "         8.8903e-02, 6.0425e-02, 3.7018e-02, 2.7843e-01],\n",
      "        [5.0879e-04, 1.5518e-02, 2.1255e-01, 5.3195e-02, 5.1860e-02, 1.9388e-01,\n",
      "         9.6918e-02, 6.3156e-02, 3.2266e-02, 2.8014e-01],\n",
      "        [6.7012e-05, 3.7519e-03, 4.0822e-02, 4.0742e-03, 5.2268e-03, 2.3610e-01,\n",
      "         1.9976e-01, 8.2250e-02, 1.7163e-02, 4.1079e-01],\n",
      "        [6.7007e-05, 2.3449e-03, 3.1481e-02, 1.6689e-02, 3.1900e-02, 2.6780e-01,\n",
      "         2.1865e-01, 1.4025e-01, 4.3848e-02, 2.4697e-01]])\n",
      "\n",
      "Source: 我 的 父亲 那 就是 他 他 是 他家 家族\n",
      "Reference: and my father -- that &apos;s him -- he\n",
      "Model: <SOS> my my said is , , he he he\n",
      "Attention Weights: tensor([[9.5793e-01, 6.8303e-08, 1.9453e-06, 1.1223e-04, 4.2577e-05, 2.7061e-02,\n",
      "         1.0127e-02, 4.6078e-06, 1.7179e-05, 4.7052e-03],\n",
      "        [6.9972e-01, 1.0065e-03, 3.3082e-02, 3.7022e-02, 1.5489e-02, 1.5698e-01,\n",
      "         4.4309e-02, 6.4973e-04, 8.8093e-04, 1.0860e-02],\n",
      "        [7.6945e-02, 6.4642e-03, 1.6718e-01, 2.3647e-01, 1.2287e-01, 2.5334e-01,\n",
      "         7.8850e-02, 2.9826e-03, 3.1516e-03, 5.1742e-02],\n",
      "        [2.0711e-01, 1.3378e-02, 1.9229e-01, 1.7328e-01, 8.6434e-02, 1.8558e-01,\n",
      "         5.7319e-02, 3.8322e-03, 3.2005e-03, 7.7579e-02],\n",
      "        [9.1567e-04, 2.0523e-04, 1.2381e-02, 3.0432e-01, 1.8145e-01, 3.8481e-01,\n",
      "         8.9755e-02, 2.5838e-03, 2.5163e-03, 2.1067e-02],\n",
      "        [1.2546e-03, 1.0914e-04, 5.9669e-03, 1.8695e-01, 1.7090e-01, 4.4162e-01,\n",
      "         1.4091e-01, 5.9762e-03, 7.9182e-03, 3.8395e-02],\n",
      "        [7.5898e-03, 3.8948e-04, 6.6040e-03, 1.2579e-01, 1.5026e-01, 3.6534e-01,\n",
      "         1.8740e-01, 1.2271e-02, 1.6827e-02, 1.2753e-01],\n",
      "        [2.0110e-02, 2.9364e-04, 4.3844e-03, 8.7727e-02, 1.0279e-01, 3.7874e-01,\n",
      "         2.5522e-01, 1.3682e-02, 2.9648e-02, 1.0740e-01],\n",
      "        [2.8666e-03, 3.1652e-05, 8.9191e-04, 4.7598e-02, 9.6258e-02, 4.3199e-01,\n",
      "         2.9612e-01, 1.1560e-02, 2.3176e-02, 8.9510e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.75, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 7.19, Minutes Elapsed: 289.18\n",
      "Sampling from val predictions...\n",
      "Source: 他们 必须 须要 赶快 <UNK> 逃走 所以 我 开始 计划\n",
      "Reference: they had to get out quickly , so i\n",
      "Model: <SOS> they they to <UNK> , , , and i\n",
      "Attention Weights: tensor([[0.0065, 0.0000, 0.0000, 0.0001, 0.0005, 0.0003, 0.0509, 0.6882, 0.0008,\n",
      "         0.2527],\n",
      "        [0.9731, 0.0047, 0.0003, 0.0001, 0.0004, 0.0001, 0.0030, 0.0110, 0.0002,\n",
      "         0.0070],\n",
      "        [0.2108, 0.1584, 0.0829, 0.0274, 0.0333, 0.0039, 0.0466, 0.1213, 0.0062,\n",
      "         0.3091],\n",
      "        [0.0341, 0.0229, 0.0456, 0.0481, 0.0555, 0.0192, 0.1737, 0.2523, 0.0158,\n",
      "         0.3329],\n",
      "        [0.0079, 0.0087, 0.0258, 0.0393, 0.0309, 0.0256, 0.2733, 0.3315, 0.0168,\n",
      "         0.2402],\n",
      "        [0.0107, 0.0033, 0.0089, 0.0226, 0.0207, 0.0211, 0.3030, 0.3929, 0.0137,\n",
      "         0.2030],\n",
      "        [0.0051, 0.0015, 0.0036, 0.0145, 0.0216, 0.0319, 0.4231, 0.2853, 0.0232,\n",
      "         0.1903],\n",
      "        [0.0089, 0.0012, 0.0027, 0.0095, 0.0159, 0.0120, 0.2502, 0.3941, 0.0185,\n",
      "         0.2871],\n",
      "        [0.0013, 0.0001, 0.0002, 0.0010, 0.0027, 0.0020, 0.1162, 0.5794, 0.0108,\n",
      "         0.2864]])\n",
      "\n",
      "Source: 不过 可悲 的 是 神经 神经系 神经系统 系统 <UNK> 疾病\n",
      "Reference: now , sadly , neurological disorders such as parkinson\n",
      "Model: <SOS> but , the is the &apos;s , a ,\n",
      "Attention Weights: tensor([[0.0000, 0.0001, 0.0000, 0.0021, 0.0088, 0.0216, 0.0049, 0.0065, 0.0765,\n",
      "         0.8793],\n",
      "        [0.0108, 0.3842, 0.0252, 0.1939, 0.0255, 0.0242, 0.0105, 0.0191, 0.0820,\n",
      "         0.2245],\n",
      "        [0.0166, 0.1705, 0.0464, 0.1656, 0.0511, 0.0466, 0.0290, 0.0519, 0.1612,\n",
      "         0.2611],\n",
      "        [0.0073, 0.1199, 0.0283, 0.1608, 0.0686, 0.0513, 0.0284, 0.0463, 0.1350,\n",
      "         0.3540],\n",
      "        [0.0083, 0.0738, 0.0307, 0.2375, 0.1029, 0.0673, 0.0326, 0.0489, 0.1091,\n",
      "         0.2889],\n",
      "        [0.0015, 0.0109, 0.0047, 0.1309, 0.1804, 0.1060, 0.0423, 0.0536, 0.0788,\n",
      "         0.3909],\n",
      "        [0.0005, 0.0032, 0.0032, 0.1407, 0.2080, 0.1142, 0.0424, 0.0542, 0.0647,\n",
      "         0.3690],\n",
      "        [0.0003, 0.0004, 0.0004, 0.0159, 0.0779, 0.0440, 0.0216, 0.0744, 0.0383,\n",
      "         0.7267],\n",
      "        [0.0000, 0.0001, 0.0000, 0.0026, 0.0375, 0.0313, 0.0165, 0.0545, 0.0433,\n",
      "         0.8142]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.90, Train Loss: 0.00, Val Loss: 4.94, Train BLEU: 0.00, Val BLEU: 7.13, Minutes Elapsed: 300.99\n",
      "Sampling from val predictions...\n",
      "Source: 现在 我们 在 <UNK> 郡 英国 教授 16 岁 青年\n",
      "Reference: now we teach entrepreneurship to <UNK> in <UNK> ,\n",
      "Model: <SOS> now , we now in the , in ,\n",
      "Attention Weights: tensor([[1.7191e-06, 9.6441e-01, 4.5390e-07, 3.2313e-06, 4.6214e-05, 1.1827e-04,\n",
      "         7.1530e-06, 2.4687e-04, 2.5970e-04, 3.4911e-02],\n",
      "        [1.1839e-03, 9.9061e-01, 6.5451e-05, 4.4704e-04, 3.1331e-04, 1.8290e-04,\n",
      "         5.7676e-05, 4.2915e-04, 3.5154e-04, 6.3574e-03],\n",
      "        [1.1642e-02, 4.2140e-01, 3.2221e-02, 5.9399e-02, 2.9612e-02, 1.9141e-02,\n",
      "         7.9722e-03, 3.9982e-02, 3.7931e-02, 3.4070e-01],\n",
      "        [2.5917e-03, 4.8054e-01, 2.3281e-02, 3.6690e-02, 3.7598e-02, 2.2125e-02,\n",
      "         5.2903e-03, 3.4359e-02, 2.9587e-02, 3.2794e-01],\n",
      "        [6.3302e-04, 2.5217e-02, 2.0143e-02, 3.1163e-02, 2.1166e-01, 1.9416e-01,\n",
      "         1.8610e-02, 8.2973e-02, 5.8176e-02, 3.5726e-01],\n",
      "        [9.2158e-05, 4.7234e-03, 2.2129e-03, 1.0336e-02, 1.2164e-01, 2.8344e-01,\n",
      "         3.7411e-02, 1.1373e-01, 7.5548e-02, 3.5087e-01],\n",
      "        [5.9999e-06, 1.2714e-03, 1.1749e-04, 2.4781e-03, 4.0689e-02, 1.6993e-01,\n",
      "         3.9415e-02, 1.9350e-01, 1.2056e-01, 4.3203e-01],\n",
      "        [1.1743e-06, 2.8520e-04, 1.0284e-04, 2.0687e-03, 1.8554e-02, 1.0352e-01,\n",
      "         4.5319e-02, 1.8118e-01, 2.0578e-01, 4.4319e-01],\n",
      "        [1.6375e-07, 2.1077e-04, 1.3060e-06, 1.9916e-04, 2.7758e-03, 1.8527e-02,\n",
      "         4.2244e-03, 8.3903e-02, 1.1100e-01, 7.7916e-01]])\n",
      "\n",
      "Source: 爱 <UNK> 顿 博士 用 这 张 子弹 <UNK> 苹果\n",
      "Reference: doc <UNK> inspired us with awe and curiosity with\n",
      "Model: <SOS> and the , is is that is that is\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0002, 0.0001, 0.0001, 0.2836, 0.0065, 0.0115, 0.0506,\n",
      "         0.6473],\n",
      "        [0.0278, 0.0246, 0.0602, 0.0114, 0.0026, 0.6129, 0.0183, 0.0143, 0.0346,\n",
      "         0.1934],\n",
      "        [0.0389, 0.0548, 0.1336, 0.0447, 0.0147, 0.2389, 0.0457, 0.0371, 0.1025,\n",
      "         0.2891],\n",
      "        [0.0153, 0.0219, 0.1399, 0.0708, 0.0139, 0.4280, 0.0404, 0.0257, 0.0369,\n",
      "         0.2073],\n",
      "        [0.0034, 0.0039, 0.0375, 0.0333, 0.0067, 0.4813, 0.0502, 0.0480, 0.0416,\n",
      "         0.2940],\n",
      "        [0.0002, 0.0005, 0.0046, 0.0134, 0.0073, 0.3486, 0.0950, 0.0905, 0.0648,\n",
      "         0.3751],\n",
      "        [0.0003, 0.0006, 0.0027, 0.0050, 0.0035, 0.2179, 0.0970, 0.1018, 0.0549,\n",
      "         0.5164],\n",
      "        [0.0001, 0.0002, 0.0005, 0.0038, 0.0030, 0.1105, 0.0775, 0.1093, 0.0705,\n",
      "         0.6245],\n",
      "        [0.0001, 0.0004, 0.0009, 0.0021, 0.0017, 0.1146, 0.0712, 0.0999, 0.0506,\n",
      "         0.6584]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.00, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 6.69, Minutes Elapsed: 308.99\n",
      "Sampling from val predictions...\n",
      "Source: 谢谢 掌声 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank you . <EOS> you <EOS> . . .\n",
      "Attention Weights: tensor([[0.0000, 0.9699, 0.0301, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0328, 0.9245, 0.0427, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2206, 0.5947, 0.1846, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2260, 0.5060, 0.2679, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0656, 0.5862, 0.3482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1445, 0.5929, 0.2626, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0768, 0.4676, 0.4556, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1110, 0.4340, 0.4550, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1035, 0.4218, 0.4747, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 禁止 在 一小 一小块 小块 土地 地上 种 食物 而\n",
      "Reference: a warrant for planting food on a piece of\n",
      "Model: <SOS> and , of in , in in , ,\n",
      "Attention Weights: tensor([[5.1660e-06, 9.8326e-05, 2.4087e-04, 1.7904e-04, 5.1768e-04, 8.9395e-04,\n",
      "         1.8617e-03, 3.5808e-02, 1.2689e-03, 9.5913e-01],\n",
      "        [6.3644e-03, 3.8397e-02, 3.6723e-02, 1.7374e-02, 1.8723e-02, 1.3291e-02,\n",
      "         1.5778e-02, 1.6646e-01, 9.2116e-03, 6.7768e-01],\n",
      "        [1.1864e-02, 5.4131e-02, 4.6179e-02, 3.2930e-02, 3.4147e-02, 2.7114e-02,\n",
      "         2.4027e-02, 1.4603e-01, 2.1570e-02, 6.0200e-01],\n",
      "        [1.1135e-02, 6.9642e-02, 7.6819e-02, 4.9643e-02, 5.2109e-02, 4.8453e-02,\n",
      "         4.1434e-02, 1.2156e-01, 2.7354e-02, 5.0185e-01],\n",
      "        [9.4511e-04, 1.9471e-02, 4.1429e-02, 4.9351e-02, 1.2130e-01, 1.1959e-01,\n",
      "         1.0354e-01, 1.7303e-01, 3.6168e-02, 3.3517e-01],\n",
      "        [3.8610e-04, 1.0637e-02, 9.3222e-03, 1.2178e-02, 6.3160e-02, 1.1890e-01,\n",
      "         1.5501e-01, 2.5387e-01, 4.7997e-02, 3.2853e-01],\n",
      "        [3.2769e-05, 5.6912e-04, 7.6105e-04, 1.2499e-03, 8.4977e-03, 3.1705e-02,\n",
      "         6.2457e-02, 4.5181e-01, 4.0212e-02, 4.0270e-01],\n",
      "        [6.9435e-05, 7.7282e-04, 1.7509e-03, 2.5085e-03, 1.1092e-02, 3.2515e-02,\n",
      "         4.0752e-02, 3.7140e-01, 4.7317e-02, 4.9182e-01],\n",
      "        [5.5528e-05, 1.2091e-03, 1.2725e-03, 2.2949e-03, 1.5082e-02, 4.7670e-02,\n",
      "         7.8565e-02, 2.6248e-01, 5.4775e-02, 5.3660e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.15, Train Loss: 0.00, Val Loss: 4.99, Train BLEU: 0.00, Val BLEU: 6.87, Minutes Elapsed: 320.85\n",
      "Sampling from val predictions...\n",
      "Source: 虐待 只能 活 在 沉默 中 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: abuse thrives only in silence . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> and &apos;s . . . . <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.0000, 0.0001, 0.0000, 0.0003, 0.0037, 0.8678, 0.1282, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0274, 0.0700, 0.0185, 0.0346, 0.1051, 0.5645, 0.1798, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0272, 0.1131, 0.0392, 0.0369, 0.0849, 0.3840, 0.3147, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0129, 0.0286, 0.0527, 0.1375, 0.2199, 0.4075, 0.1409, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0046, 0.0181, 0.0111, 0.0606, 0.2147, 0.4981, 0.1927, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0104, 0.0303, 0.0317, 0.0993, 0.1904, 0.4037, 0.2342, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0092, 0.0329, 0.0438, 0.1095, 0.1853, 0.4070, 0.2123, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.0110, 0.0041, 0.0278, 0.2552, 0.4709, 0.2278, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0081, 0.0173, 0.0246, 0.0799, 0.2570, 0.3399, 0.2731, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 我们 实验 实验室 室里 有 工业 <UNK> 的 机器 来\n",
      "Reference: so i said we have such an industrial machine\n",
      "Model: <SOS> we we we to &quot; have <UNK> , ,\n",
      "Attention Weights: tensor([[9.9786e-01, 1.6055e-07, 4.0054e-07, 4.2188e-08, 7.5726e-08, 1.5127e-06,\n",
      "         6.0054e-06, 2.4044e-06, 1.3565e-04, 1.9974e-03],\n",
      "        [9.7950e-01, 1.3915e-03, 1.0977e-03, 2.8506e-04, 3.2845e-04, 3.0395e-04,\n",
      "         5.3666e-04, 2.0615e-04, 3.1306e-03, 1.3215e-02],\n",
      "        [1.5865e-01, 7.6426e-02, 1.0641e-01, 4.8714e-02, 4.4277e-02, 3.1848e-02,\n",
      "         5.8558e-02, 2.0822e-02, 9.0983e-02, 3.6331e-01],\n",
      "        [2.3549e-01, 6.1241e-02, 8.5853e-02, 4.0214e-02, 3.2216e-02, 2.7190e-02,\n",
      "         4.7904e-02, 1.4678e-02, 7.7408e-02, 3.7781e-01],\n",
      "        [1.6702e-01, 3.6236e-02, 1.4034e-01, 1.3148e-01, 6.1725e-02, 5.7101e-02,\n",
      "         3.8901e-02, 1.4065e-02, 7.2120e-02, 2.8101e-01],\n",
      "        [1.5302e-01, 8.8742e-03, 2.6350e-02, 1.8722e-02, 3.4330e-02, 5.1678e-02,\n",
      "         3.7089e-02, 1.6153e-02, 1.6833e-01, 4.8545e-01],\n",
      "        [1.3801e-03, 2.2498e-03, 5.6076e-03, 1.1851e-02, 2.9727e-02, 4.6564e-02,\n",
      "         2.2258e-02, 5.2540e-02, 3.9445e-01, 4.3337e-01],\n",
      "        [3.9622e-04, 6.1076e-04, 2.0528e-03, 5.6565e-03, 2.2396e-02, 5.2676e-02,\n",
      "         1.9418e-02, 7.6584e-02, 4.1897e-01, 4.0124e-01],\n",
      "        [4.6436e-04, 8.3576e-05, 2.9220e-04, 3.9136e-04, 2.2817e-03, 2.1095e-02,\n",
      "         1.5216e-02, 3.4443e-02, 4.1367e-01, 5.1207e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.30, Train Loss: 0.00, Val Loss: 4.98, Train BLEU: 0.00, Val BLEU: 7.16, Minutes Elapsed: 332.75\n",
      "Sampling from val predictions...\n",
      "Source: 他们 能 消灭 所有 尸体 上 的 细菌 他们 能\n",
      "Reference: they help to kill all the bacteria . they\n",
      "Model: <SOS> they &apos;re a the the , they , they\n",
      "Attention Weights: tensor([[0.0336, 0.0000, 0.0015, 0.0002, 0.0018, 0.0072, 0.0008, 0.4108, 0.0383,\n",
      "         0.5059],\n",
      "        [0.9705, 0.0040, 0.0027, 0.0002, 0.0013, 0.0013, 0.0002, 0.0079, 0.0039,\n",
      "         0.0079],\n",
      "        [0.1222, 0.0830, 0.2340, 0.0223, 0.0360, 0.0791, 0.0107, 0.1411, 0.0323,\n",
      "         0.2394],\n",
      "        [0.0060, 0.0104, 0.1824, 0.0395, 0.0763, 0.1497, 0.0195, 0.3333, 0.0383,\n",
      "         0.1446],\n",
      "        [0.0009, 0.0016, 0.0323, 0.0381, 0.1296, 0.2283, 0.0428, 0.3958, 0.0773,\n",
      "         0.0533],\n",
      "        [0.0003, 0.0004, 0.0162, 0.0120, 0.0683, 0.1627, 0.0251, 0.5333, 0.1157,\n",
      "         0.0660],\n",
      "        [0.0003, 0.0002, 0.0034, 0.0024, 0.0264, 0.0810, 0.0103, 0.5384, 0.2440,\n",
      "         0.0936],\n",
      "        [0.0007, 0.0004, 0.0038, 0.0024, 0.0185, 0.0715, 0.0163, 0.3924, 0.2887,\n",
      "         0.2053],\n",
      "        [0.0012, 0.0005, 0.0044, 0.0043, 0.0246, 0.0861, 0.0197, 0.4244, 0.2769,\n",
      "         0.1580]])\n",
      "\n",
      "Source: 这 是 个 很 悲惨 的 事实 那 就是 朝鲜\n",
      "Reference: it &apos;s tragic that north koreans have to hide\n",
      "Model: <SOS> this is a very it &apos;s the it ,\n",
      "Attention Weights: tensor([[0.5057, 0.0000, 0.0097, 0.0000, 0.0033, 0.0001, 0.0038, 0.0790, 0.0023,\n",
      "         0.3960],\n",
      "        [0.5185, 0.0227, 0.2031, 0.0161, 0.0379, 0.0077, 0.0227, 0.0692, 0.0144,\n",
      "         0.0878],\n",
      "        [0.2058, 0.0321, 0.2698, 0.0379, 0.1086, 0.0110, 0.0495, 0.0763, 0.0247,\n",
      "         0.1844],\n",
      "        [0.0107, 0.0039, 0.2437, 0.0541, 0.3434, 0.0314, 0.0998, 0.1041, 0.0201,\n",
      "         0.0888],\n",
      "        [0.0067, 0.0020, 0.0980, 0.0352, 0.2282, 0.0329, 0.1751, 0.2719, 0.0391,\n",
      "         0.1108],\n",
      "        [0.0034, 0.0004, 0.0115, 0.0019, 0.0524, 0.0055, 0.1159, 0.6097, 0.0624,\n",
      "         0.1370],\n",
      "        [0.0027, 0.0004, 0.0215, 0.0072, 0.0936, 0.0203, 0.1141, 0.3642, 0.1173,\n",
      "         0.2587],\n",
      "        [0.0005, 0.0001, 0.0032, 0.0007, 0.0166, 0.0033, 0.0472, 0.5547, 0.1143,\n",
      "         0.2595],\n",
      "        [0.0025, 0.0001, 0.0024, 0.0003, 0.0115, 0.0023, 0.0331, 0.5952, 0.1087,\n",
      "         0.2439]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.45, Train Loss: 0.00, Val Loss: 4.94, Train BLEU: 0.00, Val BLEU: 7.32, Minutes Elapsed: 344.73\n",
      "Sampling from val predictions...\n",
      "Source: 我 也 很 幸运 幸运地 因为 这项 项发明 发明 而\n",
      "Reference: because of this invention , i was lucky to\n",
      "Model: <SOS> i &apos;m &apos;m , , i &apos;m , ,\n",
      "Attention Weights: tensor([[9.9963e-01, 1.0531e-09, 9.7017e-10, 3.9584e-09, 1.3344e-07, 1.5434e-05,\n",
      "         3.7372e-05, 2.4616e-07, 1.2757e-06, 3.1932e-04],\n",
      "        [9.8189e-01, 1.6245e-03, 5.0818e-04, 5.6221e-04, 6.2468e-04, 4.5225e-03,\n",
      "         5.2963e-03, 2.3004e-04, 3.9258e-04, 4.3519e-03],\n",
      "        [1.4841e-01, 1.3717e-01, 1.6442e-01, 6.3587e-02, 3.5693e-02, 1.5352e-01,\n",
      "         1.0067e-01, 1.0670e-02, 1.5568e-02, 1.7029e-01],\n",
      "        [1.5820e-01, 3.2453e-02, 6.0199e-02, 4.5162e-02, 2.3131e-02, 1.9834e-01,\n",
      "         1.3345e-01, 1.1618e-02, 2.6660e-02, 3.1079e-01],\n",
      "        [2.5614e-01, 8.9457e-03, 2.0209e-02, 1.8882e-02, 1.6890e-02, 2.0603e-01,\n",
      "         9.7676e-02, 1.2438e-02, 3.9232e-02, 3.2356e-01],\n",
      "        [1.6901e-01, 1.1397e-03, 4.8979e-03, 6.7855e-03, 2.5906e-02, 3.6740e-01,\n",
      "         9.6089e-02, 2.1747e-02, 5.7210e-02, 2.4981e-01],\n",
      "        [2.0850e-02, 3.2967e-05, 1.1279e-04, 5.8562e-04, 7.6675e-03, 2.7136e-01,\n",
      "         1.7448e-01, 1.2838e-02, 5.4025e-02, 4.5805e-01],\n",
      "        [2.3578e-02, 2.9682e-04, 1.1878e-03, 3.3139e-03, 9.0016e-03, 1.9170e-01,\n",
      "         2.4271e-01, 1.4978e-02, 5.8685e-02, 4.5456e-01],\n",
      "        [8.0668e-04, 1.8506e-04, 9.0506e-04, 6.3049e-03, 3.7764e-02, 4.8691e-01,\n",
      "         2.6381e-01, 2.4227e-02, 3.6816e-02, 1.4227e-01]])\n",
      "\n",
      "Source: 我 厌倦 了 这些 现实 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: i got tired of seeing this happening . <EOS>\n",
      "Model: <SOS> i i this these . . <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9994, 0.0000, 0.0000, 0.0001, 0.0004, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.8303, 0.0032, 0.0051, 0.1426, 0.0089, 0.0098, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1125, 0.0390, 0.1015, 0.6241, 0.0688, 0.0540, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0106, 0.0072, 0.0524, 0.8303, 0.0662, 0.0334, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0272, 0.0112, 0.0784, 0.6099, 0.2080, 0.0654, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0795, 0.0103, 0.0394, 0.4757, 0.2852, 0.1099, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0651, 0.0150, 0.0631, 0.4507, 0.2565, 0.1496, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0724, 0.0141, 0.0419, 0.4731, 0.2585, 0.1401, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0760, 0.0164, 0.0465, 0.3375, 0.2335, 0.2901, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.60, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 7.09, Minutes Elapsed: 356.50\n",
      "Sampling from val predictions...\n",
      "Source: 我 觉得 十分 自豪 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: and i was very proud . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> i i think that . . <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9997, 0.0000, 0.0000, 0.0002, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9820, 0.0019, 0.0010, 0.0115, 0.0036, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3185, 0.1767, 0.1039, 0.2870, 0.1140, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2993, 0.1943, 0.1402, 0.2483, 0.1179, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0412, 0.0812, 0.3614, 0.3066, 0.2096, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0625, 0.0743, 0.2234, 0.4349, 0.2049, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0927, 0.0493, 0.1678, 0.4045, 0.2857, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1221, 0.0312, 0.0930, 0.3970, 0.3565, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0560, 0.0259, 0.1035, 0.4821, 0.3326, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 我 只 想 说 一句 话 对 <UNK> 对 sam\n",
      "Reference: and if i could communicate just one thing to\n",
      "Model: <SOS> i i said want to , , the the\n",
      "Attention Weights: tensor([[9.9856e-01, 1.1620e-08, 2.2401e-07, 1.0877e-07, 1.2677e-08, 2.7936e-08,\n",
      "         1.1246e-07, 2.3508e-06, 5.6947e-06, 1.4334e-03],\n",
      "        [9.8702e-01, 6.5937e-04, 7.0953e-03, 7.6824e-04, 1.6916e-04, 1.7394e-04,\n",
      "         2.7041e-04, 3.6238e-04, 3.5142e-04, 3.1282e-03],\n",
      "        [1.6797e-01, 7.1776e-02, 5.5418e-01, 6.0294e-02, 7.4224e-03, 5.5300e-03,\n",
      "         1.1289e-02, 1.4708e-02, 1.2432e-02, 9.4401e-02],\n",
      "        [1.7574e-01, 6.3718e-02, 5.2766e-01, 7.2659e-02, 8.8113e-03, 5.2857e-03,\n",
      "         9.4636e-03, 1.4405e-02, 1.1689e-02, 1.1057e-01],\n",
      "        [7.7282e-02, 2.0566e-02, 4.3473e-01, 2.1442e-01, 2.6368e-02, 1.2817e-02,\n",
      "         2.1967e-02, 2.0969e-02, 1.9586e-02, 1.5129e-01],\n",
      "        [1.7292e-02, 5.8252e-03, 1.6399e-01, 3.1102e-01, 1.0218e-01, 4.3404e-02,\n",
      "         3.6077e-02, 3.7030e-02, 5.0089e-02, 2.3309e-01],\n",
      "        [7.3784e-03, 1.0597e-03, 4.4782e-02, 2.4809e-01, 1.2351e-01, 5.8724e-02,\n",
      "         6.2089e-02, 3.6391e-02, 7.7199e-02, 3.4078e-01],\n",
      "        [3.0171e-03, 5.0324e-05, 1.2068e-03, 2.6922e-02, 3.1728e-02, 2.7178e-02,\n",
      "         6.3989e-02, 2.9254e-02, 2.7250e-01, 5.4416e-01],\n",
      "        [8.8085e-04, 2.7915e-05, 5.1979e-04, 1.6501e-02, 3.4383e-02, 5.0045e-02,\n",
      "         8.9152e-02, 2.2907e-02, 3.5905e-01, 4.2654e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.75, Train Loss: 0.00, Val Loss: 4.92, Train BLEU: 0.00, Val BLEU: 7.34, Minutes Elapsed: 368.22\n",
      "Sampling from val predictions...\n",
      "Source: 解决 的 办法 就是 使用 多孔 沥青 <UNK> <UNK> <EOS>\n",
      "Reference: the solution for that is to make roads out\n",
      "Model: <SOS> the the is the is is the the <UNK>\n",
      "Attention Weights: tensor([[0.0002, 0.0004, 0.0382, 0.0700, 0.0042, 0.0334, 0.1289, 0.2938, 0.3445,\n",
      "         0.0864],\n",
      "        [0.0661, 0.0534, 0.5731, 0.1376, 0.0036, 0.0134, 0.0163, 0.0451, 0.0644,\n",
      "         0.0271],\n",
      "        [0.0815, 0.0382, 0.1734, 0.1275, 0.0102, 0.0503, 0.0430, 0.1729, 0.1725,\n",
      "         0.1305],\n",
      "        [0.0515, 0.0407, 0.1155, 0.1855, 0.0170, 0.0366, 0.0447, 0.2099, 0.1657,\n",
      "         0.1328],\n",
      "        [0.0098, 0.0096, 0.0520, 0.1253, 0.0419, 0.1147, 0.0788, 0.1866, 0.2286,\n",
      "         0.1526],\n",
      "        [0.0102, 0.0089, 0.0312, 0.1989, 0.0677, 0.1105, 0.0641, 0.1814, 0.1897,\n",
      "         0.1374],\n",
      "        [0.0084, 0.0083, 0.0199, 0.0995, 0.0576, 0.1584, 0.0885, 0.1831, 0.2251,\n",
      "         0.1512],\n",
      "        [0.0030, 0.0029, 0.0100, 0.0356, 0.0666, 0.2256, 0.1075, 0.1774, 0.2236,\n",
      "         0.1477],\n",
      "        [0.0118, 0.0108, 0.0267, 0.0596, 0.0911, 0.3154, 0.1371, 0.1053, 0.1425,\n",
      "         0.0998]])\n",
      "\n",
      "Source: 就 这个 问题 在 国会 会上 争论 了 很久 很久很久\n",
      "Reference: and this was debated in congress for ages and\n",
      "Model: <SOS> and , problem a , the , <EOS> ,\n",
      "Attention Weights: tensor([[1.7844e-05, 1.6282e-01, 1.8017e-01, 1.7714e-03, 8.8563e-02, 1.2602e-03,\n",
      "         1.4308e-03, 7.0039e-03, 3.7495e-03, 5.5321e-01],\n",
      "        [3.2069e-03, 6.9718e-01, 2.7610e-01, 1.5903e-03, 1.0411e-02, 5.3032e-04,\n",
      "         5.1239e-04, 5.9425e-04, 6.9727e-04, 9.1853e-03],\n",
      "        [1.2173e-02, 3.3904e-01, 3.7847e-01, 3.4740e-02, 5.4603e-02, 9.4964e-03,\n",
      "         8.6986e-03, 1.8322e-02, 1.7031e-02, 1.2743e-01],\n",
      "        [1.1380e-02, 2.9153e-01, 3.5279e-01, 5.4046e-02, 8.1889e-02, 1.0598e-02,\n",
      "         8.1320e-03, 1.7170e-02, 1.5809e-02, 1.5665e-01],\n",
      "        [3.3484e-03, 5.5161e-02, 5.3405e-01, 1.1536e-01, 1.2025e-01, 2.1956e-02,\n",
      "         1.4172e-02, 2.0153e-02, 1.8237e-02, 9.7317e-02],\n",
      "        [2.7553e-03, 1.2463e-01, 2.4745e-01, 1.0551e-01, 2.0748e-01, 4.5188e-02,\n",
      "         2.9397e-02, 3.8263e-02, 3.5234e-02, 1.6410e-01],\n",
      "        [2.3078e-05, 1.1127e-02, 1.1633e-03, 3.3297e-03, 4.1854e-01, 2.4246e-02,\n",
      "         1.5682e-02, 6.6350e-02, 4.1325e-02, 4.1822e-01],\n",
      "        [1.0871e-05, 2.2737e-03, 1.3026e-02, 2.2900e-02, 2.7379e-01, 3.2577e-02,\n",
      "         5.2836e-02, 1.6543e-01, 7.3276e-02, 3.6389e-01],\n",
      "        [7.5329e-06, 1.9490e-03, 4.8410e-04, 1.5330e-03, 5.3085e-02, 1.4494e-02,\n",
      "         2.6408e-02, 1.3529e-01, 9.7426e-02, 6.6933e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.90, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 7.55, Minutes Elapsed: 379.95\n",
      "Sampling from val predictions...\n",
      "Source: 在 塔利 塔利班 控制 阿富汗 的 那些 年 我 记得\n",
      "Reference: during taliban years , i remember there were times\n",
      "Model: <SOS> in the , , , was a the a\n",
      "Attention Weights: tensor([[8.1207e-07, 6.5133e-06, 1.6443e-05, 5.1893e-06, 1.2692e-05, 1.6708e-05,\n",
      "         1.9187e-05, 1.9421e-03, 9.6529e-01, 3.2692e-02],\n",
      "        [3.3204e-03, 3.0124e-02, 2.6883e-02, 2.5998e-03, 2.7428e-03, 4.7814e-03,\n",
      "         5.2928e-03, 4.2026e-02, 7.0211e-01, 1.8012e-01],\n",
      "        [6.3406e-03, 4.0971e-02, 5.2743e-02, 2.1948e-02, 1.1569e-02, 1.9318e-02,\n",
      "         1.5064e-02, 7.6852e-02, 3.7628e-01, 3.7891e-01],\n",
      "        [1.0884e-03, 4.3879e-03, 2.3569e-02, 3.5772e-02, 1.4433e-02, 3.1000e-02,\n",
      "         1.4870e-02, 8.5935e-02, 2.7396e-01, 5.1499e-01],\n",
      "        [2.0859e-04, 9.7893e-04, 2.5095e-03, 1.0212e-02, 1.5247e-02, 3.4286e-02,\n",
      "         2.2613e-02, 2.1458e-01, 4.3161e-01, 2.6776e-01],\n",
      "        [3.0976e-05, 2.1906e-04, 1.1804e-03, 3.9976e-03, 7.7972e-03, 2.7670e-02,\n",
      "         1.0809e-02, 1.3794e-01, 5.7147e-01, 2.3889e-01],\n",
      "        [3.7547e-05, 5.8491e-04, 3.0769e-03, 7.1979e-03, 1.2630e-02, 3.1274e-02,\n",
      "         2.0529e-02, 1.2764e-01, 3.1951e-01, 4.7752e-01],\n",
      "        [2.6200e-04, 2.2733e-03, 4.7886e-03, 1.1617e-02, 2.2375e-02, 2.1978e-02,\n",
      "         1.8893e-02, 1.1170e-01, 1.4904e-01, 6.5707e-01],\n",
      "        [8.9808e-05, 3.5510e-04, 1.5399e-03, 7.1292e-03, 1.4162e-02, 2.1086e-02,\n",
      "         3.0084e-02, 1.2463e-01, 1.4380e-01, 6.5712e-01]])\n",
      "\n",
      "Source: 但是 言语 语词 词汇 对于 政治 中心 之 作用 是非\n",
      "Reference: but it &apos;s very important that words are at\n",
      "Model: <SOS> but , &apos;s not not , it can to\n",
      "Attention Weights: tensor([[6.4082e-07, 1.8206e-04, 3.3189e-02, 2.7907e-04, 4.9074e-04, 1.9509e-01,\n",
      "         2.5230e-02, 1.2970e-02, 1.8092e-03, 7.3076e-01],\n",
      "        [7.9910e-03, 2.1906e-01, 6.1741e-01, 5.9673e-03, 2.1933e-03, 2.8097e-02,\n",
      "         1.4879e-02, 9.0794e-03, 2.7972e-03, 9.2525e-02],\n",
      "        [1.0611e-02, 1.3826e-01, 3.3667e-01, 2.2879e-02, 1.0243e-02, 2.8747e-02,\n",
      "         2.5621e-02, 2.3269e-02, 9.6865e-03, 3.9401e-01],\n",
      "        [8.4098e-03, 1.0649e-01, 3.7659e-01, 7.7090e-02, 2.9826e-02, 5.6624e-02,\n",
      "         2.9146e-02, 2.8395e-02, 1.7083e-02, 2.7035e-01],\n",
      "        [1.1227e-03, 2.8689e-02, 1.7925e-01, 7.8778e-02, 8.2181e-02, 2.3298e-01,\n",
      "         8.2406e-02, 4.7200e-02, 3.7588e-02, 2.2981e-01],\n",
      "        [3.4326e-04, 2.1903e-02, 2.3289e-01, 5.0477e-02, 4.2903e-02, 2.1521e-01,\n",
      "         5.8468e-02, 4.6630e-02, 2.3029e-02, 3.0815e-01],\n",
      "        [4.7733e-04, 2.1626e-02, 2.4830e-01, 4.3412e-02, 5.5423e-02, 2.4082e-01,\n",
      "         7.3750e-02, 6.1310e-02, 2.8649e-02, 2.2623e-01],\n",
      "        [4.0608e-05, 3.4035e-03, 1.7044e-02, 3.1766e-03, 9.9619e-03, 3.6842e-01,\n",
      "         1.6138e-01, 6.4462e-02, 2.0697e-02, 3.5141e-01],\n",
      "        [4.1496e-05, 1.9608e-03, 1.1652e-02, 1.3320e-02, 3.7180e-02, 3.1275e-01,\n",
      "         1.8246e-01, 1.3093e-01, 4.4443e-02, 2.6527e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.00, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 7.20, Minutes Elapsed: 387.95\n",
      "Sampling from val predictions...\n",
      "Source: 在 这样 寒冷 <UNK> 的 晚上 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: in the cold , windy night . <EOS> <PAD>\n",
      "Model: <SOS> and &apos;s case of , &apos;s . <EOS> was\n",
      "Attention Weights: tensor([[0.0000, 0.0213, 0.0002, 0.0021, 0.0116, 0.8984, 0.0665, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0027, 0.8264, 0.0039, 0.0115, 0.0298, 0.0937, 0.0320, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0207, 0.4273, 0.0590, 0.0650, 0.1137, 0.1901, 0.1243, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0056, 0.2419, 0.0907, 0.1010, 0.2234, 0.2270, 0.1104, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0188, 0.3440, 0.0685, 0.1011, 0.1677, 0.1637, 0.1362, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0154, 0.1916, 0.0465, 0.1354, 0.3021, 0.2002, 0.1089, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0061, 0.1000, 0.0688, 0.0687, 0.2632, 0.4005, 0.0926, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0608, 0.0435, 0.0723, 0.3092, 0.4235, 0.0883, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0005, 0.0451, 0.0297, 0.0745, 0.2319, 0.4651, 0.1532, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 一个 奄奄 奄奄一息 一息 的 女人 正 躺 在 地上\n",
      "Reference: a lifeless woman was lying on the ground ,\n",
      "Model: <SOS> and the of of the to a side ,\n",
      "Attention Weights: tensor([[0.0001, 0.0014, 0.0453, 0.0069, 0.0050, 0.0055, 0.0112, 0.0520, 0.0132,\n",
      "         0.8593],\n",
      "        [0.0303, 0.0917, 0.2070, 0.1387, 0.0967, 0.0563, 0.0512, 0.1057, 0.0303,\n",
      "         0.1920],\n",
      "        [0.0488, 0.0954, 0.1573, 0.1157, 0.0676, 0.0783, 0.0484, 0.0714, 0.0413,\n",
      "         0.2757],\n",
      "        [0.0470, 0.0841, 0.2050, 0.1411, 0.0724, 0.1143, 0.0682, 0.0559, 0.0285,\n",
      "         0.1835],\n",
      "        [0.0043, 0.0114, 0.0595, 0.0908, 0.1059, 0.1726, 0.2227, 0.1478, 0.0648,\n",
      "         0.1201],\n",
      "        [0.0003, 0.0006, 0.0176, 0.0185, 0.0125, 0.0315, 0.0782, 0.1363, 0.1635,\n",
      "         0.5409],\n",
      "        [0.0023, 0.0040, 0.0309, 0.0564, 0.0267, 0.1290, 0.0617, 0.1027, 0.1964,\n",
      "         0.3899],\n",
      "        [0.0030, 0.0022, 0.0187, 0.0280, 0.0147, 0.0311, 0.0716, 0.1424, 0.1990,\n",
      "         0.4894],\n",
      "        [0.0008, 0.0006, 0.0108, 0.0179, 0.0047, 0.0188, 0.0528, 0.1190, 0.1623,\n",
      "         0.6124]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.15, Train Loss: 0.00, Val Loss: 4.99, Train BLEU: 0.00, Val BLEU: 7.13, Minutes Elapsed: 399.77\n",
      "Sampling from val predictions...\n",
      "Source: 她 说 她 记得 第一 第一次 一次 看到 那个 地方\n",
      "Reference: she said she remembers her heart bursting out of\n",
      "Model: <SOS> she , , &quot; the she she she she\n",
      "Attention Weights: tensor([[6.9005e-05, 3.5194e-07, 2.3923e-02, 3.0051e-05, 3.0446e-04, 9.5960e-05,\n",
      "         2.4785e-03, 6.5185e-05, 8.7591e-04, 9.7216e-01],\n",
      "        [7.6911e-02, 8.0492e-04, 3.8468e-01, 1.5606e-03, 4.7907e-03, 3.4606e-03,\n",
      "         2.6704e-02, 1.4662e-03, 1.0481e-02, 4.8914e-01],\n",
      "        [4.6628e-02, 7.0033e-03, 6.3719e-01, 5.9134e-03, 1.8488e-02, 8.5397e-03,\n",
      "         3.1146e-02, 3.0992e-03, 1.6936e-02, 2.2506e-01],\n",
      "        [7.0622e-03, 2.7832e-03, 2.4447e-01, 4.7341e-02, 1.4454e-01, 3.0500e-02,\n",
      "         3.9533e-02, 1.0745e-02, 2.0012e-02, 4.5301e-01],\n",
      "        [4.3693e-03, 2.3343e-03, 4.0455e-01, 6.1580e-02, 1.1828e-01, 4.2362e-02,\n",
      "         7.9161e-02, 1.0620e-02, 1.8480e-02, 2.5826e-01],\n",
      "        [4.2823e-04, 1.7020e-04, 5.2113e-02, 3.3259e-02, 1.9363e-01, 3.4018e-02,\n",
      "         6.4207e-02, 2.6196e-02, 4.1740e-02, 5.5424e-01],\n",
      "        [1.8093e-04, 6.8298e-05, 2.4850e-02, 1.6140e-02, 7.3443e-02, 1.9770e-02,\n",
      "         9.2161e-02, 2.5441e-02, 6.0445e-02, 6.8750e-01],\n",
      "        [1.1843e-05, 4.4843e-06, 9.2164e-04, 1.5053e-03, 1.2636e-02, 4.5925e-03,\n",
      "         2.7735e-02, 3.9539e-02, 1.0416e-01, 8.0890e-01],\n",
      "        [1.9403e-05, 6.4056e-06, 2.1761e-03, 1.8593e-03, 1.1148e-02, 4.8533e-03,\n",
      "         2.2115e-02, 1.9059e-02, 6.2172e-02, 8.7659e-01]])\n",
      "\n",
      "Source: 谢谢 掌声 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank you . <EOS> you <EOS> . . .\n",
      "Attention Weights: tensor([[0.0000, 0.8621, 0.1379, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0114, 0.6970, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0742, 0.5663, 0.3596, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1453, 0.6135, 0.2412, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0264, 0.7390, 0.2346, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1151, 0.4245, 0.4604, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0550, 0.3719, 0.5731, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1140, 0.3112, 0.5748, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1269, 0.2856, 0.5875, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.30, Train Loss: 0.00, Val Loss: 4.95, Train BLEU: 0.00, Val BLEU: 7.49, Minutes Elapsed: 411.58\n",
      "Sampling from val predictions...\n",
      "Source: 我 想 不能 再 这样 下去 了 <EOS> <PAD> <PAD>\n",
      "Reference: and i figured , this has to stop .\n",
      "Model: <SOS> i i said , &quot; well , . .\n",
      "Attention Weights: tensor([[0.9989, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0008, 0.0003, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9756, 0.0188, 0.0001, 0.0008, 0.0028, 0.0001, 0.0013, 0.0006, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0938, 0.5185, 0.0308, 0.1151, 0.0984, 0.0077, 0.0996, 0.0361, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1204, 0.4420, 0.0366, 0.1179, 0.1077, 0.0101, 0.1004, 0.0649, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0144, 0.1039, 0.1344, 0.2616, 0.2433, 0.0275, 0.1258, 0.0890, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0100, 0.0400, 0.0188, 0.2025, 0.3682, 0.0506, 0.1870, 0.1229, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0263, 0.0369, 0.0030, 0.1727, 0.3962, 0.0913, 0.1861, 0.0876, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0322, 0.0155, 0.0099, 0.0834, 0.0953, 0.2687, 0.3243, 0.1706, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0516, 0.0112, 0.0081, 0.0350, 0.0607, 0.2207, 0.2480, 0.3647, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 有时 学校 <UNK> 突然 <UNK> 一周 因为 被 塔利 塔利班\n",
      "Reference: from time to time , the school would suddenly\n",
      "Model: <SOS> and , , it it , &apos;s is to\n",
      "Attention Weights: tensor([[0.0001, 0.0005, 0.0002, 0.0016, 0.0007, 0.0029, 0.4882, 0.0011, 0.0152,\n",
      "         0.4897],\n",
      "        [0.0660, 0.1060, 0.0137, 0.0237, 0.0172, 0.0195, 0.5009, 0.0084, 0.0491,\n",
      "         0.1955],\n",
      "        [0.0356, 0.0921, 0.0757, 0.0977, 0.0761, 0.0584, 0.2730, 0.0235, 0.0579,\n",
      "         0.2100],\n",
      "        [0.0062, 0.0142, 0.0189, 0.0446, 0.0321, 0.0605, 0.5591, 0.0178, 0.0288,\n",
      "         0.2178],\n",
      "        [0.0012, 0.0011, 0.0062, 0.0235, 0.0241, 0.0385, 0.3995, 0.0069, 0.0434,\n",
      "         0.4555],\n",
      "        [0.0004, 0.0011, 0.0034, 0.0294, 0.0131, 0.0964, 0.6005, 0.0224, 0.0605,\n",
      "         0.1728],\n",
      "        [0.0002, 0.0008, 0.0066, 0.0165, 0.0334, 0.0905, 0.2875, 0.0380, 0.1389,\n",
      "         0.3876],\n",
      "        [0.0007, 0.0017, 0.0040, 0.0201, 0.0201, 0.0911, 0.2854, 0.0752, 0.1630,\n",
      "         0.3386],\n",
      "        [0.0013, 0.0028, 0.0054, 0.0357, 0.0258, 0.1354, 0.3939, 0.1160, 0.0817,\n",
      "         0.2021]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.45, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.67, Minutes Elapsed: 423.43\n",
      "Sampling from val predictions...\n",
      "Source: 虽然 我 也 时常 憧憬 外面 的 世界 我 以为\n",
      "Reference: although i often wondered about the outside world ,\n",
      "Model: <SOS> and i i going my lot , of i\n",
      "Attention Weights: tensor([[3.0512e-10, 9.7985e-01, 2.1280e-08, 1.0046e-08, 4.9768e-04, 2.1336e-06,\n",
      "         1.8086e-06, 6.9630e-07, 1.9469e-02, 1.8145e-04],\n",
      "        [1.0209e-03, 9.8422e-01, 5.6252e-04, 1.0246e-04, 2.8411e-03, 2.4058e-04,\n",
      "         1.7043e-04, 9.1699e-05, 7.4969e-03, 3.2562e-03],\n",
      "        [3.4502e-02, 3.6505e-01, 5.9466e-02, 1.1341e-02, 1.1298e-01, 2.2785e-02,\n",
      "         2.0722e-02, 1.3160e-02, 8.5601e-02, 2.7439e-01],\n",
      "        [2.5240e-02, 4.2351e-01, 1.0817e-01, 2.4371e-02, 1.1232e-01, 2.0720e-02,\n",
      "         2.1926e-02, 1.4249e-02, 6.1557e-02, 1.8794e-01],\n",
      "        [6.2325e-04, 1.1963e-02, 3.8922e-02, 8.2037e-02, 5.4535e-01, 1.3773e-01,\n",
      "         4.5143e-02, 1.4775e-02, 3.3623e-02, 8.9832e-02],\n",
      "        [2.6455e-05, 2.8468e-03, 8.1608e-03, 2.3148e-02, 2.9180e-01, 2.2188e-01,\n",
      "         1.5062e-01, 6.4870e-02, 1.5252e-01, 8.4133e-02],\n",
      "        [1.3274e-05, 1.0180e-03, 2.5375e-03, 2.7428e-03, 1.9722e-01, 1.4127e-01,\n",
      "         1.1465e-01, 5.4515e-02, 3.6486e-01, 1.2118e-01],\n",
      "        [3.1454e-06, 5.0312e-04, 1.4581e-03, 2.2790e-03, 1.7919e-01, 9.2247e-02,\n",
      "         1.1951e-01, 8.8975e-02, 3.6996e-01, 1.4588e-01],\n",
      "        [3.4380e-05, 3.8685e-03, 5.6183e-03, 7.8540e-03, 2.0958e-01, 8.6163e-02,\n",
      "         8.6613e-02, 7.3461e-02, 4.0460e-01, 1.2220e-01]])\n",
      "\n",
      "Source: 客户 热衷 热衷于 自我 提升 的 这份 热情 才 是\n",
      "Reference: the passion that the person has for her own\n",
      "Model: <SOS> and , of the this of the this ,\n",
      "Attention Weights: tensor([[4.0075e-05, 5.3321e-06, 1.1711e-04, 3.3601e-03, 3.7045e-03, 5.3293e-05,\n",
      "         8.6365e-01, 1.2321e-04, 3.6005e-03, 1.2535e-01],\n",
      "        [2.2295e-02, 4.2227e-03, 2.9122e-02, 3.9645e-02, 4.3344e-02, 4.3965e-03,\n",
      "         6.9864e-01, 2.5437e-03, 1.9789e-02, 1.3600e-01],\n",
      "        [4.7712e-02, 3.6514e-02, 1.6073e-01, 1.5996e-01, 1.4958e-01, 2.1916e-02,\n",
      "         1.7845e-01, 1.6554e-02, 4.3900e-02, 1.8468e-01],\n",
      "        [1.0134e-01, 6.3342e-02, 1.3568e-01, 1.5037e-01, 8.7699e-02, 1.1339e-02,\n",
      "         1.3245e-01, 1.6262e-02, 4.3697e-02, 2.5783e-01],\n",
      "        [2.9524e-03, 2.6817e-03, 2.2013e-02, 1.4620e-01, 1.6314e-01, 1.4417e-02,\n",
      "         4.2887e-01, 1.1243e-02, 3.1070e-02, 1.7741e-01],\n",
      "        [2.8938e-03, 2.8066e-03, 1.1148e-02, 1.0609e-01, 2.2445e-01, 2.3613e-02,\n",
      "         3.4649e-01, 1.7676e-02, 4.8601e-02, 2.1624e-01],\n",
      "        [9.1192e-03, 8.9990e-03, 2.0373e-02, 1.2432e-01, 1.1782e-01, 1.6083e-02,\n",
      "         2.3148e-01, 5.0907e-02, 1.0539e-01, 3.1550e-01],\n",
      "        [6.4119e-04, 7.8218e-04, 2.7558e-03, 3.4499e-02, 9.6885e-02, 1.7361e-02,\n",
      "         4.3748e-01, 3.1807e-02, 9.0504e-02, 2.8728e-01],\n",
      "        [1.5041e-04, 2.6898e-04, 1.2073e-03, 2.0637e-02, 6.5262e-02, 1.4961e-02,\n",
      "         2.4088e-01, 1.0189e-01, 2.4140e-01, 3.1334e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.60, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 7.03, Minutes Elapsed: 435.31\n",
      "Sampling from val predictions...\n",
      "Source: 我 搬到 了 纽约 开始 了 自己 第一 第一份 一份\n",
      "Reference: i had moved to new york city for my\n",
      "Model: <SOS> i i a first the a , , the\n",
      "Attention Weights: tensor([[9.9939e-01, 2.5868e-10, 2.8261e-09, 1.1968e-07, 2.9193e-08, 1.0140e-07,\n",
      "         3.0417e-07, 1.0216e-07, 1.1951e-06, 6.1234e-04],\n",
      "        [9.8856e-01, 6.7319e-04, 8.0960e-04, 2.1613e-03, 3.4267e-04, 2.1721e-04,\n",
      "         1.5962e-04, 5.9312e-05, 2.0519e-04, 6.8101e-03],\n",
      "        [2.5210e-01, 5.0512e-02, 1.2675e-01, 1.6936e-01, 3.8548e-02, 2.9427e-02,\n",
      "         1.6301e-02, 9.5529e-03, 2.2700e-02, 2.8474e-01],\n",
      "        [1.0930e-02, 7.5964e-03, 7.4191e-02, 6.1093e-01, 8.5536e-02, 3.8778e-02,\n",
      "         3.1410e-02, 1.3328e-02, 1.6511e-02, 1.1079e-01],\n",
      "        [9.2137e-03, 5.7970e-04, 3.3477e-02, 1.7598e-01, 1.4203e-01, 1.7236e-01,\n",
      "         6.9143e-02, 2.3411e-02, 3.0198e-02, 3.4361e-01],\n",
      "        [4.1822e-03, 2.6177e-04, 8.3862e-03, 8.6617e-02, 1.4058e-01, 2.3555e-01,\n",
      "         1.7430e-01, 6.2950e-02, 5.6886e-02, 2.3029e-01],\n",
      "        [1.1579e-03, 1.1016e-04, 4.2607e-03, 8.1773e-02, 1.1298e-01, 2.0958e-01,\n",
      "         2.2586e-01, 7.4532e-02, 6.5560e-02, 2.2419e-01],\n",
      "        [1.1546e-03, 4.8845e-06, 5.0053e-04, 8.9857e-03, 3.2177e-02, 9.2731e-02,\n",
      "         1.0856e-01, 3.8488e-02, 1.0812e-01, 6.0927e-01],\n",
      "        [1.6477e-03, 1.5550e-05, 8.0297e-04, 7.4251e-03, 2.6769e-02, 1.0512e-01,\n",
      "         1.0773e-01, 5.0941e-02, 1.2599e-01, 5.7356e-01]])\n",
      "\n",
      "Source: 停电 也 变得 越来 越来越 频繁 到 了 晚上 我\n",
      "Reference: power <UNK> also became more and more frequent ,\n",
      "Model: <SOS> so , to a to to , , ,\n",
      "Attention Weights: tensor([[7.8982e-07, 1.8149e-06, 1.6842e-06, 5.0275e-05, 1.1734e-04, 3.5602e-05,\n",
      "         8.9873e-05, 1.5234e-03, 1.9252e-04, 9.9799e-01],\n",
      "        [1.3336e-02, 3.3964e-02, 3.7524e-03, 1.7014e-02, 1.8207e-02, 1.0694e-02,\n",
      "         1.1094e-02, 3.6691e-02, 5.6197e-03, 8.4963e-01],\n",
      "        [1.2777e-01, 2.0711e-01, 5.0266e-02, 4.1566e-02, 2.3762e-02, 8.9259e-03,\n",
      "         1.5675e-02, 3.1433e-02, 9.9389e-03, 4.8356e-01],\n",
      "        [7.5565e-03, 4.0638e-02, 4.8657e-02, 1.2003e-01, 8.0416e-02, 4.6370e-02,\n",
      "         3.2887e-02, 7.5762e-02, 2.4079e-02, 5.2360e-01],\n",
      "        [1.3529e-04, 1.3663e-03, 1.1123e-03, 3.7819e-02, 3.4672e-02, 1.9061e-02,\n",
      "         2.4667e-02, 7.8593e-02, 3.0862e-02, 7.7171e-01],\n",
      "        [9.5430e-04, 3.4699e-03, 4.8955e-03, 4.0452e-02, 4.7919e-02, 3.4923e-02,\n",
      "         3.3376e-02, 8.7729e-02, 3.2376e-02, 7.1391e-01],\n",
      "        [3.2099e-05, 6.0010e-04, 5.6191e-04, 1.0475e-02, 2.0536e-02, 1.5449e-02,\n",
      "         2.6029e-02, 9.3146e-02, 4.5758e-02, 7.8741e-01],\n",
      "        [7.5049e-05, 1.1349e-03, 8.1067e-04, 8.1582e-03, 1.5552e-02, 1.6828e-02,\n",
      "         3.1566e-02, 1.1005e-01, 4.2101e-02, 7.7373e-01],\n",
      "        [1.8169e-05, 5.9366e-04, 4.6910e-04, 5.6608e-03, 1.3093e-02, 1.4683e-02,\n",
      "         2.0162e-02, 9.5266e-02, 3.9936e-02, 8.1012e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.75, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.56, Minutes Elapsed: 447.20\n",
      "Sampling from val predictions...\n",
      "Source: 我们 怎样 能够 更好 地 分享 各自 的 关于 被\n",
      "Reference: how can we share more of our memories of\n",
      "Model: <SOS> we we we get the the to to ?\n",
      "Attention Weights: tensor([[9.9883e-01, 2.3840e-06, 3.2205e-08, 2.3542e-06, 8.8519e-07, 9.0665e-07,\n",
      "         1.4573e-05, 7.2529e-07, 9.4592e-06, 1.1433e-03],\n",
      "        [8.8761e-01, 9.1474e-02, 3.1903e-04, 3.0429e-03, 4.0318e-04, 1.7900e-04,\n",
      "         3.0782e-04, 8.4265e-05, 9.8458e-04, 1.5592e-02],\n",
      "        [1.6224e-01, 3.1314e-01, 8.1440e-02, 5.4785e-02, 1.6247e-02, 7.6876e-03,\n",
      "         2.9296e-02, 7.0419e-03, 2.4007e-02, 3.0411e-01],\n",
      "        [1.0654e-01, 2.0440e-01, 6.1845e-02, 1.7604e-01, 3.9388e-02, 1.8918e-02,\n",
      "         5.2771e-02, 8.9166e-03, 3.9792e-02, 2.9140e-01],\n",
      "        [9.9443e-03, 2.0842e-02, 2.0178e-02, 3.6811e-01, 9.7158e-02, 2.5461e-02,\n",
      "         8.5870e-02, 6.1705e-03, 6.6149e-02, 3.0012e-01],\n",
      "        [7.3746e-03, 1.6773e-02, 1.9103e-02, 2.5044e-01, 1.6056e-01, 5.5142e-02,\n",
      "         1.5675e-01, 1.8664e-02, 7.1700e-02, 2.4350e-01],\n",
      "        [3.1093e-03, 1.9337e-03, 2.6246e-03, 1.0611e-01, 1.5915e-01, 6.8521e-02,\n",
      "         1.7869e-01, 2.8560e-02, 1.8959e-01, 2.6171e-01],\n",
      "        [5.4823e-03, 1.0909e-03, 4.4988e-04, 2.7355e-02, 3.0611e-02, 2.8089e-02,\n",
      "         1.5861e-01, 1.0214e-02, 3.3045e-01, 4.0765e-01],\n",
      "        [1.4074e-02, 2.3576e-03, 1.7130e-03, 7.8078e-02, 6.8394e-02, 3.7001e-02,\n",
      "         1.6455e-01, 9.9647e-03, 2.2164e-01, 4.0223e-01]])\n",
      "\n",
      "Source: 现在 我 的 方法 已经 被 肯尼 肯尼亚 尼亚 的\n",
      "Reference: and my idea is also being used now all\n",
      "Model: <SOS> now , my , to to to to be\n",
      "Attention Weights: tensor([[1.5161e-07, 9.9800e-01, 1.1040e-08, 7.0663e-08, 2.0169e-07, 1.6928e-06,\n",
      "         3.0595e-06, 3.6398e-06, 1.5964e-06, 1.9924e-03],\n",
      "        [1.5082e-04, 9.9326e-01, 1.3354e-04, 1.5668e-04, 9.3920e-05, 7.2626e-05,\n",
      "         6.8666e-05, 1.4109e-04, 9.4525e-05, 5.8271e-03],\n",
      "        [3.0501e-03, 5.3566e-01, 1.9386e-02, 4.1005e-02, 1.7393e-02, 1.1189e-02,\n",
      "         1.0538e-02, 1.5117e-02, 1.3756e-02, 3.3291e-01],\n",
      "        [1.1105e-03, 4.8008e-01, 1.5236e-02, 4.6997e-02, 3.2411e-02, 2.0333e-02,\n",
      "         1.2069e-02, 1.7056e-02, 1.3297e-02, 3.6142e-01],\n",
      "        [1.3898e-04, 1.3053e-02, 1.9445e-03, 4.7643e-02, 2.2990e-01, 2.0158e-01,\n",
      "         6.6884e-02, 7.7744e-02, 4.5601e-02, 3.1551e-01],\n",
      "        [2.6825e-07, 2.5316e-02, 1.6850e-06, 1.6089e-04, 4.0820e-03, 2.4557e-02,\n",
      "         3.7421e-02, 4.9962e-02, 3.2185e-02, 8.2631e-01],\n",
      "        [2.4266e-06, 6.6190e-02, 2.5898e-05, 6.0254e-04, 1.4536e-02, 5.7180e-02,\n",
      "         1.3845e-01, 1.6570e-01, 5.4573e-02, 5.0273e-01],\n",
      "        [9.3292e-07, 1.1912e-02, 2.8449e-05, 5.1545e-04, 9.6147e-03, 5.4686e-02,\n",
      "         1.9928e-01, 1.9441e-01, 7.0495e-02, 4.5906e-01],\n",
      "        [2.6872e-06, 1.2510e-02, 5.7883e-05, 6.7578e-04, 1.1383e-02, 7.6882e-02,\n",
      "         2.1174e-01, 1.7998e-01, 7.9107e-02, 4.2765e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.90, Train Loss: 0.00, Val Loss: 4.85, Train BLEU: 0.00, Val BLEU: 7.32, Minutes Elapsed: 458.77\n",
      "Sampling from val predictions...\n",
      "Source: 当 我 今天 站 在 这里 跟 你们 演讲 的\n",
      "Reference: as i stand talking to you today , these\n",
      "Model: <SOS> when i i , , , , , i\n",
      "Attention Weights: tensor([[2.2697e-05, 9.8884e-01, 8.6167e-08, 1.9628e-07, 3.9807e-07, 9.2672e-05,\n",
      "         1.2062e-06, 3.5634e-03, 8.2896e-07, 7.4820e-03],\n",
      "        [1.6061e-02, 9.6305e-01, 1.5389e-03, 8.3243e-04, 2.6444e-04, 6.4052e-03,\n",
      "         2.2017e-04, 3.9710e-03, 4.7259e-05, 7.6096e-03],\n",
      "        [5.2752e-03, 2.9468e-01, 9.8263e-02, 5.5227e-02, 1.6527e-02, 1.6029e-01,\n",
      "         9.2785e-03, 8.5385e-02, 2.3403e-03, 2.7273e-01],\n",
      "        [4.8214e-04, 1.1855e-01, 4.8336e-02, 2.8475e-02, 1.3768e-02, 2.5367e-01,\n",
      "         8.0733e-03, 1.5853e-01, 1.7242e-03, 3.6839e-01],\n",
      "        [8.2998e-04, 7.9083e-03, 1.6026e-02, 5.6463e-02, 5.5225e-02, 5.7224e-01,\n",
      "         1.4265e-02, 1.4143e-01, 1.3291e-03, 1.3428e-01],\n",
      "        [1.8539e-03, 9.1949e-03, 5.5225e-03, 5.3763e-02, 8.3908e-02, 4.9642e-01,\n",
      "         4.3205e-02, 8.4413e-02, 7.7459e-03, 2.1397e-01],\n",
      "        [1.8655e-04, 2.8698e-03, 1.1550e-03, 3.3753e-02, 3.8975e-02, 3.0400e-01,\n",
      "         8.5277e-02, 2.1121e-01, 1.2284e-02, 3.1028e-01],\n",
      "        [7.3997e-04, 3.6823e-03, 3.2044e-03, 3.1949e-02, 4.7288e-02, 3.3342e-01,\n",
      "         5.7487e-02, 2.6819e-01, 2.3607e-02, 2.3044e-01],\n",
      "        [1.2421e-03, 4.8386e-03, 1.4205e-03, 9.6313e-03, 1.7224e-02, 1.7493e-01,\n",
      "         3.8177e-02, 1.7983e-01, 3.2294e-02, 5.4041e-01]])\n",
      "\n",
      "Source: 一年 年前 我 只是 一个 热带 带大 草原 上 帮\n",
      "Reference: so one year ago , i was just a\n",
      "Model: <SOS> one , one , , i was a a\n",
      "Attention Weights: tensor([[4.7481e-09, 8.8451e-06, 9.9953e-01, 1.4547e-07, 4.1872e-07, 3.8160e-06,\n",
      "         1.5529e-07, 2.9149e-07, 8.1867e-06, 4.4887e-04],\n",
      "        [3.0295e-03, 4.9714e-01, 4.9437e-01, 3.8554e-04, 1.2078e-04, 1.0522e-04,\n",
      "         3.0498e-05, 4.9630e-05, 4.1862e-04, 4.3572e-03],\n",
      "        [1.3256e-02, 2.4004e-01, 2.9681e-01, 6.0222e-02, 1.8882e-02, 1.2828e-02,\n",
      "         4.4938e-03, 6.0286e-03, 3.4897e-02, 3.1254e-01],\n",
      "        [1.0324e-02, 1.4448e-01, 4.7561e-01, 7.5334e-02, 1.8590e-02, 1.0726e-02,\n",
      "         4.0978e-03, 4.9474e-03, 2.4097e-02, 2.3180e-01],\n",
      "        [8.3189e-03, 6.2340e-02, 6.8204e-01, 5.6735e-02, 2.0015e-02, 1.2655e-02,\n",
      "         4.6246e-03, 4.5928e-03, 1.5442e-02, 1.3324e-01],\n",
      "        [7.2348e-04, 1.3328e-02, 7.7160e-01, 7.1041e-02, 1.3084e-02, 6.1912e-03,\n",
      "         1.9014e-03, 2.0525e-03, 8.2859e-03, 1.1179e-01],\n",
      "        [4.8759e-04, 8.1061e-03, 5.8911e-01, 6.6121e-02, 4.8889e-02, 2.5755e-02,\n",
      "         5.6568e-03, 6.3302e-03, 1.9810e-02, 2.2974e-01],\n",
      "        [9.2942e-04, 1.0914e-02, 2.9867e-01, 1.9034e-01, 1.4152e-01, 4.7350e-02,\n",
      "         1.2896e-02, 1.5795e-02, 2.9086e-02, 2.5251e-01],\n",
      "        [8.5895e-04, 3.0752e-03, 9.9545e-03, 1.4271e-02, 3.0622e-01, 2.8453e-01,\n",
      "         8.0499e-02, 3.3433e-02, 2.5458e-02, 2.4170e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.00, Train Loss: 0.00, Val Loss: 4.90, Train BLEU: 0.00, Val BLEU: 7.13, Minutes Elapsed: 466.38\n",
      "Sampling from val predictions...\n",
      "Source: 一旦 价格 价格低 价格低廉 低廉 它 也 就 能 被\n",
      "Reference: when something becomes <UNK> cost , it becomes massively\n",
      "Model: <SOS> and it it it , it it it a\n",
      "Attention Weights: tensor([[0.0002, 0.0000, 0.0011, 0.0035, 0.0002, 0.8087, 0.0009, 0.0027, 0.0008,\n",
      "         0.1820],\n",
      "        [0.0127, 0.0137, 0.0482, 0.0266, 0.0034, 0.8531, 0.0039, 0.0077, 0.0017,\n",
      "         0.0291],\n",
      "        [0.0316, 0.0421, 0.0560, 0.0529, 0.0212, 0.4767, 0.0307, 0.0478, 0.0219,\n",
      "         0.2191],\n",
      "        [0.0131, 0.0485, 0.1251, 0.1523, 0.0399, 0.3627, 0.0320, 0.0429, 0.0189,\n",
      "         0.1647],\n",
      "        [0.0025, 0.0152, 0.1766, 0.2955, 0.0665, 0.3455, 0.0204, 0.0179, 0.0076,\n",
      "         0.0524],\n",
      "        [0.0012, 0.0042, 0.0264, 0.0612, 0.0397, 0.5683, 0.0528, 0.0580, 0.0292,\n",
      "         0.1589],\n",
      "        [0.0005, 0.0005, 0.0040, 0.0084, 0.0046, 0.5759, 0.0463, 0.1166, 0.0528,\n",
      "         0.1902],\n",
      "        [0.0004, 0.0007, 0.0038, 0.0094, 0.0040, 0.1825, 0.0435, 0.1211, 0.1152,\n",
      "         0.5195],\n",
      "        [0.0001, 0.0003, 0.0014, 0.0042, 0.0036, 0.1796, 0.0578, 0.1833, 0.1203,\n",
      "         0.4493]])\n",
      "\n",
      "Source: 请 不要 要说 我 正常 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: please -- don &apos;t tell me i &apos;m normal\n",
      "Model: <SOS> i don not don tell me , &quot; .\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.9990, 0.0005, 0.0004, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3805, 0.0009, 0.0298, 0.5695, 0.0104, 0.0090, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2991, 0.0182, 0.0717, 0.4308, 0.0859, 0.0943, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0082, 0.0057, 0.1391, 0.7471, 0.0648, 0.0350, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0360, 0.0130, 0.1505, 0.5015, 0.1458, 0.1531, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0045, 0.0017, 0.0849, 0.7437, 0.1396, 0.0256, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0033, 0.0017, 0.0685, 0.7752, 0.0936, 0.0577, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0057, 0.0032, 0.0387, 0.7086, 0.1631, 0.0808, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0075, 0.0028, 0.0228, 0.5330, 0.2962, 0.1376, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.15, Train Loss: 0.00, Val Loss: 4.97, Train BLEU: 0.00, Val BLEU: 7.19, Minutes Elapsed: 477.85\n",
      "Sampling from val predictions...\n",
      "Source: 所以 我 又 有 了 个 主意 <EOS> <PAD> <PAD>\n",
      "Reference: so i had an idea . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> so i i a a . <EOS> was <EOS>\n",
      "Attention Weights: tensor([[0.0000, 0.9986, 0.0000, 0.0000, 0.0000, 0.0000, 0.0013, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0024, 0.9774, 0.0012, 0.0003, 0.0002, 0.0019, 0.0096, 0.0070, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0190, 0.3148, 0.1302, 0.0218, 0.0093, 0.0301, 0.2234, 0.2514, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0116, 0.3208, 0.2027, 0.0318, 0.0099, 0.0393, 0.2236, 0.1603, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0147, 0.0262, 0.0326, 0.1016, 0.0643, 0.2006, 0.3985, 0.1614, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0017, 0.0104, 0.0118, 0.0276, 0.0485, 0.1229, 0.5435, 0.2337, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0080, 0.0210, 0.0058, 0.0116, 0.0265, 0.0958, 0.5613, 0.2699, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0033, 0.0294, 0.0127, 0.0069, 0.0304, 0.1156, 0.4729, 0.3289, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0014, 0.0547, 0.0690, 0.0431, 0.0865, 0.1800, 0.2818, 0.2834, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 神经 神经学 经学 测试 是 无 创 性 的 而且\n",
      "Reference: the neurologist &apos;s test is non-invasive . they both\n",
      "Model: <SOS> and is is , , , , , &apos;s\n",
      "Attention Weights: tensor([[2.8394e-04, 6.7075e-02, 5.1048e-05, 6.8241e-06, 2.9428e-04, 3.2062e-05,\n",
      "         2.3003e-05, 1.4364e-04, 1.1218e-04, 9.3198e-01],\n",
      "        [1.7535e-02, 8.6893e-01, 1.9660e-03, 5.7139e-04, 7.0379e-03, 2.4986e-04,\n",
      "         2.3642e-04, 6.5859e-04, 2.0558e-04, 1.0261e-01],\n",
      "        [1.0419e-01, 2.5550e-01, 9.0084e-02, 2.4651e-02, 7.9520e-02, 2.9716e-02,\n",
      "         2.5133e-02, 5.4113e-02, 1.0870e-02, 3.2621e-01],\n",
      "        [4.8166e-02, 8.3257e-02, 2.8282e-02, 3.9601e-02, 2.6843e-01, 8.8262e-02,\n",
      "         6.3907e-02, 7.1156e-02, 1.1777e-02, 2.9716e-01],\n",
      "        [9.6413e-04, 1.9488e-02, 3.7656e-03, 2.5969e-03, 3.0264e-02, 9.0525e-03,\n",
      "         9.8369e-03, 3.9789e-02, 1.0471e-02, 8.7377e-01],\n",
      "        [5.2254e-04, 8.4107e-03, 7.0921e-04, 2.3249e-03, 1.9346e-02, 1.1423e-02,\n",
      "         2.0811e-02, 5.2155e-02, 8.4208e-03, 8.7588e-01],\n",
      "        [7.1354e-05, 9.2167e-04, 2.9084e-05, 1.8351e-04, 4.3318e-03, 2.2143e-03,\n",
      "         6.3794e-03, 5.3181e-02, 1.0755e-02, 9.2193e-01],\n",
      "        [2.8455e-05, 8.3669e-04, 3.0142e-05, 5.5829e-05, 1.7805e-03, 1.3246e-03,\n",
      "         5.1650e-03, 2.1235e-02, 1.2415e-02, 9.5713e-01],\n",
      "        [8.7272e-05, 1.6266e-03, 1.2584e-04, 2.1671e-04, 2.6352e-03, 1.4527e-03,\n",
      "         2.9375e-03, 2.8815e-02, 7.0684e-03, 9.5504e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.30, Train Loss: 0.00, Val Loss: 4.93, Train BLEU: 0.00, Val BLEU: 7.51, Minutes Elapsed: 489.30\n",
      "Sampling from val predictions...\n",
      "Source: 请 不要 要说 我 正常 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: please -- don &apos;t tell me i &apos;m normal\n",
      "Model: <SOS> i &apos;s . , just me . <EOS> .\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.9994, 0.0005, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0086, 0.0001, 0.0056, 0.9451, 0.0283, 0.0123, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1004, 0.0166, 0.1258, 0.5279, 0.0994, 0.1298, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0123, 0.0125, 0.1076, 0.4932, 0.2672, 0.1073, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0247, 0.0066, 0.0575, 0.3779, 0.2117, 0.3216, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0106, 0.0049, 0.0366, 0.6156, 0.2087, 0.1235, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0077, 0.0077, 0.0365, 0.6177, 0.1533, 0.1772, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0063, 0.0074, 0.0342, 0.4009, 0.2913, 0.2599, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0194, 0.0144, 0.0299, 0.1755, 0.3391, 0.4217, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 我们 做 过 调查 对 世界 上 100 家 标杆\n",
      "Reference: we &apos;ve done the research , and we have\n",
      "Model: <SOS> we &apos;re have a the to the we &apos;s\n",
      "Attention Weights: tensor([[9.9825e-01, 5.1529e-10, 7.9906e-09, 5.9606e-06, 1.8524e-07, 3.1576e-08,\n",
      "         3.7337e-07, 4.3502e-06, 1.1564e-06, 1.7358e-03],\n",
      "        [9.7363e-01, 1.6097e-04, 1.3597e-03, 1.6100e-02, 3.4634e-04, 2.4924e-05,\n",
      "         8.6835e-05, 2.7204e-04, 1.0323e-04, 7.9180e-03],\n",
      "        [1.9972e-01, 4.8301e-02, 9.9752e-02, 3.0865e-01, 6.2154e-02, 8.2077e-03,\n",
      "         1.8411e-02, 4.4916e-02, 1.9237e-02, 1.9066e-01],\n",
      "        [9.4221e-03, 4.5835e-03, 3.6648e-02, 5.2097e-01, 1.7583e-01, 1.2891e-02,\n",
      "         1.7440e-02, 6.1046e-02, 1.7461e-02, 1.4372e-01],\n",
      "        [7.7009e-03, 3.2317e-03, 3.5822e-02, 6.1635e-01, 1.5821e-01, 1.9765e-02,\n",
      "         1.6109e-02, 4.8730e-02, 1.5968e-02, 7.8115e-02],\n",
      "        [1.5432e-02, 1.4052e-03, 1.2581e-02, 2.6957e-01, 2.2627e-01, 3.4719e-02,\n",
      "         5.5772e-02, 1.0362e-01, 3.9375e-02, 2.4126e-01],\n",
      "        [1.9273e-02, 1.6455e-03, 3.0015e-03, 8.2694e-02, 2.4222e-01, 5.4228e-02,\n",
      "         1.7425e-01, 2.2826e-01, 6.3929e-02, 1.3050e-01],\n",
      "        [3.6737e-03, 2.8260e-05, 1.9734e-04, 4.2077e-02, 2.1307e-02, 9.2059e-03,\n",
      "         4.9586e-02, 2.3271e-01, 1.4867e-01, 4.9255e-01],\n",
      "        [1.7537e-02, 3.3030e-04, 1.7810e-03, 1.1126e-01, 6.7897e-02, 2.5968e-02,\n",
      "         6.4856e-02, 2.4196e-01, 1.6264e-01, 3.0577e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.45, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.86, Minutes Elapsed: 500.69\n",
      "Sampling from val predictions...\n",
      "Source: 并且 产生 了 很多 建议 这些 建议 貌似 <UNK> 可行\n",
      "Reference: and there were all sorts of suggestions on the\n",
      "Model: <SOS> and , , a lot of of of this\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0003, 0.0022, 0.0017, 0.7012, 0.0023, 0.0261, 0.0447,\n",
      "         0.2214],\n",
      "        [0.0077, 0.1630, 0.0737, 0.1980, 0.0329, 0.4079, 0.0080, 0.0330, 0.0203,\n",
      "         0.0555],\n",
      "        [0.0326, 0.1746, 0.1312, 0.1718, 0.0371, 0.0906, 0.0193, 0.0633, 0.1235,\n",
      "         0.1558],\n",
      "        [0.0189, 0.0803, 0.1780, 0.2327, 0.0702, 0.1239, 0.0260, 0.0514, 0.1040,\n",
      "         0.1147],\n",
      "        [0.0008, 0.0040, 0.0465, 0.3850, 0.1727, 0.2013, 0.0590, 0.0479, 0.0306,\n",
      "         0.0522],\n",
      "        [0.0003, 0.0006, 0.0112, 0.0469, 0.1156, 0.5415, 0.0463, 0.1182, 0.0458,\n",
      "         0.0736],\n",
      "        [0.0017, 0.0016, 0.0192, 0.0366, 0.1154, 0.3560, 0.0693, 0.1233, 0.1653,\n",
      "         0.1116],\n",
      "        [0.0007, 0.0005, 0.0038, 0.0117, 0.0641, 0.4372, 0.0685, 0.1438, 0.1080,\n",
      "         0.1617],\n",
      "        [0.0006, 0.0004, 0.0047, 0.0102, 0.0761, 0.3576, 0.0927, 0.1494, 0.1505,\n",
      "         0.1579]])\n",
      "\n",
      "Source: 尤其 是 如果 你 正 骑 着 自行 自行车 行车\n",
      "Reference: and especially if you then ride with your bicycle\n",
      "Model: <SOS> the the answer , if a the , ,\n",
      "Attention Weights: tensor([[4.1529e-08, 2.3623e-06, 1.2885e-07, 9.9995e-01, 6.4110e-10, 4.2444e-08,\n",
      "         9.7443e-08, 2.2476e-08, 3.4536e-08, 4.6262e-05],\n",
      "        [1.0916e-01, 6.7498e-01, 7.2861e-03, 1.9735e-01, 1.4925e-04, 1.0790e-03,\n",
      "         4.9455e-04, 8.9153e-05, 2.4598e-04, 9.1549e-03],\n",
      "        [1.8759e-01, 2.1614e-01, 1.0201e-01, 2.8164e-01, 2.7165e-03, 1.6680e-02,\n",
      "         1.2954e-02, 4.9545e-03, 1.1148e-02, 1.6418e-01],\n",
      "        [1.5089e-01, 2.2994e-01, 1.4242e-01, 1.7160e-01, 5.3678e-03, 2.7404e-02,\n",
      "         1.7916e-02, 7.9872e-03, 1.6868e-02, 2.2960e-01],\n",
      "        [7.0702e-03, 1.1987e-01, 1.0386e-01, 6.5285e-01, 2.0489e-03, 2.2905e-02,\n",
      "         1.3955e-02, 2.8477e-03, 4.7303e-03, 6.9865e-02],\n",
      "        [3.6479e-03, 1.3575e-02, 1.5625e-02, 4.2079e-01, 1.4848e-02, 1.3334e-01,\n",
      "         1.1505e-01, 2.7195e-02, 2.6926e-02, 2.2900e-01],\n",
      "        [5.0084e-04, 1.7635e-03, 2.5311e-03, 3.1386e-02, 3.8309e-02, 1.7796e-01,\n",
      "         1.9639e-01, 1.3630e-01, 1.7500e-01, 2.3986e-01],\n",
      "        [4.0291e-04, 1.0180e-03, 9.2592e-04, 1.2150e-01, 2.0857e-02, 9.0484e-02,\n",
      "         1.1475e-01, 1.0536e-01, 1.3157e-01, 4.1312e-01],\n",
      "        [1.7286e-04, 1.5646e-03, 5.9620e-04, 1.1846e-01, 1.5211e-02, 8.9645e-02,\n",
      "         1.3333e-01, 1.3116e-01, 1.6855e-01, 3.4132e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.60, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 7.13, Minutes Elapsed: 512.17\n",
      "Sampling from val predictions...\n",
      "Source: 我 以为 自己 陷入 了 疯狂 的 爱情 还 不知\n",
      "Reference: i had no idea i was falling into crazy\n",
      "Model: <SOS> i i the idea that how to to with\n",
      "Attention Weights: tensor([[9.9948e-01, 3.0519e-08, 9.0427e-09, 8.4925e-10, 4.3328e-08, 1.3373e-08,\n",
      "         1.1391e-08, 2.8034e-07, 2.8416e-07, 5.1888e-04],\n",
      "        [9.7291e-01, 2.4142e-02, 1.0409e-03, 1.2641e-04, 1.0859e-04, 2.0162e-05,\n",
      "         1.9092e-05, 1.2142e-04, 5.0973e-05, 1.4619e-03],\n",
      "        [2.7814e-01, 2.3824e-01, 1.3021e-01, 2.2864e-02, 1.6947e-02, 2.8524e-03,\n",
      "         1.8795e-03, 1.5902e-02, 1.2298e-02, 2.8067e-01],\n",
      "        [3.7562e-02, 2.8185e-02, 2.1541e-01, 1.3187e-01, 1.5074e-01, 2.5571e-02,\n",
      "         3.6329e-03, 3.0309e-02, 1.4604e-02, 3.6211e-01],\n",
      "        [3.7266e-03, 1.7263e-03, 5.4997e-02, 9.9850e-02, 2.9005e-01, 5.9094e-02,\n",
      "         2.0131e-02, 7.9282e-02, 3.9375e-02, 3.5177e-01],\n",
      "        [1.0429e-02, 1.3543e-03, 9.8988e-03, 3.5156e-02, 1.3910e-01, 1.4093e-02,\n",
      "         8.2268e-03, 1.6350e-01, 7.8142e-02, 5.4010e-01],\n",
      "        [7.6164e-03, 2.4636e-03, 2.3926e-02, 5.2185e-02, 1.8323e-01, 3.7082e-02,\n",
      "         1.7548e-02, 2.4440e-01, 1.1375e-01, 3.1781e-01],\n",
      "        [3.7131e-04, 3.1509e-04, 7.3612e-03, 1.5619e-02, 1.3135e-01, 1.4272e-01,\n",
      "         3.3305e-02, 2.2469e-01, 1.5143e-01, 2.9284e-01],\n",
      "        [4.5003e-04, 1.6906e-04, 2.4590e-03, 5.0163e-03, 7.0281e-02, 2.6222e-02,\n",
      "         1.7181e-02, 2.8351e-01, 2.0252e-01, 3.9219e-01]])\n",
      "\n",
      "Source: 如果 有 任何 <UNK> 我 就 会 入狱 并 被\n",
      "Reference: if anything seemed unnatural , i could be imprisoned\n",
      "Model: <SOS> if , &apos;s that , i i think that\n",
      "Attention Weights: tensor([[1.3055e-08, 5.1539e-07, 1.9488e-04, 5.3527e-05, 9.9616e-01, 1.6485e-05,\n",
      "         4.5682e-04, 7.7275e-05, 4.3948e-06, 3.0330e-03],\n",
      "        [9.1538e-03, 1.0881e-01, 2.8942e-01, 3.4244e-02, 4.3338e-01, 1.0487e-02,\n",
      "         2.7422e-02, 2.0519e-02, 4.0721e-03, 6.2501e-02],\n",
      "        [3.9318e-02, 1.1480e-01, 3.7195e-01, 9.0854e-02, 9.9772e-02, 2.3383e-02,\n",
      "         5.3533e-02, 4.0265e-02, 1.4276e-02, 1.5184e-01],\n",
      "        [8.9936e-03, 2.4768e-02, 4.9839e-01, 4.0691e-02, 1.4328e-01, 2.0110e-02,\n",
      "         8.0974e-02, 3.4147e-02, 7.8981e-03, 1.4076e-01],\n",
      "        [2.9117e-03, 1.2593e-02, 2.2574e-01, 7.6942e-02, 4.2713e-01, 3.6321e-02,\n",
      "         9.2588e-02, 3.6371e-02, 9.4988e-03, 7.9903e-02],\n",
      "        [4.8215e-04, 2.8711e-03, 5.2892e-02, 6.2012e-02, 5.8450e-01, 3.0418e-02,\n",
      "         8.4028e-02, 4.6986e-02, 9.4323e-03, 1.2638e-01],\n",
      "        [1.6331e-05, 4.7575e-05, 6.1083e-03, 1.9403e-02, 6.1139e-01, 1.6771e-02,\n",
      "         1.0957e-01, 5.0594e-02, 9.3871e-03, 1.7671e-01],\n",
      "        [1.1847e-04, 4.9894e-04, 8.8341e-03, 5.4933e-03, 3.2380e-01, 2.8255e-02,\n",
      "         3.8903e-01, 1.1787e-01, 1.5691e-02, 1.1041e-01],\n",
      "        [5.6722e-06, 8.2801e-05, 1.7322e-02, 1.4567e-03, 5.1138e-02, 3.2217e-02,\n",
      "         1.6826e-01, 2.3615e-01, 7.1924e-02, 4.2145e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.75, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 8.21, Minutes Elapsed: 523.50\n",
      "Sampling from val predictions...\n",
      "Source: 没人 知道 我们 是 干嘛 的 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: no one knows what the hell we do .\n",
      "Model: <SOS> nobody , we we we &apos;re are . .\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.9995, 0.0000, 0.0000, 0.0004, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0166, 0.1308, 0.8413, 0.0019, 0.0000, 0.0086, 0.0008, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2182, 0.2624, 0.1973, 0.0739, 0.0058, 0.1502, 0.0921, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0457, 0.1953, 0.6891, 0.0268, 0.0022, 0.0261, 0.0148, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0084, 0.0616, 0.5635, 0.1711, 0.0201, 0.1152, 0.0600, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0009, 0.0074, 0.5349, 0.2393, 0.0522, 0.1118, 0.0534, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0004, 0.0036, 0.2419, 0.3059, 0.0530, 0.3176, 0.0775, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0012, 0.0154, 0.3392, 0.2628, 0.0660, 0.2241, 0.0912, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0023, 0.0159, 0.4921, 0.1449, 0.0789, 0.2100, 0.0559, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 事实 事实上 这些 肌肉 强健 的 男人 若干 若干年 年后\n",
      "Reference: in fact , it &apos;s very likely that this\n",
      "Model: <SOS> in fact , these these out to to be\n",
      "Attention Weights: tensor([[2.4816e-05, 5.5075e-07, 4.0941e-01, 3.1381e-05, 1.6374e-04, 1.4982e-04,\n",
      "         3.6781e-04, 1.7103e-05, 1.1074e-04, 5.8973e-01],\n",
      "        [5.9057e-03, 5.4710e-04, 9.4266e-01, 9.4533e-04, 2.2151e-03, 9.4302e-04,\n",
      "         4.9071e-04, 9.5750e-05, 3.1376e-04, 4.5883e-02],\n",
      "        [1.2900e-02, 7.6805e-03, 6.0826e-01, 3.3337e-02, 2.8277e-02, 2.8330e-02,\n",
      "         9.8153e-03, 5.4257e-03, 1.1620e-02, 2.5436e-01],\n",
      "        [6.0160e-03, 9.1805e-03, 6.6159e-01, 6.4242e-02, 4.4852e-02, 3.6339e-02,\n",
      "         3.4365e-02, 6.7671e-03, 1.3483e-02, 1.2317e-01],\n",
      "        [6.8265e-04, 2.5253e-04, 7.1706e-01, 2.6782e-02, 3.3270e-02, 3.1847e-02,\n",
      "         1.5635e-02, 3.5553e-03, 9.0970e-03, 1.6182e-01],\n",
      "        [2.4781e-04, 4.2215e-05, 1.2903e-01, 1.8956e-02, 7.1983e-02, 1.6335e-01,\n",
      "         5.5545e-02, 1.8373e-02, 2.1095e-02, 5.2138e-01],\n",
      "        [3.4238e-04, 7.8770e-05, 7.6469e-02, 5.3964e-02, 1.4902e-01, 1.3198e-01,\n",
      "         9.6108e-02, 3.9703e-02, 4.5774e-02, 4.0655e-01],\n",
      "        [2.1866e-04, 3.5820e-05, 6.2839e-02, 3.4779e-02, 9.1707e-02, 1.8352e-01,\n",
      "         9.2828e-02, 3.6009e-02, 5.0936e-02, 4.4713e-01],\n",
      "        [3.2749e-04, 2.0102e-04, 1.0043e-01, 4.5807e-02, 9.8958e-02, 1.6581e-01,\n",
      "         1.3886e-01, 5.2895e-02, 7.7364e-02, 3.1934e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.90, Train Loss: 0.00, Val Loss: 4.86, Train BLEU: 0.00, Val BLEU: 7.31, Minutes Elapsed: 535.09\n",
      "Sampling from val predictions...\n",
      "Source: 此时 此时此刻 此刻 你 可能 在 想 哇 这 才\n",
      "Reference: right now , maybe you &apos;re thinking , &quot;\n",
      "Model: <SOS> and , , you you you , to this\n",
      "Attention Weights: tensor([[4.4588e-06, 4.1002e-06, 2.5276e-06, 9.9947e-01, 2.4181e-06, 4.5200e-07,\n",
      "         2.0551e-06, 2.2396e-07, 1.1736e-04, 3.9797e-04],\n",
      "        [9.0106e-03, 2.6165e-03, 1.2642e-02, 9.6587e-01, 1.6306e-03, 5.2062e-05,\n",
      "         4.2415e-04, 1.1364e-04, 5.0071e-03, 2.6374e-03],\n",
      "        [2.4485e-02, 4.4877e-02, 1.7664e-01, 3.0565e-01, 1.0479e-01, 1.3718e-02,\n",
      "         3.5871e-02, 2.1004e-02, 1.0576e-01, 1.6720e-01],\n",
      "        [1.3677e-02, 1.1741e-02, 5.3962e-02, 7.1090e-01, 4.3353e-02, 1.6709e-03,\n",
      "         2.6279e-02, 5.1235e-03, 4.8863e-02, 8.4429e-02],\n",
      "        [2.6557e-03, 2.8387e-03, 2.0673e-02, 4.2140e-01, 1.4789e-01, 1.7297e-02,\n",
      "         9.1441e-02, 3.0480e-02, 1.2128e-01, 1.4405e-01],\n",
      "        [1.9323e-03, 2.3988e-03, 1.7953e-02, 3.2838e-01, 1.8500e-01, 3.9297e-02,\n",
      "         1.2455e-01, 5.5878e-02, 1.4019e-01, 1.0443e-01],\n",
      "        [1.1069e-03, 1.2119e-03, 1.2539e-02, 4.5131e-01, 1.1961e-01, 1.0333e-02,\n",
      "         9.1086e-02, 2.2327e-02, 1.3875e-01, 1.5174e-01],\n",
      "        [1.6418e-03, 1.4605e-03, 3.1257e-03, 1.0908e-02, 1.0061e-01, 8.5607e-02,\n",
      "         1.4038e-01, 1.8188e-01, 3.5752e-01, 1.1685e-01],\n",
      "        [6.4151e-04, 2.0737e-04, 2.3933e-04, 2.7914e-03, 5.9650e-03, 8.6010e-03,\n",
      "         1.1461e-01, 1.2534e-01, 4.4479e-01, 2.9681e-01]])\n",
      "\n",
      "Source: 这 之后 一个 警察 对 另 一个 个人 说 报告\n",
      "Reference: after they finished questioning me , one official said\n",
      "Model: <SOS> it &apos;s , a a , a said the\n",
      "Attention Weights: tensor([[0.0025, 0.0247, 0.0002, 0.0003, 0.0000, 0.0005, 0.0010, 0.0001, 0.0158,\n",
      "         0.9549],\n",
      "        [0.2157, 0.6200, 0.0228, 0.0052, 0.0028, 0.0068, 0.0026, 0.0006, 0.0141,\n",
      "         0.1092],\n",
      "        [0.2000, 0.2820, 0.0892, 0.0490, 0.0217, 0.0290, 0.0109, 0.0031, 0.0346,\n",
      "         0.2807],\n",
      "        [0.0721, 0.0237, 0.0575, 0.0722, 0.0321, 0.0568, 0.0229, 0.0065, 0.0276,\n",
      "         0.6286],\n",
      "        [0.0193, 0.0165, 0.0828, 0.1390, 0.0915, 0.2295, 0.1215, 0.0160, 0.0275,\n",
      "         0.2564],\n",
      "        [0.0002, 0.0006, 0.0064, 0.0410, 0.0438, 0.0701, 0.3208, 0.1427, 0.1052,\n",
      "         0.2691],\n",
      "        [0.0004, 0.0006, 0.0111, 0.0379, 0.0169, 0.0724, 0.1741, 0.0354, 0.1591,\n",
      "         0.4922],\n",
      "        [0.0000, 0.0001, 0.0023, 0.0024, 0.0013, 0.0134, 0.0824, 0.0115, 0.1499,\n",
      "         0.7367],\n",
      "        [0.0000, 0.0001, 0.0022, 0.0032, 0.0051, 0.0339, 0.0816, 0.0178, 0.1463,\n",
      "         0.7098]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.00, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 7.35, Minutes Elapsed: 542.90\n",
      "Sampling from val predictions...\n",
      "Source: 但 不幸 的 是 我 得 告诉 你们 在 2007\n",
      "Reference: but unfortunately i have to inform you that in\n",
      "Model: <SOS> but unfortunately , i tell tell you you you\n",
      "Attention Weights: tensor([[7.2201e-09, 9.1318e-08, 4.8752e-06, 8.2316e-05, 9.8589e-01, 9.4865e-05,\n",
      "         2.1568e-04, 1.3195e-02, 8.4450e-07, 5.1233e-04],\n",
      "        [1.2756e-03, 3.7647e-02, 7.7014e-02, 1.3314e-01, 7.2521e-01, 7.7140e-03,\n",
      "         1.6509e-03, 1.2515e-02, 5.4325e-05, 3.7727e-03],\n",
      "        [1.0566e-02, 8.7836e-02, 9.5898e-02, 1.3248e-01, 3.7377e-01, 1.2163e-02,\n",
      "         8.6558e-03, 1.8608e-01, 1.0919e-03, 9.1466e-02],\n",
      "        [2.0531e-02, 1.3206e-01, 1.4796e-01, 1.1858e-01, 3.1349e-01, 7.5670e-02,\n",
      "         2.1873e-02, 5.0744e-02, 4.5962e-03, 1.1450e-01],\n",
      "        [1.3764e-03, 1.3261e-02, 3.1367e-02, 6.4972e-02, 6.3925e-01, 4.7446e-02,\n",
      "         3.7619e-02, 9.6535e-02, 2.2206e-03, 6.5952e-02],\n",
      "        [7.9257e-04, 3.0099e-03, 3.4886e-03, 8.8819e-03, 2.8890e-01, 1.5094e-01,\n",
      "         1.1908e-01, 3.2783e-01, 1.0983e-02, 8.6084e-02],\n",
      "        [1.4983e-04, 1.9517e-04, 1.4697e-03, 8.4886e-03, 2.8554e-01, 9.8940e-02,\n",
      "         2.9733e-01, 2.2563e-01, 1.1489e-02, 7.0767e-02],\n",
      "        [1.8403e-04, 3.6351e-04, 2.1535e-03, 8.5714e-03, 5.1295e-01, 1.0305e-01,\n",
      "         1.2315e-01, 1.5596e-01, 8.2993e-03, 8.5317e-02],\n",
      "        [4.5919e-05, 5.2408e-05, 6.2588e-04, 5.3196e-03, 2.4216e-01, 8.1748e-02,\n",
      "         1.9254e-01, 3.4051e-01, 3.6414e-02, 1.0059e-01]])\n",
      "\n",
      "Source: 但是 很快 就 有人 抱怨 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: and then somebody complained . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> but , it &apos;s &apos;t . was . .\n",
      "Attention Weights: tensor([[0.0000, 0.0001, 0.0237, 0.0011, 0.7724, 0.2028, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0080, 0.1268, 0.7220, 0.0103, 0.0823, 0.0506, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0247, 0.0852, 0.4650, 0.0266, 0.2008, 0.1977, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0151, 0.0831, 0.4069, 0.0746, 0.1633, 0.2570, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0111, 0.0638, 0.3752, 0.1623, 0.1328, 0.2548, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0146, 0.0424, 0.0626, 0.1530, 0.5760, 0.1514, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0354, 0.0452, 0.0970, 0.1703, 0.4254, 0.2267, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0022, 0.0066, 0.1433, 0.0666, 0.4475, 0.3338, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0072, 0.0137, 0.0385, 0.0791, 0.4111, 0.4504, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.15, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 7.31, Minutes Elapsed: 554.56\n",
      "Sampling from val predictions...\n",
      "Source: 我 回答 说 当然 不 啦 我 当然 不怕 <UNK>\n",
      "Reference: and i &apos;m like , &quot; hell no ,\n",
      "Model: <SOS> i i said , , &quot; , , ,\n",
      "Attention Weights: tensor([[9.9061e-01, 2.8677e-09, 4.2691e-08, 2.8222e-06, 5.5076e-09, 7.1770e-07,\n",
      "         3.8099e-03, 1.7338e-05, 3.9498e-03, 1.6108e-03],\n",
      "        [9.5206e-01, 1.1343e-03, 4.9827e-03, 2.3411e-02, 1.0758e-04, 1.2571e-03,\n",
      "         4.8966e-03, 2.1317e-03, 3.6397e-03, 6.3783e-03],\n",
      "        [5.4453e-01, 2.4832e-02, 1.2354e-01, 9.8443e-02, 1.9869e-03, 8.0886e-03,\n",
      "         3.4663e-02, 1.8863e-02, 4.8863e-02, 9.6189e-02],\n",
      "        [2.2807e-01, 5.2935e-02, 2.2328e-01, 2.0850e-01, 4.9141e-03, 1.9269e-02,\n",
      "         4.2531e-02, 3.4698e-02, 7.0862e-02, 1.1493e-01],\n",
      "        [2.2216e-02, 5.1110e-02, 3.2172e-01, 2.7823e-01, 1.0873e-02, 5.8592e-02,\n",
      "         5.7258e-02, 3.5674e-02, 5.9207e-02, 1.0512e-01],\n",
      "        [2.5720e-02, 7.1495e-03, 1.8712e-01, 1.7035e-01, 1.3673e-02, 8.9258e-02,\n",
      "         2.1562e-01, 4.2789e-02, 8.8032e-02, 1.6028e-01],\n",
      "        [1.3215e-02, 4.9970e-03, 1.9250e-01, 4.0911e-01, 1.5162e-02, 5.8368e-02,\n",
      "         6.5169e-02, 8.0663e-02, 5.8184e-02, 1.0263e-01],\n",
      "        [4.0886e-02, 2.0053e-03, 2.1058e-01, 2.7305e-01, 1.3359e-02, 6.6528e-02,\n",
      "         2.4422e-01, 6.3205e-02, 3.2160e-02, 5.4005e-02],\n",
      "        [3.6602e-02, 3.7859e-03, 2.9132e-01, 3.3824e-01, 2.7943e-02, 6.9841e-02,\n",
      "         1.1915e-01, 5.1683e-02, 2.8369e-02, 3.3055e-02]])\n",
      "\n",
      "Source: 因为 我 当时 是 个 小孩 小孩子 孩子 我 可以\n",
      "Reference: since i was a small boy , i used\n",
      "Model: <SOS> because i was a a girl , i i\n",
      "Attention Weights: tensor([[7.6270e-06, 5.6593e-01, 9.7375e-08, 7.7782e-08, 1.6684e-07, 8.8133e-07,\n",
      "         4.6096e-06, 4.1822e-07, 3.8305e-01, 5.1004e-02],\n",
      "        [2.7544e-03, 9.8624e-01, 5.2665e-04, 8.9077e-06, 8.3411e-06, 7.9713e-06,\n",
      "         8.2618e-06, 5.1309e-06, 5.9226e-03, 4.5207e-03],\n",
      "        [6.1608e-02, 5.7843e-01, 8.3058e-02, 4.2305e-03, 1.0988e-03, 2.4585e-03,\n",
      "         2.8485e-03, 1.4690e-03, 3.3830e-02, 2.3097e-01],\n",
      "        [5.8648e-02, 3.4922e-01, 2.0515e-01, 9.1707e-02, 2.3050e-02, 9.3162e-03,\n",
      "         6.8334e-03, 3.4569e-03, 2.7986e-02, 2.2463e-01],\n",
      "        [3.6930e-02, 2.5850e-02, 2.4904e-02, 1.2686e-01, 1.0745e-01, 1.7772e-01,\n",
      "         1.3607e-01, 1.0608e-02, 8.6304e-02, 2.6729e-01],\n",
      "        [4.4757e-03, 1.5258e-02, 2.1688e-03, 7.5261e-03, 2.4809e-02, 9.9098e-02,\n",
      "         1.0808e-01, 2.0661e-02, 3.6359e-01, 3.5434e-01],\n",
      "        [1.1082e-04, 1.0265e-03, 1.0924e-05, 5.1951e-05, 8.1657e-05, 2.4707e-03,\n",
      "         6.1119e-03, 3.0018e-03, 8.1086e-01, 1.7627e-01],\n",
      "        [2.6305e-04, 8.7563e-04, 4.5382e-06, 5.5599e-05, 4.3854e-05, 1.7183e-03,\n",
      "         5.3473e-03, 7.5910e-03, 8.4452e-01, 1.3958e-01],\n",
      "        [5.0523e-05, 1.1678e-03, 8.4398e-06, 1.1020e-04, 1.3164e-04, 2.4007e-03,\n",
      "         1.0830e-02, 4.4229e-03, 5.5403e-01, 4.2685e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.30, Train Loss: 0.00, Val Loss: 4.93, Train BLEU: 0.00, Val BLEU: 7.78, Minutes Elapsed: 566.14\n",
      "Sampling from val predictions...\n",
      "Source: 谢谢 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank you . <EOS> you <EOS> . . .\n",
      "Attention Weights: tensor([[0.0434, 0.9566, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4752, 0.5248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5612, 0.4388, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5118, 0.4882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4583, 0.5417, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2818, 0.7182, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3410, 0.6590, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3416, 0.6584, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3940, 0.6060, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 早先 我 在 战争 中长 长大 周末 在 朋友 朋友家\n",
      "Reference: long before that i had grown up with the\n",
      "Model: <SOS> i , i was i in in in a\n",
      "Attention Weights: tensor([[5.2547e-12, 9.9995e-01, 4.5746e-09, 5.0009e-10, 4.0931e-08, 6.0412e-09,\n",
      "         3.4686e-09, 4.6482e-08, 2.3051e-08, 5.1788e-05],\n",
      "        [9.5898e-05, 9.9741e-01, 2.4469e-04, 4.4421e-05, 1.3535e-04, 1.8250e-05,\n",
      "         1.4433e-05, 5.1835e-05, 4.4591e-05, 1.9383e-03],\n",
      "        [2.5491e-02, 4.8876e-01, 8.5881e-02, 3.3426e-02, 2.6994e-02, 1.2521e-02,\n",
      "         1.1945e-02, 3.3913e-02, 2.9547e-02, 2.5152e-01],\n",
      "        [9.5827e-04, 6.2394e-01, 5.1427e-02, 3.7340e-02, 2.3484e-02, 7.4542e-03,\n",
      "         5.7356e-03, 1.5418e-02, 1.2392e-02, 2.2185e-01],\n",
      "        [2.8091e-04, 1.6913e-01, 5.3385e-02, 1.3025e-01, 1.5316e-01, 3.7281e-02,\n",
      "         3.6866e-02, 5.5093e-02, 2.3477e-02, 3.4108e-01],\n",
      "        [2.1971e-05, 4.4158e-01, 2.6248e-03, 1.6293e-02, 1.1316e-01, 1.3211e-02,\n",
      "         8.1975e-03, 1.5437e-02, 1.0588e-02, 3.7889e-01],\n",
      "        [4.6837e-05, 1.1502e-02, 8.3489e-03, 3.8939e-02, 1.9416e-01, 1.2826e-01,\n",
      "         7.5854e-02, 1.0795e-01, 5.9744e-02, 3.7519e-01],\n",
      "        [2.1042e-07, 2.2437e-04, 2.2371e-05, 1.0985e-03, 3.1804e-02, 2.2825e-02,\n",
      "         3.6462e-02, 2.7487e-02, 6.0312e-02, 8.1976e-01],\n",
      "        [4.7107e-07, 1.9743e-03, 1.3873e-05, 7.0970e-04, 2.6742e-02, 1.6685e-02,\n",
      "         1.9024e-02, 2.5149e-02, 8.1898e-02, 8.2780e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.45, Train Loss: 0.00, Val Loss: 4.90, Train BLEU: 0.00, Val BLEU: 7.88, Minutes Elapsed: 577.68\n",
      "Sampling from val predictions...\n",
      "Source: 没人 能 独自 自创 创立 一个 企业 没人 <EOS> <PAD>\n",
      "Reference: nobody started a company alone . no one .\n",
      "Model: <SOS> nobody &apos;s the a , a a was .\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0025, 0.0142, 0.1243, 0.0028, 0.0580, 0.7004, 0.0978,\n",
      "         0.0000],\n",
      "        [0.0191, 0.1005, 0.1468, 0.2255, 0.1871, 0.0125, 0.0792, 0.1675, 0.0618,\n",
      "         0.0000],\n",
      "        [0.0811, 0.1109, 0.1878, 0.2074, 0.0779, 0.0260, 0.0318, 0.0688, 0.2082,\n",
      "         0.0000],\n",
      "        [0.0464, 0.0386, 0.1180, 0.2270, 0.1754, 0.0654, 0.0703, 0.0949, 0.1640,\n",
      "         0.0000],\n",
      "        [0.0019, 0.0029, 0.0197, 0.2014, 0.4571, 0.1016, 0.0819, 0.0968, 0.0365,\n",
      "         0.0000],\n",
      "        [0.0027, 0.0018, 0.0174, 0.1368, 0.4094, 0.0369, 0.1069, 0.1466, 0.1416,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0096, 0.0278, 0.0422, 0.1566, 0.0814, 0.2689, 0.2169, 0.1942,\n",
      "         0.0000],\n",
      "        [0.0099, 0.0152, 0.0408, 0.1090, 0.2920, 0.1571, 0.1950, 0.1061, 0.0751,\n",
      "         0.0000],\n",
      "        [0.0008, 0.0050, 0.0096, 0.0727, 0.4078, 0.1379, 0.1547, 0.1286, 0.0826,\n",
      "         0.0000]])\n",
      "\n",
      "Source: <UNK> 下 浸 着 些 树 的 枝干 经常 常会\n",
      "Reference: the skeletal tree limbs submerged in lake volta often\n",
      "Model: <SOS> <UNK> <UNK> and , a the the air a\n",
      "Attention Weights: tensor([[4.1636e-05, 1.7998e-06, 1.8838e-07, 3.1001e-05, 4.6074e-06, 1.1549e-04,\n",
      "         1.3931e-05, 1.6045e-05, 3.5538e-04, 9.9942e-01],\n",
      "        [7.7414e-02, 1.8352e-02, 9.2740e-04, 5.8935e-02, 4.8887e-03, 2.3600e-02,\n",
      "         3.1289e-03, 1.0806e-03, 5.9364e-03, 8.0574e-01],\n",
      "        [7.3735e-02, 7.4080e-02, 9.6666e-03, 1.3242e-01, 1.2504e-02, 5.2840e-02,\n",
      "         6.4525e-03, 5.7354e-03, 2.3397e-02, 6.0917e-01],\n",
      "        [8.1413e-02, 7.6187e-02, 4.3125e-02, 1.7267e-01, 5.5162e-02, 1.2035e-01,\n",
      "         1.4355e-02, 1.9293e-02, 2.7951e-02, 3.8949e-01],\n",
      "        [6.1327e-03, 1.0470e-02, 4.4031e-03, 6.1752e-02, 8.6625e-02, 2.6880e-01,\n",
      "         4.6193e-02, 6.4863e-02, 8.5610e-02, 3.6515e-01],\n",
      "        [7.7840e-03, 7.2532e-03, 2.8948e-03, 4.4626e-02, 7.5563e-02, 2.7687e-01,\n",
      "         7.3638e-02, 5.5167e-02, 1.0093e-01, 3.5528e-01],\n",
      "        [9.1598e-03, 4.7730e-03, 1.4618e-03, 2.1205e-02, 3.7215e-02, 1.9100e-01,\n",
      "         4.5042e-02, 6.1703e-02, 1.6852e-01, 4.5992e-01],\n",
      "        [9.6120e-03, 5.0113e-03, 2.7730e-03, 1.5561e-02, 2.5513e-02, 9.7754e-02,\n",
      "         3.9668e-02, 7.0338e-02, 1.8952e-01, 5.4425e-01],\n",
      "        [5.3505e-03, 6.8112e-03, 5.2178e-03, 2.9390e-02, 5.0845e-02, 2.5770e-01,\n",
      "         2.1212e-02, 3.9400e-02, 9.3222e-02, 4.9085e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.60, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 6.92, Minutes Elapsed: 589.19\n",
      "Sampling from val predictions...\n",
      "Source: 我 确实 实有 太多 从没 机会 穿 的 8 英寸\n",
      "Reference: i do have too many <UNK> heels which i\n",
      "Model: <SOS> i i that , , , , that the\n",
      "Attention Weights: tensor([[9.9989e-01, 1.0009e-10, 2.6677e-09, 1.3811e-09, 1.6250e-07, 1.7159e-09,\n",
      "         1.1257e-09, 5.2270e-09, 3.8573e-06, 1.0476e-04],\n",
      "        [9.9160e-01, 1.5836e-03, 3.7743e-04, 2.5491e-04, 2.0879e-03, 1.0998e-04,\n",
      "         5.5538e-05, 4.0992e-05, 1.0058e-03, 2.8812e-03],\n",
      "        [2.2950e-01, 1.0605e-01, 1.1388e-01, 4.8664e-02, 7.8543e-02, 1.1311e-02,\n",
      "         7.5574e-03, 5.4794e-03, 1.6150e-01, 2.3752e-01],\n",
      "        [3.6622e-02, 6.2872e-02, 6.3904e-01, 1.0503e-01, 6.3781e-02, 4.9568e-03,\n",
      "         3.4639e-03, 1.0579e-03, 4.3904e-02, 3.9273e-02],\n",
      "        [1.6179e-02, 7.5089e-03, 1.2840e-01, 1.2546e-01, 2.6570e-01, 2.9312e-02,\n",
      "         1.3626e-02, 6.7540e-03, 1.9598e-01, 2.1108e-01],\n",
      "        [6.7519e-03, 3.0136e-03, 1.2850e-01, 1.0258e-01, 2.8723e-01, 9.8731e-02,\n",
      "         6.2447e-02, 2.6517e-02, 1.6138e-01, 1.2284e-01],\n",
      "        [1.3076e-02, 2.4572e-03, 4.1164e-02, 2.7237e-02, 2.8523e-01, 5.7720e-02,\n",
      "         1.6553e-02, 1.1570e-02, 2.9094e-01, 2.5406e-01],\n",
      "        [1.5286e-02, 6.2058e-04, 5.5946e-03, 8.2233e-03, 1.8031e-01, 6.7453e-02,\n",
      "         2.0860e-02, 1.9249e-02, 3.6321e-01, 3.1919e-01],\n",
      "        [7.8765e-03, 1.6530e-05, 9.1734e-05, 2.1568e-04, 1.0166e-02, 4.7694e-03,\n",
      "         1.5703e-03, 3.7383e-03, 5.0028e-01, 4.7127e-01]])\n",
      "\n",
      "Source: 新奥尔良 奥尔良 拥有 世界 上 很多 最 漂亮 的 建筑\n",
      "Reference: the city has some of the most beautiful architecture\n",
      "Model: <SOS> and the majority the to , and and world\n",
      "Attention Weights: tensor([[0.0001, 0.0002, 0.0006, 0.0000, 0.0306, 0.0123, 0.0012, 0.0004, 0.0010,\n",
      "         0.9536],\n",
      "        [0.0020, 0.0379, 0.6968, 0.0009, 0.1758, 0.0132, 0.0015, 0.0004, 0.0009,\n",
      "         0.0707],\n",
      "        [0.0028, 0.0216, 0.4729, 0.0206, 0.2056, 0.0402, 0.0130, 0.0049, 0.0076,\n",
      "         0.2108],\n",
      "        [0.0275, 0.0370, 0.2531, 0.0351, 0.2237, 0.0553, 0.0420, 0.0096, 0.0284,\n",
      "         0.2882],\n",
      "        [0.0064, 0.0184, 0.1573, 0.0400, 0.2737, 0.1521, 0.0412, 0.0109, 0.0240,\n",
      "         0.2759],\n",
      "        [0.0007, 0.0012, 0.0360, 0.0214, 0.2129, 0.3551, 0.0812, 0.0279, 0.0462,\n",
      "         0.2175],\n",
      "        [0.0003, 0.0006, 0.0138, 0.0013, 0.1245, 0.1577, 0.0880, 0.0566, 0.0709,\n",
      "         0.4863],\n",
      "        [0.0015, 0.0022, 0.0177, 0.0019, 0.1972, 0.1530, 0.0805, 0.0284, 0.0364,\n",
      "         0.4813],\n",
      "        [0.0038, 0.0054, 0.0267, 0.0029, 0.1551, 0.1053, 0.0505, 0.0207, 0.0266,\n",
      "         0.6030]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.75, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.92, Minutes Elapsed: 600.81\n",
      "Sampling from val predictions...\n",
      "Source: 我 想 这 就是 内罗毕 公园 的 狮子 <UNK> 子会\n",
      "Reference: and i think this is why the nairobi national\n",
      "Model: <SOS> i think think that &apos;s the the is of\n",
      "Attention Weights: tensor([[9.9999e-01, 1.8092e-11, 3.7643e-08, 4.0665e-11, 3.3726e-10, 4.0383e-10,\n",
      "         6.5990e-10, 9.1837e-08, 1.9922e-06, 6.0059e-06],\n",
      "        [9.9680e-01, 2.2592e-03, 8.4154e-04, 1.7709e-05, 1.8335e-06, 7.3361e-07,\n",
      "         1.1560e-06, 5.4813e-06, 2.5507e-05, 4.7991e-05],\n",
      "        [1.5915e-01, 1.6334e-01, 5.2078e-01, 2.1874e-02, 5.5894e-03, 1.7779e-03,\n",
      "         1.2144e-03, 8.5181e-03, 4.7896e-02, 6.9871e-02],\n",
      "        [1.7272e-01, 1.8484e-01, 4.0874e-01, 3.6859e-02, 4.9186e-03, 2.0219e-03,\n",
      "         1.8125e-03, 1.5205e-02, 4.9615e-02, 1.2327e-01],\n",
      "        [5.7086e-03, 2.3273e-02, 8.3223e-01, 4.5018e-02, 2.5240e-02, 3.3768e-03,\n",
      "         1.1447e-03, 8.0759e-03, 1.4349e-02, 4.1586e-02],\n",
      "        [1.6385e-02, 6.0255e-03, 3.5561e-01, 1.4515e-01, 1.8609e-02, 1.2742e-02,\n",
      "         3.7100e-03, 3.0946e-02, 9.7305e-02, 3.1352e-01],\n",
      "        [1.1898e-02, 2.0844e-04, 4.5333e-02, 2.8835e-02, 5.6672e-02, 4.1355e-02,\n",
      "         1.8960e-02, 1.2619e-01, 2.1921e-02, 6.4863e-01],\n",
      "        [9.5500e-03, 1.0078e-03, 8.9348e-02, 5.8079e-02, 1.3212e-01, 1.1351e-01,\n",
      "         6.4200e-02, 1.0414e-01, 4.1219e-02, 3.8682e-01],\n",
      "        [2.3807e-03, 2.6479e-05, 2.7404e-02, 6.5047e-03, 1.1055e-02, 2.9803e-02,\n",
      "         3.8053e-02, 7.0137e-02, 1.1081e-01, 7.0383e-01]])\n",
      "\n",
      "Source: 我 想要 让 他们 知道 我们 将 为 他们 作证\n",
      "Reference: i wanted them to know that we will be\n",
      "Model: <SOS> i i them know know that they would be\n",
      "Attention Weights: tensor([[9.9993e-01, 8.8926e-09, 3.1711e-07, 3.1814e-05, 6.4975e-10, 2.4167e-05,\n",
      "         1.8807e-10, 1.6452e-09, 8.7718e-08, 1.6612e-05],\n",
      "        [7.7250e-01, 8.0589e-02, 7.2312e-03, 1.3912e-01, 6.8549e-06, 4.4845e-04,\n",
      "         1.6422e-06, 1.4878e-06, 1.5773e-05, 8.3542e-05],\n",
      "        [3.4084e-02, 6.5507e-02, 3.5096e-01, 5.1801e-01, 5.2869e-04, 9.6878e-03,\n",
      "         3.9578e-04, 3.8792e-04, 2.1646e-03, 1.8277e-02],\n",
      "        [8.4323e-03, 1.7113e-02, 3.1341e-01, 6.0954e-01, 3.8765e-03, 2.6132e-02,\n",
      "         1.4794e-03, 1.1230e-03, 4.2334e-03, 1.4666e-02],\n",
      "        [5.4284e-03, 3.8790e-03, 1.7174e-01, 5.5294e-01, 3.6786e-02, 1.0784e-01,\n",
      "         2.8251e-02, 2.8353e-02, 2.0122e-02, 4.4663e-02],\n",
      "        [6.4662e-03, 1.1159e-03, 5.4472e-02, 1.0856e-01, 3.1253e-02, 4.1938e-01,\n",
      "         8.2947e-02, 7.9377e-02, 5.5780e-02, 1.6065e-01],\n",
      "        [5.0998e-03, 1.4750e-03, 4.6206e-02, 1.2699e-01, 5.3821e-02, 4.7819e-01,\n",
      "         3.3575e-02, 4.4007e-02, 6.9457e-02, 1.4118e-01],\n",
      "        [4.5610e-03, 4.7423e-04, 1.1721e-02, 1.6840e-02, 1.2305e-02, 5.3257e-01,\n",
      "         8.5550e-02, 9.0748e-02, 9.2409e-02, 1.5282e-01],\n",
      "        [7.6993e-03, 4.6949e-05, 2.8938e-03, 9.0065e-03, 1.5084e-03, 2.8103e-01,\n",
      "         1.1653e-01, 1.5251e-01, 1.5972e-01, 2.6906e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.90, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 7.34, Minutes Elapsed: 612.34\n",
      "Sampling from val predictions...\n",
      "Source: 我们 怎样 能够 更好 地 分享 各自 的 关于 被\n",
      "Reference: how can we share more of our memories of\n",
      "Model: <SOS> we have we get the to to and in\n",
      "Attention Weights: tensor([[9.9501e-01, 3.0010e-07, 1.2805e-08, 6.4108e-07, 5.3373e-07, 1.0596e-06,\n",
      "         1.3380e-05, 1.5704e-06, 3.0007e-05, 4.9459e-03],\n",
      "        [9.6217e-01, 2.8250e-02, 1.6551e-04, 9.9903e-04, 2.9092e-04, 1.0501e-04,\n",
      "         1.0272e-04, 7.1093e-05, 6.1032e-04, 7.2401e-03],\n",
      "        [1.4755e-01, 1.8815e-01, 1.0244e-01, 5.9377e-02, 2.6200e-02, 1.4113e-02,\n",
      "         3.4017e-02, 1.6409e-02, 5.4992e-02, 3.5675e-01],\n",
      "        [8.5024e-02, 1.5139e-01, 8.4436e-02, 1.9412e-01, 3.7606e-02, 1.5881e-02,\n",
      "         2.3854e-02, 6.9594e-03, 3.8763e-02, 3.6196e-01],\n",
      "        [1.4480e-02, 1.7343e-02, 4.9028e-02, 2.7697e-01, 1.1237e-01, 5.2742e-02,\n",
      "         9.1618e-02, 1.6760e-02, 8.2514e-02, 2.8617e-01],\n",
      "        [1.2314e-02, 2.4142e-02, 4.3015e-02, 1.6725e-01, 1.2172e-01, 5.3740e-02,\n",
      "         1.2737e-01, 2.7664e-02, 7.7505e-02, 3.4528e-01],\n",
      "        [8.1209e-03, 1.1790e-03, 4.3021e-03, 1.8020e-01, 2.1801e-01, 9.4743e-02,\n",
      "         9.8038e-02, 1.4943e-02, 1.2394e-01, 2.5652e-01],\n",
      "        [4.7681e-02, 2.7847e-03, 3.1874e-03, 1.3012e-01, 1.2703e-01, 9.9079e-02,\n",
      "         1.0077e-01, 1.0680e-02, 1.9491e-01, 2.8376e-01],\n",
      "        [6.1983e-02, 2.9138e-03, 2.3738e-03, 9.2916e-02, 7.3784e-02, 4.6612e-02,\n",
      "         2.9912e-02, 3.6525e-03, 1.2599e-01, 5.5986e-01]])\n",
      "\n",
      "Source: 事件 发生 的 顺序 在 相机 里 有时 有时候 时候\n",
      "Reference: the order at which events take place in the\n",
      "Model: <SOS> the &apos;s of the in in the in the\n",
      "Attention Weights: tensor([[0.0010, 0.0005, 0.0002, 0.0023, 0.0201, 0.0007, 0.0012, 0.0058, 0.0030,\n",
      "         0.9652],\n",
      "        [0.1786, 0.1478, 0.0458, 0.1263, 0.0753, 0.0141, 0.0121, 0.0221, 0.0172,\n",
      "         0.3608],\n",
      "        [0.0546, 0.1079, 0.0698, 0.0871, 0.0897, 0.0164, 0.0275, 0.0320, 0.0291,\n",
      "         0.4859],\n",
      "        [0.1008, 0.1268, 0.0716, 0.0875, 0.1637, 0.0364, 0.0379, 0.0248, 0.0410,\n",
      "         0.3095],\n",
      "        [0.0576, 0.1101, 0.1326, 0.1133, 0.1800, 0.0228, 0.0237, 0.0272, 0.0259,\n",
      "         0.3069],\n",
      "        [0.0216, 0.0390, 0.0244, 0.0465, 0.1314, 0.0161, 0.0266, 0.0197, 0.0188,\n",
      "         0.6558],\n",
      "        [0.0125, 0.0127, 0.0327, 0.0393, 0.2149, 0.0318, 0.0712, 0.0411, 0.0351,\n",
      "         0.5086],\n",
      "        [0.0072, 0.0067, 0.0143, 0.0343, 0.2259, 0.0892, 0.0928, 0.1064, 0.0731,\n",
      "         0.3501],\n",
      "        [0.0018, 0.0029, 0.0016, 0.0068, 0.0490, 0.0249, 0.0461, 0.1150, 0.0759,\n",
      "         0.6760]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.00, Train Loss: 0.00, Val Loss: 4.95, Train BLEU: 0.00, Val BLEU: 7.37, Minutes Elapsed: 620.18\n",
      "Sampling from val predictions...\n",
      "Source: 我们 不一 一定 定能 常常 碰到 我们 的 邻居 所以\n",
      "Reference: we don &apos;t bump into every neighbor , so\n",
      "Model: <SOS> we don &apos;t want to , , , our\n",
      "Attention Weights: tensor([[9.9930e-01, 8.4778e-10, 1.3596e-08, 4.9413e-07, 1.6173e-07, 5.1783e-07,\n",
      "         6.3099e-05, 2.2391e-08, 4.1147e-07, 6.3335e-04],\n",
      "        [9.9774e-01, 1.1219e-04, 1.4630e-04, 2.5521e-04, 2.6917e-06, 1.2855e-05,\n",
      "         8.2369e-04, 1.3135e-06, 6.8896e-06, 8.9536e-04],\n",
      "        [6.9375e-01, 3.7660e-02, 3.2681e-02, 3.2087e-02, 2.3402e-03, 4.7550e-03,\n",
      "         3.7349e-02, 1.9766e-03, 7.6049e-03, 1.4979e-01],\n",
      "        [2.6923e-02, 1.9536e-02, 1.3971e-01, 3.4838e-01, 5.4953e-02, 6.0586e-02,\n",
      "         2.4470e-01, 2.8926e-03, 7.4841e-03, 9.4832e-02],\n",
      "        [1.0780e-02, 5.8703e-03, 9.2052e-02, 5.9715e-01, 2.6559e-02, 4.3851e-02,\n",
      "         1.0461e-01, 1.8057e-03, 6.1637e-03, 1.1116e-01],\n",
      "        [1.3245e-03, 1.0288e-03, 1.5413e-02, 2.7013e-01, 1.7288e-01, 2.0284e-01,\n",
      "         2.1494e-01, 1.6280e-02, 3.2417e-02, 7.2761e-02],\n",
      "        [5.2344e-03, 1.9523e-04, 1.8527e-03, 3.0215e-02, 6.1161e-02, 1.8909e-01,\n",
      "         5.4658e-01, 6.9899e-03, 2.8204e-02, 1.3048e-01],\n",
      "        [3.4293e-03, 1.0703e-04, 1.0528e-03, 2.3563e-02, 2.8334e-02, 1.5993e-01,\n",
      "         5.4611e-01, 3.2028e-03, 3.5201e-02, 1.9907e-01],\n",
      "        [3.7852e-03, 9.7316e-05, 5.6620e-04, 1.3295e-02, 1.3404e-02, 7.8285e-02,\n",
      "         6.6366e-01, 2.5877e-03, 2.7142e-02, 1.9717e-01]])\n",
      "\n",
      "Source: 我 没有 一天 不想 这些 美丽 但 却 受到 不公\n",
      "Reference: there is not a day that goes by that\n",
      "Model: <SOS> i i no reason lot of i don ,\n",
      "Attention Weights: tensor([[9.9982e-01, 3.1397e-09, 2.7569e-06, 1.3498e-09, 1.3773e-05, 1.9365e-07,\n",
      "         8.3136e-08, 7.4319e-08, 2.9989e-08, 1.6577e-04],\n",
      "        [9.7955e-01, 5.7581e-04, 1.4402e-02, 1.9013e-05, 5.0478e-03, 3.2487e-05,\n",
      "         9.9009e-06, 1.2346e-05, 2.8634e-06, 3.4323e-04],\n",
      "        [2.4241e-01, 8.9357e-02, 4.6441e-01, 7.2062e-03, 1.1885e-01, 6.9858e-03,\n",
      "         6.4887e-03, 4.7875e-03, 2.1771e-03, 5.7328e-02],\n",
      "        [1.8191e-01, 5.1138e-03, 5.5631e-01, 9.3487e-03, 7.6597e-02, 3.0256e-02,\n",
      "         1.0936e-02, 6.4378e-03, 3.3092e-03, 1.1978e-01],\n",
      "        [1.6407e-01, 3.9115e-03, 5.7115e-01, 3.8911e-03, 5.1393e-02, 2.4703e-02,\n",
      "         1.3388e-02, 4.7404e-03, 2.0275e-03, 1.6073e-01],\n",
      "        [7.2494e-02, 2.1935e-03, 4.7051e-01, 3.0957e-03, 1.5692e-01, 3.1932e-02,\n",
      "         2.1249e-02, 8.1655e-03, 3.3001e-03, 2.3014e-01],\n",
      "        [1.6417e-01, 2.9700e-03, 3.4664e-01, 1.1083e-02, 1.6013e-01, 5.1126e-02,\n",
      "         4.4836e-02, 8.8997e-03, 4.2953e-03, 2.0586e-01],\n",
      "        [1.0878e-01, 1.3414e-03, 1.5473e-01, 7.3806e-04, 5.4359e-01, 1.3588e-02,\n",
      "         8.8002e-03, 7.7915e-03, 1.9085e-03, 1.5873e-01],\n",
      "        [1.4718e-02, 2.3360e-04, 2.4983e-01, 5.9002e-03, 5.5572e-01, 4.8145e-02,\n",
      "         2.0221e-02, 1.0956e-02, 3.8347e-03, 9.0445e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.15, Train Loss: 0.00, Val Loss: 4.98, Train BLEU: 0.00, Val BLEU: 7.21, Minutes Elapsed: 631.79\n",
      "Sampling from val predictions...\n",
      "Source: 孩子 们 从 家里 被 带出 出来 被 贩卖 被迫\n",
      "Reference: children are taken from their families and trafficked and\n",
      "Model: <SOS> children children to to the , to the and\n",
      "Attention Weights: tensor([[1.4088e-08, 9.0374e-05, 7.5294e-07, 1.8851e-07, 1.6226e-06, 5.4531e-06,\n",
      "         1.6879e-05, 1.5882e-05, 1.0863e-03, 9.9878e-01],\n",
      "        [3.9087e-03, 6.8948e-01, 8.7160e-03, 9.7862e-04, 8.3566e-04, 1.3035e-03,\n",
      "         1.9502e-03, 1.1877e-03, 2.0595e-02, 2.7104e-01],\n",
      "        [5.0219e-03, 5.1518e-01, 4.4917e-02, 2.2000e-03, 3.5872e-03, 2.7386e-03,\n",
      "         1.5707e-03, 2.2296e-03, 3.8468e-02, 3.8408e-01],\n",
      "        [3.2384e-03, 5.6283e-02, 3.6080e-02, 1.6888e-02, 6.0070e-03, 1.5899e-02,\n",
      "         1.0017e-02, 7.6422e-03, 2.9580e-02, 8.1836e-01],\n",
      "        [5.5569e-03, 5.5640e-02, 6.8560e-02, 4.3160e-02, 5.4926e-02, 6.8736e-02,\n",
      "         4.3867e-02, 1.6343e-02, 7.1130e-02, 5.7208e-01],\n",
      "        [5.3818e-04, 1.7476e-03, 2.5873e-03, 2.0396e-03, 5.8788e-03, 2.9838e-02,\n",
      "         4.2751e-02, 1.4467e-02, 9.6337e-02, 8.0382e-01],\n",
      "        [2.2104e-03, 4.0794e-03, 2.9208e-03, 3.8106e-03, 3.9462e-03, 4.4929e-02,\n",
      "         1.2460e-01, 1.4642e-02, 1.4017e-01, 6.5869e-01],\n",
      "        [5.7730e-05, 5.5524e-05, 1.8180e-05, 2.8512e-05, 1.0472e-04, 5.4363e-03,\n",
      "         2.3493e-02, 3.8941e-03, 1.8476e-01, 7.8215e-01],\n",
      "        [3.6016e-04, 9.3053e-04, 1.7067e-04, 1.0677e-04, 3.7702e-04, 1.8094e-02,\n",
      "         1.6589e-02, 6.4961e-03, 2.0619e-01, 7.5069e-01]])\n",
      "\n",
      "Source: 他们 捕杀 了 我们 的 家畜 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: they kill our livestock . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> they they our our . . <EOS> . .\n",
      "Attention Weights: tensor([[0.0013, 0.0001, 0.0001, 0.0994, 0.0001, 0.4486, 0.4504, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.8304, 0.0903, 0.0012, 0.0664, 0.0005, 0.0080, 0.0031, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0412, 0.1011, 0.0578, 0.6173, 0.0120, 0.1073, 0.0631, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0032, 0.0080, 0.0334, 0.8078, 0.0085, 0.1123, 0.0268, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0064, 0.0174, 0.0772, 0.6246, 0.0408, 0.1533, 0.0803, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0094, 0.0155, 0.0617, 0.2347, 0.0638, 0.3669, 0.2480, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0196, 0.0168, 0.0761, 0.4876, 0.0783, 0.1568, 0.1647, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0105, 0.0225, 0.0190, 0.6083, 0.0492, 0.1636, 0.1269, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0103, 0.0044, 0.0273, 0.6382, 0.0200, 0.1275, 0.1723, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.30, Train Loss: 0.00, Val Loss: 4.95, Train BLEU: 0.00, Val BLEU: 7.91, Minutes Elapsed: 643.38\n",
      "Sampling from val predictions...\n",
      "Source: 在 塔利 塔利班 控制 阿富汗 的 那些 年 我 记得\n",
      "Reference: during taliban years , i remember there were times\n",
      "Model: <SOS> in the time in i was a my a\n",
      "Attention Weights: tensor([[3.0700e-08, 9.2475e-07, 2.9235e-06, 2.0192e-08, 6.2469e-06, 1.5164e-06,\n",
      "         1.6124e-05, 2.3411e-03, 9.9335e-01, 4.2807e-03],\n",
      "        [8.0743e-05, 2.8615e-03, 1.5963e-03, 2.4474e-05, 2.6845e-04, 1.9780e-04,\n",
      "         1.7626e-03, 3.4203e-02, 9.4437e-01, 1.4635e-02],\n",
      "        [4.0007e-03, 4.2090e-02, 6.9496e-02, 8.4214e-03, 1.2583e-02, 1.7167e-02,\n",
      "         2.8929e-02, 1.1527e-01, 5.0244e-01, 1.9960e-01],\n",
      "        [1.2654e-03, 2.6069e-02, 7.5956e-02, 1.7824e-02, 3.0661e-02, 3.3870e-02,\n",
      "         2.7759e-02, 1.2833e-01, 1.7943e-01, 4.7883e-01],\n",
      "        [5.2582e-05, 1.2703e-03, 6.3700e-03, 4.6469e-03, 7.8119e-02, 1.4121e-02,\n",
      "         3.0270e-02, 1.9325e-01, 6.0920e-01, 6.2703e-02],\n",
      "        [5.3132e-06, 6.5125e-05, 6.4040e-04, 4.4636e-04, 1.6236e-02, 5.0203e-03,\n",
      "         6.9777e-03, 1.3045e-01, 7.5011e-01, 9.0049e-02],\n",
      "        [2.5110e-05, 4.2047e-04, 4.6877e-03, 2.5118e-03, 2.0609e-02, 1.1046e-02,\n",
      "         2.1945e-02, 1.9189e-01, 4.1708e-01, 3.2978e-01],\n",
      "        [1.1574e-04, 9.5947e-04, 3.4998e-03, 7.1494e-03, 4.9507e-02, 1.1998e-02,\n",
      "         2.2973e-02, 1.9176e-01, 3.3484e-01, 3.7720e-01],\n",
      "        [1.4601e-05, 1.4105e-04, 9.9717e-04, 1.6059e-03, 1.7333e-02, 6.4921e-03,\n",
      "         2.4818e-02, 1.2648e-01, 1.7826e-01, 6.4386e-01]])\n",
      "\n",
      "Source: 现在 我 所讲 讲述 的 这个 疯狂 爱情 的 故事\n",
      "Reference: i &apos;m here to tell you the story of\n",
      "Model: <SOS> now i going to this about a story of\n",
      "Attention Weights: tensor([[1.1896e-09, 9.9998e-01, 3.5637e-11, 4.9487e-09, 1.0801e-10, 1.2853e-05,\n",
      "         2.0956e-10, 4.6306e-09, 1.8192e-09, 1.1382e-05],\n",
      "        [1.4465e-03, 9.8414e-01, 8.6720e-05, 2.9068e-04, 6.1950e-05, 1.1161e-02,\n",
      "         6.9065e-06, 8.4662e-05, 4.9954e-05, 2.6679e-03],\n",
      "        [1.2515e-02, 4.2387e-01, 3.0855e-02, 6.4210e-02, 7.2724e-03, 1.2327e-01,\n",
      "         3.0850e-03, 1.4755e-02, 1.3419e-02, 3.0675e-01],\n",
      "        [3.5978e-03, 1.0215e-01, 1.9428e-02, 8.7859e-02, 1.2653e-02, 4.3089e-01,\n",
      "         2.0738e-03, 1.0245e-02, 5.5494e-03, 3.2555e-01],\n",
      "        [3.7165e-03, 9.4794e-02, 4.9745e-02, 1.5975e-01, 2.5800e-02, 4.3546e-01,\n",
      "         4.7370e-03, 2.2157e-02, 7.9278e-03, 1.9591e-01],\n",
      "        [3.4949e-04, 5.5326e-02, 2.3018e-02, 1.5471e-01, 3.7053e-02, 1.5740e-01,\n",
      "         1.4617e-02, 4.8988e-02, 3.1230e-02, 4.7731e-01],\n",
      "        [3.5971e-04, 3.7093e-02, 1.8465e-02, 2.2133e-01, 2.8428e-02, 3.0104e-01,\n",
      "         2.0493e-02, 7.2227e-02, 1.9334e-02, 2.8123e-01],\n",
      "        [2.6571e-04, 3.2027e-02, 1.0575e-02, 1.3754e-01, 2.0166e-02, 3.4652e-01,\n",
      "         2.0112e-02, 1.0059e-01, 2.9777e-02, 3.0242e-01],\n",
      "        [3.0863e-04, 2.7524e-02, 1.1396e-02, 1.3060e-01, 1.5886e-02, 3.1149e-01,\n",
      "         3.7572e-02, 1.0550e-01, 3.7956e-02, 3.2177e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.45, Train Loss: 0.00, Val Loss: 4.92, Train BLEU: 0.00, Val BLEU: 7.63, Minutes Elapsed: 654.86\n",
      "Sampling from val predictions...\n",
      "Source: 家庭 家庭暴力 暴力 模式 的 第三 第三阶段 三阶 阶段 就是\n",
      "Reference: the next step in the domestic violence pattern is\n",
      "Model: <SOS> and , step of the , is , ,\n",
      "Attention Weights: tensor([[7.5957e-07, 4.6678e-04, 1.2508e-05, 2.5550e-04, 1.6334e-04, 1.0688e-03,\n",
      "         9.1689e-04, 6.8521e-01, 6.1908e-04, 3.1128e-01],\n",
      "        [5.0609e-04, 4.5195e-02, 5.8496e-03, 1.3557e-02, 2.0894e-03, 2.2087e-02,\n",
      "         5.4104e-03, 7.5516e-01, 3.7315e-03, 1.4641e-01],\n",
      "        [2.5937e-03, 1.9340e-02, 1.2418e-01, 2.0426e-01, 2.6613e-02, 7.2407e-02,\n",
      "         1.9383e-02, 2.6578e-01, 2.1067e-02, 2.4437e-01],\n",
      "        [2.6729e-02, 6.5561e-02, 1.2706e-01, 1.2964e-01, 1.8454e-02, 4.8007e-02,\n",
      "         4.0880e-02, 2.6012e-01, 2.7622e-02, 2.5592e-01],\n",
      "        [2.7458e-02, 5.4343e-02, 5.4095e-02, 6.4947e-02, 2.5810e-02, 5.2784e-02,\n",
      "         2.1861e-02, 3.1963e-01, 2.6948e-02, 3.5212e-01],\n",
      "        [1.6285e-03, 5.2810e-03, 2.2398e-02, 5.8642e-02, 3.6740e-02, 1.6451e-01,\n",
      "         1.5634e-02, 1.4683e-01, 5.8168e-02, 4.9017e-01],\n",
      "        [2.4178e-04, 1.2444e-03, 9.0756e-03, 3.2555e-02, 6.7263e-03, 4.0246e-02,\n",
      "         1.3695e-02, 5.0662e-01, 3.8816e-02, 3.5078e-01],\n",
      "        [1.2259e-03, 3.6567e-03, 1.2289e-02, 1.8750e-02, 4.5887e-03, 2.6054e-02,\n",
      "         2.3220e-02, 3.7739e-01, 4.8649e-02, 4.8418e-01],\n",
      "        [4.4044e-05, 6.9324e-05, 4.0212e-04, 1.3427e-03, 7.7275e-04, 7.7006e-03,\n",
      "         8.0677e-03, 1.5333e-01, 6.0249e-02, 7.6802e-01]])\n",
      "\n",
      "Source: 新奥尔良 奥尔良 拥有 世界 上 很多 最 漂亮 的 建筑\n",
      "Reference: the city has some of the most beautiful architecture\n",
      "Model: <SOS> the the of the the and and of of\n",
      "Attention Weights: tensor([[0.0016, 0.0016, 0.0011, 0.0000, 0.0022, 0.0141, 0.0000, 0.0001, 0.0001,\n",
      "         0.9792],\n",
      "        [0.0132, 0.2043, 0.6159, 0.0035, 0.0472, 0.0168, 0.0006, 0.0009, 0.0004,\n",
      "         0.0972],\n",
      "        [0.0022, 0.0186, 0.4714, 0.0335, 0.1675, 0.0993, 0.0158, 0.0133, 0.0102,\n",
      "         0.1683],\n",
      "        [0.0452, 0.0769, 0.3058, 0.0393, 0.1743, 0.1023, 0.0167, 0.0074, 0.0098,\n",
      "         0.2223],\n",
      "        [0.0036, 0.0129, 0.2085, 0.0323, 0.2562, 0.1644, 0.0197, 0.0082, 0.0092,\n",
      "         0.2853],\n",
      "        [0.0007, 0.0023, 0.0626, 0.0325, 0.1235, 0.3281, 0.0717, 0.0378, 0.0328,\n",
      "         0.3079],\n",
      "        [0.0014, 0.0054, 0.0474, 0.0121, 0.0638, 0.2349, 0.0435, 0.0588, 0.0360,\n",
      "         0.4966],\n",
      "        [0.0012, 0.0054, 0.0429, 0.0186, 0.0728, 0.2322, 0.0496, 0.1058, 0.0305,\n",
      "         0.4408],\n",
      "        [0.0095, 0.0209, 0.0488, 0.0093, 0.0753, 0.1688, 0.0259, 0.0461, 0.0115,\n",
      "         0.5840]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.60, Train Loss: 0.00, Val Loss: 4.93, Train BLEU: 0.00, Val BLEU: 7.53, Minutes Elapsed: 666.27\n",
      "Sampling from val predictions...\n",
      "Source: 这里 只有 一个 出口 就是 我 来时 走过 的 楼梯\n",
      "Reference: i had only one way out : the stairs\n",
      "Model: <SOS> and here a idea that , i i i\n",
      "Attention Weights: tensor([[1.0900e-06, 1.8955e-06, 8.6091e-07, 6.5066e-04, 7.8507e-05, 9.9888e-01,\n",
      "         8.6444e-07, 6.1189e-06, 8.7461e-07, 3.7555e-04],\n",
      "        [6.1898e-01, 1.4868e-01, 1.3772e-02, 7.1956e-02, 1.7538e-02, 9.9263e-02,\n",
      "         3.1994e-04, 2.1039e-03, 1.7232e-03, 2.5658e-02],\n",
      "        [1.8854e-01, 2.3452e-01, 7.3584e-02, 1.4655e-01, 4.3892e-02, 9.7838e-02,\n",
      "         3.5123e-03, 1.2523e-02, 1.1337e-02, 1.8771e-01],\n",
      "        [1.7490e-01, 1.1577e-01, 1.8040e-01, 2.1245e-01, 1.3377e-02, 1.9040e-02,\n",
      "         5.3163e-03, 1.9536e-02, 1.1674e-02, 2.4753e-01],\n",
      "        [1.7397e-02, 1.3338e-02, 6.9347e-02, 3.2257e-01, 6.7040e-02, 7.5257e-02,\n",
      "         1.6858e-02, 4.4074e-02, 1.9570e-02, 3.5455e-01],\n",
      "        [9.1991e-03, 5.8760e-03, 1.7366e-02, 2.7562e-01, 9.6216e-02, 1.5867e-01,\n",
      "         4.2813e-03, 1.8397e-02, 1.4386e-02, 3.9999e-01],\n",
      "        [9.9249e-04, 7.8232e-04, 1.8660e-03, 4.0878e-02, 3.8752e-02, 4.9231e-01,\n",
      "         1.0826e-02, 3.4047e-02, 2.8427e-02, 3.5112e-01],\n",
      "        [1.0006e-03, 9.1429e-04, 4.5318e-03, 4.0489e-02, 2.8091e-02, 6.2497e-01,\n",
      "         1.6107e-02, 6.4944e-02, 2.0061e-02, 1.9889e-01],\n",
      "        [1.7126e-05, 4.6350e-05, 6.4997e-04, 1.6699e-02, 2.2973e-02, 3.6254e-01,\n",
      "         1.5771e-02, 7.4919e-02, 2.1220e-02, 4.8516e-01]])\n",
      "\n",
      "Source: 不过 可悲 的 是 神经 神经系 神经系统 系统 <UNK> 疾病\n",
      "Reference: now , sadly , neurological disorders such as parkinson\n",
      "Model: <SOS> but what the &apos;s is is a a as\n",
      "Attention Weights: tensor([[3.1721e-07, 6.4333e-06, 1.0243e-04, 4.9641e-04, 7.2034e-02, 7.6050e-02,\n",
      "         4.5544e-03, 5.6227e-03, 2.0204e-02, 8.2093e-01],\n",
      "        [1.8848e-03, 2.2921e-01, 9.1245e-02, 5.6264e-02, 5.0567e-02, 7.4059e-02,\n",
      "         1.2598e-02, 1.6070e-02, 7.6607e-02, 3.9150e-01],\n",
      "        [2.1730e-02, 2.8557e-01, 8.3176e-02, 9.5924e-02, 4.5060e-02, 3.4829e-02,\n",
      "         1.1862e-02, 9.3807e-03, 1.4326e-01, 2.6920e-01],\n",
      "        [7.7954e-03, 2.1003e-01, 4.4049e-02, 8.0761e-02, 8.6026e-02, 6.4711e-02,\n",
      "         3.1795e-02, 1.6005e-02, 6.1044e-02, 3.9778e-01],\n",
      "        [1.0702e-02, 7.8344e-02, 6.2326e-02, 7.5473e-02, 1.6062e-01, 1.4827e-01,\n",
      "         6.3851e-02, 3.5650e-02, 7.1558e-02, 2.9321e-01],\n",
      "        [2.4652e-03, 2.5703e-02, 1.0597e-02, 8.4336e-02, 4.2117e-01, 1.3791e-01,\n",
      "         4.1888e-02, 3.0776e-02, 5.5858e-02, 1.8930e-01],\n",
      "        [2.3297e-04, 2.4439e-03, 7.6202e-03, 4.3278e-02, 4.5310e-01, 1.4905e-01,\n",
      "         4.7665e-02, 3.4522e-02, 3.2840e-02, 2.2925e-01],\n",
      "        [1.7825e-04, 1.9904e-04, 8.6886e-04, 1.5357e-02, 2.3500e-01, 4.3104e-02,\n",
      "         1.2460e-02, 4.0181e-02, 1.2718e-02, 6.3994e-01],\n",
      "        [3.6999e-05, 6.1348e-05, 1.6264e-04, 5.4047e-03, 1.2210e-01, 5.1270e-02,\n",
      "         1.7956e-02, 2.8325e-02, 2.5577e-02, 7.4910e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.75, Train Loss: 0.00, Val Loss: 4.93, Train BLEU: 0.00, Val BLEU: 8.11, Minutes Elapsed: 677.70\n",
      "Sampling from val predictions...\n",
      "Source: 由此 开始 了 我 深入 现代 奴隶 奴隶制 奴隶制度 制度\n",
      "Reference: thus began my journey into modern day slavery .\n",
      "Model: <SOS> and , i i to , , , <EOS>\n",
      "Attention Weights: tensor([[4.3404e-09, 2.4998e-08, 3.1061e-07, 9.9998e-01, 4.9918e-09, 3.9853e-08,\n",
      "         3.0458e-08, 2.6442e-08, 4.9304e-09, 1.9376e-05],\n",
      "        [1.6914e-02, 7.9983e-02, 2.5333e-02, 8.6522e-01, 1.0682e-04, 2.7540e-04,\n",
      "         1.1947e-04, 2.0615e-04, 1.9847e-04, 1.1641e-02],\n",
      "        [3.6736e-02, 2.0850e-01, 1.3320e-01, 2.6529e-01, 1.7757e-02, 1.7255e-02,\n",
      "         8.4947e-03, 1.5982e-02, 2.1265e-02, 2.7553e-01],\n",
      "        [5.6364e-03, 3.4505e-02, 8.1851e-02, 7.7168e-01, 2.7704e-03, 3.2294e-03,\n",
      "         2.0741e-03, 3.6706e-03, 5.1872e-03, 8.9393e-02],\n",
      "        [1.9137e-04, 1.3222e-03, 6.0152e-02, 4.4409e-01, 6.1264e-03, 1.4078e-02,\n",
      "         8.2931e-03, 1.2923e-02, 1.1081e-02, 4.4174e-01],\n",
      "        [6.5945e-06, 6.6871e-05, 4.6436e-04, 4.3727e-02, 3.3709e-02, 2.5617e-01,\n",
      "         5.7558e-02, 8.4012e-02, 5.5268e-02, 4.6902e-01],\n",
      "        [2.2415e-05, 5.2489e-05, 6.5944e-04, 5.0548e-02, 1.2050e-02, 8.9579e-02,\n",
      "         3.5961e-02, 1.3082e-01, 1.2446e-01, 5.5585e-01],\n",
      "        [7.4787e-06, 4.4462e-05, 1.4276e-03, 4.7498e-01, 4.4959e-03, 2.0887e-02,\n",
      "         2.5760e-02, 5.4343e-02, 3.1822e-02, 3.8623e-01],\n",
      "        [7.6588e-06, 2.7219e-05, 2.7389e-04, 8.9032e-02, 6.7086e-04, 1.2379e-02,\n",
      "         6.9941e-03, 3.7434e-02, 2.5184e-02, 8.2800e-01]])\n",
      "\n",
      "Source: 首先 我 要 表扬 下 你们 对于 模特 界 的\n",
      "Reference: and first , i <UNK> you on your model\n",
      "Model: <SOS> and first &apos;m i want to you call of\n",
      "Attention Weights: tensor([[5.5430e-11, 9.9970e-01, 3.5615e-09, 6.9786e-09, 1.5429e-08, 2.3820e-04,\n",
      "         5.0497e-09, 7.1964e-07, 5.8948e-09, 6.5854e-05],\n",
      "        [1.4313e-03, 9.9530e-01, 4.9481e-04, 5.1129e-04, 2.6192e-05, 1.4458e-03,\n",
      "         5.6124e-06, 1.3365e-04, 1.3553e-05, 6.4061e-04],\n",
      "        [6.2842e-02, 3.3428e-01, 1.2827e-01, 9.2456e-02, 1.3131e-02, 1.1114e-01,\n",
      "         3.1033e-03, 2.8184e-02, 1.0302e-02, 2.1629e-01],\n",
      "        [5.5250e-02, 4.2276e-01, 1.7334e-01, 1.0003e-01, 1.4883e-02, 8.0168e-02,\n",
      "         1.6990e-03, 1.1606e-02, 4.9925e-03, 1.3527e-01],\n",
      "        [1.3079e-02, 5.1751e-01, 1.0455e-01, 6.8140e-02, 3.8349e-02, 6.2755e-02,\n",
      "         3.8039e-03, 1.2944e-02, 1.0080e-02, 1.6878e-01],\n",
      "        [5.4416e-03, 3.5250e-01, 2.4883e-01, 1.6446e-01, 1.5684e-02, 1.0641e-01,\n",
      "         1.3631e-03, 7.8923e-03, 2.0727e-03, 9.5335e-02],\n",
      "        [1.3280e-03, 5.4720e-02, 2.1857e-02, 7.1734e-02, 2.1652e-02, 6.9779e-01,\n",
      "         4.2650e-03, 2.4805e-02, 3.0183e-03, 9.8833e-02],\n",
      "        [9.6321e-06, 4.1526e-03, 1.6079e-03, 6.5796e-03, 1.6245e-02, 1.9577e-01,\n",
      "         9.5017e-02, 4.9429e-01, 3.8763e-02, 1.4757e-01],\n",
      "        [8.9084e-07, 4.2511e-04, 9.2960e-04, 1.1116e-03, 4.1919e-03, 6.5327e-02,\n",
      "         3.6404e-02, 4.6928e-01, 4.5765e-02, 3.7656e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.90, Train Loss: 0.00, Val Loss: 4.92, Train BLEU: 0.00, Val BLEU: 7.89, Minutes Elapsed: 689.22\n",
      "Sampling from val predictions...\n",
      "Source: 所以 我 伟大 的 梦想 是 当 我 长大 以后\n",
      "Reference: so my big dream is to become an aircraft\n",
      "Model: <SOS> so my i was is that when , when\n",
      "Attention Weights: tensor([[4.2530e-07, 9.9891e-01, 4.7072e-08, 3.3112e-09, 1.7752e-09, 1.1226e-08,\n",
      "         1.6427e-05, 7.5908e-04, 4.9481e-08, 3.1434e-04],\n",
      "        [9.9695e-04, 9.7984e-01, 1.5880e-02, 4.1696e-04, 2.1251e-04, 1.8160e-04,\n",
      "         2.7624e-04, 7.0725e-04, 2.9104e-06, 1.4902e-03],\n",
      "        [3.6944e-03, 2.7179e-01, 5.4821e-01, 2.4462e-02, 3.3422e-02, 3.9260e-03,\n",
      "         8.8193e-03, 3.8135e-03, 6.5094e-04, 1.0121e-01],\n",
      "        [1.9401e-03, 3.8710e-01, 3.5138e-01, 4.9331e-02, 4.4491e-02, 1.2833e-02,\n",
      "         2.7194e-02, 7.9116e-03, 2.6260e-04, 1.1756e-01],\n",
      "        [1.4219e-04, 4.6013e-03, 1.5745e-02, 3.9729e-02, 4.6621e-02, 4.2176e-01,\n",
      "         3.7232e-01, 1.6572e-02, 7.1681e-04, 8.1793e-02],\n",
      "        [1.6323e-05, 8.1318e-04, 1.5193e-03, 2.0715e-03, 3.8111e-03, 1.1626e-01,\n",
      "         7.9236e-01, 4.6136e-02, 2.2133e-03, 3.4798e-02],\n",
      "        [2.0616e-07, 7.0164e-04, 1.6447e-04, 6.5276e-05, 1.3680e-04, 4.3262e-03,\n",
      "         3.0094e-01, 3.0698e-01, 2.1010e-03, 3.8458e-01],\n",
      "        [7.7327e-07, 4.2176e-03, 4.0498e-04, 1.5667e-04, 2.1096e-04, 2.1989e-03,\n",
      "         1.4197e-01, 4.2634e-01, 9.9099e-03, 4.1459e-01],\n",
      "        [9.2574e-08, 4.7548e-04, 1.9587e-04, 1.6713e-05, 5.9381e-05, 6.0270e-04,\n",
      "         7.8488e-02, 3.4529e-01, 1.7971e-02, 5.5690e-01]])\n",
      "\n",
      "Source: 在 死 之前 我 想 过 隐居 的 生活 <EOS>\n",
      "Reference: &quot; before i die , i want to live\n",
      "Model: <SOS> in i , to to i i to be\n",
      "Attention Weights: tensor([[4.1052e-09, 3.7056e-07, 1.9950e-07, 9.9889e-01, 2.5959e-06, 1.3784e-07,\n",
      "         6.5000e-06, 2.0492e-06, 1.0567e-03, 4.6219e-05],\n",
      "        [6.0995e-04, 1.4280e-02, 2.2567e-03, 9.2933e-01, 3.3322e-03, 8.8385e-04,\n",
      "         1.2480e-03, 3.4964e-03, 2.1632e-02, 2.2933e-02],\n",
      "        [1.0611e-03, 1.8109e-02, 2.5167e-02, 5.1937e-01, 7.0124e-02, 5.6459e-03,\n",
      "         2.4655e-02, 2.0639e-02, 1.3129e-01, 1.8394e-01],\n",
      "        [9.9273e-03, 5.6898e-02, 8.7038e-02, 3.6692e-01, 1.4355e-01, 3.4519e-02,\n",
      "         4.3629e-02, 3.7916e-02, 1.1919e-01, 1.0042e-01],\n",
      "        [6.6986e-04, 2.3820e-03, 1.0101e-02, 3.3932e-01, 1.4949e-01, 5.7150e-02,\n",
      "         7.5756e-02, 4.0160e-02, 1.2174e-01, 2.0324e-01],\n",
      "        [2.0035e-03, 1.1133e-02, 6.0231e-02, 1.7656e-01, 6.9695e-02, 4.7301e-02,\n",
      "         1.5156e-01, 5.5900e-02, 2.8466e-01, 1.4096e-01],\n",
      "        [8.0795e-04, 1.8848e-03, 4.6482e-03, 1.9560e-01, 1.4199e-01, 4.1363e-02,\n",
      "         9.2362e-02, 5.3225e-02, 3.5749e-01, 1.1062e-01],\n",
      "        [1.0255e-04, 6.9548e-04, 6.5021e-04, 2.7958e-01, 1.9191e-01, 5.0961e-02,\n",
      "         1.8731e-01, 6.2065e-02, 1.5471e-01, 7.2013e-02],\n",
      "        [2.0789e-04, 4.5008e-04, 5.0909e-04, 2.4411e-03, 1.6341e-02, 2.3213e-02,\n",
      "         1.9733e-01, 1.1989e-01, 5.1275e-01, 1.2686e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9.00, Train Loss: 0.00, Val Loss: 4.99, Train BLEU: 0.00, Val BLEU: 6.98, Minutes Elapsed: 696.98\n",
      "Sampling from val predictions...\n",
      "Source: 我们 正在 做 什么 呢 我们 正在 研究 这些 <UNK>\n",
      "Reference: so what &apos;s being done ? well , we\n",
      "Model: <SOS> we &apos;re &apos;re we about ? we &apos;s we\n",
      "Attention Weights: tensor([[9.9878e-01, 7.4893e-08, 6.6106e-08, 3.0894e-07, 1.0373e-05, 2.1186e-04,\n",
      "         5.6463e-08, 8.7830e-07, 8.1080e-04, 1.9017e-04],\n",
      "        [9.2154e-01, 4.8724e-02, 3.3991e-04, 2.4002e-03, 2.9001e-04, 1.5472e-02,\n",
      "         6.7491e-06, 2.2873e-05, 1.0522e-02, 6.7785e-04],\n",
      "        [2.3156e-01, 4.1856e-01, 9.8094e-02, 9.9979e-02, 2.0156e-02, 3.0383e-02,\n",
      "         1.7965e-03, 4.0465e-03, 5.0472e-02, 4.4956e-02],\n",
      "        [2.0844e-01, 4.3856e-01, 6.0529e-02, 1.2827e-01, 1.5930e-02, 3.4837e-02,\n",
      "         1.4978e-03, 2.8211e-03, 5.2866e-02, 5.6253e-02],\n",
      "        [9.9537e-02, 3.3143e-01, 1.3516e-01, 2.1063e-01, 2.4511e-02, 4.7438e-02,\n",
      "         3.8439e-03, 7.2321e-03, 6.7990e-02, 7.2231e-02],\n",
      "        [1.2286e-01, 4.7676e-02, 1.4609e-01, 2.4827e-01, 1.7015e-01, 1.4521e-01,\n",
      "         3.8536e-03, 8.4540e-03, 4.3762e-02, 6.3685e-02],\n",
      "        [1.3109e-01, 4.8604e-03, 3.5762e-02, 9.3032e-02, 2.8575e-01, 3.3459e-01,\n",
      "         3.0910e-03, 7.7269e-03, 3.8004e-02, 6.6094e-02],\n",
      "        [4.5180e-02, 8.9311e-04, 3.8696e-03, 2.4681e-02, 1.6753e-01, 6.2460e-01,\n",
      "         6.9827e-03, 1.2104e-02, 6.2556e-02, 5.1610e-02],\n",
      "        [1.5459e-02, 6.3733e-03, 1.8054e-02, 6.7923e-02, 9.4407e-02, 4.5065e-01,\n",
      "         7.8766e-02, 6.3637e-02, 1.6846e-01, 3.6268e-02]])\n",
      "\n",
      "Source: 太 不 简单 了 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: extraordinary . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it , very . . . . . .\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0005, 0.9989, 0.0006, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4395, 0.0179, 0.1870, 0.3538, 0.0018, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4747, 0.1146, 0.2268, 0.1420, 0.0417, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0517, 0.1001, 0.1236, 0.6778, 0.0468, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2128, 0.2091, 0.2309, 0.2230, 0.1242, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0437, 0.1163, 0.0696, 0.6592, 0.1112, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0912, 0.2848, 0.0868, 0.3223, 0.2150, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1228, 0.3193, 0.1196, 0.1910, 0.2474, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0902, 0.2248, 0.1429, 0.2717, 0.2704, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9.15, Train Loss: 0.00, Val Loss: 5.02, Train BLEU: 0.00, Val BLEU: 7.42, Minutes Elapsed: 708.59\n",
      "Sampling from val predictions...\n",
      "Source: 他们 第一 第一天 一天 来时 看到 了 稻草 <UNK> <UNK>\n",
      "Reference: they will come the first day and they see\n",
      "Model: <SOS> they they the the the in , the first\n",
      "Attention Weights: tensor([[0.1501, 0.0000, 0.0000, 0.0002, 0.0049, 0.0003, 0.0030, 0.0164, 0.6992,\n",
      "         0.1261],\n",
      "        [0.9753, 0.0029, 0.0036, 0.0060, 0.0039, 0.0003, 0.0013, 0.0011, 0.0043,\n",
      "         0.0012],\n",
      "        [0.4970, 0.0784, 0.0857, 0.0634, 0.0506, 0.0055, 0.0324, 0.0205, 0.0999,\n",
      "         0.0665],\n",
      "        [0.1737, 0.0317, 0.0217, 0.0303, 0.0994, 0.0106, 0.0739, 0.1477, 0.3050,\n",
      "         0.1060],\n",
      "        [0.0764, 0.0509, 0.0371, 0.0949, 0.2060, 0.0583, 0.1553, 0.1457, 0.1277,\n",
      "         0.0476],\n",
      "        [0.0637, 0.0254, 0.0330, 0.0630, 0.1672, 0.0342, 0.1867, 0.1203, 0.2066,\n",
      "         0.0999],\n",
      "        [0.2577, 0.0463, 0.0099, 0.0077, 0.0486, 0.0095, 0.0337, 0.0376, 0.2949,\n",
      "         0.2541],\n",
      "        [0.2575, 0.0544, 0.0213, 0.0199, 0.0883, 0.0377, 0.0598, 0.0933, 0.2058,\n",
      "         0.1621],\n",
      "        [0.1375, 0.0415, 0.0164, 0.0215, 0.0490, 0.0392, 0.0874, 0.1397, 0.3094,\n",
      "         0.1584]])\n",
      "\n",
      "Source: 这 是 一张 卫星 星图 拍摄 了 夜晚 的 朝鲜\n",
      "Reference: this is a satellite picture showing north korea at\n",
      "Model: <SOS> this &apos;s a the that that a the ,\n",
      "Attention Weights: tensor([[3.2958e-03, 2.9284e-06, 5.2866e-05, 3.8678e-04, 3.4733e-05, 9.4864e-05,\n",
      "         3.5934e-04, 1.1372e-04, 3.5781e-04, 9.9530e-01],\n",
      "        [2.0477e-01, 9.6981e-02, 5.7457e-02, 2.5358e-02, 7.3955e-03, 5.9770e-03,\n",
      "         1.0237e-02, 4.7368e-03, 7.0223e-03, 5.8006e-01],\n",
      "        [1.5471e-01, 1.0422e-01, 1.1786e-01, 8.1060e-02, 1.5554e-02, 2.1912e-02,\n",
      "         1.5797e-02, 1.2422e-02, 2.1024e-02, 4.5545e-01],\n",
      "        [3.6153e-02, 1.7189e-02, 1.4173e-01, 2.6472e-01, 3.0600e-02, 3.6774e-02,\n",
      "         1.0255e-02, 1.0748e-02, 1.1432e-02, 4.4040e-01],\n",
      "        [1.9433e-02, 4.0111e-03, 5.4398e-02, 2.1328e-01, 6.8879e-02, 8.2522e-02,\n",
      "         4.7157e-02, 3.2356e-02, 2.5485e-02, 4.5248e-01],\n",
      "        [3.7625e-02, 7.7041e-03, 2.3727e-02, 1.1356e-01, 1.9101e-02, 5.4062e-02,\n",
      "         2.7151e-02, 1.3363e-02, 2.1238e-02, 6.8247e-01],\n",
      "        [9.2265e-03, 7.1775e-03, 4.1869e-03, 6.2028e-02, 3.0394e-02, 1.0169e-01,\n",
      "         1.1213e-01, 6.5761e-02, 2.7562e-02, 5.7985e-01],\n",
      "        [3.6458e-04, 5.6397e-05, 5.2078e-04, 1.7963e-02, 7.8493e-03, 2.2393e-02,\n",
      "         2.0922e-02, 2.9137e-02, 1.4225e-02, 8.8657e-01],\n",
      "        [8.2347e-05, 3.1543e-06, 3.3965e-05, 5.1849e-03, 2.9317e-03, 1.4072e-02,\n",
      "         2.7044e-02, 3.9184e-02, 1.4443e-02, 8.9702e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9.30, Train Loss: 0.00, Val Loss: 5.00, Train BLEU: 0.00, Val BLEU: 7.63, Minutes Elapsed: 720.11\n",
      "Sampling from val predictions...\n",
      "Source: 我们 的 科学 科学家 学家 和 工程 工程师 们 解决\n",
      "Reference: our scientists and engineers are the ones that are\n",
      "Model: <SOS> our scientists and to are to to to the\n",
      "Attention Weights: tensor([[9.9995e-01, 3.0947e-10, 2.5240e-05, 1.9145e-07, 3.3074e-09, 9.8344e-08,\n",
      "         2.1744e-08, 6.4185e-08, 1.6249e-06, 2.1796e-05],\n",
      "        [2.0051e-01, 3.0280e-04, 7.6937e-01, 2.5662e-02, 2.0855e-04, 1.5028e-03,\n",
      "         1.9256e-04, 2.3240e-04, 3.3655e-04, 1.6877e-03],\n",
      "        [1.2681e-01, 2.2400e-03, 4.2741e-01, 1.0950e-01, 7.0610e-03, 3.4450e-02,\n",
      "         1.0285e-02, 1.6805e-02, 5.3443e-02, 2.1199e-01],\n",
      "        [4.6971e-02, 7.4403e-05, 5.3785e-02, 7.1277e-02, 9.8579e-03, 1.8532e-01,\n",
      "         2.4468e-02, 1.2303e-01, 3.6510e-01, 1.2011e-01],\n",
      "        [6.0778e-03, 3.7529e-05, 3.0337e-02, 3.5306e-02, 6.6563e-03, 1.4235e-01,\n",
      "         7.5665e-02, 1.5683e-01, 3.6359e-01, 1.8314e-01],\n",
      "        [2.8389e-03, 1.4101e-05, 1.8145e-02, 3.3006e-02, 1.5928e-02, 2.3673e-01,\n",
      "         4.6807e-02, 1.7803e-01, 3.4711e-01, 1.2140e-01],\n",
      "        [3.1583e-03, 5.2172e-07, 8.2009e-04, 5.4625e-04, 1.4335e-04, 5.3710e-03,\n",
      "         7.3861e-03, 3.7162e-02, 2.2653e-01, 7.1888e-01],\n",
      "        [4.8206e-03, 8.5836e-05, 1.6721e-02, 7.0671e-03, 4.4376e-03, 7.4808e-02,\n",
      "         5.6588e-02, 6.3127e-02, 2.5514e-01, 5.1720e-01],\n",
      "        [1.9035e-02, 1.4168e-05, 1.1062e-02, 3.3977e-03, 3.6234e-03, 5.0648e-02,\n",
      "         1.8051e-02, 3.7918e-02, 4.5562e-01, 4.0063e-01]])\n",
      "\n",
      "Source: 在 西藏 <UNK> <UNK> 文化 中 <UNK> 尤其 显得 重要\n",
      "Reference: in tibetan culture , they are performing very important\n",
      "Model: <SOS> in the , , , <UNK> the in <UNK>\n",
      "Attention Weights: tensor([[3.4384e-05, 1.0487e-03, 2.3353e-05, 5.9663e-06, 4.3695e-04, 7.9562e-03,\n",
      "         6.3396e-05, 2.7558e-04, 6.2535e-04, 9.8953e-01],\n",
      "        [1.1930e-02, 4.2859e-01, 2.1174e-02, 1.0198e-02, 1.3484e-01, 7.1549e-02,\n",
      "         3.0187e-03, 1.6056e-02, 2.6957e-02, 2.7568e-01],\n",
      "        [1.6617e-02, 1.8236e-01, 5.0266e-02, 3.5135e-02, 1.5916e-01, 1.2824e-01,\n",
      "         1.4699e-02, 1.6872e-02, 2.6201e-02, 3.7045e-01],\n",
      "        [3.3571e-02, 7.4956e-02, 1.8907e-02, 1.3171e-02, 1.4119e-01, 1.9384e-01,\n",
      "         1.0931e-02, 1.7568e-02, 4.1692e-02, 4.5418e-01],\n",
      "        [6.1602e-03, 4.7158e-03, 3.6902e-03, 3.8980e-03, 1.2263e-01, 3.4187e-01,\n",
      "         2.0957e-02, 2.2441e-02, 4.8496e-02, 4.2515e-01],\n",
      "        [3.7855e-05, 3.4259e-04, 5.3544e-04, 4.0997e-04, 2.3736e-02, 1.9992e-01,\n",
      "         2.7313e-03, 1.2141e-02, 4.0903e-02, 7.1924e-01],\n",
      "        [4.6058e-04, 3.3267e-03, 3.5925e-03, 2.9310e-03, 9.6423e-02, 3.3996e-01,\n",
      "         1.0438e-02, 6.0552e-02, 6.9159e-02, 4.1316e-01],\n",
      "        [2.2753e-04, 1.7519e-03, 2.5268e-03, 1.5595e-03, 2.7178e-02, 1.0692e-01,\n",
      "         5.9119e-03, 6.1519e-02, 6.5815e-02, 7.2659e-01],\n",
      "        [6.4244e-03, 2.5315e-02, 1.2848e-02, 8.4769e-03, 1.2508e-01, 2.0483e-01,\n",
      "         1.7247e-02, 1.0192e-01, 8.5334e-02, 4.1252e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9.45, Train Loss: 0.00, Val Loss: 4.97, Train BLEU: 0.00, Val BLEU: 7.96, Minutes Elapsed: 731.57\n",
      "Sampling from val predictions...\n",
      "Source: 这 是 一家 一家人 家人 的 肖像 <EOS> <PAD> <PAD>\n",
      "Reference: this is a family portrait . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> it &apos;s a computer deal . <EOS> <EOS> .\n",
      "Attention Weights: tensor([[0.0207, 0.0002, 0.0001, 0.0001, 0.0000, 0.0005, 0.8792, 0.0992, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5325, 0.3354, 0.0334, 0.0041, 0.0021, 0.0068, 0.0590, 0.0267, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3552, 0.3145, 0.1391, 0.0290, 0.0143, 0.0156, 0.0526, 0.0797, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1199, 0.1169, 0.2829, 0.2066, 0.1111, 0.0475, 0.0962, 0.0189, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0642, 0.0639, 0.0889, 0.0487, 0.1022, 0.0681, 0.3688, 0.1951, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1212, 0.0696, 0.0704, 0.0659, 0.1476, 0.0282, 0.1649, 0.3322, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0979, 0.0905, 0.0724, 0.0426, 0.0512, 0.0460, 0.1628, 0.4367, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0456, 0.0444, 0.0381, 0.0265, 0.0537, 0.0313, 0.1271, 0.6334, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1320, 0.1654, 0.1129, 0.0397, 0.0604, 0.0246, 0.0833, 0.3817, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 英语 在 韩国 太重 重要 了 所以 我 必须 须要\n",
      "Reference: english was so important in south korea , so\n",
      "Model: <SOS> and was : in in the u.s. , and\n",
      "Attention Weights: tensor([[1.0036e-08, 1.7898e-06, 8.7714e-07, 3.8786e-07, 2.6891e-07, 8.7126e-07,\n",
      "         4.3887e-02, 9.5310e-01, 5.4960e-08, 3.0077e-03],\n",
      "        [2.1048e-02, 1.6770e-02, 2.3125e-02, 7.6671e-03, 5.7971e-03, 9.8015e-03,\n",
      "         1.6936e-01, 5.2248e-01, 5.0524e-04, 2.2344e-01],\n",
      "        [5.5409e-02, 2.8989e-02, 6.2781e-02, 3.0575e-02, 2.0435e-02, 3.7103e-02,\n",
      "         1.6358e-01, 2.2398e-01, 5.5666e-03, 3.7158e-01],\n",
      "        [5.0298e-02, 1.0879e-01, 1.3470e-01, 9.0937e-02, 6.2560e-02, 5.3621e-02,\n",
      "         2.2064e-01, 4.8624e-02, 2.7572e-03, 2.2708e-01],\n",
      "        [6.7175e-03, 2.2301e-02, 1.0677e-01, 2.5734e-01, 1.0396e-01, 3.4742e-02,\n",
      "         2.4035e-01, 1.0649e-01, 3.6695e-03, 1.1766e-01],\n",
      "        [4.4617e-03, 6.7278e-03, 1.7202e-01, 1.4898e-01, 2.4895e-02, 3.6396e-02,\n",
      "         2.2067e-01, 1.0040e-01, 4.3211e-03, 2.8114e-01],\n",
      "        [6.7039e-04, 2.1258e-03, 1.8977e-02, 1.2768e-02, 6.0007e-03, 6.7527e-03,\n",
      "         1.2547e-01, 3.4449e-01, 1.2081e-03, 4.8154e-01],\n",
      "        [4.2863e-05, 5.4622e-04, 3.5305e-03, 2.3837e-02, 2.8504e-02, 3.9559e-02,\n",
      "         3.9451e-01, 2.8885e-01, 7.5020e-03, 2.1312e-01],\n",
      "        [2.7847e-05, 2.1846e-04, 7.8257e-04, 1.5211e-03, 2.2779e-03, 1.9967e-02,\n",
      "         1.0510e-01, 5.7644e-01, 6.0194e-03, 2.8764e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9.60, Train Loss: 0.00, Val Loss: 4.97, Train BLEU: 0.00, Val BLEU: 7.21, Minutes Elapsed: 742.75\n",
      "Sampling from val predictions...\n",
      "Source: 事实 事实上 我 的 学生 <UNK> 我 得 提 一下\n",
      "Reference: actually , my student <UNK> , i have to\n",
      "Model: <SOS> in , my , , i i i a\n",
      "Attention Weights: tensor([[2.5645e-08, 2.7168e-09, 9.9798e-01, 9.5475e-08, 8.4001e-06, 5.0184e-06,\n",
      "         1.4189e-03, 7.8382e-08, 2.8097e-07, 5.8964e-04],\n",
      "        [3.0368e-03, 1.1205e-04, 9.6890e-01, 1.8341e-04, 2.8853e-03, 5.4124e-03,\n",
      "         1.2977e-02, 1.4330e-04, 5.7255e-04, 5.7778e-03],\n",
      "        [1.2388e-02, 7.6328e-03, 5.1935e-01, 2.4957e-02, 6.9269e-02, 1.2100e-01,\n",
      "         5.6399e-02, 2.2313e-02, 4.7568e-02, 1.1912e-01],\n",
      "        [8.4363e-03, 8.2437e-03, 5.7636e-01, 2.1006e-02, 8.2859e-02, 6.8301e-02,\n",
      "         4.6424e-02, 1.7429e-02, 2.4318e-02, 1.4663e-01],\n",
      "        [9.3166e-03, 2.6339e-03, 2.3888e-01, 7.2033e-02, 1.9636e-01, 5.6857e-02,\n",
      "         1.4895e-01, 3.4632e-02, 4.1719e-02, 1.9862e-01],\n",
      "        [3.7782e-04, 2.1964e-04, 1.4596e-01, 2.0379e-02, 3.0394e-01, 1.0419e-01,\n",
      "         2.6039e-01, 4.6909e-02, 2.6471e-02, 9.1169e-02],\n",
      "        [9.5288e-05, 1.3619e-05, 4.7185e-02, 4.5744e-03, 2.7371e-01, 5.1199e-02,\n",
      "         4.0880e-01, 6.3158e-02, 6.9243e-02, 8.2026e-02],\n",
      "        [1.4048e-05, 6.4068e-06, 1.0401e-01, 8.9703e-04, 2.3860e-02, 3.5855e-02,\n",
      "         4.2923e-01, 6.0045e-02, 8.1313e-02, 2.6477e-01],\n",
      "        [9.3975e-05, 1.1489e-05, 4.8126e-02, 6.9833e-04, 4.9699e-02, 3.4245e-02,\n",
      "         3.6588e-01, 7.4924e-02, 9.8092e-02, 3.2823e-01]])\n",
      "\n",
      "Source: 截至 七月 仅 在 日本 境内 我们 就 设立 了\n",
      "Reference: within japan , by july , we &apos;d <UNK>\n",
      "Model: <SOS> and , few , the , the the the\n",
      "Attention Weights: tensor([[7.2779e-09, 2.7836e-04, 1.5119e-05, 1.6109e-03, 4.9746e-04, 1.2163e-02,\n",
      "         9.4096e-01, 9.3701e-05, 2.8375e-04, 4.4101e-02],\n",
      "        [2.4562e-03, 1.7872e-01, 4.4330e-02, 2.4945e-02, 3.1905e-02, 3.0808e-02,\n",
      "         5.8850e-01, 1.4484e-03, 2.5150e-03, 9.4372e-02],\n",
      "        [2.1507e-02, 3.5433e-01, 7.1170e-02, 3.3815e-02, 4.8163e-02, 8.2675e-03,\n",
      "         3.5015e-02, 6.7174e-03, 1.9929e-02, 4.0108e-01],\n",
      "        [9.8886e-03, 6.3471e-02, 2.0094e-01, 3.0073e-01, 2.8834e-02, 2.1046e-02,\n",
      "         1.5834e-01, 1.4001e-02, 8.4202e-03, 1.9433e-01],\n",
      "        [1.9271e-03, 2.8584e-02, 3.5403e-02, 3.3328e-01, 1.7403e-01, 3.3740e-02,\n",
      "         2.5672e-01, 1.3454e-02, 1.4655e-02, 1.0821e-01],\n",
      "        [1.6327e-03, 3.1141e-02, 1.6263e-02, 1.1482e-01, 2.5717e-01, 3.1266e-02,\n",
      "         2.0630e-01, 3.6430e-02, 3.5256e-02, 2.6973e-01],\n",
      "        [1.2180e-03, 1.6013e-02, 2.6092e-02, 2.8341e-01, 2.1337e-01, 1.0090e-01,\n",
      "         2.3329e-01, 1.4918e-02, 1.3208e-02, 9.7580e-02],\n",
      "        [1.6097e-04, 4.9519e-03, 2.7105e-03, 5.1658e-02, 2.0418e-01, 3.0104e-02,\n",
      "         4.4201e-01, 1.6208e-02, 2.4719e-02, 2.2330e-01],\n",
      "        [3.9589e-05, 2.9037e-03, 1.6333e-03, 4.0407e-02, 2.5387e-01, 2.8190e-02,\n",
      "         2.2108e-01, 5.4507e-02, 7.7265e-02, 3.2011e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9.75, Train Loss: 0.00, Val Loss: 4.99, Train BLEU: 0.00, Val BLEU: 8.31, Minutes Elapsed: 754.10\n",
      "Sampling from val predictions...\n",
      "Source: 这 是 一个 意外 而 他 以后 不会 再 伤害\n",
      "Reference: it was an isolated incident , and he was\n",
      "Model: <SOS> this &apos;s a dream . , he he was\n",
      "Attention Weights: tensor([[8.0497e-03, 6.7575e-06, 7.9066e-06, 4.3300e-06, 6.0791e-04, 9.7307e-01,\n",
      "         1.6464e-03, 1.1068e-06, 4.1743e-04, 1.6188e-02],\n",
      "        [3.9141e-01, 1.1051e-01, 3.1595e-02, 1.3593e-02, 3.0264e-02, 2.7798e-01,\n",
      "         7.9075e-02, 5.1634e-04, 6.8334e-03, 5.8229e-02],\n",
      "        [2.4381e-01, 2.3517e-01, 7.0735e-02, 3.5390e-02, 6.9386e-02, 2.1720e-01,\n",
      "         3.6593e-02, 2.0454e-03, 1.5125e-02, 7.4551e-02],\n",
      "        [2.6565e-03, 4.9450e-03, 4.9655e-02, 2.2392e-01, 3.8476e-01, 2.5271e-01,\n",
      "         5.0792e-02, 1.5182e-03, 4.9752e-03, 2.4066e-02],\n",
      "        [9.5807e-04, 2.6247e-04, 3.2405e-03, 1.8513e-02, 1.9074e-01, 6.6521e-01,\n",
      "         1.0038e-01, 5.4612e-03, 6.3897e-03, 8.8471e-03],\n",
      "        [2.3420e-03, 2.1645e-03, 1.4924e-03, 6.8923e-03, 2.2218e-01, 5.2540e-01,\n",
      "         1.9365e-01, 1.7976e-03, 1.8170e-02, 2.5916e-02],\n",
      "        [7.0299e-04, 2.0046e-04, 9.0889e-05, 2.2823e-04, 5.0149e-02, 7.7685e-01,\n",
      "         9.7084e-02, 7.0494e-04, 2.3759e-02, 5.0234e-02],\n",
      "        [3.2740e-04, 5.3131e-05, 1.3800e-04, 5.2598e-05, 9.8265e-03, 2.7865e-01,\n",
      "         2.8187e-01, 3.9237e-03, 9.9240e-02, 3.2591e-01],\n",
      "        [9.7523e-05, 5.9592e-06, 3.0497e-05, 2.1860e-06, 9.4123e-04, 9.6319e-02,\n",
      "         6.5554e-02, 6.8729e-04, 2.5962e-01, 5.7674e-01]])\n",
      "\n",
      "Source: 这些 迹象 表明 如果 我们 能 有 一个 表达 自己\n",
      "Reference: together , we &apos;ve shown how powerful our public\n",
      "Model: <SOS> and these just , have a a can can\n",
      "Attention Weights: tensor([[1.2221e-01, 1.6589e-05, 8.1406e-03, 1.0922e-04, 7.8847e-01, 8.7743e-07,\n",
      "         3.4250e-06, 7.1893e-05, 2.6694e-05, 8.0945e-02],\n",
      "        [7.7834e-01, 1.4824e-03, 3.9732e-02, 2.9921e-04, 1.7835e-01, 9.1956e-05,\n",
      "         8.8253e-05, 2.7646e-05, 1.8160e-05, 1.5692e-03],\n",
      "        [3.6674e-01, 4.7534e-02, 2.2178e-01, 1.9499e-02, 1.3026e-01, 1.6182e-02,\n",
      "         1.4563e-02, 6.3427e-03, 6.2915e-03, 1.7080e-01],\n",
      "        [1.1617e-01, 7.7200e-03, 1.7522e-01, 8.2336e-02, 4.7565e-01, 7.2842e-03,\n",
      "         1.4534e-02, 4.5919e-03, 1.9258e-03, 1.1457e-01],\n",
      "        [4.1988e-01, 4.6959e-03, 1.1299e-01, 2.1945e-02, 3.0387e-01, 2.3391e-02,\n",
      "         2.6727e-02, 2.6540e-03, 2.0520e-03, 8.1788e-02],\n",
      "        [6.9478e-02, 6.1940e-03, 2.7172e-02, 2.1466e-02, 9.4496e-02, 1.0325e-01,\n",
      "         2.3763e-01, 3.5931e-02, 6.8230e-03, 3.9756e-01],\n",
      "        [2.0505e-02, 2.1273e-04, 6.0236e-03, 5.9413e-04, 3.4245e-02, 9.2552e-03,\n",
      "         6.2465e-02, 8.4345e-02, 1.9857e-02, 7.6250e-01],\n",
      "        [3.7660e-02, 2.9832e-03, 1.2096e-02, 2.7529e-03, 4.0075e-02, 5.9689e-02,\n",
      "         1.2640e-01, 1.7599e-01, 1.2427e-01, 4.1808e-01],\n",
      "        [5.7293e-04, 7.6907e-06, 1.0253e-03, 7.7312e-05, 7.2548e-02, 1.8569e-02,\n",
      "         1.7333e-02, 4.2702e-02, 2.1763e-02, 8.2540e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9.90, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 7.51, Minutes Elapsed: 765.23\n",
      "Sampling from val predictions...\n",
      "Source: 现在 含有 双 <UNK> 芬 酸 的 <UNK> 已经 被禁\n",
      "Reference: this drug has now been banned for veterinary use\n",
      "Model: <SOS> now , a now the the by the ,\n",
      "Attention Weights: tensor([[0.0013, 0.0000, 0.0094, 0.0002, 0.0001, 0.0002, 0.0021, 0.0003, 0.0005,\n",
      "         0.9858],\n",
      "        [0.1539, 0.0999, 0.4169, 0.0163, 0.0043, 0.0089, 0.0132, 0.0012, 0.0017,\n",
      "         0.2836],\n",
      "        [0.0499, 0.0968, 0.2436, 0.1105, 0.0138, 0.0243, 0.0256, 0.0187, 0.0080,\n",
      "         0.4088],\n",
      "        [0.0320, 0.0143, 0.3003, 0.0480, 0.0729, 0.1091, 0.0584, 0.0153, 0.0164,\n",
      "         0.3332],\n",
      "        [0.0127, 0.0040, 0.1324, 0.0561, 0.1251, 0.3291, 0.0774, 0.0077, 0.0145,\n",
      "         0.2410],\n",
      "        [0.0005, 0.0004, 0.0549, 0.0592, 0.0640, 0.1503, 0.0389, 0.0110, 0.0332,\n",
      "         0.5875],\n",
      "        [0.0003, 0.0004, 0.0488, 0.0737, 0.0885, 0.2555, 0.0849, 0.0096, 0.0187,\n",
      "         0.4196],\n",
      "        [0.0006, 0.0018, 0.0574, 0.0768, 0.0972, 0.1770, 0.0971, 0.0239, 0.0447,\n",
      "         0.4235],\n",
      "        [0.0003, 0.0011, 0.0314, 0.0567, 0.0308, 0.0292, 0.0781, 0.0203, 0.0274,\n",
      "         0.7246]])\n",
      "\n",
      "Source: 据 保守 估计 当今 全世界 世界 有 超过 2700 万\n",
      "Reference: a conservative estimate tells us there are more than\n",
      "Model: <SOS> it , of , the a the a billion\n",
      "Attention Weights: tensor([[7.7064e-07, 2.3703e-06, 2.6613e-04, 5.0976e-02, 6.0076e-06, 9.7923e-07,\n",
      "         3.6649e-05, 1.1837e-04, 2.8524e-02, 9.2007e-01],\n",
      "        [8.4169e-02, 2.5553e-02, 1.9727e-02, 2.8768e-01, 2.4943e-03, 1.1321e-03,\n",
      "         2.5718e-03, 3.6758e-03, 8.1853e-02, 4.9114e-01],\n",
      "        [5.6952e-02, 6.3816e-02, 5.7560e-02, 1.0971e-01, 1.4028e-02, 1.6299e-02,\n",
      "         3.2813e-02, 2.5683e-02, 2.0742e-01, 4.1571e-01],\n",
      "        [1.6190e-02, 1.8295e-02, 7.1215e-02, 1.2765e-01, 1.2917e-02, 1.0944e-02,\n",
      "         3.8459e-02, 1.9985e-02, 1.8873e-01, 4.9562e-01],\n",
      "        [3.4957e-04, 3.7356e-04, 5.6616e-03, 7.0190e-02, 2.4758e-03, 3.5958e-03,\n",
      "         2.2544e-01, 2.1932e-02, 2.1581e-01, 4.5418e-01],\n",
      "        [6.5641e-04, 7.2240e-04, 1.8748e-02, 3.0121e-01, 2.7723e-02, 2.3998e-02,\n",
      "         3.8936e-02, 5.9246e-02, 1.2980e-01, 3.9896e-01],\n",
      "        [1.8973e-04, 4.5019e-05, 3.8664e-04, 3.7564e-02, 1.7683e-03, 2.3375e-03,\n",
      "         1.9725e-02, 7.4139e-02, 2.9403e-01, 5.6982e-01],\n",
      "        [4.8926e-04, 8.6799e-05, 4.7158e-04, 2.2775e-02, 3.8843e-03, 4.1976e-03,\n",
      "         2.2004e-02, 7.1054e-02, 3.2850e-01, 5.4654e-01],\n",
      "        [4.0073e-04, 1.3250e-04, 5.4683e-04, 3.2482e-02, 4.9950e-03, 9.4221e-03,\n",
      "         3.0409e-02, 7.3373e-02, 2.8016e-01, 5.6808e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10.00, Train Loss: 0.00, Val Loss: 5.04, Train BLEU: 0.00, Val BLEU: 6.94, Minutes Elapsed: 770.82\n",
      "Sampling from val predictions...\n",
      "Source: 作为 替代 他们 同意 先用 着 头衔 <UNK> 但 他们\n",
      "Reference: instead , they agreed to use the title &quot;\n",
      "Model: <SOS> and , they they to and the <UNK> ,\n",
      "Attention Weights: tensor([[1.1262e-06, 2.0431e-05, 8.6112e-01, 9.0979e-05, 7.2057e-05, 5.2807e-05,\n",
      "         3.3459e-03, 1.7388e-03, 2.1227e-03, 1.3143e-01],\n",
      "        [2.5158e-03, 4.4613e-02, 9.4208e-01, 1.3007e-03, 3.3130e-04, 8.1984e-05,\n",
      "         8.3601e-04, 3.4398e-04, 9.2069e-04, 6.9802e-03],\n",
      "        [1.5614e-02, 2.2411e-01, 3.8531e-01, 2.3587e-02, 5.5000e-03, 4.8771e-03,\n",
      "         1.3660e-02, 5.2438e-02, 2.0145e-02, 2.5476e-01],\n",
      "        [1.4769e-03, 2.7556e-02, 7.2659e-01, 3.6428e-02, 7.3962e-03, 8.9890e-03,\n",
      "         8.4888e-03, 2.8192e-02, 1.2025e-02, 1.4286e-01],\n",
      "        [1.3944e-03, 1.5328e-02, 1.9757e-01, 2.5808e-01, 6.4915e-02, 6.4708e-02,\n",
      "         5.9199e-02, 6.7999e-02, 2.6444e-02, 2.4436e-01],\n",
      "        [2.2833e-04, 2.1277e-03, 2.7341e-02, 2.6681e-01, 1.0783e-01, 1.2136e-01,\n",
      "         2.8872e-01, 2.8863e-02, 3.0213e-02, 1.2651e-01],\n",
      "        [3.8536e-04, 1.5429e-03, 4.2271e-02, 3.6674e-01, 1.2967e-01, 1.1893e-01,\n",
      "         1.6887e-01, 1.8808e-02, 5.5152e-02, 9.7629e-02],\n",
      "        [1.0556e-03, 4.8071e-03, 2.5918e-02, 2.9355e-01, 7.8729e-02, 1.0717e-01,\n",
      "         2.2401e-01, 3.7959e-02, 7.5436e-02, 1.5137e-01],\n",
      "        [4.1258e-05, 1.8570e-04, 8.5550e-03, 1.0702e-01, 1.5129e-01, 1.7718e-01,\n",
      "         1.4964e-01, 1.7437e-02, 2.1047e-01, 1.7818e-01]])\n",
      "\n",
      "Source: 5 月 13 日 我 来到 日本 <UNK> 渡 县\n",
      "Reference: on may the 13th , i made my way\n",
      "Model: <SOS> and the of of of , i a a\n",
      "Attention Weights: tensor([[3.1061e-06, 2.4330e-09, 1.5492e-06, 1.9779e-06, 9.9983e-01, 9.3600e-10,\n",
      "         7.1413e-08, 7.2508e-08, 2.0701e-06, 1.6073e-04],\n",
      "        [1.0247e-01, 2.0702e-03, 5.4167e-02, 7.7762e-03, 7.7495e-01, 9.4834e-06,\n",
      "         1.5676e-04, 5.6731e-04, 8.3052e-04, 5.7009e-02],\n",
      "        [3.3177e-01, 4.4817e-02, 1.2173e-01, 9.8665e-02, 2.2121e-01, 2.1367e-03,\n",
      "         7.5837e-03, 1.4764e-02, 2.0661e-02, 1.3667e-01],\n",
      "        [2.2417e-01, 1.8755e-02, 1.8051e-01, 6.9677e-02, 3.2581e-01, 2.7163e-04,\n",
      "         2.1255e-03, 4.9459e-03, 6.8928e-03, 1.6684e-01],\n",
      "        [8.6156e-02, 8.8553e-03, 3.1474e-02, 1.7900e-01, 4.5161e-01, 3.9791e-04,\n",
      "         2.2543e-03, 3.7041e-03, 6.5846e-03, 2.2997e-01],\n",
      "        [2.6154e-02, 3.2776e-03, 1.9888e-02, 7.7729e-02, 4.3036e-01, 1.0739e-04,\n",
      "         1.5894e-03, 8.3077e-03, 6.3837e-03, 4.2621e-01],\n",
      "        [1.6674e-01, 5.8660e-03, 4.1027e-02, 9.1910e-02, 4.5487e-01, 1.2388e-03,\n",
      "         3.7263e-03, 7.5872e-03, 1.2020e-02, 2.1501e-01],\n",
      "        [4.9884e-02, 1.1704e-03, 2.1750e-02, 3.3130e-02, 6.8510e-01, 8.2858e-03,\n",
      "         1.2492e-02, 8.1151e-03, 3.6688e-02, 1.4339e-01],\n",
      "        [2.0609e-03, 5.0052e-05, 6.3386e-04, 1.5210e-02, 5.8159e-02, 5.7324e-02,\n",
      "         3.1068e-01, 3.6415e-02, 1.6183e-01, 3.5763e-01]])\n",
      "\n",
      "Model training completed in 770 minutes with 4.85 best validation loss and 8.31 best validation BLEU.\n"
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=500, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=True, print_attn=True, inspect_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_single_learning_curve(experiment_results[0]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_val_bleu</th>\n",
       "      <th>runtime</th>\n",
       "      <th>total_params</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>dt_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zh-rnn-additive-attn-2018-12-12 05:40:57</td>\n",
       "      <td>4.847539</td>\n",
       "      <td>8.307246</td>\n",
       "      <td>770.818851</td>\n",
       "      <td>71591344</td>\n",
       "      <td>53591344</td>\n",
       "      <td>2018-12-12 18:31:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model_name  best_val_loss  best_val_bleu  \\\n",
       "0  zh-rnn-additive-attn-2018-12-12 05:40:57       4.847539       8.307246   \n",
       "\n",
       "      runtime  total_params  trainable_params           dt_created  \n",
       "0  770.818851      71591344          53591344  2018-12-12 18:31:51  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(experiment_results)[['model_name', 'best_val_loss', 'best_val_bleu', 'runtime', \n",
    "                                       'total_params', 'trainable_params', 'dt_created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model \n",
    "MODEL_NAME_TO_RELOAD = 'zh-rnn-additive-attn-2018-12-12 05:40:57'\n",
    "checkpoint = torch.load('model_checkpoints/{}.pth.tar'.format(MODEL_NAME_TO_RELOAD), map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance on validation set \n",
    "val_loss, val_bleu, val_hyp_idxs, val_ref_idxs, val_source_idxs, val_hyp_tokens, val_ref_tokens, val_source_tokens,\\\n",
    "val_attn = evaluate(model=model, loader=loaders_full['dev'], \n",
    "                    src_id2token=vocab[SRC_LANG]['id2token'], targ_id2token=vocab[TARG_LANG]['id2token'])\n",
    "print(\"Validation BLEU: {:.2f} | Validation Loss: {:.2f}\".format(val_bleu, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set \n",
    "test_loss, test_bleu, test_hyp_idxs, test_ref_idxs, test_source_idxs, test_hyp_tokens, test_ref_tokens, test_source_tokens,\\\n",
    "test_attn = evaluate(model=model, loader=loaders_full['test'], \n",
    "                     src_id2token=vocab[SRC_LANG]['id2token'], targ_id2token=vocab[TARG_LANG]['id2token'])\n",
    "print(\"Test BLEU: {:.2f} | Test Loss: {:.2f}\".format(test_bleu, test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
