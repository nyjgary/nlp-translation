{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import evaluate, train_and_eval, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params required for generating data loaders \n",
    "\n",
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture params \n",
    "NETWORK_TYPE = 'rnn'\n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0 \n",
    "DEC_DROPOUT = 0  \n",
    "ATTENTION_TYPE = 'additive'\n",
    "\n",
    "# training params  \n",
    "NUM_EPOCHS = 10 \n",
    "LR = 0.00015 \n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = False\n",
    "\n",
    "# name the model and experiment \n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    EXPERIMENT_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    EXPERIMENT_NAME = '{}-cnn'.format(SRC_LANG)\n",
    "MODEL_NAME = '{}-{}'.format(EXPERIMENT_NAME, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n",
    "          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n",
    "          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n",
    "          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n",
    "          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n",
    "          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "if ATTENTION_TYPE == 'without': \n",
    "    # without attention \n",
    "    decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                         targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)\n",
    "    \n",
    "else: \n",
    "    # with attention \n",
    "    decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                             num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, \n",
    "                             src_max_sentence_len=SRC_MAX_SENTENCE_LEN, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                             dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                    vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.20, Train BLEU: 0.00, Val BLEU: 0.02, Minutes Elapsed: 0.10\n",
      "Sampling from val predictions...\n",
      "Source: 事实 事实上 我 的 学生 <UNK> 我 得 提 一下\n",
      "Reference: actually , my student <UNK> , i have to\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0809, 0.0843, 0.0889, 0.0973, 0.1038, 0.0983, 0.0981, 0.1059, 0.1160,\n",
      "         0.1265],\n",
      "        [0.0807, 0.0842, 0.0888, 0.0972, 0.1038, 0.0986, 0.0982, 0.1060, 0.1161,\n",
      "         0.1265],\n",
      "        [0.0807, 0.0842, 0.0888, 0.0971, 0.1038, 0.0986, 0.0982, 0.1060, 0.1160,\n",
      "         0.1265],\n",
      "        [0.0808, 0.0842, 0.0888, 0.0971, 0.1037, 0.0986, 0.0982, 0.1060, 0.1160,\n",
      "         0.1265],\n",
      "        [0.0808, 0.0842, 0.0888, 0.0971, 0.1037, 0.0986, 0.0982, 0.1060, 0.1160,\n",
      "         0.1265],\n",
      "        [0.0808, 0.0842, 0.0888, 0.0971, 0.1037, 0.0986, 0.0982, 0.1060, 0.1160,\n",
      "         0.1265],\n",
      "        [0.0809, 0.0843, 0.0888, 0.0971, 0.1037, 0.0986, 0.0982, 0.1060, 0.1160,\n",
      "         0.1265],\n",
      "        [0.0809, 0.0843, 0.0888, 0.0971, 0.1037, 0.0986, 0.0982, 0.1059, 0.1160,\n",
      "         0.1265],\n",
      "        [0.0809, 0.0843, 0.0888, 0.0971, 0.1037, 0.0986, 0.0981, 0.1059, 0.1160,\n",
      "         0.1265]])\n",
      "\n",
      "Source: 在 我们 的 谈话 结束 后 我 感觉 非常 的\n",
      "Reference: after we finished talking , i felt so horrible\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0805, 0.0858, 0.0900, 0.0938, 0.0951, 0.0982, 0.1047, 0.1105, 0.1155,\n",
      "         0.1259],\n",
      "        [0.0804, 0.0858, 0.0900, 0.0938, 0.0951, 0.0982, 0.1047, 0.1106, 0.1155,\n",
      "         0.1259],\n",
      "        [0.0804, 0.0859, 0.0901, 0.0939, 0.0951, 0.0982, 0.1047, 0.1105, 0.1155,\n",
      "         0.1258],\n",
      "        [0.0804, 0.0859, 0.0901, 0.0939, 0.0951, 0.0982, 0.1047, 0.1105, 0.1154,\n",
      "         0.1257],\n",
      "        [0.0804, 0.0859, 0.0901, 0.0939, 0.0951, 0.0982, 0.1047, 0.1105, 0.1154,\n",
      "         0.1257],\n",
      "        [0.0805, 0.0860, 0.0901, 0.0939, 0.0951, 0.0981, 0.1047, 0.1105, 0.1154,\n",
      "         0.1257],\n",
      "        [0.0805, 0.0860, 0.0901, 0.0939, 0.0951, 0.0981, 0.1046, 0.1105, 0.1154,\n",
      "         0.1257],\n",
      "        [0.0805, 0.0860, 0.0901, 0.0939, 0.0951, 0.0981, 0.1046, 0.1105, 0.1154,\n",
      "         0.1257],\n",
      "        [0.0805, 0.0860, 0.0901, 0.0939, 0.0951, 0.0981, 0.1046, 0.1105, 0.1154,\n",
      "         0.1258]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.15, Train Loss: 0.00, Val Loss: 6.10, Train BLEU: 0.00, Val BLEU: 2.98, Minutes Elapsed: 8.53\n",
      "Sampling from val predictions...\n",
      "Source: <UNK> 下 浸 着 些 树 的 枝干 经常 常会\n",
      "Reference: the skeletal tree limbs submerged in lake volta often\n",
      "Model: <SOS> and : , , , , , , ,\n",
      "Attention Weights: tensor([[1.0487e-10, 3.1911e-05, 3.8588e-04, 1.0076e-03, 1.6733e-03, 2.4577e-03,\n",
      "         3.8465e-03, 7.5345e-03, 6.4566e-02, 9.1850e-01],\n",
      "        [2.8080e-09, 1.8259e-05, 1.3503e-04, 3.1442e-04, 5.0374e-04, 7.3250e-04,\n",
      "         1.1567e-03, 2.4103e-03, 2.7258e-02, 9.6747e-01],\n",
      "        [2.8382e-08, 3.3209e-05, 1.7181e-04, 3.5188e-04, 5.3081e-04, 7.4294e-04,\n",
      "         1.1354e-03, 2.3119e-03, 2.4324e-02, 9.7040e-01],\n",
      "        [2.0151e-07, 7.3138e-05, 3.0023e-04, 5.5355e-04, 7.8390e-04, 1.0390e-03,\n",
      "         1.4820e-03, 2.7317e-03, 2.2728e-02, 9.7031e-01],\n",
      "        [5.5284e-07, 1.6157e-04, 6.1722e-04, 1.0819e-03, 1.4767e-03, 1.8882e-03,\n",
      "         2.5587e-03, 4.3214e-03, 2.7867e-02, 9.6003e-01],\n",
      "        [1.1888e-06, 3.0531e-04, 1.1234e-03, 1.9116e-03, 2.5493e-03, 3.1833e-03,\n",
      "         4.1647e-03, 6.5988e-03, 3.4869e-02, 9.4529e-01],\n",
      "        [2.7871e-06, 5.6777e-04, 1.9905e-03, 3.2909e-03, 4.2975e-03, 5.2534e-03,\n",
      "         6.6619e-03, 9.9635e-03, 4.3785e-02, 9.2419e-01],\n",
      "        [4.9636e-06, 8.5409e-04, 2.9027e-03, 4.7151e-03, 6.0787e-03, 7.3346e-03,\n",
      "         9.1228e-03, 1.3157e-02, 5.1345e-02, 9.0448e-01],\n",
      "        [6.7068e-06, 1.0507e-03, 3.5260e-03, 5.6858e-03, 7.2878e-03, 8.7390e-03,\n",
      "         1.0766e-02, 1.5244e-02, 5.5971e-02, 8.9172e-01]])\n",
      "\n",
      "Source: 但 让 我 坚定 <UNK> 信念 的 是 即使 现实\n",
      "Reference: but what lifted my heart and strengthened my soul\n",
      "Model: <SOS> and &apos;s , , , , , , ,\n",
      "Attention Weights: tensor([[2.5370e-09, 2.1859e-08, 8.1464e-08, 1.4001e-07, 1.3673e-07, 4.1805e-04,\n",
      "         1.2727e-03, 3.5501e-03, 3.2402e-02, 9.6236e-01],\n",
      "        [2.6982e-06, 7.0157e-06, 1.1635e-05, 1.2600e-05, 9.7164e-06, 1.1473e-03,\n",
      "         2.3901e-03, 4.9697e-03, 2.9389e-02, 9.6206e-01],\n",
      "        [4.9602e-06, 1.0661e-05, 1.6402e-05, 1.7875e-05, 1.3041e-05, 9.2695e-04,\n",
      "         1.8741e-03, 3.9985e-03, 2.6095e-02, 9.6704e-01],\n",
      "        [5.5287e-06, 1.2639e-05, 2.0899e-05, 2.5203e-05, 2.1022e-05, 1.0812e-03,\n",
      "         2.0713e-03, 4.1914e-03, 2.5212e-02, 9.6736e-01],\n",
      "        [3.9188e-06, 1.0474e-05, 1.9650e-05, 2.6794e-05, 2.7099e-05, 1.3110e-03,\n",
      "         2.4557e-03, 4.8207e-03, 2.7038e-02, 9.6429e-01],\n",
      "        [3.6187e-06, 1.0621e-05, 2.1631e-05, 3.1885e-05, 3.7158e-05, 1.7711e-03,\n",
      "         3.2320e-03, 6.1154e-03, 3.1163e-02, 9.5761e-01],\n",
      "        [6.4953e-06, 1.8967e-05, 3.8758e-05, 5.8085e-05, 7.3189e-05, 2.9409e-03,\n",
      "         5.1043e-03, 9.0379e-03, 3.9070e-02, 9.4365e-01],\n",
      "        [8.7249e-06, 2.5252e-05, 5.1395e-05, 7.7255e-05, 1.0042e-04, 3.7992e-03,\n",
      "         6.4605e-03, 1.1096e-02, 4.4254e-02, 9.3413e-01],\n",
      "        [1.1597e-05, 3.3079e-05, 6.6708e-05, 9.9971e-05, 1.3198e-04, 4.6873e-03,\n",
      "         7.8347e-03, 1.3125e-02, 4.9021e-02, 9.2499e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.30, Train Loss: 0.00, Val Loss: 5.94, Train BLEU: 0.00, Val BLEU: 4.50, Minutes Elapsed: 16.97\n",
      "Sampling from val predictions...\n",
      "Source: 而且 实际 实际上 这 全 是因为 因为 一个 勇敢 勇敢的人\n",
      "Reference: and this was actually all down to the bravery\n",
      "Model: <SOS> and , , , , , , , ,\n",
      "Attention Weights: tensor([[8.1897e-06, 2.2304e-05, 4.3928e-05, 1.9887e-05, 2.7869e-05, 1.1376e-04,\n",
      "         1.3800e-04, 6.2727e-04, 2.8584e-02, 9.7041e-01],\n",
      "        [5.0666e-04, 6.0882e-04, 7.3138e-04, 5.2586e-04, 5.9107e-04, 1.0845e-03,\n",
      "         1.3044e-03, 3.4158e-03, 4.6756e-02, 9.4448e-01],\n",
      "        [2.1398e-07, 2.6004e-07, 3.7651e-07, 2.5817e-07, 3.7792e-07, 1.4864e-06,\n",
      "         4.5495e-06, 8.8245e-05, 1.6588e-02, 9.8332e-01],\n",
      "        [1.5345e-07, 1.9323e-07, 2.8420e-07, 2.0636e-07, 3.0587e-07, 1.1969e-06,\n",
      "         3.9186e-06, 7.9322e-05, 1.5649e-02, 9.8427e-01],\n",
      "        [1.2580e-06, 1.5190e-06, 1.9980e-06, 1.6764e-06, 2.2130e-06, 5.7108e-06,\n",
      "         1.4264e-05, 1.5322e-04, 1.5780e-02, 9.8404e-01],\n",
      "        [2.0006e-07, 2.6953e-07, 3.8852e-07, 3.2990e-07, 4.7514e-07, 1.5726e-06,\n",
      "         5.3107e-06, 9.9333e-05, 1.7072e-02, 9.8282e-01],\n",
      "        [1.4621e-05, 1.7584e-05, 2.1665e-05, 1.9587e-05, 2.3836e-05, 4.5930e-05,\n",
      "         8.8904e-05, 5.1167e-04, 2.1782e-02, 9.7747e-01],\n",
      "        [2.2477e-05, 2.7138e-05, 3.3259e-05, 3.0168e-05, 3.6374e-05, 6.7755e-05,\n",
      "         1.2578e-04, 6.5466e-04, 2.3603e-02, 9.7540e-01],\n",
      "        [7.1315e-05, 8.6708e-05, 1.0445e-04, 9.5816e-05, 1.1268e-04, 1.9243e-04,\n",
      "         3.2263e-04, 1.3075e-03, 3.0486e-02, 9.6722e-01]])\n",
      "\n",
      "Source: 我们 可以 多次 <UNK> <UNK> 重复 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: several times we can repeat that . <EOS> <PAD>\n",
      "Model: <SOS> it &apos;s , <UNK> . . . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.1383, 0.0671, 0.0289, 0.0853, 0.1880, 0.4014, 0.0910, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2406, 0.1735, 0.1015, 0.1143, 0.1361, 0.1565, 0.0775, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1025, 0.0940, 0.0956, 0.1285, 0.1554, 0.2199, 0.2040, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0955, 0.0945, 0.0997, 0.1275, 0.1524, 0.2253, 0.2050, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0964, 0.0992, 0.1069, 0.1344, 0.1555, 0.2132, 0.1945, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0992, 0.1032, 0.1113, 0.1406, 0.1592, 0.2009, 0.1857, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0926, 0.0971, 0.1052, 0.1418, 0.1659, 0.2072, 0.1903, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0890, 0.0936, 0.1021, 0.1440, 0.1705, 0.2087, 0.1921, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0896, 0.0941, 0.1026, 0.1455, 0.1714, 0.2059, 0.1908, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.45, Train Loss: 0.00, Val Loss: 5.81, Train BLEU: 0.00, Val BLEU: 3.51, Minutes Elapsed: 25.30\n",
      "Sampling from val predictions...\n",
      "Source: 政客 们 小心 小心翼翼 翼翼 地 选择 用词 并 利用\n",
      "Reference: politicians try to pick words and use words to\n",
      "Model: <SOS> and , the to , the and , ,\n",
      "Attention Weights: tensor([[5.3791e-08, 1.3675e-07, 6.4965e-08, 7.2069e-08, 1.1002e-06, 3.0207e-06,\n",
      "         2.1626e-05, 4.7636e-04, 3.7852e-02, 9.6165e-01],\n",
      "        [6.8255e-08, 7.4823e-08, 6.7624e-08, 8.7110e-08, 2.5001e-07, 6.3955e-07,\n",
      "         5.5756e-06, 1.6988e-04, 2.1705e-02, 9.7812e-01],\n",
      "        [1.3461e-07, 1.2825e-07, 1.5464e-07, 2.1953e-07, 3.4031e-07, 6.9825e-07,\n",
      "         5.4135e-06, 1.4407e-04, 1.8135e-02, 9.8171e-01],\n",
      "        [1.9557e-05, 1.8983e-05, 2.2674e-05, 2.8870e-05, 3.7584e-05, 5.5373e-05,\n",
      "         1.6338e-04, 1.0008e-03, 2.6377e-02, 9.7228e-01],\n",
      "        [2.0610e-06, 2.0894e-06, 2.5740e-06, 3.5205e-06, 5.1546e-06, 8.9774e-06,\n",
      "         3.7241e-05, 3.8300e-04, 1.8787e-02, 9.8077e-01],\n",
      "        [1.1464e-06, 1.2072e-06, 1.4889e-06, 2.0310e-06, 3.0671e-06, 5.4551e-06,\n",
      "         2.4788e-05, 3.0120e-04, 1.7968e-02, 9.8169e-01],\n",
      "        [1.3815e-06, 1.4772e-06, 1.8276e-06, 2.4827e-06, 3.7152e-06, 6.3764e-06,\n",
      "         2.7809e-05, 3.2543e-04, 1.8933e-02, 9.8070e-01],\n",
      "        [2.0755e-06, 2.1439e-06, 2.6589e-06, 3.6113e-06, 5.0327e-06, 8.1691e-06,\n",
      "         3.3780e-05, 3.7413e-04, 2.0689e-02, 9.7888e-01],\n",
      "        [6.4753e-07, 6.6018e-07, 8.3533e-07, 1.1675e-06, 1.6828e-06, 2.9776e-06,\n",
      "         1.5757e-05, 2.5037e-04, 1.9598e-02, 9.8013e-01]])\n",
      "\n",
      "Source: 同 陪审 陪审团 主席 的 意思 相似 <EOS> <PAD> <PAD>\n",
      "Reference: it was like the foreman of the jury .\n",
      "Model: <SOS> and &apos;s a , . of <EOS> . .\n",
      "Attention Weights: tensor([[0.2711, 0.0461, 0.1113, 0.2815, 0.1314, 0.0448, 0.0207, 0.0930, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1623, 0.1015, 0.1283, 0.1788, 0.1477, 0.1013, 0.0726, 0.1075, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0992, 0.1045, 0.1167, 0.1306, 0.1350, 0.1421, 0.1283, 0.1436, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0951, 0.1063, 0.1188, 0.1298, 0.1320, 0.1379, 0.1297, 0.1504, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0724, 0.0798, 0.1026, 0.1320, 0.1444, 0.1562, 0.1390, 0.1738, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0770, 0.0858, 0.1052, 0.1288, 0.1404, 0.1550, 0.1410, 0.1668, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0727, 0.0809, 0.1035, 0.1307, 0.1398, 0.1519, 0.1402, 0.1803, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0903, 0.0978, 0.1126, 0.1277, 0.1332, 0.1398, 0.1343, 0.1642, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0760, 0.0835, 0.1053, 0.1306, 0.1371, 0.1459, 0.1377, 0.1840, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.60, Train Loss: 0.00, Val Loss: 5.70, Train BLEU: 0.00, Val BLEU: 2.25, Minutes Elapsed: 33.63\n",
      "Sampling from val predictions...\n",
      "Source: 如果 听完 这些 她们 还是 说 不 不 cameron 我\n",
      "Reference: if , after this amazing list , they still\n",
      "Model: <SOS> so , &apos;s &apos;s , , , , ,\n",
      "Attention Weights: tensor([[1.9381e-08, 4.2579e-08, 1.5923e-07, 4.4692e-07, 2.2560e-06, 1.9105e-05,\n",
      "         2.5075e-04, 4.4912e-03, 8.6547e-03, 9.8658e-01],\n",
      "        [3.8240e-08, 5.0950e-08, 6.9650e-08, 1.4189e-07, 5.8168e-07, 5.1157e-06,\n",
      "         6.0085e-05, 1.1754e-03, 3.4811e-03, 9.9528e-01],\n",
      "        [2.1442e-06, 2.4661e-06, 2.8481e-06, 4.2813e-06, 1.0331e-05, 4.4974e-05,\n",
      "         2.5324e-04, 2.3904e-03, 5.9830e-03, 9.9131e-01],\n",
      "        [5.3053e-07, 6.2220e-07, 7.3618e-07, 1.1883e-06, 3.2154e-06, 1.6586e-05,\n",
      "         1.1635e-04, 1.4408e-03, 3.9744e-03, 9.9445e-01],\n",
      "        [5.1475e-07, 6.0225e-07, 7.1109e-07, 1.1490e-06, 3.1434e-06, 1.6462e-05,\n",
      "         1.1661e-04, 1.4526e-03, 3.9797e-03, 9.9443e-01],\n",
      "        [2.4137e-08, 3.1433e-08, 4.0902e-08, 8.3800e-08, 3.5219e-07, 3.4282e-06,\n",
      "         4.4032e-05, 9.4914e-04, 3.0692e-03, 9.9593e-01],\n",
      "        [2.5334e-07, 3.1161e-07, 3.7806e-07, 6.3867e-07, 1.8084e-06, 1.0037e-05,\n",
      "         7.9825e-05, 1.1630e-03, 3.4133e-03, 9.9533e-01],\n",
      "        [4.7980e-07, 5.7768e-07, 6.9326e-07, 1.1546e-06, 3.1399e-06, 1.5673e-05,\n",
      "         1.0954e-04, 1.3813e-03, 3.8661e-03, 9.9462e-01],\n",
      "        [1.4653e-07, 1.8094e-07, 2.2330e-07, 4.0102e-07, 1.2777e-06, 8.1056e-06,\n",
      "         7.1183e-05, 1.1194e-03, 3.3432e-03, 9.9546e-01]])\n",
      "\n",
      "Source: 此时 此时此刻 此刻 你 可能 在 想 哇 这 才\n",
      "Reference: right now , maybe you &apos;re thinking , &quot;\n",
      "Model: <SOS> and , the i i i , to ,\n",
      "Attention Weights: tensor([[1.8358e-09, 2.9019e-09, 3.1403e-07, 7.8749e-07, 4.2835e-06, 3.2437e-05,\n",
      "         3.3456e-04, 4.4770e-03, 7.6566e-02, 9.1858e-01],\n",
      "        [3.4882e-09, 6.1345e-09, 1.4561e-08, 4.1191e-08, 2.6474e-07, 3.5558e-06,\n",
      "         4.9818e-05, 9.8962e-04, 3.1379e-02, 9.6758e-01],\n",
      "        [3.2999e-07, 4.4867e-07, 5.4711e-07, 1.0164e-06, 3.2080e-06, 1.8582e-05,\n",
      "         1.2480e-04, 1.3243e-03, 2.8293e-02, 9.7023e-01],\n",
      "        [4.3131e-07, 6.1002e-07, 7.1896e-07, 1.3006e-06, 4.0680e-06, 2.1402e-05,\n",
      "         1.3287e-04, 1.3201e-03, 2.7555e-02, 9.7096e-01],\n",
      "        [1.1174e-07, 1.6497e-07, 1.9967e-07, 4.1180e-07, 1.5996e-06, 1.1048e-05,\n",
      "         8.6624e-05, 1.0667e-03, 2.6307e-02, 9.7253e-01],\n",
      "        [7.7540e-08, 1.1208e-07, 1.3600e-07, 2.8928e-07, 1.1671e-06, 8.8932e-06,\n",
      "         7.6204e-05, 1.0205e-03, 2.6552e-02, 9.7234e-01],\n",
      "        [8.8309e-07, 1.2218e-06, 1.3977e-06, 2.4442e-06, 7.1413e-06, 3.3816e-05,\n",
      "         1.8692e-04, 1.6306e-03, 2.9852e-02, 9.6828e-01],\n",
      "        [6.6358e-05, 8.4474e-05, 8.9122e-05, 1.2104e-04, 2.1884e-04, 5.2511e-04,\n",
      "         1.4610e-03, 6.0027e-03, 5.0340e-02, 9.4109e-01],\n",
      "        [6.9491e-06, 8.8705e-06, 9.6329e-06, 1.3875e-05, 2.8204e-05, 9.1220e-05,\n",
      "         3.6797e-04, 2.4262e-03, 3.4593e-02, 9.6245e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.75, Train Loss: 0.00, Val Loss: 5.61, Train BLEU: 0.00, Val BLEU: 4.66, Minutes Elapsed: 42.00\n",
      "Sampling from val predictions...\n",
      "Source: 我 想要 种 一整 整个 街区 的 菜园 在 这个\n",
      "Reference: i want to plant a whole block of gardens\n",
      "Model: <SOS> i i to know a a , of the\n",
      "Attention Weights: tensor([[2.1375e-05, 5.6818e-06, 5.7977e-06, 3.2664e-06, 3.6019e-06, 5.7657e-06,\n",
      "         3.5365e-05, 2.7051e-04, 2.4499e-02, 9.7515e-01],\n",
      "        [4.9032e-07, 3.8694e-07, 3.6470e-07, 5.6142e-07, 8.5449e-07, 2.8332e-06,\n",
      "         1.4712e-05, 1.8728e-04, 1.7635e-02, 9.8216e-01],\n",
      "        [4.7734e-09, 5.3346e-09, 6.8118e-09, 1.2880e-08, 2.9671e-08, 1.6150e-07,\n",
      "         2.2091e-06, 6.5754e-05, 1.4656e-02, 9.8528e-01],\n",
      "        [1.3142e-09, 1.5609e-09, 2.0108e-09, 4.8301e-09, 1.3938e-08, 1.2626e-07,\n",
      "         2.6105e-06, 1.0914e-04, 2.1762e-02, 9.7813e-01],\n",
      "        [1.6281e-09, 1.9514e-09, 2.5431e-09, 6.0731e-09, 1.7523e-08, 1.4731e-07,\n",
      "         3.0765e-06, 1.1696e-04, 2.1567e-02, 9.7831e-01],\n",
      "        [1.1941e-07, 1.3605e-07, 1.5689e-07, 2.5628e-07, 4.5623e-07, 1.6162e-06,\n",
      "         1.1786e-05, 1.8547e-04, 1.9301e-02, 9.8050e-01],\n",
      "        [1.8807e-06, 2.0602e-06, 2.2511e-06, 3.0970e-06, 4.4619e-06, 1.0077e-05,\n",
      "         3.9132e-05, 3.1988e-04, 1.9469e-02, 9.8015e-01],\n",
      "        [9.4560e-06, 1.0149e-05, 1.0867e-05, 1.3709e-05, 1.8071e-05, 3.2626e-05,\n",
      "         9.2295e-05, 5.0625e-04, 2.0656e-02, 9.7865e-01],\n",
      "        [3.2659e-05, 3.5342e-05, 3.7560e-05, 4.6249e-05, 5.8216e-05, 9.6489e-05,\n",
      "         2.2441e-04, 9.6153e-04, 2.6260e-02, 9.7225e-01]])\n",
      "\n",
      "Source: 在 这 之后 不久 当 我 走过 一个 火车 火车站\n",
      "Reference: soon after , when i was walking past a\n",
      "Model: <SOS> and , i , , was , to ,\n",
      "Attention Weights: tensor([[6.5002e-06, 5.6533e-06, 6.1783e-07, 1.2482e-06, 1.5651e-06, 7.9627e-06,\n",
      "         1.4089e-05, 1.5159e-04, 1.4313e-02, 9.8550e-01],\n",
      "        [1.3591e-07, 1.2959e-07, 1.1552e-07, 1.6718e-07, 1.9194e-07, 6.6457e-07,\n",
      "         4.0443e-06, 9.8480e-05, 1.2841e-02, 9.8705e-01],\n",
      "        [7.9145e-09, 9.0211e-09, 1.1354e-08, 1.7470e-08, 2.5205e-08, 1.0569e-07,\n",
      "         9.6544e-07, 4.6439e-05, 1.1111e-02, 9.8884e-01],\n",
      "        [7.8082e-10, 9.2453e-10, 1.2945e-09, 2.3126e-09, 3.9540e-09, 2.9450e-08,\n",
      "         5.2785e-07, 5.3734e-05, 1.5839e-02, 9.8411e-01],\n",
      "        [5.0769e-11, 6.4053e-11, 9.9384e-11, 2.0775e-10, 4.6762e-10, 6.9104e-09,\n",
      "         2.9499e-07, 8.0381e-05, 2.6191e-02, 9.7373e-01],\n",
      "        [2.4608e-11, 3.2919e-11, 5.6975e-11, 1.4525e-10, 4.2378e-10, 1.1304e-08,\n",
      "         7.3056e-07, 2.1685e-04, 4.3320e-02, 9.5646e-01],\n",
      "        [1.2507e-11, 1.7327e-11, 3.1445e-11, 8.6455e-11, 2.8575e-10, 9.5674e-09,\n",
      "         7.6683e-07, 2.5861e-04, 4.8135e-02, 9.5161e-01],\n",
      "        [2.1703e-11, 2.8504e-11, 5.2843e-11, 1.4557e-10, 4.1419e-10, 1.2881e-08,\n",
      "         9.9395e-07, 2.6443e-04, 4.5584e-02, 9.5415e-01],\n",
      "        [2.9646e-10, 3.5592e-10, 5.1721e-10, 9.3086e-10, 1.6987e-09, 1.5417e-08,\n",
      "         3.9087e-07, 6.2578e-05, 1.9680e-02, 9.8026e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.90, Train Loss: 0.00, Val Loss: 5.53, Train BLEU: 0.00, Val BLEU: 3.12, Minutes Elapsed: 50.33\n",
      "Sampling from val predictions...\n",
      "Source: 我们 这些 西方 援助 <UNK> 在 过去 50 年 里\n",
      "Reference: we western donor countries have given the african continent\n",
      "Model: <SOS> we we to to , the to to of\n",
      "Attention Weights: tensor([[9.9923e-01, 3.0208e-04, 1.1665e-04, 1.6363e-05, 3.3151e-06, 1.0554e-04,\n",
      "         7.2704e-06, 4.9613e-05, 3.8136e-05, 1.2805e-04],\n",
      "        [9.7290e-01, 1.0833e-02, 6.4697e-03, 1.8672e-03, 8.0861e-04, 4.1720e-03,\n",
      "         7.5741e-04, 1.0470e-03, 6.2839e-04, 5.1593e-04],\n",
      "        [6.1523e-02, 3.2729e-02, 3.4596e-02, 3.4650e-02, 3.7275e-02, 4.4949e-02,\n",
      "         4.8926e-02, 1.1707e-01, 1.6107e-01, 4.2721e-01],\n",
      "        [5.4848e-03, 3.4954e-03, 4.0214e-03, 4.3475e-03, 5.6360e-03, 7.2785e-03,\n",
      "         9.5349e-03, 6.3978e-02, 1.2378e-01, 7.7245e-01],\n",
      "        [2.3277e-03, 1.4540e-03, 1.6394e-03, 1.7605e-03, 2.4086e-03, 3.3445e-03,\n",
      "         4.6762e-03, 4.8922e-02, 1.0540e-01, 8.2807e-01],\n",
      "        [3.6865e-03, 2.0075e-03, 2.2511e-03, 2.4059e-03, 3.3589e-03, 4.5632e-03,\n",
      "         6.1460e-03, 5.5556e-02, 1.1459e-01, 8.0544e-01],\n",
      "        [3.6661e-03, 1.7052e-03, 1.9161e-03, 2.0223e-03, 2.7945e-03, 3.9379e-03,\n",
      "         5.2188e-03, 5.1823e-02, 1.0907e-01, 8.1784e-01],\n",
      "        [2.0078e-05, 1.1237e-05, 1.4401e-05, 1.6533e-05, 2.1012e-05, 5.4861e-05,\n",
      "         1.1626e-04, 1.2810e-02, 4.7232e-02, 9.3970e-01],\n",
      "        [3.2709e-04, 1.8436e-04, 2.1782e-04, 2.4179e-04, 3.1479e-04, 5.5903e-04,\n",
      "         8.9574e-04, 2.5024e-02, 7.0106e-02, 9.0213e-01]])\n",
      "\n",
      "Source: 正好 相反 我 是 一个 很 强硬 的 女性 深爱\n",
      "Reference: instead , i was a very strong woman in\n",
      "Model: <SOS> and , , you to to of , ,\n",
      "Attention Weights: tensor([[7.0733e-02, 1.6567e-03, 9.2524e-01, 2.3112e-05, 7.6918e-05, 4.6317e-05,\n",
      "         1.0999e-05, 1.0852e-04, 8.6567e-05, 2.0155e-03],\n",
      "        [9.2510e-02, 8.0211e-03, 8.9190e-01, 1.0645e-03, 1.9665e-03, 1.2037e-03,\n",
      "         5.4927e-04, 1.0912e-03, 7.1759e-04, 9.7282e-04],\n",
      "        [1.4253e-02, 1.2553e-02, 3.5179e-02, 1.5351e-02, 1.8389e-02, 2.5752e-02,\n",
      "         2.4931e-02, 6.3513e-02, 9.0314e-02, 6.9976e-01],\n",
      "        [2.5547e-05, 2.3987e-05, 1.2391e-04, 5.2917e-05, 8.0503e-05, 2.7310e-04,\n",
      "         2.7220e-04, 4.1125e-03, 1.1135e-02, 9.8390e-01],\n",
      "        [5.7538e-05, 5.3454e-05, 2.6509e-04, 1.1140e-04, 1.6256e-04, 4.9560e-04,\n",
      "         4.8689e-04, 5.9088e-03, 1.4429e-02, 9.7803e-01],\n",
      "        [2.6318e-05, 2.5682e-05, 1.2141e-04, 5.8682e-05, 1.0201e-04, 3.4608e-04,\n",
      "         3.6040e-04, 5.1581e-03, 1.3116e-02, 9.8069e-01],\n",
      "        [9.9314e-06, 9.4985e-06, 6.9256e-05, 2.5352e-05, 4.7286e-05, 2.0131e-04,\n",
      "         2.0003e-04, 4.2043e-03, 1.1173e-02, 9.8406e-01],\n",
      "        [1.6982e-05, 1.5391e-05, 1.1896e-04, 3.5089e-05, 6.2760e-05, 2.3726e-04,\n",
      "         2.3618e-04, 4.3921e-03, 1.1529e-02, 9.8336e-01],\n",
      "        [1.5128e-05, 1.3800e-05, 9.9676e-05, 3.1646e-05, 5.6589e-05, 2.2055e-04,\n",
      "         2.2017e-04, 4.2997e-03, 1.1558e-02, 9.8348e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.00, Train Loss: 0.00, Val Loss: 5.44, Train BLEU: 0.00, Val BLEU: 5.33, Minutes Elapsed: 55.92\n",
      "Sampling from val predictions...\n",
      "Source: 现在 如果 我 用 一个 激光 <UNK> 并且 在 万亿\n",
      "Reference: now if i take a laser pointer and turn\n",
      "Model: <SOS> and , i want to to of , and\n",
      "Attention Weights: tensor([[1.0470e-05, 8.7758e-06, 9.9867e-01, 3.9582e-06, 1.3407e-06, 2.2003e-06,\n",
      "         6.4459e-06, 3.6657e-05, 1.1967e-04, 1.1455e-03],\n",
      "        [1.1484e-03, 1.0638e-03, 9.9213e-01, 2.1848e-04, 1.5999e-04, 2.4300e-04,\n",
      "         6.6610e-04, 1.0347e-03, 1.1413e-03, 2.1934e-03],\n",
      "        [3.8296e-03, 5.6154e-03, 7.6691e-01, 7.3149e-03, 7.3130e-03, 1.1676e-02,\n",
      "         2.7826e-02, 2.8738e-02, 4.6174e-02, 9.4604e-02],\n",
      "        [2.0018e-03, 2.9388e-03, 5.5939e-01, 5.7055e-03, 7.0103e-03, 1.1415e-02,\n",
      "         2.2619e-02, 3.8177e-02, 8.4108e-02, 2.6664e-01],\n",
      "        [1.3922e-04, 2.1272e-04, 1.2885e-01, 3.3600e-03, 4.1162e-03, 5.6704e-03,\n",
      "         6.4126e-03, 2.9952e-02, 1.2167e-01, 6.9962e-01],\n",
      "        [7.2769e-04, 9.4854e-04, 1.4880e-01, 1.1319e-02, 1.5994e-02, 1.9687e-02,\n",
      "         1.8642e-02, 7.5598e-02, 1.7472e-01, 5.3357e-01],\n",
      "        [1.0513e-04, 1.4099e-04, 1.8331e-01, 3.6268e-03, 4.1469e-03, 4.7524e-03,\n",
      "         4.1462e-03, 3.5387e-02, 1.2561e-01, 6.3878e-01],\n",
      "        [5.8559e-04, 7.1114e-04, 1.9535e-01, 4.1722e-03, 5.6956e-03, 7.0236e-03,\n",
      "         6.4643e-03, 4.3997e-02, 1.2734e-01, 6.0866e-01],\n",
      "        [6.1455e-04, 7.1882e-04, 1.1382e-01, 5.2657e-03, 7.5390e-03, 9.1367e-03,\n",
      "         8.1093e-03, 5.2019e-02, 1.5204e-01, 6.5074e-01]])\n",
      "\n",
      "Source: 我 在 那里 成长 我 在 那里 养育 我 的\n",
      "Reference: i grew up there . i raised my sons\n",
      "Model: <SOS> i i to to i i i to ,\n",
      "Attention Weights: tensor([[9.8442e-01, 3.4851e-06, 7.2964e-07, 1.6509e-06, 1.4196e-02, 2.2594e-06,\n",
      "         1.6525e-06, 1.6567e-05, 1.2831e-03, 7.4859e-05],\n",
      "        [9.6872e-01, 1.4860e-04, 8.9104e-05, 1.4760e-04, 2.3884e-02, 1.4388e-04,\n",
      "         1.3289e-04, 6.1355e-04, 5.1090e-03, 1.0099e-03],\n",
      "        [1.7889e-02, 1.6039e-03, 1.3391e-03, 2.9616e-03, 3.5036e-01, 9.8969e-03,\n",
      "         8.9924e-03, 3.8283e-02, 4.2250e-01, 1.4618e-01],\n",
      "        [3.9164e-03, 1.1725e-03, 1.5992e-03, 3.8398e-03, 4.0364e-01, 1.5055e-02,\n",
      "         1.4515e-02, 4.5596e-02, 3.8782e-01, 1.2284e-01],\n",
      "        [3.1458e-04, 5.9728e-05, 8.0064e-05, 2.6205e-04, 4.8500e-01, 2.2552e-03,\n",
      "         1.7608e-03, 1.1485e-02, 4.3454e-01, 6.4246e-02],\n",
      "        [3.2075e-04, 4.2433e-05, 4.9815e-05, 1.6513e-04, 4.9608e-01, 1.3008e-03,\n",
      "         1.0042e-03, 9.4829e-03, 4.3478e-01, 5.6776e-02],\n",
      "        [4.5429e-04, 5.1407e-05, 5.7632e-05, 1.6777e-04, 4.9210e-01, 1.3512e-03,\n",
      "         1.1149e-03, 9.2967e-03, 4.3805e-01, 5.7356e-02],\n",
      "        [7.0289e-04, 5.8008e-05, 6.2416e-05, 1.9477e-04, 4.0238e-01, 1.8696e-03,\n",
      "         1.6117e-03, 1.0924e-02, 4.9814e-01, 8.4056e-02],\n",
      "        [2.2003e-04, 5.8487e-05, 9.2411e-05, 3.7075e-04, 4.2318e-01, 3.7875e-03,\n",
      "         3.0255e-03, 1.4905e-02, 4.6790e-01, 8.6461e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.15, Train Loss: 0.00, Val Loss: 5.40, Train BLEU: 0.00, Val BLEU: 4.53, Minutes Elapsed: 64.24\n",
      "Sampling from val predictions...\n",
      "Source: 像 这样 的 幻灯 幻灯片 不仅 不仅仅 仅仅 乏味 无聊\n",
      "Reference: a slide like this is not only boring ,\n",
      "Model: <SOS> and the of the , , , , ,\n",
      "Attention Weights: tensor([[0.0014, 0.0027, 0.0018, 0.0011, 0.0004, 0.0022, 0.0010, 0.0058, 0.0291,\n",
      "         0.9546],\n",
      "        [0.0136, 0.0254, 0.0140, 0.0123, 0.0063, 0.0190, 0.0112, 0.0341, 0.0949,\n",
      "         0.7692],\n",
      "        [0.0041, 0.0076, 0.0067, 0.0062, 0.0033, 0.0093, 0.0056, 0.0127, 0.0522,\n",
      "         0.8922],\n",
      "        [0.0024, 0.0054, 0.0041, 0.0042, 0.0019, 0.0064, 0.0033, 0.0088, 0.0418,\n",
      "         0.9217],\n",
      "        [0.0012, 0.0028, 0.0015, 0.0017, 0.0009, 0.0033, 0.0019, 0.0052, 0.0277,\n",
      "         0.9538],\n",
      "        [0.0000, 0.0000, 0.0001, 0.0002, 0.0001, 0.0007, 0.0005, 0.0016, 0.0112,\n",
      "         0.9855],\n",
      "        [0.0004, 0.0009, 0.0012, 0.0017, 0.0015, 0.0049, 0.0036, 0.0086, 0.0349,\n",
      "         0.9423],\n",
      "        [0.0001, 0.0003, 0.0004, 0.0005, 0.0004, 0.0018, 0.0012, 0.0038, 0.0207,\n",
      "         0.9708],\n",
      "        [0.0001, 0.0001, 0.0003, 0.0006, 0.0007, 0.0027, 0.0022, 0.0056, 0.0248,\n",
      "         0.9630]])\n",
      "\n",
      "Source: 不久 久之 之后 我 参加 做 志愿 志愿者 愿者 的\n",
      "Reference: soon after , an organization i volunteer with ,\n",
      "Model: <SOS> and , , , i , i to ,\n",
      "Attention Weights: tensor([[1.6220e-05, 1.0261e-05, 1.2888e-05, 9.9010e-01, 5.8458e-06, 5.8354e-05,\n",
      "         1.3306e-05, 2.0143e-05, 1.9836e-04, 9.5682e-03],\n",
      "        [3.7722e-04, 3.4334e-04, 4.1441e-04, 9.7401e-01, 1.3505e-04, 7.4639e-04,\n",
      "         2.9674e-04, 4.8407e-04, 2.4156e-03, 2.0778e-02],\n",
      "        [1.7492e-03, 2.6875e-03, 5.1140e-03, 6.6763e-01, 3.2705e-03, 1.7515e-02,\n",
      "         7.7569e-03, 1.2247e-02, 3.3730e-02, 2.4830e-01],\n",
      "        [2.1172e-03, 3.1160e-03, 5.1551e-03, 6.7530e-01, 5.6480e-03, 2.1242e-02,\n",
      "         1.2445e-02, 1.8243e-02, 3.8215e-02, 2.1852e-01],\n",
      "        [2.0579e-03, 2.8015e-03, 4.4807e-03, 4.9415e-01, 1.0069e-02, 3.0028e-02,\n",
      "         2.1018e-02, 3.0526e-02, 5.1795e-02, 3.5307e-01],\n",
      "        [1.6637e-03, 2.4286e-03, 3.8420e-03, 5.0954e-01, 6.7258e-03, 2.1623e-02,\n",
      "         1.3938e-02, 2.0955e-02, 3.9012e-02, 3.8027e-01],\n",
      "        [1.5321e-03, 2.0022e-03, 3.3821e-03, 6.9266e-01, 5.3220e-03, 1.8586e-02,\n",
      "         1.1859e-02, 1.8149e-02, 3.4857e-02, 2.1165e-01],\n",
      "        [1.0142e-03, 1.3734e-03, 2.5300e-03, 4.6609e-01, 6.7569e-03, 2.5819e-02,\n",
      "         1.6465e-02, 2.6906e-02, 5.1979e-02, 4.0106e-01],\n",
      "        [1.4182e-03, 2.1837e-03, 5.2824e-03, 2.9595e-01, 1.6007e-02, 4.4863e-02,\n",
      "         3.2292e-02, 5.1141e-02, 8.3123e-02, 4.6774e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.30, Train Loss: 0.00, Val Loss: 5.35, Train BLEU: 0.00, Val BLEU: 5.08, Minutes Elapsed: 72.61\n",
      "Sampling from val predictions...\n",
      "Source: 没 必要 去 诊所 做 例行 的 检查 <EOS> <PAD>\n",
      "Reference: no need to go to the clinic for a\n",
      "Model: <SOS> it &apos;s the be a the . . <EOS>\n",
      "Attention Weights: tensor([[0.1725, 0.0042, 0.1377, 0.0558, 0.0707, 0.0214, 0.3092, 0.1518, 0.0765,\n",
      "         0.0000],\n",
      "        [0.1160, 0.0085, 0.1089, 0.0484, 0.0562, 0.0218, 0.3151, 0.1731, 0.1518,\n",
      "         0.0000],\n",
      "        [0.0241, 0.0070, 0.1075, 0.0547, 0.0666, 0.0151, 0.3062, 0.1999, 0.2189,\n",
      "         0.0000],\n",
      "        [0.0356, 0.0151, 0.1166, 0.0704, 0.0860, 0.0249, 0.3116, 0.1638, 0.1760,\n",
      "         0.0000],\n",
      "        [0.0220, 0.0180, 0.0655, 0.0850, 0.1304, 0.0472, 0.2782, 0.1447, 0.2091,\n",
      "         0.0000],\n",
      "        [0.0197, 0.0189, 0.0627, 0.0862, 0.1308, 0.0580, 0.2699, 0.1516, 0.2020,\n",
      "         0.0000],\n",
      "        [0.0249, 0.0271, 0.0576, 0.0906, 0.1318, 0.0751, 0.2493, 0.1536, 0.1900,\n",
      "         0.0000],\n",
      "        [0.0240, 0.0278, 0.0528, 0.0882, 0.1320, 0.0883, 0.2434, 0.1541, 0.1893,\n",
      "         0.0000],\n",
      "        [0.0439, 0.0442, 0.0649, 0.0871, 0.1072, 0.0906, 0.1765, 0.1530, 0.2325,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 让 我 兴奋 的 是 我 看见 我 学校 中\n",
      "Reference: the exciting thing is that i see students at\n",
      "Model: <SOS> and i i i i i i to i\n",
      "Attention Weights: tensor([[0.0136, 0.9465, 0.0000, 0.0003, 0.0000, 0.0219, 0.0001, 0.0154, 0.0001,\n",
      "         0.0020],\n",
      "        [0.0135, 0.9390, 0.0001, 0.0016, 0.0004, 0.0220, 0.0010, 0.0160, 0.0008,\n",
      "         0.0056],\n",
      "        [0.0241, 0.2746, 0.0077, 0.0516, 0.0136, 0.2261, 0.0265, 0.1958, 0.0233,\n",
      "         0.1569],\n",
      "        [0.0235, 0.2219, 0.0058, 0.0498, 0.0141, 0.2648, 0.0323, 0.2200, 0.0271,\n",
      "         0.1407],\n",
      "        [0.0060, 0.1190, 0.0019, 0.0280, 0.0067, 0.3569, 0.0224, 0.3057, 0.0177,\n",
      "         0.1357],\n",
      "        [0.0020, 0.0405, 0.0016, 0.0237, 0.0057, 0.3222, 0.0236, 0.3368, 0.0188,\n",
      "         0.2252],\n",
      "        [0.0032, 0.0400, 0.0017, 0.0240, 0.0071, 0.3554, 0.0257, 0.3625, 0.0190,\n",
      "         0.1615],\n",
      "        [0.0025, 0.0312, 0.0022, 0.0242, 0.0080, 0.3142, 0.0276, 0.3479, 0.0220,\n",
      "         0.2200],\n",
      "        [0.0008, 0.0083, 0.0071, 0.0331, 0.0156, 0.2639, 0.0474, 0.3300, 0.0416,\n",
      "         0.2523]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.45, Train Loss: 0.00, Val Loss: 5.28, Train BLEU: 0.00, Val BLEU: 5.36, Minutes Elapsed: 81.00\n",
      "Sampling from val predictions...\n",
      "Source: 而 这 张 由 <UNK> brown 制作 的 示例 幻灯\n",
      "Reference: instead , this example slide by <UNK> brown is\n",
      "Model: <SOS> and , this is a the the <UNK> ,\n",
      "Attention Weights: tensor([[0.0011, 0.9501, 0.0001, 0.0253, 0.0005, 0.0001, 0.0027, 0.0031, 0.0018,\n",
      "         0.0153],\n",
      "        [0.0026, 0.9159, 0.0003, 0.0522, 0.0010, 0.0002, 0.0054, 0.0048, 0.0028,\n",
      "         0.0147],\n",
      "        [0.0044, 0.3337, 0.0029, 0.3898, 0.0125, 0.0011, 0.0305, 0.0314, 0.0168,\n",
      "         0.1768],\n",
      "        [0.0017, 0.0977, 0.0021, 0.3885, 0.0180, 0.0016, 0.0416, 0.0430, 0.0207,\n",
      "         0.3850],\n",
      "        [0.0021, 0.1335, 0.0023, 0.4122, 0.0132, 0.0014, 0.0419, 0.0401, 0.0208,\n",
      "         0.3325],\n",
      "        [0.0023, 0.0593, 0.0015, 0.2796, 0.0078, 0.0012, 0.0678, 0.0835, 0.0373,\n",
      "         0.4597],\n",
      "        [0.0011, 0.0394, 0.0006, 0.1927, 0.0047, 0.0006, 0.0469, 0.0745, 0.0323,\n",
      "         0.6073],\n",
      "        [0.0018, 0.0322, 0.0010, 0.1269, 0.0075, 0.0009, 0.0546, 0.0825, 0.0364,\n",
      "         0.6563],\n",
      "        [0.0003, 0.0064, 0.0004, 0.0368, 0.0029, 0.0009, 0.0404, 0.0913, 0.0490,\n",
      "         0.7716]])\n",
      "\n",
      "Source: 谢谢 掌声 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank you . <EOS> you . <EOS> . .\n",
      "Attention Weights: tensor([[0.0023, 0.6165, 0.3813, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0257, 0.7469, 0.2274, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1822, 0.4908, 0.3270, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1842, 0.3849, 0.4308, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1222, 0.3669, 0.5109, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1957, 0.3652, 0.4391, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1747, 0.3582, 0.4671, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1756, 0.3291, 0.4953, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1774, 0.3210, 0.5017, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.60, Train Loss: 0.00, Val Loss: 5.24, Train BLEU: 0.00, Val BLEU: 4.18, Minutes Elapsed: 89.37\n",
      "Sampling from val predictions...\n",
      "Source: 我 只是 在 6 秒 内 就 彻底 彻底改变 改变\n",
      "Reference: i just totally transformed what you thought of me\n",
      "Model: <SOS> i i , to the the the , the\n",
      "Attention Weights: tensor([[9.9890e-01, 4.7416e-07, 1.0696e-05, 4.1313e-04, 2.5443e-05, 1.4521e-05,\n",
      "         2.9462e-05, 4.2602e-05, 5.7808e-05, 5.0945e-04],\n",
      "        [9.9494e-01, 1.0037e-05, 1.8826e-04, 1.0770e-03, 2.9090e-04, 1.6890e-04,\n",
      "         2.6610e-04, 3.3915e-04, 4.6210e-04, 2.2565e-03],\n",
      "        [1.4092e-01, 2.3998e-03, 5.0148e-02, 3.9880e-01, 4.9020e-02, 2.0662e-02,\n",
      "         2.8515e-02, 2.5275e-02, 3.7344e-02, 2.4692e-01],\n",
      "        [2.8892e-02, 6.9762e-05, 6.6926e-03, 6.6067e-01, 1.6517e-02, 4.8431e-03,\n",
      "         9.4902e-03, 8.1594e-03, 1.2482e-02, 2.5218e-01],\n",
      "        [1.6621e-02, 8.3666e-05, 4.8413e-03, 4.8121e-01, 1.9402e-02, 7.4152e-03,\n",
      "         1.6408e-02, 1.5659e-02, 2.5173e-02, 4.1319e-01],\n",
      "        [1.6449e-02, 3.4369e-05, 2.1020e-03, 2.6097e-01, 1.3135e-02, 6.1872e-03,\n",
      "         1.7498e-02, 2.1592e-02, 3.9925e-02, 6.2211e-01],\n",
      "        [4.5821e-03, 3.2215e-06, 4.8670e-04, 8.8465e-02, 6.0410e-03, 3.9571e-03,\n",
      "         1.2797e-02, 2.3436e-02, 4.4330e-02, 8.1590e-01],\n",
      "        [2.4149e-03, 5.8678e-06, 5.9494e-04, 1.3894e-01, 7.9712e-03, 6.1643e-03,\n",
      "         1.7784e-02, 2.5512e-02, 4.5328e-02, 7.5528e-01],\n",
      "        [2.1443e-04, 5.0912e-06, 4.7868e-04, 1.7887e-01, 9.0383e-03, 9.6452e-03,\n",
      "         2.8505e-02, 3.8924e-02, 6.4577e-02, 6.6974e-01]])\n",
      "\n",
      "Source: 我 认为 答案 已经 藏 在 问题 之中 <EOS> <PAD>\n",
      "Reference: so i figured that the problem is the solution\n",
      "Model: <SOS> i i i to to a is the .\n",
      "Attention Weights: tensor([[0.9661, 0.0003, 0.0007, 0.0006, 0.0082, 0.0025, 0.0041, 0.0142, 0.0032,\n",
      "         0.0000],\n",
      "        [0.9471, 0.0005, 0.0013, 0.0009, 0.0099, 0.0035, 0.0053, 0.0224, 0.0091,\n",
      "         0.0000],\n",
      "        [0.2787, 0.0222, 0.0631, 0.0249, 0.1501, 0.0546, 0.0617, 0.1875, 0.1573,\n",
      "         0.0000],\n",
      "        [0.2801, 0.0158, 0.0510, 0.0190, 0.1528, 0.0478, 0.0606, 0.2206, 0.1522,\n",
      "         0.0000],\n",
      "        [0.0367, 0.0090, 0.0470, 0.0326, 0.2289, 0.0888, 0.0888, 0.2594, 0.2088,\n",
      "         0.0000],\n",
      "        [0.0301, 0.0071, 0.0287, 0.0256, 0.2048, 0.0816, 0.0941, 0.3031, 0.2249,\n",
      "         0.0000],\n",
      "        [0.0386, 0.0091, 0.0266, 0.0312, 0.1905, 0.0928, 0.1131, 0.3045, 0.1936,\n",
      "         0.0000],\n",
      "        [0.1130, 0.0195, 0.0369, 0.0342, 0.1537, 0.0808, 0.0992, 0.2457, 0.2169,\n",
      "         0.0000],\n",
      "        [0.0964, 0.0178, 0.0322, 0.0299, 0.1730, 0.0804, 0.0983, 0.2761, 0.1959,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.75, Train Loss: 0.00, Val Loss: 5.22, Train BLEU: 0.00, Val BLEU: 5.67, Minutes Elapsed: 97.78\n",
      "Sampling from val predictions...\n",
      "Source: 还 不算 毁坏 得 太严 严重 但是 被 <UNK> <UNK>\n",
      "Reference: not terribly damaged , but where the water had\n",
      "Model: <SOS> it &apos;s , , the &apos;s &apos;s was is\n",
      "Attention Weights: tensor([[0.0040, 0.0006, 0.0121, 0.1429, 0.0478, 0.0240, 0.0134, 0.6078, 0.0808,\n",
      "         0.0667],\n",
      "        [0.0334, 0.0026, 0.0315, 0.1739, 0.0464, 0.0147, 0.0073, 0.4747, 0.1233,\n",
      "         0.0922],\n",
      "        [0.0096, 0.0042, 0.0344, 0.2221, 0.0855, 0.0335, 0.0142, 0.2895, 0.1729,\n",
      "         0.1342],\n",
      "        [0.0044, 0.0023, 0.0201, 0.2720, 0.1235, 0.0431, 0.0154, 0.3594, 0.0947,\n",
      "         0.0651],\n",
      "        [0.0046, 0.0020, 0.0159, 0.1676, 0.1149, 0.0474, 0.0187, 0.4291, 0.1150,\n",
      "         0.0847],\n",
      "        [0.0044, 0.0015, 0.0089, 0.0970, 0.0571, 0.0404, 0.0186, 0.5535, 0.1143,\n",
      "         0.1043],\n",
      "        [0.0052, 0.0024, 0.0130, 0.1224, 0.0863, 0.0529, 0.0258, 0.4432, 0.1331,\n",
      "         0.1158],\n",
      "        [0.0034, 0.0026, 0.0123, 0.1124, 0.1057, 0.0781, 0.0425, 0.4135, 0.1243,\n",
      "         0.1053],\n",
      "        [0.0029, 0.0023, 0.0099, 0.1008, 0.1041, 0.0874, 0.0495, 0.3764, 0.1411,\n",
      "         0.1256]])\n",
      "\n",
      "Source: 我们 已经 有 大约 50 人参 参与 到 我们 的\n",
      "Reference: we &apos;ve had , like , 50 people come\n",
      "Model: <SOS> we we the a lot , , we ,\n",
      "Attention Weights: tensor([[9.9988e-01, 1.1496e-06, 2.9751e-06, 1.3134e-06, 3.0350e-05, 2.8035e-06,\n",
      "         7.2653e-06, 6.3267e-06, 2.6889e-05, 3.8728e-05],\n",
      "        [9.9029e-01, 1.5655e-04, 3.6943e-04, 1.1739e-04, 3.1575e-03, 1.7316e-04,\n",
      "         3.0505e-04, 2.6235e-04, 1.0388e-03, 4.1265e-03],\n",
      "        [4.3409e-02, 9.3835e-03, 4.9266e-02, 1.2053e-02, 5.6284e-01, 1.2498e-02,\n",
      "         1.2189e-02, 1.1117e-02, 1.5995e-02, 2.7125e-01],\n",
      "        [2.9764e-03, 1.2997e-03, 1.4551e-02, 5.8568e-03, 7.7154e-01, 9.9702e-03,\n",
      "         1.4174e-02, 9.2430e-03, 1.6954e-02, 1.5344e-01],\n",
      "        [1.6030e-03, 2.5710e-04, 3.9930e-03, 2.5050e-03, 5.7928e-01, 9.9223e-03,\n",
      "         2.8568e-02, 2.0411e-02, 7.4551e-02, 2.7891e-01],\n",
      "        [9.0995e-04, 1.3782e-04, 1.6877e-03, 1.0064e-03, 4.4553e-01, 8.8495e-03,\n",
      "         4.3360e-02, 2.8812e-02, 1.4897e-01, 3.2074e-01],\n",
      "        [1.7785e-02, 7.9115e-04, 5.6811e-03, 2.9536e-03, 2.7830e-01, 1.5152e-02,\n",
      "         4.5703e-02, 3.7217e-02, 1.5485e-01, 4.4157e-01],\n",
      "        [1.4563e-03, 1.6361e-04, 1.4253e-03, 1.3886e-03, 2.1867e-01, 1.9614e-02,\n",
      "         8.7073e-02, 5.7047e-02, 2.6083e-01, 3.5233e-01],\n",
      "        [6.0367e-03, 3.2453e-04, 2.8977e-03, 2.1723e-03, 2.1513e-01, 1.8129e-02,\n",
      "         6.3604e-02, 4.6834e-02, 2.1306e-01, 4.3181e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.90, Train Loss: 0.00, Val Loss: 5.17, Train BLEU: 0.00, Val BLEU: 5.53, Minutes Elapsed: 106.26\n",
      "Sampling from val predictions...\n",
      "Source: 在 这个 世界 上有 <UNK> 大类 <UNK> <EOS> <PAD> <PAD>\n",
      "Reference: so there &apos;s two types of <UNK> in this\n",
      "Model: <SOS> and &apos;s &apos;s a a of in . <EOS>\n",
      "Attention Weights: tensor([[0.0038, 0.5578, 0.0005, 0.1096, 0.0341, 0.2415, 0.0282, 0.0244, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0102, 0.5849, 0.0004, 0.0686, 0.0684, 0.1579, 0.0571, 0.0524, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0088, 0.3678, 0.0037, 0.1261, 0.1295, 0.1645, 0.1062, 0.0934, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0096, 0.1949, 0.0077, 0.1665, 0.1664, 0.1995, 0.1354, 0.1201, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0093, 0.0894, 0.0167, 0.2452, 0.1310, 0.3141, 0.1044, 0.0898, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0164, 0.0767, 0.0128, 0.2105, 0.1319, 0.3518, 0.1079, 0.0919, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0314, 0.1101, 0.0069, 0.1310, 0.1514, 0.2587, 0.1556, 0.1549, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0232, 0.0959, 0.0038, 0.1140, 0.1551, 0.2514, 0.1729, 0.1837, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0374, 0.0886, 0.0139, 0.1101, 0.1550, 0.2103, 0.1822, 0.2024, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 这 就是 我们 做 的 你 看到 的 就是 做\n",
      "Reference: so that &apos;s what we did here . you\n",
      "Model: <SOS> this &apos;s &apos;s what we &apos;re to do we\n",
      "Attention Weights: tensor([[0.9744, 0.0000, 0.0170, 0.0001, 0.0005, 0.0046, 0.0000, 0.0002, 0.0003,\n",
      "         0.0028],\n",
      "        [0.4892, 0.0033, 0.3359, 0.0037, 0.0163, 0.0724, 0.0008, 0.0040, 0.0044,\n",
      "         0.0699],\n",
      "        [0.1607, 0.0082, 0.3378, 0.0181, 0.0510, 0.1186, 0.0048, 0.0173, 0.0195,\n",
      "         0.2639],\n",
      "        [0.0942, 0.0090, 0.3289, 0.0269, 0.0597, 0.1038, 0.0097, 0.0231, 0.0259,\n",
      "         0.3188],\n",
      "        [0.0437, 0.0078, 0.2037, 0.0437, 0.1053, 0.2292, 0.0174, 0.0370, 0.0401,\n",
      "         0.2723],\n",
      "        [0.0374, 0.0074, 0.1788, 0.0431, 0.1272, 0.2934, 0.0129, 0.0336, 0.0372,\n",
      "         0.2290],\n",
      "        [0.0282, 0.0016, 0.0929, 0.0209, 0.0809, 0.2891, 0.0086, 0.0276, 0.0299,\n",
      "         0.4204],\n",
      "        [0.0026, 0.0017, 0.0470, 0.0349, 0.0965, 0.3023, 0.0251, 0.0575, 0.0555,\n",
      "         0.3771],\n",
      "        [0.0013, 0.0008, 0.0254, 0.0300, 0.0926, 0.3121, 0.0333, 0.0833, 0.0857,\n",
      "         0.3354]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.00, Train Loss: 0.00, Val Loss: 5.16, Train BLEU: 0.00, Val BLEU: 5.95, Minutes Elapsed: 111.96\n",
      "Sampling from val predictions...\n",
      "Source: 超过 70 的 家庭 家庭暴力 暴力 谋杀 发生 生在 受害\n",
      "Reference: over 70 percent of domestic violence murders happen after\n",
      "Model: <SOS> they , , of the , , the the\n",
      "Attention Weights: tensor([[0.9852, 0.0042, 0.0003, 0.0000, 0.0001, 0.0003, 0.0003, 0.0006, 0.0011,\n",
      "         0.0079],\n",
      "        [0.7484, 0.0794, 0.0090, 0.0006, 0.0020, 0.0042, 0.0035, 0.0063, 0.0129,\n",
      "         0.1338],\n",
      "        [0.0394, 0.3571, 0.0838, 0.0033, 0.0115, 0.0168, 0.0114, 0.0180, 0.0362,\n",
      "         0.4225],\n",
      "        [0.1655, 0.0345, 0.0158, 0.0009, 0.0072, 0.0163, 0.0130, 0.0357, 0.0799,\n",
      "         0.6314],\n",
      "        [0.0317, 0.0416, 0.0204, 0.0015, 0.0092, 0.0154, 0.0107, 0.0285, 0.0640,\n",
      "         0.7770],\n",
      "        [0.0111, 0.0063, 0.0039, 0.0003, 0.0033, 0.0148, 0.0134, 0.0485, 0.1089,\n",
      "         0.7894],\n",
      "        [0.0062, 0.0062, 0.0023, 0.0003, 0.0041, 0.0162, 0.0157, 0.0592, 0.1275,\n",
      "         0.7622],\n",
      "        [0.0017, 0.0030, 0.0014, 0.0002, 0.0025, 0.0156, 0.0163, 0.0644, 0.1361,\n",
      "         0.7587],\n",
      "        [0.0001, 0.0016, 0.0005, 0.0001, 0.0015, 0.0119, 0.0148, 0.0568, 0.1229,\n",
      "         0.7898]])\n",
      "\n",
      "Source: 谢谢 掌声 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank you . <EOS> you . <EOS> . <EOS>\n",
      "Attention Weights: tensor([[0.0022, 0.8440, 0.1537, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0161, 0.6803, 0.3037, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1494, 0.5248, 0.3258, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1772, 0.4198, 0.4030, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1153, 0.4217, 0.4630, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1546, 0.3748, 0.4706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1662, 0.3762, 0.4576, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1554, 0.3361, 0.5086, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1563, 0.3408, 0.5029, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.15, Train Loss: 0.00, Val Loss: 5.16, Train BLEU: 0.00, Val BLEU: 5.45, Minutes Elapsed: 120.46\n",
      "Sampling from val predictions...\n",
      "Source: 你 会 好奇 是否 会 有效 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: and now you &apos;re of course curious if it\n",
      "Model: <SOS> you you you you to to , <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9997, 0.0001, 0.0000, 0.0000, 0.0002, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9877, 0.0034, 0.0004, 0.0003, 0.0047, 0.0019, 0.0016, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2873, 0.1340, 0.0922, 0.0554, 0.1384, 0.1335, 0.1592, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2339, 0.1010, 0.0896, 0.0673, 0.1587, 0.1719, 0.1777, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2686, 0.0794, 0.0610, 0.0562, 0.1788, 0.1778, 0.1782, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0041, 0.0171, 0.1329, 0.1647, 0.3567, 0.2137, 0.1106, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0122, 0.0293, 0.1110, 0.1422, 0.3650, 0.2053, 0.1350, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0077, 0.0114, 0.0450, 0.0982, 0.4410, 0.2477, 0.1489, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0321, 0.0207, 0.0337, 0.0686, 0.3965, 0.2629, 0.1856, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 所以 当 那个 中国 警官 快 检查 查到 我家 家人\n",
      "Reference: as the chinese officer approached my family , i\n",
      "Model: <SOS> so when , , , the the , ,\n",
      "Attention Weights: tensor([[0.0233, 0.0318, 0.0056, 0.0105, 0.0120, 0.0142, 0.0140, 0.0219, 0.1489,\n",
      "         0.7177],\n",
      "        [0.0412, 0.1766, 0.0442, 0.0321, 0.0249, 0.0267, 0.0203, 0.0298, 0.0992,\n",
      "         0.5048],\n",
      "        [0.0112, 0.1303, 0.0708, 0.0298, 0.0121, 0.0123, 0.0056, 0.0115, 0.0491,\n",
      "         0.6672],\n",
      "        [0.0065, 0.0895, 0.0570, 0.0299, 0.0120, 0.0114, 0.0059, 0.0101, 0.0400,\n",
      "         0.7376],\n",
      "        [0.0168, 0.1569, 0.0701, 0.0479, 0.0231, 0.0193, 0.0109, 0.0171, 0.0581,\n",
      "         0.5797],\n",
      "        [0.0060, 0.0533, 0.0200, 0.0207, 0.0142, 0.0114, 0.0064, 0.0114, 0.0531,\n",
      "         0.8036],\n",
      "        [0.0012, 0.0094, 0.0052, 0.0120, 0.0097, 0.0081, 0.0044, 0.0103, 0.0961,\n",
      "         0.8436],\n",
      "        [0.0001, 0.0010, 0.0019, 0.0187, 0.0266, 0.0244, 0.0201, 0.0353, 0.1837,\n",
      "         0.6881],\n",
      "        [0.0000, 0.0002, 0.0006, 0.0086, 0.0200, 0.0187, 0.0230, 0.0456, 0.2072,\n",
      "         0.6762]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.45, Train Loss: 0.00, Val Loss: 5.09, Train BLEU: 0.00, Val BLEU: 6.28, Minutes Elapsed: 137.57\n",
      "Sampling from val predictions...\n",
      "Source: 我 想来 分享 几个 有用 的 方法 告诉 你们 如何\n",
      "Reference: i want to share a few keys on how\n",
      "Model: <SOS> i i &apos;t talk you you of of you\n",
      "Attention Weights: tensor([[9.9901e-01, 4.2356e-06, 2.0018e-05, 5.1257e-05, 1.8826e-05, 2.8452e-05,\n",
      "         4.9210e-05, 6.2292e-05, 6.1820e-04, 1.4020e-04],\n",
      "        [9.7254e-01, 1.8791e-03, 2.8316e-03, 4.1769e-03, 9.8110e-04, 1.4111e-03,\n",
      "         1.2891e-03, 1.0200e-03, 7.6350e-03, 6.2367e-03],\n",
      "        [3.0915e-01, 5.0041e-02, 1.7342e-01, 1.3195e-01, 3.5661e-02, 3.1244e-02,\n",
      "         2.7190e-02, 2.2105e-02, 4.8850e-02, 1.7038e-01],\n",
      "        [7.0002e-02, 3.0114e-02, 1.7410e-01, 2.3377e-01, 7.9726e-02, 6.1836e-02,\n",
      "         5.5285e-02, 3.7666e-02, 9.3483e-02, 1.6402e-01],\n",
      "        [1.7606e-02, 4.4573e-03, 7.5887e-02, 3.2848e-01, 9.9809e-02, 4.9348e-02,\n",
      "         5.8683e-02, 2.7117e-02, 8.3174e-02, 2.5544e-01],\n",
      "        [1.5078e-02, 6.7567e-03, 7.0405e-02, 2.3901e-01, 1.0592e-01, 7.9504e-02,\n",
      "         7.9640e-02, 5.1214e-02, 1.1616e-01, 2.3631e-01],\n",
      "        [7.6181e-03, 6.2971e-04, 1.0859e-02, 6.9425e-02, 7.7634e-02, 9.3291e-02,\n",
      "         1.2734e-01, 1.0380e-01, 3.4134e-01, 1.6807e-01],\n",
      "        [8.6880e-03, 7.3303e-04, 7.7179e-03, 4.4528e-02, 5.8855e-02, 7.3152e-02,\n",
      "         1.2588e-01, 1.1837e-01, 3.9892e-01, 1.6315e-01],\n",
      "        [1.0865e-02, 6.1362e-04, 5.9229e-03, 3.7406e-02, 5.6756e-02, 7.4078e-02,\n",
      "         1.2669e-01, 1.1779e-01, 3.9916e-01, 1.7072e-01]])\n",
      "\n",
      "Source: 我 无法 否认 家里 的确 确有 过 艰难 的 时候\n",
      "Reference: now , i cannot deny that there have been\n",
      "Model: <SOS> i i i i to that that of a\n",
      "Attention Weights: tensor([[0.9987, 0.0000, 0.0003, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0001,\n",
      "         0.0006],\n",
      "        [0.9653, 0.0026, 0.0126, 0.0006, 0.0029, 0.0016, 0.0009, 0.0010, 0.0023,\n",
      "         0.0103],\n",
      "        [0.1982, 0.0481, 0.3425, 0.0275, 0.0456, 0.0232, 0.0141, 0.0138, 0.0288,\n",
      "         0.2583],\n",
      "        [0.2505, 0.0290, 0.2502, 0.0227, 0.0473, 0.0244, 0.0144, 0.0141, 0.0339,\n",
      "         0.3135],\n",
      "        [0.2166, 0.0302, 0.2719, 0.0296, 0.0549, 0.0272, 0.0153, 0.0157, 0.0348,\n",
      "         0.3037],\n",
      "        [0.0181, 0.0299, 0.3087, 0.0940, 0.1430, 0.0595, 0.0308, 0.0307, 0.0471,\n",
      "         0.2381],\n",
      "        [0.0130, 0.0061, 0.1614, 0.0509, 0.1842, 0.0657, 0.0333, 0.0353, 0.0688,\n",
      "         0.3812],\n",
      "        [0.0061, 0.0014, 0.0455, 0.0229, 0.1709, 0.0780, 0.0721, 0.0819, 0.1444,\n",
      "         0.3769],\n",
      "        [0.0047, 0.0005, 0.0122, 0.0107, 0.0770, 0.0539, 0.0705, 0.0796, 0.2002,\n",
      "         0.4906]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.60, Train Loss: 0.00, Val Loss: 5.06, Train BLEU: 0.00, Val BLEU: 5.00, Minutes Elapsed: 146.16\n",
      "Sampling from val predictions...\n",
      "Source: 我们 怎么 怎么样 能够 从 邻里 <UNK> <UNK> <UNK> 更多\n",
      "Reference: how can we lend and borrow more things without\n",
      "Model: <SOS> we we we do the <UNK> , the ,\n",
      "Attention Weights: tensor([[0.9987, 0.0001, 0.0002, 0.0000, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001,\n",
      "         0.0003],\n",
      "        [0.9736, 0.0015, 0.0072, 0.0005, 0.0019, 0.0038, 0.0027, 0.0025, 0.0015,\n",
      "         0.0047],\n",
      "        [0.1050, 0.0792, 0.2062, 0.0450, 0.0755, 0.0885, 0.1031, 0.0794, 0.0652,\n",
      "         0.1530],\n",
      "        [0.1328, 0.0396, 0.0867, 0.0312, 0.1244, 0.1438, 0.0885, 0.0695, 0.0522,\n",
      "         0.2313],\n",
      "        [0.0159, 0.0050, 0.0639, 0.0220, 0.1186, 0.3059, 0.1094, 0.0759, 0.0394,\n",
      "         0.2439],\n",
      "        [0.0123, 0.0111, 0.0605, 0.0429, 0.1262, 0.2629, 0.1123, 0.0854, 0.0487,\n",
      "         0.2377],\n",
      "        [0.0045, 0.0021, 0.0055, 0.0076, 0.0951, 0.1766, 0.1023, 0.1052, 0.0644,\n",
      "         0.4368],\n",
      "        [0.0103, 0.0044, 0.0060, 0.0093, 0.0747, 0.1703, 0.1055, 0.1131, 0.0837,\n",
      "         0.4230],\n",
      "        [0.0035, 0.0009, 0.0015, 0.0031, 0.0354, 0.1095, 0.0724, 0.0933, 0.0714,\n",
      "         0.6089]])\n",
      "\n",
      "Source: 包厢 餐厅 <UNK> 业内 被 人 熟知 是 强迫 卖淫\n",
      "Reference: cabin restaurants , as they &apos;re known in the\n",
      "Model: <SOS> and , a a , , , , ,\n",
      "Attention Weights: tensor([[0.0321, 0.0047, 0.0079, 0.0440, 0.0369, 0.0608, 0.2718, 0.0532, 0.0877,\n",
      "         0.4009],\n",
      "        [0.0956, 0.0242, 0.0342, 0.1068, 0.0503, 0.0743, 0.1910, 0.0420, 0.0637,\n",
      "         0.3179],\n",
      "        [0.0670, 0.1058, 0.0686, 0.0810, 0.0389, 0.0620, 0.0716, 0.0318, 0.0394,\n",
      "         0.4339],\n",
      "        [0.0090, 0.0084, 0.0099, 0.1490, 0.0611, 0.1025, 0.2309, 0.0345, 0.0324,\n",
      "         0.3623],\n",
      "        [0.0013, 0.0018, 0.0029, 0.0532, 0.0472, 0.1152, 0.3022, 0.0455, 0.0379,\n",
      "         0.3927],\n",
      "        [0.0055, 0.0092, 0.0088, 0.0624, 0.0544, 0.0990, 0.1411, 0.0572, 0.0534,\n",
      "         0.5090],\n",
      "        [0.0057, 0.0107, 0.0086, 0.0640, 0.0455, 0.0876, 0.1253, 0.0428, 0.0364,\n",
      "         0.5734],\n",
      "        [0.0001, 0.0006, 0.0011, 0.0085, 0.0343, 0.1289, 0.1589, 0.0740, 0.0511,\n",
      "         0.5426],\n",
      "        [0.0001, 0.0003, 0.0007, 0.0071, 0.0371, 0.1247, 0.2235, 0.0803, 0.0618,\n",
      "         0.4645]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.75, Train Loss: 0.00, Val Loss: 5.04, Train BLEU: 0.00, Val BLEU: 6.53, Minutes Elapsed: 154.82\n",
      "Sampling from val predictions...\n",
      "Source: 我 为什么 什么 没有 逃跑 我 有 很多 的 机会\n",
      "Reference: why didn &apos;t i walk out ? i could\n",
      "Model: <SOS> i i i do me to my i i\n",
      "Attention Weights: tensor([[9.9941e-01, 5.3388e-06, 6.7444e-07, 2.7541e-06, 7.4000e-06, 5.2128e-04,\n",
      "         1.1426e-06, 3.9001e-06, 2.5536e-06, 4.3526e-05],\n",
      "        [9.9050e-01, 2.5457e-03, 2.8763e-04, 3.1367e-04, 2.4028e-04, 4.2302e-03,\n",
      "         4.1092e-05, 1.5099e-04, 1.0500e-04, 1.5875e-03],\n",
      "        [4.7551e-01, 8.8900e-02, 8.2968e-02, 6.4983e-02, 3.5706e-02, 5.3801e-02,\n",
      "         9.2906e-03, 1.2379e-02, 1.2192e-02, 1.6427e-01],\n",
      "        [4.4274e-01, 4.3807e-02, 4.8593e-02, 7.7606e-02, 5.1353e-02, 9.3002e-02,\n",
      "         9.9789e-03, 1.8638e-02, 1.4496e-02, 1.9979e-01],\n",
      "        [5.3346e-02, 1.1932e-02, 2.8562e-02, 1.2509e-01, 1.0819e-01, 3.5157e-01,\n",
      "         1.6400e-02, 2.9485e-02, 1.2678e-02, 2.6276e-01],\n",
      "        [3.7550e-02, 8.0183e-03, 2.4904e-02, 1.0940e-01, 1.4643e-01, 3.4662e-01,\n",
      "         2.9885e-02, 4.3608e-02, 2.1302e-02, 2.3228e-01],\n",
      "        [1.2327e-02, 1.0744e-02, 2.5109e-02, 8.2991e-02, 1.1029e-01, 4.0735e-01,\n",
      "         3.7556e-02, 5.8199e-02, 2.5729e-02, 2.2971e-01],\n",
      "        [5.2886e-03, 1.2762e-03, 2.7463e-03, 2.2003e-02, 8.0987e-02, 5.2602e-01,\n",
      "         3.6239e-02, 9.5601e-02, 3.5052e-02, 1.9479e-01],\n",
      "        [3.8480e-03, 7.0428e-04, 1.8172e-03, 1.4714e-02, 5.7574e-02, 3.6143e-01,\n",
      "         7.8742e-02, 1.7188e-01, 7.1710e-02, 2.3757e-01]])\n",
      "\n",
      "Source: 如果 有 任何 <UNK> 我 就 会 入狱 并 被\n",
      "Reference: if anything seemed unnatural , i could be imprisoned\n",
      "Model: <SOS> if , &apos;s the , , i &apos;t to\n",
      "Attention Weights: tensor([[0.0002, 0.0026, 0.0039, 0.0082, 0.7374, 0.0076, 0.0863, 0.0079, 0.0098,\n",
      "         0.1362],\n",
      "        [0.0046, 0.0791, 0.0858, 0.1113, 0.4957, 0.0169, 0.0592, 0.0079, 0.0131,\n",
      "         0.1262],\n",
      "        [0.0119, 0.0781, 0.1891, 0.0971, 0.1484, 0.0375, 0.0649, 0.0245, 0.0392,\n",
      "         0.3091],\n",
      "        [0.0008, 0.0073, 0.0545, 0.0348, 0.3042, 0.0236, 0.0708, 0.0181, 0.0214,\n",
      "         0.4645],\n",
      "        [0.0007, 0.0036, 0.0240, 0.0226, 0.5415, 0.0236, 0.1098, 0.0205, 0.0208,\n",
      "         0.2330],\n",
      "        [0.0003, 0.0010, 0.0049, 0.0067, 0.2450, 0.0537, 0.2344, 0.0923, 0.0651,\n",
      "         0.2966],\n",
      "        [0.0000, 0.0002, 0.0005, 0.0011, 0.2702, 0.0232, 0.3003, 0.0382, 0.0566,\n",
      "         0.3097],\n",
      "        [0.0000, 0.0001, 0.0007, 0.0013, 0.1592, 0.0234, 0.2059, 0.0320, 0.0557,\n",
      "         0.5216],\n",
      "        [0.0000, 0.0001, 0.0009, 0.0012, 0.0625, 0.0390, 0.1569, 0.1428, 0.1199,\n",
      "         0.4766]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.90, Train Loss: 0.00, Val Loss: 5.02, Train BLEU: 0.00, Val BLEU: 6.64, Minutes Elapsed: 163.46\n",
      "Sampling from val predictions...\n",
      "Source: 有 一天 晚上 我 拿 着手 手电 手电筒 电筒 在\n",
      "Reference: so one night , i was walking around the\n",
      "Model: <SOS> i &apos;s &apos;s i , was to to the\n",
      "Attention Weights: tensor([[0.0093, 0.1456, 0.0002, 0.8019, 0.0040, 0.0005, 0.0005, 0.0005, 0.0006,\n",
      "         0.0370],\n",
      "        [0.0609, 0.5283, 0.0040, 0.3532, 0.0054, 0.0006, 0.0011, 0.0012, 0.0013,\n",
      "         0.0440],\n",
      "        [0.0556, 0.1836, 0.0217, 0.2746, 0.0633, 0.0130, 0.0149, 0.0150, 0.0173,\n",
      "         0.3410],\n",
      "        [0.0310, 0.1327, 0.0164, 0.4473, 0.0653, 0.0106, 0.0087, 0.0083, 0.0096,\n",
      "         0.2703],\n",
      "        [0.0153, 0.0505, 0.0088, 0.5354, 0.1007, 0.0259, 0.0167, 0.0166, 0.0199,\n",
      "         0.2101],\n",
      "        [0.0059, 0.0155, 0.0034, 0.3992, 0.1353, 0.0414, 0.0258, 0.0273, 0.0304,\n",
      "         0.3158],\n",
      "        [0.0028, 0.0107, 0.0014, 0.2154, 0.0959, 0.0524, 0.0294, 0.0338, 0.0419,\n",
      "         0.5162],\n",
      "        [0.0003, 0.0005, 0.0004, 0.0228, 0.0601, 0.0465, 0.0418, 0.0530, 0.0571,\n",
      "         0.7175],\n",
      "        [0.0004, 0.0010, 0.0007, 0.0225, 0.0427, 0.0560, 0.0437, 0.0648, 0.0998,\n",
      "         0.6685]])\n",
      "\n",
      "Source: 为什么 什么 因为 西红柿 <UNK> 了 并且 光 在 西红柿\n",
      "Reference: why is that ? because the tomato is actually\n",
      "Model: <SOS> why is because ? ? ? is is in\n",
      "Attention Weights: tensor([[0.0980, 0.0440, 0.2326, 0.0781, 0.0041, 0.0179, 0.0291, 0.0775, 0.0557,\n",
      "         0.3630],\n",
      "        [0.1985, 0.2020, 0.2452, 0.1144, 0.0162, 0.0158, 0.0147, 0.0250, 0.0215,\n",
      "         0.1467],\n",
      "        [0.0337, 0.0860, 0.5183, 0.0437, 0.0322, 0.0161, 0.0108, 0.0189, 0.0280,\n",
      "         0.2121],\n",
      "        [0.0109, 0.0261, 0.2882, 0.0975, 0.0465, 0.0365, 0.0226, 0.0400, 0.0509,\n",
      "         0.3808],\n",
      "        [0.0219, 0.0459, 0.2875, 0.1369, 0.0559, 0.0465, 0.0387, 0.0421, 0.0527,\n",
      "         0.2718],\n",
      "        [0.0072, 0.0152, 0.1322, 0.0791, 0.0441, 0.0551, 0.0530, 0.0787, 0.0948,\n",
      "         0.4406],\n",
      "        [0.0031, 0.0066, 0.0926, 0.0384, 0.0271, 0.0549, 0.0434, 0.0889, 0.1177,\n",
      "         0.5274],\n",
      "        [0.0008, 0.0015, 0.0446, 0.0208, 0.0122, 0.0410, 0.0320, 0.0885, 0.1312,\n",
      "         0.6275],\n",
      "        [0.0011, 0.0029, 0.0979, 0.0364, 0.0201, 0.0676, 0.0757, 0.1283, 0.1601,\n",
      "         0.4098]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.00, Train Loss: 0.00, Val Loss: 5.01, Train BLEU: 0.00, Val BLEU: 6.44, Minutes Elapsed: 169.29\n",
      "Sampling from val predictions...\n",
      "Source: 我 获得 了 乘飞机 飞机 来 ted 的 机会 这\n",
      "Reference: i got a chance to come by plane for\n",
      "Model: <SOS> i i that a of i the the and\n",
      "Attention Weights: tensor([[0.9956, 0.0002, 0.0001, 0.0000, 0.0000, 0.0007, 0.0000, 0.0001, 0.0002,\n",
      "         0.0030],\n",
      "        [0.9440, 0.0264, 0.0046, 0.0021, 0.0003, 0.0026, 0.0004, 0.0006, 0.0008,\n",
      "         0.0182],\n",
      "        [0.1809, 0.1943, 0.2517, 0.0519, 0.0105, 0.0402, 0.0073, 0.0102, 0.0118,\n",
      "         0.2412],\n",
      "        [0.0155, 0.0564, 0.4922, 0.1593, 0.0177, 0.0924, 0.0094, 0.0079, 0.0088,\n",
      "         0.1403],\n",
      "        [0.0097, 0.0186, 0.1693, 0.0927, 0.0267, 0.2211, 0.0136, 0.0198, 0.0286,\n",
      "         0.4000],\n",
      "        [0.0153, 0.0153, 0.0569, 0.0605, 0.0221, 0.2575, 0.0117, 0.0301, 0.0555,\n",
      "         0.4750],\n",
      "        [0.0057, 0.0039, 0.0263, 0.0205, 0.0042, 0.1404, 0.0058, 0.0194, 0.0376,\n",
      "         0.7362],\n",
      "        [0.0039, 0.0068, 0.0836, 0.0598, 0.0230, 0.2188, 0.0190, 0.0364, 0.0566,\n",
      "         0.4921],\n",
      "        [0.0022, 0.0013, 0.0115, 0.0123, 0.0088, 0.1998, 0.0133, 0.0442, 0.0888,\n",
      "         0.6178]])\n",
      "\n",
      "Source: 我 告诉 你们 一个 秘密 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: let me tell you a secret . <EOS> <PAD>\n",
      "Model: <SOS> i i show you a . . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9988, 0.0000, 0.0011, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9667, 0.0002, 0.0319, 0.0003, 0.0004, 0.0005, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1058, 0.0424, 0.7748, 0.0228, 0.0231, 0.0311, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0222, 0.0110, 0.6218, 0.1592, 0.1015, 0.0843, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0235, 0.0158, 0.3782, 0.2269, 0.2225, 0.1331, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0225, 0.0266, 0.3441, 0.2435, 0.2457, 0.1176, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0143, 0.0358, 0.1333, 0.2718, 0.3890, 0.1558, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0303, 0.0578, 0.1793, 0.1974, 0.3495, 0.1856, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0372, 0.0489, 0.1143, 0.2116, 0.3555, 0.2324, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.15, Train Loss: 0.00, Val Loss: 5.04, Train BLEU: 0.00, Val BLEU: 6.69, Minutes Elapsed: 177.88\n",
      "Sampling from val predictions...\n",
      "Source: 食物 是 问题 食物 也 是 解决 解决方案 方案 <EOS>\n",
      "Reference: food is the problem and food is the solution\n",
      "Model: <SOS> and is is , of the . . .\n",
      "Attention Weights: tensor([[0.0092, 0.0250, 0.4062, 0.0135, 0.2026, 0.0670, 0.0823, 0.0247, 0.1084,\n",
      "         0.0611],\n",
      "        [0.1167, 0.1025, 0.4341, 0.0274, 0.0638, 0.0261, 0.0370, 0.0255, 0.0878,\n",
      "         0.0791],\n",
      "        [0.0363, 0.0668, 0.3231, 0.0211, 0.0502, 0.0376, 0.0580, 0.0343, 0.1349,\n",
      "         0.2376],\n",
      "        [0.0035, 0.0266, 0.5686, 0.0280, 0.1299, 0.0579, 0.0574, 0.0172, 0.0547,\n",
      "         0.0563],\n",
      "        [0.0054, 0.0134, 0.3973, 0.0480, 0.1162, 0.0543, 0.0877, 0.0415, 0.1017,\n",
      "         0.1346],\n",
      "        [0.0183, 0.0214, 0.2763, 0.0484, 0.1068, 0.0453, 0.0782, 0.0460, 0.1265,\n",
      "         0.2327],\n",
      "        [0.0069, 0.0124, 0.0794, 0.0145, 0.1855, 0.0538, 0.0954, 0.0459, 0.1591,\n",
      "         0.3471],\n",
      "        [0.0048, 0.0119, 0.0833, 0.0205, 0.1729, 0.0753, 0.1258, 0.0566, 0.1573,\n",
      "         0.2916],\n",
      "        [0.0016, 0.0054, 0.0638, 0.0148, 0.2006, 0.0965, 0.1751, 0.0686, 0.1828,\n",
      "         0.1907]])\n",
      "\n",
      "Source: 照片 是 一个 提醒 和 <UNK> 提醒 我们 一些 人\n",
      "Reference: a photo is a reminder of someone or something\n",
      "Model: <SOS> and is is a , , , , ,\n",
      "Attention Weights: tensor([[0.0046, 0.0098, 0.0098, 0.0042, 0.0352, 0.0068, 0.0197, 0.5480, 0.0076,\n",
      "         0.3543],\n",
      "        [0.2537, 0.0792, 0.1362, 0.0429, 0.0960, 0.0288, 0.0186, 0.1214, 0.0069,\n",
      "         0.2163],\n",
      "        [0.1416, 0.0958, 0.0976, 0.0496, 0.0776, 0.0644, 0.0209, 0.0464, 0.0162,\n",
      "         0.3897],\n",
      "        [0.0969, 0.0792, 0.1517, 0.0881, 0.1164, 0.0705, 0.0391, 0.0698, 0.0106,\n",
      "         0.2778],\n",
      "        [0.0125, 0.0263, 0.2269, 0.1076, 0.1771, 0.0697, 0.0382, 0.0998, 0.0063,\n",
      "         0.2357],\n",
      "        [0.0033, 0.0053, 0.1387, 0.0958, 0.1725, 0.0526, 0.0612, 0.1915, 0.0073,\n",
      "         0.2718],\n",
      "        [0.0037, 0.0025, 0.0362, 0.0479, 0.0945, 0.0307, 0.0692, 0.3474, 0.0130,\n",
      "         0.3549],\n",
      "        [0.0016, 0.0011, 0.0169, 0.0169, 0.0583, 0.0198, 0.0426, 0.4014, 0.0073,\n",
      "         0.4342],\n",
      "        [0.0014, 0.0012, 0.0137, 0.0194, 0.0577, 0.0221, 0.0670, 0.3638, 0.0166,\n",
      "         0.4372]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.30, Train Loss: 0.00, Val Loss: 5.02, Train BLEU: 0.00, Val BLEU: 6.98, Minutes Elapsed: 186.53\n",
      "Sampling from val predictions...\n",
      "Source: 而且 实际 实际上 这 全 是因为 因为 一个 勇敢 勇敢的人\n",
      "Reference: and this was actually all down to the bravery\n",
      "Model: <SOS> and , fact actually a because because the ,\n",
      "Attention Weights: tensor([[0.0042, 0.0077, 0.0021, 0.5701, 0.0209, 0.0030, 0.2465, 0.0172, 0.0078,\n",
      "         0.1206],\n",
      "        [0.0550, 0.0761, 0.0626, 0.5708, 0.0352, 0.0111, 0.0783, 0.0178, 0.0078,\n",
      "         0.0853],\n",
      "        [0.0257, 0.0886, 0.0436, 0.3280, 0.0817, 0.0263, 0.1512, 0.0302, 0.0199,\n",
      "         0.2049],\n",
      "        [0.0182, 0.0615, 0.0374, 0.3432, 0.0827, 0.0263, 0.1657, 0.0272, 0.0156,\n",
      "         0.2222],\n",
      "        [0.0050, 0.0113, 0.0184, 0.2269, 0.1065, 0.0354, 0.3964, 0.0302, 0.0198,\n",
      "         0.1500],\n",
      "        [0.0029, 0.0064, 0.0065, 0.1704, 0.0725, 0.0308, 0.4755, 0.0428, 0.0209,\n",
      "         0.1713],\n",
      "        [0.0009, 0.0022, 0.0016, 0.1451, 0.0509, 0.0193, 0.3494, 0.1041, 0.0295,\n",
      "         0.2969],\n",
      "        [0.0002, 0.0005, 0.0003, 0.0566, 0.0262, 0.0114, 0.2766, 0.1629, 0.0630,\n",
      "         0.4022],\n",
      "        [0.0004, 0.0010, 0.0005, 0.0991, 0.0253, 0.0113, 0.3194, 0.1538, 0.0501,\n",
      "         0.3391]])\n",
      "\n",
      "Source: 他们 知道 我们 的 住处 吗 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: do they know where we live ? <EOS> <PAD>\n",
      "Model: <SOS> they know know us ? &apos;re . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9877, 0.0005, 0.0057, 0.0006, 0.0017, 0.0024, 0.0015, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9580, 0.0020, 0.0225, 0.0022, 0.0038, 0.0057, 0.0059, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1447, 0.0800, 0.3108, 0.0685, 0.0704, 0.1642, 0.1615, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0173, 0.0195, 0.6538, 0.0619, 0.1191, 0.0733, 0.0551, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0097, 0.0126, 0.5022, 0.0848, 0.2297, 0.0894, 0.0715, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0267, 0.0203, 0.2269, 0.0999, 0.2925, 0.1794, 0.1543, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0185, 0.0167, 0.2839, 0.1047, 0.2788, 0.1495, 0.1479, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0108, 0.0192, 0.1533, 0.1338, 0.3835, 0.1898, 0.1098, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0590, 0.0403, 0.1713, 0.0858, 0.2701, 0.1880, 0.1855, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.45, Train Loss: 0.00, Val Loss: 4.97, Train BLEU: 0.00, Val BLEU: 6.72, Minutes Elapsed: 195.16\n",
      "Sampling from val predictions...\n",
      "Source: 和 <UNK> 还有 <UNK> 一起 合作 我们 希望 记录 录下\n",
      "Reference: with <UNK> and <UNK> , we &apos;re aiming to\n",
      "Model: <SOS> and <UNK> , , , we we the to\n",
      "Attention Weights: tensor([[0.0062, 0.0033, 0.0234, 0.0056, 0.0393, 0.4646, 0.3402, 0.0050, 0.0142,\n",
      "         0.0983],\n",
      "        [0.0470, 0.0528, 0.1054, 0.0335, 0.0680, 0.3386, 0.3010, 0.0019, 0.0066,\n",
      "         0.0452],\n",
      "        [0.0533, 0.0406, 0.2349, 0.0644, 0.1375, 0.1810, 0.0687, 0.0034, 0.0151,\n",
      "         0.2013],\n",
      "        [0.0032, 0.0052, 0.0461, 0.0184, 0.1394, 0.5168, 0.1898, 0.0049, 0.0100,\n",
      "         0.0662],\n",
      "        [0.0004, 0.0010, 0.0082, 0.0056, 0.0487, 0.5051, 0.4027, 0.0021, 0.0063,\n",
      "         0.0199],\n",
      "        [0.0011, 0.0017, 0.0122, 0.0089, 0.0545, 0.3561, 0.3812, 0.0219, 0.0334,\n",
      "         0.1290],\n",
      "        [0.0002, 0.0004, 0.0030, 0.0022, 0.0156, 0.2104, 0.6142, 0.0127, 0.0391,\n",
      "         0.1021],\n",
      "        [0.0010, 0.0012, 0.0082, 0.0059, 0.0283, 0.1660, 0.2316, 0.0400, 0.1173,\n",
      "         0.4005],\n",
      "        [0.0002, 0.0003, 0.0007, 0.0014, 0.0037, 0.0356, 0.0687, 0.0526, 0.3055,\n",
      "         0.5313]])\n",
      "\n",
      "Source: 除了 这个 伤 以外 他 还 患有 肺结核 结核 即使\n",
      "Reference: on top of that , <UNK> has tuberculosis ,\n",
      "Model: <SOS> and this way , , he &apos;s , ,\n",
      "Attention Weights: tensor([[3.0827e-04, 9.8168e-02, 3.2487e-04, 3.1812e-03, 8.8634e-01, 6.9877e-04,\n",
      "         2.0336e-03, 7.7918e-04, 1.0102e-03, 7.1548e-03],\n",
      "        [6.3372e-03, 7.5478e-01, 3.6888e-03, 1.5986e-02, 2.0672e-01, 1.2040e-03,\n",
      "         1.7101e-03, 1.4454e-03, 1.1596e-03, 6.9699e-03],\n",
      "        [1.8128e-02, 3.8509e-01, 4.8040e-02, 7.1291e-02, 2.3639e-01, 1.9747e-02,\n",
      "         2.2498e-02, 1.8691e-02, 1.8975e-02, 1.6115e-01],\n",
      "        [6.9747e-03, 1.8481e-01, 3.2390e-02, 1.4533e-01, 5.5754e-01, 6.8058e-03,\n",
      "         6.1045e-03, 6.0616e-03, 4.3495e-03, 4.9642e-02],\n",
      "        [1.2457e-03, 1.4315e-01, 1.1184e-02, 1.0628e-01, 7.0149e-01, 4.3210e-03,\n",
      "         4.5925e-03, 3.0876e-03, 2.7347e-03, 2.1914e-02],\n",
      "        [2.6406e-04, 5.4255e-02, 7.8707e-03, 6.5022e-02, 8.2190e-01, 6.6452e-03,\n",
      "         7.1271e-03, 3.4160e-03, 4.4312e-03, 2.9074e-02],\n",
      "        [1.2979e-03, 1.3880e-01, 1.6713e-02, 1.3565e-01, 5.6777e-01, 1.6288e-02,\n",
      "         1.5934e-02, 1.0863e-02, 1.0001e-02, 8.6681e-02],\n",
      "        [4.2236e-04, 8.0448e-02, 1.5547e-02, 5.7149e-02, 2.7493e-01, 6.5942e-02,\n",
      "         9.8559e-02, 4.4722e-02, 5.8713e-02, 3.0357e-01],\n",
      "        [9.2012e-06, 2.6459e-03, 4.5217e-04, 6.4173e-03, 6.9177e-01, 1.0766e-02,\n",
      "         5.1019e-02, 2.1506e-02, 5.3481e-02, 1.6193e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.60, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 6.32, Minutes Elapsed: 203.81\n",
      "Sampling from val predictions...\n",
      "Source: 在 这里 工作 的 人 通常 都 要 忍受 客人\n",
      "Reference: the workers here often endure tragic sexual abuse at\n",
      "Model: <SOS> in the of the the in the <EOS> ,\n",
      "Attention Weights: tensor([[0.0010, 0.0435, 0.0012, 0.0087, 0.0350, 0.0201, 0.0057, 0.3108, 0.0242,\n",
      "         0.5496],\n",
      "        [0.0047, 0.6843, 0.0171, 0.0211, 0.0365, 0.0199, 0.0045, 0.0347, 0.0059,\n",
      "         0.1712],\n",
      "        [0.0124, 0.3505, 0.0529, 0.0474, 0.0665, 0.0251, 0.0094, 0.0508, 0.0118,\n",
      "         0.3733],\n",
      "        [0.0128, 0.1636, 0.0748, 0.0893, 0.1522, 0.0654, 0.0207, 0.0755, 0.0180,\n",
      "         0.3278],\n",
      "        [0.0016, 0.0492, 0.0135, 0.0616, 0.1411, 0.0504, 0.0117, 0.1058, 0.0133,\n",
      "         0.5518],\n",
      "        [0.0010, 0.0234, 0.0091, 0.0581, 0.1601, 0.0474, 0.0191, 0.1565, 0.0281,\n",
      "         0.4973],\n",
      "        [0.0002, 0.0035, 0.0011, 0.0137, 0.0543, 0.0293, 0.0190, 0.2379, 0.0517,\n",
      "         0.5894],\n",
      "        [0.0001, 0.0012, 0.0004, 0.0047, 0.0187, 0.0141, 0.0143, 0.2726, 0.0755,\n",
      "         0.5984],\n",
      "        [0.0001, 0.0007, 0.0002, 0.0022, 0.0102, 0.0077, 0.0106, 0.2409, 0.0654,\n",
      "         0.6620]])\n",
      "\n",
      "Source: 每 一个 病例 的 情况 都 是 特殊 的 所以\n",
      "Reference: it manifests in each individual differently , hence why\n",
      "Model: <SOS> and the a the of , , <EOS> ,\n",
      "Attention Weights: tensor([[0.0022, 0.0037, 0.0047, 0.0097, 0.0317, 0.0078, 0.0421, 0.0826, 0.0282,\n",
      "         0.7872],\n",
      "        [0.0536, 0.1310, 0.1129, 0.0391, 0.0813, 0.0113, 0.0190, 0.0348, 0.0187,\n",
      "         0.4983],\n",
      "        [0.0683, 0.1759, 0.1262, 0.0536, 0.0538, 0.0161, 0.0303, 0.0281, 0.0273,\n",
      "         0.4205],\n",
      "        [0.0226, 0.1040, 0.1315, 0.0533, 0.0879, 0.0249, 0.0290, 0.0709, 0.0437,\n",
      "         0.4322],\n",
      "        [0.0161, 0.0461, 0.0394, 0.0489, 0.1035, 0.0338, 0.0547, 0.0766, 0.0613,\n",
      "         0.5195],\n",
      "        [0.0115, 0.0297, 0.0291, 0.0529, 0.1297, 0.0653, 0.1018, 0.0972, 0.0803,\n",
      "         0.4026],\n",
      "        [0.0097, 0.0182, 0.0178, 0.0350, 0.1126, 0.0525, 0.0936, 0.1318, 0.1028,\n",
      "         0.4260],\n",
      "        [0.0041, 0.0072, 0.0067, 0.0197, 0.0479, 0.0415, 0.0906, 0.1128, 0.1507,\n",
      "         0.5188],\n",
      "        [0.0015, 0.0043, 0.0027, 0.0064, 0.0146, 0.0199, 0.0632, 0.1008, 0.1638,\n",
      "         0.6229]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.75, Train Loss: 0.00, Val Loss: 4.94, Train BLEU: 0.00, Val BLEU: 7.19, Minutes Elapsed: 212.51\n",
      "Sampling from val predictions...\n",
      "Source: 当 狮子 晚上 走近 的 时候 看到 的 就是 这个\n",
      "Reference: and that &apos;s how it looks to lions when\n",
      "Model: <SOS> when the the when the the to , ,\n",
      "Attention Weights: tensor([[0.0683, 0.0036, 0.0015, 0.0102, 0.0072, 0.0094, 0.0131, 0.0225, 0.0148,\n",
      "         0.8494],\n",
      "        [0.5116, 0.1168, 0.0180, 0.0228, 0.0078, 0.0082, 0.0077, 0.0103, 0.0062,\n",
      "         0.2907],\n",
      "        [0.1928, 0.1163, 0.0770, 0.0554, 0.0378, 0.0212, 0.0144, 0.0246, 0.0135,\n",
      "         0.4470],\n",
      "        [0.1481, 0.1460, 0.0765, 0.0489, 0.0361, 0.0164, 0.0114, 0.0197, 0.0092,\n",
      "         0.4877],\n",
      "        [0.1543, 0.0438, 0.1033, 0.1245, 0.0406, 0.0367, 0.0208, 0.0227, 0.0148,\n",
      "         0.4385],\n",
      "        [0.1300, 0.0349, 0.0859, 0.1299, 0.0456, 0.0589, 0.0308, 0.0300, 0.0265,\n",
      "         0.4274],\n",
      "        [0.0205, 0.0109, 0.0206, 0.0595, 0.0375, 0.0488, 0.0478, 0.0514, 0.0333,\n",
      "         0.6697],\n",
      "        [0.0179, 0.0116, 0.0389, 0.1353, 0.0407, 0.0882, 0.0828, 0.0589, 0.0550,\n",
      "         0.4707],\n",
      "        [0.0081, 0.0045, 0.0109, 0.0736, 0.0354, 0.1006, 0.0961, 0.0930, 0.1080,\n",
      "         0.4697]])\n",
      "\n",
      "Source: 很 感谢 你 <UNK> 谢谢 掌声 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: thank you so much . thank you . <EOS>\n",
      "Model: <SOS> oh , . much . <EOS> . . <EOS>\n",
      "Attention Weights: tensor([[0.0065, 0.0450, 0.8259, 0.0070, 0.0084, 0.0904, 0.0169, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0328, 0.2475, 0.6789, 0.0066, 0.0020, 0.0177, 0.0146, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0359, 0.1715, 0.4140, 0.0774, 0.0363, 0.1200, 0.1449, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0338, 0.1415, 0.5582, 0.0644, 0.0458, 0.1222, 0.0342, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0086, 0.0421, 0.2282, 0.0758, 0.0876, 0.5043, 0.0532, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0058, 0.0208, 0.1447, 0.0723, 0.2906, 0.3880, 0.0778, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0112, 0.0177, 0.1539, 0.0835, 0.2085, 0.4111, 0.1141, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0059, 0.0131, 0.2306, 0.0573, 0.1704, 0.4117, 0.1110, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0143, 0.0361, 0.1229, 0.1139, 0.2993, 0.3467, 0.0669, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3.90, Train Loss: 0.00, Val Loss: 4.92, Train BLEU: 0.00, Val BLEU: 7.19, Minutes Elapsed: 221.23\n",
      "Sampling from val predictions...\n",
      "Source: 我 看起 看起来 起来 不 像是 典型 的 家庭 家庭暴力\n",
      "Reference: i don &apos;t look like a typical domestic violence\n",
      "Model: <SOS> i &apos;m &apos;t think to the , of of\n",
      "Attention Weights: tensor([[9.9973e-01, 5.6267e-06, 1.0566e-05, 7.1387e-05, 6.1621e-06, 6.8263e-06,\n",
      "         1.6511e-05, 8.8537e-06, 4.2448e-06, 1.4318e-04],\n",
      "        [9.8544e-01, 1.2189e-03, 5.2041e-03, 4.0752e-03, 1.1756e-04, 1.2874e-04,\n",
      "         2.2044e-04, 1.7806e-04, 9.2738e-05, 3.3287e-03],\n",
      "        [2.8442e-01, 6.0567e-02, 7.6388e-02, 2.4338e-01, 4.0994e-02, 2.1815e-02,\n",
      "         2.6340e-02, 1.9242e-02, 1.3268e-02, 2.1358e-01],\n",
      "        [8.4444e-03, 1.0825e-02, 7.4507e-02, 6.2519e-01, 4.6685e-02, 2.1452e-02,\n",
      "         6.2805e-02, 2.5372e-02, 9.9057e-03, 1.1481e-01],\n",
      "        [1.7307e-02, 6.4188e-03, 5.7267e-02, 5.0658e-01, 5.6849e-02, 2.3462e-02,\n",
      "         6.7806e-02, 3.1521e-02, 1.0876e-02, 2.2191e-01],\n",
      "        [7.5491e-03, 1.5369e-03, 1.1421e-02, 4.8569e-01, 4.4385e-02, 3.9005e-02,\n",
      "         1.4345e-01, 5.0582e-02, 1.7291e-02, 1.9909e-01],\n",
      "        [8.2089e-03, 1.8610e-03, 6.0377e-03, 2.4602e-01, 5.7526e-02, 5.6422e-02,\n",
      "         2.3220e-01, 6.3263e-02, 4.3199e-02, 2.8526e-01],\n",
      "        [5.2159e-03, 6.9004e-04, 2.2985e-03, 1.0408e-01, 2.5378e-02, 2.9229e-02,\n",
      "         1.6646e-01, 8.0272e-02, 5.0958e-02, 5.3542e-01],\n",
      "        [1.5576e-03, 1.3680e-04, 3.8715e-04, 7.0089e-02, 1.6660e-02, 2.0643e-02,\n",
      "         1.4148e-01, 6.0294e-02, 4.6431e-02, 6.4232e-01]])\n",
      "\n",
      "Source: 音乐 而且 因为 我们 有 一个 可以 运行 如此 之\n",
      "Reference: and because we have a camera that can run\n",
      "Model: <SOS> and , , &apos;s a , , , can\n",
      "Attention Weights: tensor([[0.0040, 0.0158, 0.2940, 0.5895, 0.0037, 0.0050, 0.0404, 0.0007, 0.0002,\n",
      "         0.0468],\n",
      "        [0.0670, 0.2294, 0.4828, 0.1606, 0.0029, 0.0040, 0.0138, 0.0006, 0.0002,\n",
      "         0.0386],\n",
      "        [0.0440, 0.1445, 0.4464, 0.0821, 0.0323, 0.0241, 0.0351, 0.0065, 0.0034,\n",
      "         0.1816],\n",
      "        [0.0725, 0.1457, 0.3339, 0.1153, 0.0403, 0.0309, 0.0471, 0.0085, 0.0046,\n",
      "         0.2012],\n",
      "        [0.0218, 0.0867, 0.2720, 0.1975, 0.0606, 0.0524, 0.0639, 0.0092, 0.0043,\n",
      "         0.2316],\n",
      "        [0.0007, 0.0039, 0.0949, 0.0550, 0.1997, 0.2172, 0.1776, 0.0352, 0.0051,\n",
      "         0.2106],\n",
      "        [0.0005, 0.0047, 0.2179, 0.1105, 0.1164, 0.1385, 0.1521, 0.0456, 0.0090,\n",
      "         0.2047],\n",
      "        [0.0007, 0.0043, 0.2436, 0.1304, 0.0578, 0.0866, 0.2345, 0.0645, 0.0162,\n",
      "         0.1615],\n",
      "        [0.0006, 0.0018, 0.0298, 0.2086, 0.0636, 0.1150, 0.3994, 0.0420, 0.0115,\n",
      "         0.1277]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.00, Train Loss: 0.00, Val Loss: 4.93, Train BLEU: 0.00, Val BLEU: 6.83, Minutes Elapsed: 227.07\n",
      "Sampling from val predictions...\n",
      "Source: 奇怪 奇怪的是 那些 地方 我 都 去过 <EOS> <PAD> <PAD>\n",
      "Reference: oddly , i had been to many of these\n",
      "Model: <SOS> and &apos;s i don &apos;t . . them .\n",
      "Attention Weights: tensor([[0.0420, 0.0035, 0.0016, 0.0118, 0.9285, 0.0004, 0.0120, 0.0002, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0977, 0.0162, 0.0351, 0.0874, 0.7343, 0.0023, 0.0249, 0.0022, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0861, 0.0244, 0.1346, 0.1662, 0.3903, 0.0270, 0.1125, 0.0589, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0084, 0.0038, 0.0325, 0.1081, 0.7763, 0.0085, 0.0559, 0.0066, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0046, 0.0021, 0.0189, 0.0643, 0.6473, 0.0262, 0.2131, 0.0234, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.0005, 0.0058, 0.0246, 0.2689, 0.0987, 0.5691, 0.0313, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0013, 0.0006, 0.0043, 0.0126, 0.1741, 0.1047, 0.6498, 0.0524, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0061, 0.0019, 0.0045, 0.0200, 0.2525, 0.0922, 0.5256, 0.0972, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0088, 0.0042, 0.0088, 0.0340, 0.2865, 0.0873, 0.4871, 0.0832, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 所以 如果 摄影 摄影师 就 在 这里 灯光 就 在\n",
      "Reference: so if the photographer is right there and the\n",
      "Model: <SOS> so , , look is in , , this\n",
      "Attention Weights: tensor([[0.0087, 0.0016, 0.0285, 0.0070, 0.0497, 0.0179, 0.4961, 0.0067, 0.0714,\n",
      "         0.3125],\n",
      "        [0.0257, 0.0220, 0.2582, 0.1275, 0.0759, 0.0230, 0.2019, 0.0088, 0.0557,\n",
      "         0.2013],\n",
      "        [0.0227, 0.0267, 0.5070, 0.1109, 0.0502, 0.0052, 0.0988, 0.0009, 0.0183,\n",
      "         0.1593],\n",
      "        [0.0185, 0.0195, 0.3608, 0.1145, 0.0703, 0.0199, 0.1044, 0.0036, 0.0283,\n",
      "         0.2601],\n",
      "        [0.0088, 0.0093, 0.2112, 0.1450, 0.0598, 0.0166, 0.1999, 0.0030, 0.0212,\n",
      "         0.3253],\n",
      "        [0.0189, 0.0199, 0.1065, 0.1194, 0.1890, 0.0575, 0.1981, 0.0140, 0.0416,\n",
      "         0.2351],\n",
      "        [0.0005, 0.0004, 0.0263, 0.0357, 0.0808, 0.0351, 0.5782, 0.0107, 0.0479,\n",
      "         0.1844],\n",
      "        [0.0016, 0.0018, 0.0342, 0.0374, 0.0860, 0.0566, 0.4889, 0.0277, 0.0843,\n",
      "         0.1815],\n",
      "        [0.0015, 0.0015, 0.0166, 0.0156, 0.0600, 0.0455, 0.5533, 0.0221, 0.0989,\n",
      "         0.1850]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.15, Train Loss: 0.00, Val Loss: 4.99, Train BLEU: 0.00, Val BLEU: 7.10, Minutes Elapsed: 235.72\n",
      "Sampling from val predictions...\n",
      "Source: 你 想 过为 为什么 什么 点 <UNK> 符号 <UNK> 子弹\n",
      "Reference: have you ever wondered why they &apos;re called bullet\n",
      "Model: <SOS> you you ever to why &quot; ? &apos;t to\n",
      "Attention Weights: tensor([[9.9978e-01, 9.3167e-05, 1.5307e-05, 1.4223e-05, 6.0434e-06, 1.5508e-05,\n",
      "         2.0773e-06, 1.6376e-05, 6.6691e-06, 4.7339e-05],\n",
      "        [9.8949e-01, 4.6342e-03, 1.8930e-03, 1.2901e-03, 2.0645e-04, 4.5765e-04,\n",
      "         1.5741e-04, 3.6025e-04, 3.8731e-04, 1.1267e-03],\n",
      "        [3.1489e-01, 1.8404e-01, 1.2758e-01, 1.0212e-01, 2.3460e-02, 3.5534e-02,\n",
      "         2.6033e-02, 2.1862e-02, 5.0858e-02, 1.1362e-01],\n",
      "        [1.1723e-01, 2.3399e-02, 1.4177e-01, 2.1694e-01, 1.0863e-01, 7.4737e-02,\n",
      "         2.9674e-02, 6.1619e-02, 4.1483e-02, 1.8452e-01],\n",
      "        [1.3706e-02, 3.0992e-02, 1.4838e-01, 2.9007e-01, 8.9303e-02, 9.9923e-02,\n",
      "         4.5181e-02, 3.9578e-02, 5.5358e-02, 1.8751e-01],\n",
      "        [7.1497e-03, 1.0218e-02, 9.5100e-02, 1.5112e-01, 1.0772e-01, 9.7194e-02,\n",
      "         5.0137e-02, 7.6982e-02, 7.5303e-02, 3.2908e-01],\n",
      "        [2.8113e-03, 3.0378e-03, 2.1637e-02, 4.2647e-02, 4.9289e-02, 7.0717e-02,\n",
      "         3.9127e-02, 1.0439e-01, 8.5857e-02, 5.8048e-01],\n",
      "        [7.0973e-04, 1.1672e-03, 7.9647e-03, 1.3705e-02, 2.4280e-02, 5.2117e-02,\n",
      "         2.1889e-02, 1.0120e-01, 7.3865e-02, 7.0310e-01],\n",
      "        [3.9813e-04, 9.1744e-04, 1.0835e-02, 1.1376e-02, 2.5999e-02, 7.8364e-02,\n",
      "         1.6807e-02, 1.1724e-01, 5.0629e-02, 6.8743e-01]])\n",
      "\n",
      "Source: 我 的 邻居 邻居们 都 听说 了 这个 办法 <EOS>\n",
      "Reference: and my neighboring homes heard about this idea .\n",
      "Model: <SOS> my my was is , this this . <EOS>\n",
      "Attention Weights: tensor([[0.9814, 0.0005, 0.0009, 0.0036, 0.0002, 0.0005, 0.0007, 0.0093, 0.0021,\n",
      "         0.0009],\n",
      "        [0.9075, 0.0056, 0.0128, 0.0416, 0.0004, 0.0012, 0.0011, 0.0216, 0.0026,\n",
      "         0.0055],\n",
      "        [0.1891, 0.0295, 0.1401, 0.1877, 0.0109, 0.0398, 0.0261, 0.2073, 0.0629,\n",
      "         0.1065],\n",
      "        [0.2391, 0.0330, 0.1045, 0.1348, 0.0120, 0.0309, 0.0287, 0.2245, 0.0678,\n",
      "         0.1246],\n",
      "        [0.0205, 0.0040, 0.0212, 0.1716, 0.0174, 0.0621, 0.0408, 0.4772, 0.0578,\n",
      "         0.1275],\n",
      "        [0.0161, 0.0007, 0.0046, 0.0819, 0.0110, 0.0543, 0.0415, 0.6820, 0.0379,\n",
      "         0.0699],\n",
      "        [0.0161, 0.0003, 0.0018, 0.0141, 0.0031, 0.0178, 0.0283, 0.8110, 0.0373,\n",
      "         0.0702],\n",
      "        [0.0106, 0.0011, 0.0045, 0.0478, 0.0099, 0.0392, 0.0426, 0.6340, 0.1175,\n",
      "         0.0928],\n",
      "        [0.0052, 0.0006, 0.0029, 0.0469, 0.0156, 0.0418, 0.0394, 0.4355, 0.2667,\n",
      "         0.1455]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.30, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 7.46, Minutes Elapsed: 244.38\n",
      "Sampling from val predictions...\n",
      "Source: 事实 实是 康 纳 在 一个 周五 的 晚上 回到\n",
      "Reference: instead , <UNK> came home one friday evening and\n",
      "Model: <SOS> in fact , &apos;s a a a the the\n",
      "Attention Weights: tensor([[0.0069, 0.0066, 0.0267, 0.3678, 0.0454, 0.0343, 0.0180, 0.0096, 0.0078,\n",
      "         0.4770],\n",
      "        [0.0595, 0.3716, 0.1034, 0.2400, 0.0232, 0.0212, 0.0118, 0.0083, 0.0068,\n",
      "         0.1542],\n",
      "        [0.0403, 0.1482, 0.1754, 0.2089, 0.0930, 0.0630, 0.0218, 0.0273, 0.0171,\n",
      "         0.2051],\n",
      "        [0.0056, 0.0226, 0.0908, 0.3698, 0.2074, 0.0978, 0.0240, 0.0120, 0.0072,\n",
      "         0.1627],\n",
      "        [0.0048, 0.0156, 0.0614, 0.3345, 0.2222, 0.1395, 0.0401, 0.0229, 0.0172,\n",
      "         0.1418],\n",
      "        [0.0001, 0.0001, 0.0046, 0.1760, 0.2230, 0.2061, 0.0718, 0.0422, 0.0280,\n",
      "         0.2483],\n",
      "        [0.0001, 0.0001, 0.0014, 0.0264, 0.0710, 0.3232, 0.1623, 0.0722, 0.0696,\n",
      "         0.2737],\n",
      "        [0.0001, 0.0001, 0.0011, 0.0186, 0.0767, 0.2192, 0.0893, 0.0912, 0.0862,\n",
      "         0.4175],\n",
      "        [0.0001, 0.0001, 0.0007, 0.0126, 0.0577, 0.1625, 0.0741, 0.0827, 0.0911,\n",
      "         0.5185]])\n",
      "\n",
      "Source: 我 不是 那个 国家 的 公民 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: i wasn &apos;t a citizen of that country .\n",
      "Model: <SOS> i &apos;m not really to . <EOS> . <EOS>\n",
      "Attention Weights: tensor([[0.9995, 0.0000, 0.0000, 0.0000, 0.0000, 0.0003, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9989, 0.0001, 0.0002, 0.0001, 0.0001, 0.0004, 0.0004, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.7260, 0.0473, 0.0670, 0.0207, 0.0271, 0.0648, 0.0472, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0585, 0.0302, 0.2340, 0.2176, 0.1080, 0.2400, 0.1117, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0384, 0.0060, 0.0731, 0.2704, 0.1170, 0.3181, 0.1771, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0345, 0.0059, 0.0627, 0.2427, 0.1062, 0.2827, 0.2653, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0553, 0.0101, 0.0540, 0.1738, 0.0999, 0.3246, 0.2823, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1061, 0.0097, 0.0481, 0.1252, 0.0759, 0.2973, 0.3378, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0381, 0.0165, 0.0497, 0.1611, 0.0936, 0.3441, 0.2969, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.45, Train Loss: 0.00, Val Loss: 4.90, Train BLEU: 0.00, Val BLEU: 7.29, Minutes Elapsed: 253.03\n",
      "Sampling from val predictions...\n",
      "Source: <UNK> 声 微波 微波炉 声 你们 也许 会同 同意 这条\n",
      "Reference: you probably all agree with me that this is\n",
      "Model: <SOS> <UNK> : , that , that , the ,\n",
      "Attention Weights: tensor([[0.0009, 0.0211, 0.3504, 0.0542, 0.0096, 0.5528, 0.0065, 0.0010, 0.0003,\n",
      "         0.0033],\n",
      "        [0.0209, 0.0957, 0.6942, 0.0410, 0.0118, 0.1282, 0.0037, 0.0010, 0.0003,\n",
      "         0.0031],\n",
      "        [0.0481, 0.2405, 0.4353, 0.0768, 0.0342, 0.0731, 0.0095, 0.0103, 0.0069,\n",
      "         0.0653],\n",
      "        [0.0368, 0.0683, 0.1182, 0.0888, 0.0757, 0.0801, 0.0424, 0.0565, 0.0492,\n",
      "         0.3839],\n",
      "        [0.0202, 0.0455, 0.1148, 0.1199, 0.0801, 0.1217, 0.0611, 0.0641, 0.0597,\n",
      "         0.3130],\n",
      "        [0.0115, 0.0329, 0.0692, 0.0974, 0.1158, 0.1256, 0.0634, 0.0673, 0.0677,\n",
      "         0.3491],\n",
      "        [0.0093, 0.0241, 0.0978, 0.0996, 0.0925, 0.1968, 0.0756, 0.0743, 0.0556,\n",
      "         0.2744],\n",
      "        [0.0016, 0.0072, 0.0291, 0.0670, 0.0875, 0.2591, 0.1575, 0.0888, 0.0877,\n",
      "         0.2146],\n",
      "        [0.0007, 0.0025, 0.0088, 0.0286, 0.0470, 0.2066, 0.0973, 0.0899, 0.1024,\n",
      "         0.4161]])\n",
      "\n",
      "Source: 现在 光束 进入 瓶子 也 就是 我们 的 子弹 穿过\n",
      "Reference: so , the pulse enters the bottle , our\n",
      "Model: <SOS> now , &apos;s the is this the of the\n",
      "Attention Weights: tensor([[0.0009, 0.6591, 0.0011, 0.0046, 0.0847, 0.0189, 0.1203, 0.0074, 0.0020,\n",
      "         0.1010],\n",
      "        [0.0010, 0.9654, 0.0063, 0.0048, 0.0059, 0.0017, 0.0047, 0.0011, 0.0004,\n",
      "         0.0087],\n",
      "        [0.0116, 0.5806, 0.0660, 0.0389, 0.0475, 0.0209, 0.0235, 0.0226, 0.0120,\n",
      "         0.1765],\n",
      "        [0.0033, 0.4971, 0.0874, 0.0644, 0.0603, 0.0238, 0.0358, 0.0205, 0.0093,\n",
      "         0.1982],\n",
      "        [0.0029, 0.4935, 0.1069, 0.0762, 0.0648, 0.0266, 0.0329, 0.0189, 0.0080,\n",
      "         0.1693],\n",
      "        [0.0072, 0.2938, 0.0478, 0.1016, 0.2112, 0.0714, 0.0869, 0.0186, 0.0103,\n",
      "         0.1511],\n",
      "        [0.0004, 0.1021, 0.0145, 0.0648, 0.4176, 0.0793, 0.1543, 0.0160, 0.0041,\n",
      "         0.1469],\n",
      "        [0.0001, 0.0111, 0.0017, 0.0217, 0.4529, 0.1092, 0.2676, 0.0244, 0.0076,\n",
      "         0.1036],\n",
      "        [0.0001, 0.0041, 0.0010, 0.0123, 0.3445, 0.1417, 0.3267, 0.0349, 0.0199,\n",
      "         0.1148]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.60, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 6.97, Minutes Elapsed: 261.63\n",
      "Sampling from val predictions...\n",
      "Source: 我们 的 工作 地点 不在 办公 办公室 公室 <EOS> <PAD>\n",
      "Reference: we don &apos;t work from offices . <EOS> <PAD>\n",
      "Model: <SOS> our our our have our . . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9900, 0.0006, 0.0003, 0.0005, 0.0004, 0.0008, 0.0013, 0.0043, 0.0018,\n",
      "         0.0000],\n",
      "        [0.9667, 0.0043, 0.0047, 0.0022, 0.0007, 0.0014, 0.0033, 0.0108, 0.0058,\n",
      "         0.0000],\n",
      "        [0.2870, 0.0676, 0.1370, 0.0828, 0.0296, 0.0397, 0.0759, 0.1393, 0.1412,\n",
      "         0.0000],\n",
      "        [0.2988, 0.0207, 0.0679, 0.1484, 0.0809, 0.0683, 0.0937, 0.1545, 0.0668,\n",
      "         0.0000],\n",
      "        [0.2345, 0.0361, 0.0715, 0.1295, 0.0585, 0.0661, 0.0808, 0.1549, 0.1681,\n",
      "         0.0000],\n",
      "        [0.1293, 0.0116, 0.0283, 0.1297, 0.1092, 0.1215, 0.1308, 0.2032, 0.1364,\n",
      "         0.0000],\n",
      "        [0.0917, 0.0161, 0.0366, 0.1194, 0.1142, 0.1294, 0.1269, 0.1955, 0.1703,\n",
      "         0.0000],\n",
      "        [0.0510, 0.0132, 0.0233, 0.1166, 0.1360, 0.1444, 0.0786, 0.1769, 0.2599,\n",
      "         0.0000],\n",
      "        [0.2186, 0.0103, 0.0236, 0.0820, 0.0684, 0.1269, 0.0733, 0.1598, 0.2370,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 那些 时刻 令人 触动 也 非常 常有 激励 意义 我\n",
      "Reference: now , it was emotional and it was inspiring\n",
      "Model: <SOS> and , the &apos;s not that and , ,\n",
      "Attention Weights: tensor([[0.0002, 0.0004, 0.0005, 0.0033, 0.0236, 0.0021, 0.0014, 0.0068, 0.0105,\n",
      "         0.9512],\n",
      "        [0.0446, 0.0816, 0.0990, 0.0668, 0.0592, 0.0101, 0.0078, 0.0158, 0.0218,\n",
      "         0.5933],\n",
      "        [0.0579, 0.1014, 0.1005, 0.0684, 0.0850, 0.0141, 0.0099, 0.0160, 0.0204,\n",
      "         0.5263],\n",
      "        [0.0286, 0.0905, 0.1253, 0.0967, 0.1142, 0.0153, 0.0088, 0.0159, 0.0185,\n",
      "         0.4863],\n",
      "        [0.0432, 0.1191, 0.2038, 0.0993, 0.1188, 0.0288, 0.0178, 0.0229, 0.0229,\n",
      "         0.3235],\n",
      "        [0.0208, 0.0380, 0.2061, 0.0981, 0.1114, 0.0439, 0.0234, 0.0196, 0.0263,\n",
      "         0.4123],\n",
      "        [0.0364, 0.0566, 0.1978, 0.1205, 0.1436, 0.0485, 0.0281, 0.0308, 0.0352,\n",
      "         0.3024],\n",
      "        [0.0273, 0.0307, 0.0473, 0.1474, 0.1584, 0.0190, 0.0086, 0.0144, 0.0167,\n",
      "         0.5302],\n",
      "        [0.0079, 0.0198, 0.0479, 0.1398, 0.2736, 0.0617, 0.0346, 0.0335, 0.0284,\n",
      "         0.3529]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.75, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.50, Minutes Elapsed: 270.24\n",
      "Sampling from val predictions...\n",
      "Source: 呃 在 过去 的 几个 世纪 里 我们 定义 里\n",
      "Reference: well , for the past few centuries we have\n",
      "Model: <SOS> and , the the , of , , we\n",
      "Attention Weights: tensor([[0.0048, 0.0028, 0.0063, 0.0037, 0.0224, 0.0800, 0.0211, 0.4404, 0.0093,\n",
      "         0.4092],\n",
      "        [0.1212, 0.0363, 0.1667, 0.0223, 0.0431, 0.0867, 0.0174, 0.3257, 0.0050,\n",
      "         0.1757],\n",
      "        [0.0417, 0.0746, 0.1909, 0.0681, 0.0750, 0.0442, 0.0257, 0.0524, 0.0195,\n",
      "         0.4080],\n",
      "        [0.0186, 0.0457, 0.2137, 0.0349, 0.0964, 0.0643, 0.0141, 0.1100, 0.0092,\n",
      "         0.3930],\n",
      "        [0.0142, 0.0447, 0.2305, 0.0686, 0.1425, 0.0631, 0.0172, 0.0602, 0.0162,\n",
      "         0.3429],\n",
      "        [0.0029, 0.0102, 0.0655, 0.0390, 0.1866, 0.3035, 0.0510, 0.0930, 0.0151,\n",
      "         0.2332],\n",
      "        [0.0053, 0.0146, 0.0881, 0.0410, 0.1511, 0.3137, 0.0564, 0.1435, 0.0104,\n",
      "         0.1759],\n",
      "        [0.0011, 0.0038, 0.0147, 0.0114, 0.0753, 0.3521, 0.0706, 0.2913, 0.0296,\n",
      "         0.1501],\n",
      "        [0.0002, 0.0007, 0.0006, 0.0011, 0.0109, 0.1695, 0.0740, 0.5922, 0.0432,\n",
      "         0.1076]])\n",
      "\n",
      "Source: 我 向 所有 所有人 有人 求助 警察 邻居 我 的\n",
      "Reference: i told everyone : the police , my neighbors\n",
      "Model: <SOS> i i &apos;t &quot; i , of my my\n",
      "Attention Weights: tensor([[0.9990, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0006,\n",
      "         0.0002],\n",
      "        [0.9924, 0.0028, 0.0009, 0.0018, 0.0001, 0.0001, 0.0001, 0.0001, 0.0009,\n",
      "         0.0007],\n",
      "        [0.3394, 0.2143, 0.1457, 0.0998, 0.0262, 0.0127, 0.0095, 0.0095, 0.0212,\n",
      "         0.1217],\n",
      "        [0.0407, 0.0913, 0.2278, 0.2364, 0.1062, 0.0563, 0.0286, 0.0302, 0.0608,\n",
      "         0.1217],\n",
      "        [0.0235, 0.0441, 0.1316, 0.2001, 0.1280, 0.0961, 0.0649, 0.0562, 0.1188,\n",
      "         0.1367],\n",
      "        [0.0020, 0.0017, 0.0066, 0.0116, 0.0278, 0.0877, 0.1173, 0.1328, 0.4616,\n",
      "         0.1509],\n",
      "        [0.0023, 0.0009, 0.0057, 0.0119, 0.0260, 0.0742, 0.1020, 0.1341, 0.4414,\n",
      "         0.2017],\n",
      "        [0.0083, 0.0020, 0.0121, 0.0288, 0.1062, 0.1101, 0.1512, 0.1535, 0.2299,\n",
      "         0.1979],\n",
      "        [0.0012, 0.0002, 0.0009, 0.0067, 0.0226, 0.0429, 0.0825, 0.1219, 0.5521,\n",
      "         0.1692]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.90, Train Loss: 0.00, Val Loss: 4.87, Train BLEU: 0.00, Val BLEU: 7.41, Minutes Elapsed: 278.88\n",
      "Sampling from val predictions...\n",
      "Source: 虽然 偶尔 会 有 一些 些小 小型 殖民 殖民地 的\n",
      "Reference: there were occasional presidents of little colonial councils and\n",
      "Model: <SOS> and , a many of the , , ,\n",
      "Attention Weights: tensor([[0.3252, 0.0662, 0.4461, 0.0053, 0.0049, 0.0030, 0.0064, 0.0133, 0.0021,\n",
      "         0.1277],\n",
      "        [0.1858, 0.0941, 0.6900, 0.0063, 0.0019, 0.0005, 0.0004, 0.0015, 0.0006,\n",
      "         0.0189],\n",
      "        [0.0635, 0.1016, 0.4749, 0.0674, 0.0131, 0.0041, 0.0027, 0.0115, 0.0086,\n",
      "         0.2525],\n",
      "        [0.0192, 0.0194, 0.2020, 0.1043, 0.1232, 0.0296, 0.0250, 0.0623, 0.0217,\n",
      "         0.3932],\n",
      "        [0.0201, 0.0133, 0.0594, 0.0311, 0.0567, 0.0363, 0.0166, 0.0710, 0.0365,\n",
      "         0.6592],\n",
      "        [0.0113, 0.0114, 0.0414, 0.0245, 0.0398, 0.0594, 0.0266, 0.1127, 0.0494,\n",
      "         0.6236],\n",
      "        [0.0033, 0.0049, 0.0513, 0.0274, 0.0567, 0.0768, 0.0509, 0.2285, 0.0391,\n",
      "         0.4611],\n",
      "        [0.0005, 0.0010, 0.0117, 0.0102, 0.0248, 0.0824, 0.0962, 0.3423, 0.0560,\n",
      "         0.3749],\n",
      "        [0.0001, 0.0001, 0.0015, 0.0028, 0.0065, 0.0204, 0.0482, 0.2687, 0.0496,\n",
      "         0.6019]])\n",
      "\n",
      "Source: 在 沥青 <UNK> 路上 上会 溅 起 很多 水 <EOS>\n",
      "Reference: then you can have a lot of splash water\n",
      "Model: <SOS> in the the the the , of people in\n",
      "Attention Weights: tensor([[0.0020, 0.0022, 0.0030, 0.0055, 0.5830, 0.1982, 0.0215, 0.0772, 0.0924,\n",
      "         0.0150],\n",
      "        [0.0207, 0.0186, 0.0336, 0.0271, 0.5918, 0.1367, 0.0093, 0.0569, 0.0732,\n",
      "         0.0320],\n",
      "        [0.0449, 0.0531, 0.0469, 0.0719, 0.2018, 0.1358, 0.0331, 0.0853, 0.1888,\n",
      "         0.1385],\n",
      "        [0.0212, 0.0274, 0.0260, 0.0710, 0.2489, 0.1610, 0.0357, 0.0916, 0.1621,\n",
      "         0.1551],\n",
      "        [0.0455, 0.0542, 0.0723, 0.0693, 0.1094, 0.1499, 0.0576, 0.2388, 0.1138,\n",
      "         0.0893],\n",
      "        [0.0182, 0.0268, 0.0316, 0.0558, 0.1258, 0.2012, 0.0806, 0.2766, 0.1161,\n",
      "         0.0674],\n",
      "        [0.0237, 0.0292, 0.0243, 0.0613, 0.1673, 0.2121, 0.0881, 0.2147, 0.1182,\n",
      "         0.0610],\n",
      "        [0.0293, 0.0216, 0.0208, 0.0440, 0.2045, 0.1653, 0.0898, 0.1926, 0.1422,\n",
      "         0.0899],\n",
      "        [0.0426, 0.0286, 0.0287, 0.0376, 0.2033, 0.1360, 0.0379, 0.1605, 0.1594,\n",
      "         0.1655]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.00, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.35, Minutes Elapsed: 284.66\n",
      "Sampling from val predictions...\n",
      "Source: 我 向 所有 所有人 有人 求助 警察 邻居 我 的\n",
      "Reference: i told everyone : the police , my neighbors\n",
      "Model: <SOS> i i &apos;t , &quot; i of the ,\n",
      "Attention Weights: tensor([[0.9971, 0.0001, 0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0003, 0.0018,\n",
      "         0.0003],\n",
      "        [0.9741, 0.0085, 0.0021, 0.0074, 0.0015, 0.0006, 0.0006, 0.0009, 0.0022,\n",
      "         0.0023],\n",
      "        [0.2339, 0.2638, 0.1150, 0.0982, 0.0493, 0.0175, 0.0142, 0.0157, 0.0243,\n",
      "         0.1681],\n",
      "        [0.0230, 0.0882, 0.2411, 0.2998, 0.1218, 0.0514, 0.0363, 0.0307, 0.0414,\n",
      "         0.0662],\n",
      "        [0.0114, 0.0391, 0.1265, 0.2444, 0.1937, 0.0921, 0.0812, 0.0744, 0.0746,\n",
      "         0.0625],\n",
      "        [0.0021, 0.0058, 0.0213, 0.0305, 0.0644, 0.1159, 0.1535, 0.2029, 0.3074,\n",
      "         0.0963],\n",
      "        [0.0015, 0.0012, 0.0056, 0.0102, 0.0311, 0.0793, 0.1385, 0.2374, 0.3786,\n",
      "         0.1166],\n",
      "        [0.0025, 0.0016, 0.0074, 0.0212, 0.1120, 0.1023, 0.1756, 0.2271, 0.2548,\n",
      "         0.0955],\n",
      "        [0.0006, 0.0003, 0.0018, 0.0174, 0.0579, 0.0535, 0.1207, 0.2132, 0.4592,\n",
      "         0.0754]])\n",
      "\n",
      "Source: 这 两个 词 来自 于 拉丁 词根 <UNK> 意 即\n",
      "Reference: the two words come from the latin root &quot;\n",
      "Model: <SOS> and the of these the &quot; and and and\n",
      "Attention Weights: tensor([[0.9914, 0.0007, 0.0045, 0.0003, 0.0001, 0.0004, 0.0000, 0.0001, 0.0005,\n",
      "         0.0020],\n",
      "        [0.7503, 0.0786, 0.1422, 0.0057, 0.0008, 0.0014, 0.0002, 0.0011, 0.0026,\n",
      "         0.0170],\n",
      "        [0.2333, 0.0785, 0.2821, 0.0454, 0.0230, 0.0113, 0.0047, 0.0281, 0.0299,\n",
      "         0.2636],\n",
      "        [0.1881, 0.0530, 0.3002, 0.0521, 0.0245, 0.0156, 0.0052, 0.0422, 0.0314,\n",
      "         0.2877],\n",
      "        [0.1539, 0.0146, 0.3735, 0.0495, 0.0261, 0.0181, 0.0032, 0.0348, 0.0335,\n",
      "         0.2928],\n",
      "        [0.0833, 0.0077, 0.2513, 0.1248, 0.0329, 0.0350, 0.0053, 0.0615, 0.0425,\n",
      "         0.3558],\n",
      "        [0.0154, 0.0014, 0.0547, 0.2119, 0.0618, 0.1711, 0.0130, 0.0560, 0.0779,\n",
      "         0.3369],\n",
      "        [0.0018, 0.0001, 0.0235, 0.1478, 0.0795, 0.1867, 0.0303, 0.0434, 0.1270,\n",
      "         0.3599],\n",
      "        [0.0080, 0.0003, 0.0343, 0.1023, 0.0655, 0.1571, 0.0311, 0.0684, 0.1690,\n",
      "         0.3639]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.15, Train Loss: 0.00, Val Loss: 4.98, Train BLEU: 0.00, Val BLEU: 7.27, Minutes Elapsed: 293.34\n",
      "Sampling from val predictions...\n",
      "Source: 赞比亚 比亚 人 说 没错 这 就是 我们 不在 这里\n",
      "Reference: and the <UNK> said , &quot; yes , that\n",
      "Model: <SOS> and &apos;s said news , &quot; that is that\n",
      "Attention Weights: tensor([[0.1065, 0.0164, 0.1131, 0.0075, 0.0018, 0.5924, 0.0065, 0.0365, 0.0008,\n",
      "         0.1185],\n",
      "        [0.2081, 0.2058, 0.3706, 0.0318, 0.0070, 0.1205, 0.0030, 0.0229, 0.0007,\n",
      "         0.0295],\n",
      "        [0.0494, 0.1038, 0.1881, 0.0775, 0.0245, 0.1309, 0.0204, 0.0368, 0.0071,\n",
      "         0.3614],\n",
      "        [0.0689, 0.1086, 0.1412, 0.0729, 0.0227, 0.1474, 0.0163, 0.0260, 0.0048,\n",
      "         0.3913],\n",
      "        [0.0727, 0.1687, 0.3035, 0.1056, 0.0253, 0.1179, 0.0111, 0.0151, 0.0039,\n",
      "         0.1763],\n",
      "        [0.0024, 0.0094, 0.0647, 0.0789, 0.0709, 0.5296, 0.0205, 0.0339, 0.0040,\n",
      "         0.1858],\n",
      "        [0.0012, 0.0025, 0.0146, 0.0360, 0.0587, 0.5627, 0.0432, 0.0891, 0.0079,\n",
      "         0.1842],\n",
      "        [0.0028, 0.0027, 0.0068, 0.0281, 0.0282, 0.5824, 0.0244, 0.1371, 0.0057,\n",
      "         0.1817],\n",
      "        [0.0032, 0.0080, 0.0235, 0.0582, 0.0599, 0.4038, 0.0400, 0.1358, 0.0162,\n",
      "         0.2515]])\n",
      "\n",
      "Source: 我 厌倦 了 这些 现实 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: i got tired of seeing this happening . <EOS>\n",
      "Model: <SOS> i i this these this . . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9941, 0.0000, 0.0001, 0.0048, 0.0008, 0.0002, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9532, 0.0012, 0.0019, 0.0386, 0.0020, 0.0031, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1123, 0.0145, 0.0520, 0.7169, 0.0577, 0.0467, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0080, 0.0086, 0.0517, 0.8223, 0.0758, 0.0336, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0179, 0.0030, 0.0269, 0.7298, 0.1536, 0.0688, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0264, 0.0024, 0.0194, 0.6592, 0.1311, 0.1615, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0173, 0.0062, 0.0282, 0.3425, 0.3260, 0.2798, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0313, 0.0231, 0.0319, 0.4438, 0.2950, 0.1749, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0431, 0.0237, 0.0299, 0.2046, 0.3944, 0.3042, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.45, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 7.70, Minutes Elapsed: 310.85\n",
      "Sampling from val predictions...\n",
      "Source: 最近 一次 旅程 我们 在 路上 走 着 她 突然\n",
      "Reference: recently , on one trip , we were walking\n",
      "Model: <SOS> and the the we time , we were the\n",
      "Attention Weights: tensor([[0.0009, 0.0134, 0.0072, 0.4918, 0.0006, 0.0019, 0.0041, 0.0041, 0.4367,\n",
      "         0.0393],\n",
      "        [0.0471, 0.6075, 0.0417, 0.2141, 0.0008, 0.0020, 0.0011, 0.0032, 0.0664,\n",
      "         0.0160],\n",
      "        [0.0311, 0.2693, 0.0737, 0.0743, 0.0136, 0.0162, 0.0119, 0.0275, 0.1805,\n",
      "         0.3017],\n",
      "        [0.0045, 0.0673, 0.1396, 0.4787, 0.0152, 0.0146, 0.0059, 0.0105, 0.0937,\n",
      "         0.1701],\n",
      "        [0.0055, 0.0470, 0.0678, 0.1894, 0.0410, 0.0465, 0.0138, 0.0294, 0.2607,\n",
      "         0.2989],\n",
      "        [0.0022, 0.0259, 0.0807, 0.4857, 0.0409, 0.0417, 0.0228, 0.0251, 0.1364,\n",
      "         0.1385],\n",
      "        [0.0011, 0.0088, 0.0585, 0.6059, 0.0155, 0.0338, 0.0173, 0.0204, 0.1754,\n",
      "         0.0634],\n",
      "        [0.0001, 0.0009, 0.0050, 0.3379, 0.0182, 0.0616, 0.0216, 0.0319, 0.4260,\n",
      "         0.0968],\n",
      "        [0.0001, 0.0015, 0.0052, 0.1255, 0.0500, 0.0695, 0.0170, 0.0306, 0.5723,\n",
      "         0.1283]])\n",
      "\n",
      "Source: 我 记得 他们 布满 <UNK> <UNK> <UNK> 的 眼睛 里\n",
      "Reference: i remember looking into their tired , <UNK> eyes\n",
      "Model: <SOS> i remember them <UNK> the <UNK> , , i\n",
      "Attention Weights: tensor([[9.9902e-01, 9.7860e-06, 9.5451e-04, 2.6812e-06, 3.6756e-07, 4.1729e-07,\n",
      "         3.6651e-07, 8.5672e-07, 1.6913e-06, 4.7875e-06],\n",
      "        [8.1200e-01, 4.3710e-03, 1.8142e-01, 8.8810e-04, 1.1566e-04, 8.9589e-05,\n",
      "         9.1026e-05, 6.2449e-05, 1.2382e-04, 8.4685e-04],\n",
      "        [1.1553e-01, 2.1634e-02, 7.1921e-01, 4.3327e-02, 1.0117e-02, 7.3674e-03,\n",
      "         8.6024e-03, 6.2687e-03, 6.9501e-03, 6.0996e-02],\n",
      "        [1.4716e-02, 9.3554e-03, 5.9343e-01, 2.4126e-01, 1.6738e-02, 1.2962e-02,\n",
      "         1.3657e-02, 1.5061e-02, 1.4982e-02, 6.7835e-02],\n",
      "        [8.6141e-03, 3.7682e-03, 1.5526e-01, 1.8786e-01, 5.2842e-02, 4.9781e-02,\n",
      "         4.8936e-02, 8.8713e-02, 5.9385e-02, 3.4484e-01],\n",
      "        [3.2702e-03, 3.6367e-04, 5.5936e-02, 1.6157e-01, 3.3910e-02, 4.1463e-02,\n",
      "         3.8121e-02, 1.1985e-01, 1.0289e-01, 4.4263e-01],\n",
      "        [2.4622e-03, 2.7925e-04, 1.0897e-01, 5.3176e-02, 2.1802e-02, 3.3554e-02,\n",
      "         3.3704e-02, 1.1610e-01, 1.3541e-01, 4.9454e-01],\n",
      "        [2.0334e-03, 1.0915e-04, 1.7557e-02, 1.2568e-02, 1.2805e-02, 2.4587e-02,\n",
      "         2.5616e-02, 1.4163e-01, 1.8288e-01, 5.8022e-01],\n",
      "        [2.9216e-03, 2.1177e-04, 2.0263e-02, 1.6075e-02, 1.6616e-02, 3.1066e-02,\n",
      "         2.7382e-02, 1.3924e-01, 2.5779e-01, 4.8843e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.60, Train Loss: 0.00, Val Loss: 4.87, Train BLEU: 0.00, Val BLEU: 7.38, Minutes Elapsed: 319.60\n",
      "Sampling from val predictions...\n",
      "Source: 任何 健康 或者 患有 帕金森 疾病 的 人 都 能\n",
      "Reference: anyone healthy or with parkinson &apos;s can call in\n",
      "Model: <SOS> and , , , , , or , do\n",
      "Attention Weights: tensor([[0.1114, 0.0015, 0.0164, 0.0031, 0.0292, 0.0948, 0.0285, 0.0683, 0.0050,\n",
      "         0.6418],\n",
      "        [0.5318, 0.0467, 0.0974, 0.0050, 0.0390, 0.0406, 0.0196, 0.0278, 0.0037,\n",
      "         0.1885],\n",
      "        [0.2849, 0.0723, 0.0918, 0.0173, 0.0393, 0.0394, 0.0358, 0.0592, 0.0159,\n",
      "         0.3440],\n",
      "        [0.0802, 0.0091, 0.0625, 0.0481, 0.0785, 0.1471, 0.0327, 0.0694, 0.0190,\n",
      "         0.4536],\n",
      "        [0.0386, 0.0035, 0.0252, 0.0339, 0.0803, 0.3253, 0.0615, 0.1197, 0.0217,\n",
      "         0.2903],\n",
      "        [0.0373, 0.0036, 0.0255, 0.0224, 0.0395, 0.1761, 0.0620, 0.1540, 0.0257,\n",
      "         0.4538],\n",
      "        [0.0085, 0.0016, 0.0104, 0.0272, 0.0379, 0.1887, 0.1026, 0.2188, 0.0607,\n",
      "         0.3436],\n",
      "        [0.0019, 0.0002, 0.0012, 0.0029, 0.0140, 0.0648, 0.0522, 0.1988, 0.0385,\n",
      "         0.6255],\n",
      "        [0.0010, 0.0001, 0.0005, 0.0011, 0.0066, 0.0708, 0.0601, 0.1946, 0.0611,\n",
      "         0.6041]])\n",
      "\n",
      "Source: 尽管 我们 都 是 朝鲜 <UNK> 族人 但是 我们 的\n",
      "Reference: we are all korean , but inside , we\n",
      "Model: <SOS> and , all of , , , &apos;re ,\n",
      "Attention Weights: tensor([[1.8179e-03, 9.8861e-01, 3.3085e-06, 1.4376e-04, 3.2278e-04, 1.4329e-04,\n",
      "         2.1091e-03, 1.1829e-03, 5.1595e-03, 5.1169e-04],\n",
      "        [4.4876e-03, 9.9312e-01, 1.3192e-04, 7.5949e-04, 2.4261e-04, 8.3734e-05,\n",
      "         2.9228e-04, 8.3916e-05, 4.0116e-04, 3.9908e-04],\n",
      "        [1.0213e-01, 4.1089e-01, 3.5893e-02, 1.3698e-01, 4.9104e-02, 5.4564e-02,\n",
      "         2.8838e-02, 1.9396e-02, 1.9222e-02, 1.4298e-01],\n",
      "        [1.1473e-02, 1.2545e-01, 1.7789e-02, 9.7336e-02, 3.3148e-01, 8.1440e-02,\n",
      "         1.4065e-01, 4.5124e-02, 3.9304e-02, 1.0996e-01],\n",
      "        [7.8618e-04, 1.0989e-02, 4.8847e-04, 6.1368e-03, 1.0128e-01, 1.7486e-01,\n",
      "         5.0574e-01, 1.0371e-01, 3.7892e-02, 5.8116e-02],\n",
      "        [2.9843e-03, 8.1573e-03, 2.9530e-03, 1.9211e-02, 1.4004e-01, 1.7296e-01,\n",
      "         3.5628e-01, 8.5386e-02, 2.8003e-02, 1.8402e-01],\n",
      "        [4.0050e-03, 1.9773e-02, 7.8686e-04, 1.1594e-02, 5.4574e-02, 6.6159e-02,\n",
      "         4.5556e-01, 1.8064e-01, 7.8195e-02, 1.2871e-01],\n",
      "        [2.9158e-03, 5.2812e-02, 7.0336e-04, 8.3370e-03, 4.1821e-02, 5.0309e-02,\n",
      "         2.0704e-01, 8.6683e-02, 2.1396e-01, 3.3542e-01],\n",
      "        [8.2153e-05, 7.2972e-03, 3.9348e-04, 1.0683e-02, 1.3803e-01, 3.9224e-02,\n",
      "         2.4916e-01, 8.4692e-02, 1.6447e-01, 3.0596e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.75, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 7.36, Minutes Elapsed: 328.42\n",
      "Sampling from val predictions...\n",
      "Source: 一年 之内 我 做 了 27 个 项目 政府 的\n",
      "Reference: in a year , i had 27 projects going\n",
      "Model: <SOS> i , , i i &apos;ve a a and\n",
      "Attention Weights: tensor([[0.0213, 0.0114, 0.9403, 0.0001, 0.0003, 0.0061, 0.0049, 0.0002, 0.0023,\n",
      "         0.0130],\n",
      "        [0.1530, 0.0645, 0.7678, 0.0001, 0.0006, 0.0065, 0.0016, 0.0002, 0.0009,\n",
      "         0.0048],\n",
      "        [0.0946, 0.1861, 0.2771, 0.0096, 0.0241, 0.1560, 0.0329, 0.0072, 0.0215,\n",
      "         0.1910],\n",
      "        [0.0914, 0.3052, 0.3778, 0.0133, 0.0138, 0.0937, 0.0104, 0.0042, 0.0081,\n",
      "         0.0821],\n",
      "        [0.0323, 0.1430, 0.6091, 0.0047, 0.0105, 0.0729, 0.0131, 0.0030, 0.0079,\n",
      "         0.1035],\n",
      "        [0.0017, 0.0087, 0.5959, 0.0049, 0.0229, 0.2548, 0.0257, 0.0012, 0.0036,\n",
      "         0.0806],\n",
      "        [0.0013, 0.0078, 0.3203, 0.0106, 0.0527, 0.2700, 0.1641, 0.0175, 0.0367,\n",
      "         0.1191],\n",
      "        [0.0003, 0.0009, 0.0299, 0.0034, 0.0280, 0.4781, 0.1727, 0.0415, 0.0998,\n",
      "         0.1455],\n",
      "        [0.0001, 0.0002, 0.0060, 0.0007, 0.0059, 0.0856, 0.0991, 0.0733, 0.3877,\n",
      "         0.3414]])\n",
      "\n",
      "Source: 我 要 介绍 你们 认识 <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: i want to introduce you to kofi . <EOS>\n",
      "Model: <SOS> i &apos;m to show you to a . .\n",
      "Attention Weights: tensor([[0.9991, 0.0001, 0.0000, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9955, 0.0008, 0.0010, 0.0026, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3930, 0.2139, 0.2098, 0.1267, 0.0135, 0.0214, 0.0216, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0598, 0.0678, 0.3042, 0.4540, 0.0652, 0.0283, 0.0208, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0065, 0.0088, 0.0794, 0.6473, 0.1535, 0.0750, 0.0295, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0125, 0.0093, 0.0748, 0.4784, 0.2822, 0.0967, 0.0460, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0095, 0.0026, 0.0063, 0.4200, 0.3205, 0.1659, 0.0751, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0121, 0.0092, 0.0195, 0.1595, 0.5395, 0.1898, 0.0703, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0032, 0.0037, 0.0092, 0.1605, 0.6345, 0.1263, 0.0625, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.00, Train Loss: 0.00, Val Loss: 4.87, Train BLEU: 0.00, Val BLEU: 7.99, Minutes Elapsed: 343.25\n",
      "Sampling from val predictions...\n",
      "Source: 这 并 不是 说 母亲 们 对于 我们 的 成功\n",
      "Reference: it &apos;s not to say that our mothers aren\n",
      "Model: <SOS> it &apos;s not just same a &quot; &apos;s is\n",
      "Attention Weights: tensor([[9.9830e-01, 6.5903e-04, 1.1509e-05, 7.8337e-05, 3.3273e-05, 2.4720e-04,\n",
      "         1.8772e-05, 2.9311e-04, 4.7967e-06, 3.5243e-04],\n",
      "        [8.0270e-01, 1.5196e-01, 8.2817e-03, 2.3124e-02, 1.2342e-03, 7.1492e-03,\n",
      "         1.9429e-04, 1.3834e-03, 7.6615e-05, 3.8961e-03],\n",
      "        [4.1430e-01, 3.9583e-01, 4.4463e-02, 5.3404e-02, 1.0381e-02, 2.3661e-02,\n",
      "         3.1313e-03, 9.3099e-03, 4.4755e-03, 4.1040e-02],\n",
      "        [4.7287e-03, 6.8583e-02, 8.3758e-02, 5.3043e-01, 7.5378e-02, 1.3361e-01,\n",
      "         1.1473e-02, 5.2728e-02, 4.4529e-03, 3.4856e-02],\n",
      "        [7.3195e-03, 1.8133e-02, 2.3350e-02, 3.7193e-01, 1.1823e-01, 3.1794e-01,\n",
      "         2.6056e-02, 7.9137e-02, 5.0171e-03, 3.2888e-02],\n",
      "        [7.8177e-03, 4.9594e-03, 6.4370e-03, 2.8138e-01, 9.6554e-02, 3.4971e-01,\n",
      "         2.0347e-02, 1.7564e-01, 4.0496e-03, 5.3113e-02],\n",
      "        [1.9933e-03, 3.1114e-03, 2.7274e-03, 9.1238e-02, 1.3728e-01, 3.0906e-01,\n",
      "         5.4643e-02, 3.4090e-01, 5.7774e-03, 5.3264e-02],\n",
      "        [9.4570e-04, 1.1621e-03, 6.8212e-04, 3.1437e-02, 5.4056e-02, 4.2687e-01,\n",
      "         5.5542e-02, 3.0493e-01, 1.9664e-02, 1.0470e-01],\n",
      "        [7.9850e-05, 1.8286e-04, 1.5421e-04, 1.1164e-02, 7.1675e-03, 1.9437e-01,\n",
      "         3.2808e-02, 3.9067e-01, 2.7435e-02, 3.3597e-01]])\n",
      "\n",
      "Source: 而 如果 这些 照片 不先 清洗 干净 我们 也 无法\n",
      "Reference: we couldn &apos;t <UNK> the photo unless it was\n",
      "Model: <SOS> and , &apos;t have this . . we &apos;re\n",
      "Attention Weights: tensor([[0.0074, 0.0010, 0.7156, 0.0052, 0.0067, 0.0199, 0.0096, 0.1344, 0.0066,\n",
      "         0.0936],\n",
      "        [0.0281, 0.0076, 0.9101, 0.0016, 0.0017, 0.0035, 0.0012, 0.0197, 0.0024,\n",
      "         0.0242],\n",
      "        [0.0252, 0.0163, 0.6154, 0.0174, 0.0102, 0.0159, 0.0063, 0.0265, 0.0302,\n",
      "         0.2367],\n",
      "        [0.0098, 0.0169, 0.6000, 0.0488, 0.0558, 0.0290, 0.0054, 0.0366, 0.0227,\n",
      "         0.1751],\n",
      "        [0.0031, 0.0059, 0.4117, 0.0751, 0.1131, 0.0731, 0.0159, 0.0467, 0.0269,\n",
      "         0.2284],\n",
      "        [0.0008, 0.0013, 0.1883, 0.1135, 0.2176, 0.1875, 0.0426, 0.0627, 0.0326,\n",
      "         0.1531],\n",
      "        [0.0002, 0.0002, 0.0081, 0.0200, 0.0917, 0.3310, 0.1591, 0.2293, 0.0352,\n",
      "         0.1252],\n",
      "        [0.0003, 0.0002, 0.0053, 0.0116, 0.0384, 0.2568, 0.1445, 0.2543, 0.0502,\n",
      "         0.2384],\n",
      "        [0.0001, 0.0001, 0.0137, 0.0039, 0.0130, 0.0705, 0.0229, 0.4178, 0.0486,\n",
      "         0.4094]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.15, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 7.47, Minutes Elapsed: 352.09\n",
      "Sampling from val predictions...\n",
      "Source: 开车 车行 行驶 在 加纳 的 一条 路上 与 free\n",
      "Reference: driving down a road in ghana with partners of\n",
      "Model: <SOS> and the , the , the , , ,\n",
      "Attention Weights: tensor([[0.0184, 0.0064, 0.0487, 0.0144, 0.0236, 0.0113, 0.0019, 0.0129, 0.0551,\n",
      "         0.8073],\n",
      "        [0.1142, 0.1038, 0.4047, 0.0268, 0.0659, 0.0128, 0.0034, 0.0123, 0.0322,\n",
      "         0.2241],\n",
      "        [0.0246, 0.0523, 0.1576, 0.0335, 0.0590, 0.0402, 0.0077, 0.0222, 0.1021,\n",
      "         0.5008],\n",
      "        [0.0016, 0.0054, 0.1531, 0.0982, 0.0856, 0.0248, 0.0056, 0.0148, 0.0676,\n",
      "         0.5433],\n",
      "        [0.0004, 0.0013, 0.0802, 0.1045, 0.1588, 0.0129, 0.0035, 0.0145, 0.0605,\n",
      "         0.5635],\n",
      "        [0.0004, 0.0007, 0.0936, 0.0374, 0.0829, 0.0129, 0.0043, 0.0261, 0.1139,\n",
      "         0.6277],\n",
      "        [0.0001, 0.0002, 0.0146, 0.0112, 0.0332, 0.0054, 0.0022, 0.0170, 0.0894,\n",
      "         0.8267],\n",
      "        [0.0000, 0.0000, 0.0051, 0.0125, 0.0293, 0.0082, 0.0045, 0.0309, 0.1433,\n",
      "         0.7661],\n",
      "        [0.0000, 0.0000, 0.0017, 0.0027, 0.0122, 0.0023, 0.0009, 0.0096, 0.1101,\n",
      "         0.8604]])\n",
      "\n",
      "Source: 我 并不知道 不知 知道 该 从 这个 实验 里 期待\n",
      "Reference: i didn &apos;t know what to expect from this\n",
      "Model: <SOS> i don &apos;t know that she was , this\n",
      "Attention Weights: tensor([[9.9997e-01, 8.2242e-06, 5.1807e-06, 2.4229e-07, 4.4100e-07, 1.1910e-07,\n",
      "         6.7037e-06, 1.8295e-07, 5.8750e-07, 1.0380e-05],\n",
      "        [9.9798e-01, 1.1554e-03, 3.8939e-04, 2.7222e-05, 1.2812e-05, 3.8523e-06,\n",
      "         1.6153e-04, 3.4059e-06, 1.0571e-05, 2.5453e-04],\n",
      "        [8.3102e-01, 1.0803e-02, 1.5828e-02, 1.2870e-02, 7.5251e-03, 3.2664e-03,\n",
      "         2.3036e-02, 1.1534e-03, 3.8818e-03, 9.0618e-02],\n",
      "        [9.4737e-02, 2.6167e-02, 9.4877e-02, 1.6917e-01, 9.2369e-02, 5.3858e-02,\n",
      "         2.4176e-01, 1.1670e-02, 2.4219e-02, 1.9117e-01],\n",
      "        [8.7620e-03, 1.4485e-03, 2.6431e-03, 2.9993e-02, 1.3848e-01, 5.7265e-02,\n",
      "         4.7548e-01, 1.5741e-02, 1.9141e-02, 2.5105e-01],\n",
      "        [5.2368e-03, 5.9892e-04, 6.2176e-04, 8.5339e-03, 7.0293e-02, 3.7715e-02,\n",
      "         6.3020e-01, 3.1820e-02, 2.6000e-02, 1.8898e-01],\n",
      "        [1.3903e-02, 2.0887e-03, 2.6600e-03, 7.7064e-03, 2.3472e-02, 1.3478e-02,\n",
      "         4.1599e-01, 5.4916e-02, 5.7575e-02, 4.0821e-01],\n",
      "        [7.1274e-03, 2.3815e-04, 2.7267e-04, 1.0325e-03, 3.8320e-03, 1.7918e-03,\n",
      "         2.6769e-01, 1.0291e-02, 2.7799e-02, 6.7992e-01],\n",
      "        [5.9806e-03, 1.4343e-03, 2.5917e-03, 4.7207e-03, 1.6789e-02, 1.1022e-02,\n",
      "         3.3830e-01, 9.6694e-02, 9.9805e-02, 4.2267e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.30, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 8.24, Minutes Elapsed: 360.85\n",
      "Sampling from val predictions...\n",
      "Source: 但 可口 可口可乐 可乐 可 没有 资助 这个 实验 <EOS>\n",
      "Reference: but , coca-cola did not sponsor this research .\n",
      "Model: <SOS> but it , &apos;s not have this <EOS> .\n",
      "Attention Weights: tensor([[0.0099, 0.0091, 0.0189, 0.0025, 0.1599, 0.0006, 0.0008, 0.7808, 0.0120,\n",
      "         0.0054],\n",
      "        [0.0053, 0.2086, 0.5916, 0.0127, 0.0872, 0.0014, 0.0018, 0.0640, 0.0038,\n",
      "         0.0237],\n",
      "        [0.0127, 0.0612, 0.0713, 0.0728, 0.4847, 0.0126, 0.0096, 0.1717, 0.0211,\n",
      "         0.0824],\n",
      "        [0.0049, 0.0374, 0.0803, 0.1090, 0.4048, 0.0335, 0.0226, 0.1557, 0.0435,\n",
      "         0.1084],\n",
      "        [0.0040, 0.0229, 0.0624, 0.0775, 0.3615, 0.0481, 0.0382, 0.2223, 0.0601,\n",
      "         0.1030],\n",
      "        [0.0014, 0.0015, 0.0100, 0.0159, 0.1361, 0.0708, 0.1369, 0.4644, 0.1138,\n",
      "         0.0491],\n",
      "        [0.0042, 0.0096, 0.0076, 0.0116, 0.0862, 0.0480, 0.0885, 0.5468, 0.0802,\n",
      "         0.1173],\n",
      "        [0.0051, 0.0103, 0.0139, 0.0130, 0.0604, 0.0459, 0.1197, 0.4824, 0.1401,\n",
      "         0.1093],\n",
      "        [0.0053, 0.0061, 0.0031, 0.0020, 0.0245, 0.0262, 0.0883, 0.4702, 0.1773,\n",
      "         0.1971]])\n",
      "\n",
      "Source: 所以 在 那 之后 5 年 我 <UNK> <UNK> 陪\n",
      "Reference: so for the next five years , i dressed\n",
      "Model: <SOS> so , in years 2000 years , i was\n",
      "Attention Weights: tensor([[0.0004, 0.0001, 0.0003, 0.0030, 0.0137, 0.0026, 0.9448, 0.0017, 0.0028,\n",
      "         0.0306],\n",
      "        [0.0423, 0.0210, 0.0585, 0.1856, 0.1735, 0.0289, 0.3595, 0.0285, 0.0221,\n",
      "         0.0801],\n",
      "        [0.0172, 0.0302, 0.1283, 0.3983, 0.1101, 0.0339, 0.1240, 0.0464, 0.0332,\n",
      "         0.0782],\n",
      "        [0.0071, 0.0060, 0.0752, 0.4038, 0.0818, 0.0354, 0.2724, 0.0291, 0.0203,\n",
      "         0.0688],\n",
      "        [0.0093, 0.0061, 0.0471, 0.3280, 0.2434, 0.0717, 0.1980, 0.0249, 0.0173,\n",
      "         0.0543],\n",
      "        [0.0062, 0.0046, 0.0322, 0.2183, 0.1085, 0.0919, 0.4111, 0.0359, 0.0265,\n",
      "         0.0647],\n",
      "        [0.0021, 0.0014, 0.0080, 0.0883, 0.0753, 0.0780, 0.6300, 0.0351, 0.0260,\n",
      "         0.0558],\n",
      "        [0.0012, 0.0007, 0.0033, 0.0431, 0.0290, 0.0835, 0.7025, 0.0410, 0.0293,\n",
      "         0.0663],\n",
      "        [0.0002, 0.0003, 0.0024, 0.0346, 0.0308, 0.0684, 0.7575, 0.0266, 0.0186,\n",
      "         0.0606]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.45, Train Loss: 0.00, Val Loss: 4.87, Train BLEU: 0.00, Val BLEU: 7.82, Minutes Elapsed: 369.57\n",
      "Sampling from val predictions...\n",
      "Source: 非法 <UNK> 性交 性交易 交易 是 我们 在 听到 奴役\n",
      "Reference: sex trafficking is what we often think of when\n",
      "Model: <SOS> and the , is we &apos;re in in in\n",
      "Attention Weights: tensor([[0.0034, 0.0052, 0.1237, 0.0934, 0.0811, 0.0784, 0.5715, 0.0033, 0.0064,\n",
      "         0.0335],\n",
      "        [0.0156, 0.0506, 0.2500, 0.3084, 0.0953, 0.0340, 0.2389, 0.0009, 0.0018,\n",
      "         0.0044],\n",
      "        [0.0223, 0.0628, 0.2538, 0.2301, 0.1577, 0.0687, 0.0772, 0.0119, 0.0132,\n",
      "         0.1024],\n",
      "        [0.0008, 0.0023, 0.0259, 0.0302, 0.1513, 0.3099, 0.4218, 0.0260, 0.0126,\n",
      "         0.0191],\n",
      "        [0.0002, 0.0009, 0.0052, 0.0108, 0.0630, 0.1926, 0.5206, 0.0594, 0.0497,\n",
      "         0.0976],\n",
      "        [0.0007, 0.0006, 0.0071, 0.0046, 0.0124, 0.0417, 0.6146, 0.0816, 0.0825,\n",
      "         0.1542],\n",
      "        [0.0006, 0.0013, 0.0110, 0.0097, 0.0055, 0.0074, 0.3431, 0.0960, 0.1317,\n",
      "         0.3937],\n",
      "        [0.0005, 0.0013, 0.0041, 0.0030, 0.0044, 0.0080, 0.0554, 0.1647, 0.2479,\n",
      "         0.5106],\n",
      "        [0.0004, 0.0007, 0.0041, 0.0024, 0.0033, 0.0066, 0.0476, 0.1235, 0.2976,\n",
      "         0.5136]])\n",
      "\n",
      "Source: 那么 我 为什么 什么 做 这个 呢 现场 <UNK> <EOS>\n",
      "Reference: so why did i do that ? <EOS> <PAD>\n",
      "Model: <SOS> and , i i do this ? ? &apos;s\n",
      "Attention Weights: tensor([[0.0007, 0.9893, 0.0031, 0.0008, 0.0002, 0.0031, 0.0003, 0.0012, 0.0007,\n",
      "         0.0005],\n",
      "        [0.0120, 0.9376, 0.0453, 0.0020, 0.0001, 0.0019, 0.0001, 0.0003, 0.0004,\n",
      "         0.0003],\n",
      "        [0.0591, 0.4154, 0.2998, 0.0515, 0.0098, 0.0432, 0.0114, 0.0259, 0.0457,\n",
      "         0.0382],\n",
      "        [0.0136, 0.4221, 0.2934, 0.1082, 0.0079, 0.0970, 0.0073, 0.0174, 0.0170,\n",
      "         0.0161],\n",
      "        [0.0055, 0.3249, 0.1275, 0.1245, 0.0216, 0.2807, 0.0225, 0.0517, 0.0251,\n",
      "         0.0159],\n",
      "        [0.0002, 0.0176, 0.0064, 0.0275, 0.0149, 0.7570, 0.0163, 0.0807, 0.0530,\n",
      "         0.0264],\n",
      "        [0.0006, 0.0152, 0.0222, 0.0389, 0.0229, 0.6698, 0.0325, 0.1187, 0.0484,\n",
      "         0.0309],\n",
      "        [0.0025, 0.0249, 0.0215, 0.0303, 0.0194, 0.4853, 0.0998, 0.2260, 0.0643,\n",
      "         0.0259],\n",
      "        [0.0010, 0.0196, 0.0099, 0.0234, 0.0101, 0.2086, 0.0944, 0.1765, 0.2549,\n",
      "         0.2017]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.60, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 7.88, Minutes Elapsed: 378.26\n",
      "Sampling from val predictions...\n",
      "Source: 禁止 在 一小 一小块 小块 土地 地上 种 食物 而\n",
      "Reference: a warrant for planting food on a piece of\n",
      "Model: <SOS> and in years in , , , , ,\n",
      "Attention Weights: tensor([[0.0017, 0.0152, 0.0034, 0.0008, 0.0025, 0.0133, 0.0069, 0.1175, 0.0017,\n",
      "         0.8370],\n",
      "        [0.0401, 0.2604, 0.0825, 0.0253, 0.0511, 0.0647, 0.0189, 0.1121, 0.0034,\n",
      "         0.3414],\n",
      "        [0.0208, 0.2789, 0.0829, 0.0376, 0.0406, 0.0446, 0.0150, 0.0995, 0.0075,\n",
      "         0.3726],\n",
      "        [0.0183, 0.2575, 0.0386, 0.0197, 0.0439, 0.0705, 0.0324, 0.0810, 0.0072,\n",
      "         0.4309],\n",
      "        [0.0027, 0.0513, 0.0355, 0.0171, 0.0534, 0.1362, 0.0379, 0.1598, 0.0144,\n",
      "         0.4917],\n",
      "        [0.0022, 0.0595, 0.0060, 0.0012, 0.0094, 0.1171, 0.0483, 0.2421, 0.0177,\n",
      "         0.4965],\n",
      "        [0.0008, 0.0414, 0.0219, 0.0063, 0.0281, 0.1064, 0.0375, 0.3153, 0.0126,\n",
      "         0.4297],\n",
      "        [0.0003, 0.0101, 0.0063, 0.0016, 0.0052, 0.0342, 0.0181, 0.2862, 0.0154,\n",
      "         0.6226],\n",
      "        [0.0007, 0.0152, 0.0091, 0.0022, 0.0160, 0.0638, 0.0520, 0.1819, 0.0270,\n",
      "         0.6320]])\n",
      "\n",
      "Source: 因为 <UNK> 数量 的 <UNK> 野狗 开始 在 牲畜 尸体\n",
      "Reference: because there are no <UNK> , there &apos;s been\n",
      "Model: <SOS> because the &apos;s a , , , there a\n",
      "Attention Weights: tensor([[0.0471, 0.0027, 0.0067, 0.0153, 0.0127, 0.0915, 0.0479, 0.0362, 0.0115,\n",
      "         0.7285],\n",
      "        [0.2911, 0.0550, 0.1157, 0.0717, 0.0589, 0.1617, 0.0523, 0.0210, 0.0125,\n",
      "         0.1601],\n",
      "        [0.1793, 0.0992, 0.2051, 0.1619, 0.0593, 0.0515, 0.0179, 0.0131, 0.0059,\n",
      "         0.2069],\n",
      "        [0.1109, 0.1633, 0.2087, 0.1598, 0.0651, 0.0914, 0.0254, 0.0131, 0.0050,\n",
      "         0.1572],\n",
      "        [0.0408, 0.0529, 0.0638, 0.1625, 0.0653, 0.0755, 0.0555, 0.0590, 0.0268,\n",
      "         0.3980],\n",
      "        [0.1000, 0.0675, 0.0983, 0.0651, 0.0668, 0.1359, 0.0592, 0.0277, 0.0239,\n",
      "         0.3558],\n",
      "        [0.1857, 0.0641, 0.0443, 0.0276, 0.0460, 0.0877, 0.0441, 0.0194, 0.0177,\n",
      "         0.4635],\n",
      "        [0.0952, 0.0364, 0.0156, 0.0109, 0.0304, 0.0936, 0.0447, 0.0528, 0.0271,\n",
      "         0.5933],\n",
      "        [0.0661, 0.0474, 0.0407, 0.0372, 0.0389, 0.0949, 0.0371, 0.0360, 0.0314,\n",
      "         0.5704]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.75, Train Loss: 0.00, Val Loss: 4.88, Train BLEU: 0.00, Val BLEU: 7.87, Minutes Elapsed: 386.94\n",
      "Sampling from val predictions...\n",
      "Source: 就 这个 问题 在 国会 会上 争论 了 很久 很久很久\n",
      "Reference: and this was debated in congress for ages and\n",
      "Model: <SOS> and , problem is , the , the ,\n",
      "Attention Weights: tensor([[0.0015, 0.7608, 0.1062, 0.0088, 0.0633, 0.0003, 0.0012, 0.0011, 0.0019,\n",
      "         0.0550],\n",
      "        [0.0029, 0.9323, 0.0570, 0.0002, 0.0059, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "         0.0012],\n",
      "        [0.0105, 0.4201, 0.3945, 0.0325, 0.0325, 0.0054, 0.0045, 0.0077, 0.0088,\n",
      "         0.0836],\n",
      "        [0.0140, 0.4115, 0.2783, 0.0510, 0.0596, 0.0041, 0.0031, 0.0061, 0.0058,\n",
      "         0.1665],\n",
      "        [0.0013, 0.1272, 0.6195, 0.0456, 0.1628, 0.0041, 0.0028, 0.0035, 0.0037,\n",
      "         0.0295],\n",
      "        [0.0033, 0.1187, 0.2354, 0.0650, 0.3541, 0.0089, 0.0100, 0.0086, 0.0094,\n",
      "         0.1866],\n",
      "        [0.0001, 0.0596, 0.0188, 0.0363, 0.7088, 0.0055, 0.0054, 0.0087, 0.0050,\n",
      "         0.1517],\n",
      "        [0.0001, 0.0193, 0.0705, 0.0564, 0.5120, 0.0065, 0.0164, 0.0149, 0.0199,\n",
      "         0.2840],\n",
      "        [0.0004, 0.0389, 0.0091, 0.0237, 0.5818, 0.0072, 0.0218, 0.0164, 0.0171,\n",
      "         0.2837]])\n",
      "\n",
      "Source: 对 一些 修 图 师 这段 经历 为 他们 建立\n",
      "Reference: for some of them , it &apos;s given them\n",
      "Model: <SOS> and , , , , , &apos;s like to\n",
      "Attention Weights: tensor([[0.0005, 0.0005, 0.0163, 0.0949, 0.1111, 0.1900, 0.0177, 0.0421, 0.3537,\n",
      "         0.1732],\n",
      "        [0.0297, 0.1725, 0.2073, 0.1549, 0.0806, 0.1454, 0.0069, 0.0133, 0.1171,\n",
      "         0.0723],\n",
      "        [0.0591, 0.1702, 0.2212, 0.2062, 0.0819, 0.0570, 0.0059, 0.0117, 0.0416,\n",
      "         0.1449],\n",
      "        [0.0063, 0.0429, 0.1381, 0.4326, 0.0937, 0.1733, 0.0054, 0.0074, 0.0358,\n",
      "         0.0646],\n",
      "        [0.0032, 0.0198, 0.0677, 0.4713, 0.1650, 0.1632, 0.0067, 0.0070, 0.0277,\n",
      "         0.0684],\n",
      "        [0.0017, 0.0076, 0.0745, 0.5636, 0.1564, 0.1131, 0.0107, 0.0089, 0.0295,\n",
      "         0.0340],\n",
      "        [0.0005, 0.0030, 0.0294, 0.2037, 0.1641, 0.4263, 0.0127, 0.0135, 0.0778,\n",
      "         0.0690],\n",
      "        [0.0015, 0.0059, 0.0173, 0.1165, 0.2085, 0.3484, 0.0333, 0.0235, 0.1319,\n",
      "         0.1130],\n",
      "        [0.0000, 0.0004, 0.0016, 0.0239, 0.0795, 0.1921, 0.0718, 0.0589, 0.4454,\n",
      "         0.1263]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.90, Train Loss: 0.00, Val Loss: 4.86, Train BLEU: 0.00, Val BLEU: 7.90, Minutes Elapsed: 395.66\n",
      "Sampling from val predictions...\n",
      "Source: 新大 新大陆 大陆 的 <UNK> 主要 生长 在 美洲 大陆\n",
      "Reference: there are the new world <UNK> that are mainly\n",
      "Model: <SOS> the the thousands of of of the the the\n",
      "Attention Weights: tensor([[0.0250, 0.0378, 0.0153, 0.1015, 0.0098, 0.0686, 0.0283, 0.0359, 0.0136,\n",
      "         0.6642],\n",
      "        [0.0264, 0.0114, 0.1457, 0.1192, 0.0155, 0.2285, 0.0876, 0.0455, 0.0368,\n",
      "         0.2833],\n",
      "        [0.0099, 0.0132, 0.1221, 0.1390, 0.0523, 0.1737, 0.1128, 0.0318, 0.0228,\n",
      "         0.3224],\n",
      "        [0.0547, 0.0676, 0.1540, 0.1061, 0.0308, 0.0981, 0.0671, 0.0386, 0.0571,\n",
      "         0.3260],\n",
      "        [0.0094, 0.0035, 0.0089, 0.0356, 0.0140, 0.2509, 0.2410, 0.0355, 0.0601,\n",
      "         0.3411],\n",
      "        [0.0034, 0.0028, 0.0050, 0.0119, 0.0113, 0.1493, 0.1888, 0.0179, 0.0315,\n",
      "         0.5780],\n",
      "        [0.0008, 0.0010, 0.0012, 0.0029, 0.0084, 0.1154, 0.2772, 0.0383, 0.0604,\n",
      "         0.4945],\n",
      "        [0.0020, 0.0020, 0.0020, 0.0009, 0.0027, 0.0500, 0.0786, 0.0439, 0.0793,\n",
      "         0.7384],\n",
      "        [0.0007, 0.0005, 0.0010, 0.0004, 0.0018, 0.0684, 0.1880, 0.1462, 0.2051,\n",
      "         0.3880]])\n",
      "\n",
      "Source: 所以 你 已经 拿到 了 这个 奖学 奖学金 richard <UNK>\n",
      "Reference: so you got this scholarship . yep . <EOS>\n",
      "Model: <SOS> so you you to , . <EOS> . <EOS>\n",
      "Attention Weights: tensor([[2.1373e-03, 9.7872e-01, 8.9484e-06, 4.5708e-05, 4.1402e-04, 1.4558e-02,\n",
      "         9.6619e-05, 3.3454e-04, 2.5448e-03, 1.1420e-03],\n",
      "        [1.6467e-03, 9.9742e-01, 4.0851e-05, 4.4971e-05, 1.3724e-05, 2.4620e-04,\n",
      "         1.7656e-05, 3.0468e-05, 2.6575e-04, 2.7321e-04],\n",
      "        [1.8209e-02, 6.2138e-01, 3.7602e-02, 3.5356e-02, 2.6463e-02, 8.5668e-02,\n",
      "         1.3586e-02, 2.3165e-02, 6.0204e-02, 7.8366e-02],\n",
      "        [9.8349e-03, 5.0972e-01, 3.4568e-02, 4.1224e-02, 2.8421e-02, 1.6259e-01,\n",
      "         1.7710e-02, 2.5471e-02, 8.6737e-02, 8.3727e-02],\n",
      "        [3.6613e-03, 1.7135e-02, 5.1916e-03, 5.2301e-02, 8.8271e-02, 7.7225e-01,\n",
      "         1.2876e-02, 1.2294e-02, 2.5942e-02, 1.0082e-02],\n",
      "        [8.6091e-03, 2.2641e-02, 4.8969e-03, 3.4466e-02, 5.4801e-02, 4.6916e-01,\n",
      "         7.3115e-02, 1.1477e-01, 1.3602e-01, 8.1517e-02],\n",
      "        [3.0873e-03, 9.4844e-03, 3.4642e-03, 1.6804e-02, 4.4397e-02, 2.1467e-01,\n",
      "         1.1269e-01, 2.2454e-01, 2.8546e-01, 8.5398e-02],\n",
      "        [9.5268e-03, 7.8992e-02, 1.5765e-03, 7.2032e-03, 1.1961e-02, 2.7892e-01,\n",
      "         4.6406e-02, 9.0677e-02, 2.3340e-01, 2.4134e-01],\n",
      "        [2.6728e-03, 2.9477e-02, 2.3012e-03, 6.0204e-03, 3.7095e-02, 2.6401e-01,\n",
      "         6.0159e-02, 1.1048e-01, 2.5824e-01, 2.2954e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.00, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.92, Minutes Elapsed: 401.51\n",
      "Sampling from val predictions...\n",
      "Source: 当 她们 走出 矿井 时 全身 <UNK> <EOS> <PAD> <PAD>\n",
      "Reference: when they came out of the shaft , they\n",
      "Model: <SOS> when when have to the the , , the\n",
      "Attention Weights: tensor([[0.0764, 0.9172, 0.0001, 0.0010, 0.0003, 0.0017, 0.0016, 0.0017, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0150, 0.9809, 0.0002, 0.0013, 0.0000, 0.0005, 0.0011, 0.0009, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1152, 0.5684, 0.0376, 0.0529, 0.0177, 0.0393, 0.0846, 0.0843, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0191, 0.1164, 0.0694, 0.1958, 0.0237, 0.1019, 0.2501, 0.2236, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0201, 0.0628, 0.1366, 0.5810, 0.0585, 0.0532, 0.0515, 0.0363, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0381, 0.0614, 0.0912, 0.5200, 0.1179, 0.0785, 0.0503, 0.0425, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0154, 0.0674, 0.0553, 0.4167, 0.1094, 0.1502, 0.0952, 0.0903, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0241, 0.0354, 0.0606, 0.2912, 0.2100, 0.1785, 0.0987, 0.1015, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0739, 0.1030, 0.0224, 0.1514, 0.0754, 0.1121, 0.2172, 0.2448, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 第一 第一件 一件 是 他 也 毕业 于 <UNK> <UNK>\n",
      "Reference: one was that he , too , had just\n",
      "Model: <SOS> the first , he he he was . was\n",
      "Attention Weights: tensor([[0.0094, 0.0157, 0.0005, 0.0004, 0.9723, 0.0004, 0.0005, 0.0003, 0.0003,\n",
      "         0.0003],\n",
      "        [0.0739, 0.0956, 0.0320, 0.0033, 0.7885, 0.0013, 0.0019, 0.0007, 0.0017,\n",
      "         0.0010],\n",
      "        [0.1008, 0.0626, 0.1157, 0.0450, 0.5494, 0.0251, 0.0177, 0.0220, 0.0282,\n",
      "         0.0335],\n",
      "        [0.0210, 0.0169, 0.0235, 0.0442, 0.8319, 0.0192, 0.0134, 0.0102, 0.0113,\n",
      "         0.0084],\n",
      "        [0.0025, 0.0038, 0.0040, 0.0213, 0.7956, 0.0477, 0.0682, 0.0172, 0.0223,\n",
      "         0.0173],\n",
      "        [0.0172, 0.0150, 0.0064, 0.0155, 0.5584, 0.0606, 0.0731, 0.0531, 0.1135,\n",
      "         0.0872],\n",
      "        [0.0007, 0.0004, 0.0001, 0.0013, 0.0526, 0.0657, 0.3342, 0.1671, 0.1982,\n",
      "         0.1796],\n",
      "        [0.0023, 0.0008, 0.0006, 0.0037, 0.2097, 0.1672, 0.2328, 0.1529, 0.1275,\n",
      "         0.1026],\n",
      "        [0.0008, 0.0006, 0.0002, 0.0015, 0.1731, 0.0865, 0.2156, 0.1449, 0.1761,\n",
      "         0.2006]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.15, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 7.47, Minutes Elapsed: 410.17\n",
      "Sampling from val predictions...\n",
      "Source: 没人 能 独自 自创 创立 一个 企业 没人 <EOS> <PAD>\n",
      "Reference: nobody started a company alone . no one .\n",
      "Model: <SOS> &quot; &apos;s , the , a a was .\n",
      "Attention Weights: tensor([[0.2586, 0.0110, 0.0105, 0.0137, 0.0187, 0.4035, 0.0362, 0.2321, 0.0158,\n",
      "         0.0000],\n",
      "        [0.4510, 0.1184, 0.0806, 0.0147, 0.0259, 0.0556, 0.0167, 0.1086, 0.1284,\n",
      "         0.0000],\n",
      "        [0.1152, 0.0537, 0.1151, 0.0235, 0.0266, 0.0374, 0.0150, 0.0853, 0.5281,\n",
      "         0.0000],\n",
      "        [0.0460, 0.0372, 0.2687, 0.1466, 0.0840, 0.1044, 0.0366, 0.1180, 0.1585,\n",
      "         0.0000],\n",
      "        [0.0040, 0.0058, 0.0420, 0.1231, 0.1159, 0.3335, 0.0525, 0.1927, 0.1305,\n",
      "         0.0000],\n",
      "        [0.0044, 0.0016, 0.0082, 0.0440, 0.0927, 0.1458, 0.1920, 0.2108, 0.3005,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0006, 0.0107, 0.0323, 0.1200, 0.2876, 0.0768, 0.2400, 0.2296,\n",
      "         0.0000],\n",
      "        [0.0010, 0.0009, 0.0179, 0.0665, 0.1742, 0.3664, 0.0881, 0.1512, 0.1338,\n",
      "         0.0000],\n",
      "        [0.0006, 0.0010, 0.0167, 0.0365, 0.1141, 0.2956, 0.1062, 0.2556, 0.1736,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 很多 <UNK> 被 <UNK> 中毒 的 原因 是因为 因为 在\n",
      "Reference: the reason is that they &apos;re getting poisoned ,\n",
      "Model: <SOS> the the of the the of of in ,\n",
      "Attention Weights: tensor([[0.1586, 0.0015, 0.0329, 0.0003, 0.0586, 0.0523, 0.3313, 0.0014, 0.1899,\n",
      "         0.1731],\n",
      "        [0.1802, 0.0256, 0.0872, 0.0075, 0.1545, 0.0326, 0.2151, 0.0085, 0.2172,\n",
      "         0.0715],\n",
      "        [0.2637, 0.0427, 0.1184, 0.0197, 0.0729, 0.0190, 0.1364, 0.0068, 0.1023,\n",
      "         0.2180],\n",
      "        [0.2433, 0.0474, 0.0980, 0.0173, 0.0728, 0.0226, 0.1023, 0.0127, 0.1057,\n",
      "         0.2779],\n",
      "        [0.1169, 0.0235, 0.0715, 0.0223, 0.0864, 0.0249, 0.1837, 0.0051, 0.1041,\n",
      "         0.3616],\n",
      "        [0.0838, 0.0053, 0.0244, 0.0062, 0.0954, 0.0259, 0.2058, 0.0068, 0.2454,\n",
      "         0.3009],\n",
      "        [0.0173, 0.0018, 0.0062, 0.0026, 0.0444, 0.0079, 0.0866, 0.0026, 0.1815,\n",
      "         0.6491],\n",
      "        [0.0219, 0.0017, 0.0036, 0.0037, 0.0364, 0.0208, 0.2292, 0.0085, 0.1584,\n",
      "         0.5159],\n",
      "        [0.0078, 0.0011, 0.0019, 0.0017, 0.0241, 0.0180, 0.3204, 0.0176, 0.2518,\n",
      "         0.3556]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.30, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 8.07, Minutes Elapsed: 418.85\n",
      "Sampling from val predictions...\n",
      "Source: 答案 很 简单 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: the answer is easy . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it , is yes . <EOS> . . <EOS>\n",
      "Attention Weights: tensor([[0.0009, 0.0057, 0.9928, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0601, 0.0161, 0.8988, 0.0250, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1761, 0.1998, 0.4437, 0.1804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1191, 0.1497, 0.5951, 0.1361, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0515, 0.0629, 0.7504, 0.1352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1085, 0.0676, 0.5515, 0.2725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1023, 0.0602, 0.4755, 0.3620, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0986, 0.1004, 0.3213, 0.4797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1397, 0.0631, 0.3603, 0.4369, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Source: 我们 是 一个 自由 的 组织 由来 来自 不同 同行\n",
      "Reference: what we do , we &apos;re a <UNK> kind\n",
      "Model: <SOS> we &apos;re we is we a a a ,\n",
      "Attention Weights: tensor([[9.9941e-01, 6.7227e-06, 2.9206e-06, 7.5174e-07, 1.9760e-06, 8.1515e-07,\n",
      "         1.3488e-05, 2.5998e-05, 4.0608e-06, 5.2909e-04],\n",
      "        [9.9135e-01, 3.9374e-03, 2.1949e-03, 1.9092e-04, 1.5239e-04, 2.2622e-05,\n",
      "         6.6420e-05, 1.3595e-04, 1.9685e-05, 1.9299e-03],\n",
      "        [3.4506e-01, 1.7730e-01, 1.4935e-01, 5.2460e-02, 2.9418e-02, 1.2296e-02,\n",
      "         2.0278e-02, 3.2202e-02, 1.2083e-02, 1.6955e-01],\n",
      "        [2.3906e-01, 9.9226e-02, 2.5480e-01, 6.8783e-02, 4.5831e-02, 1.6085e-02,\n",
      "         2.3121e-02, 4.1967e-02, 1.6918e-02, 1.9421e-01],\n",
      "        [1.0211e-01, 9.0789e-02, 3.6939e-01, 1.7935e-01, 9.2184e-02, 4.0158e-02,\n",
      "         3.1532e-02, 3.5547e-02, 9.1195e-03, 4.9816e-02],\n",
      "        [1.4120e-01, 5.1153e-02, 1.9742e-01, 7.5375e-02, 6.8878e-02, 6.1788e-02,\n",
      "         8.7477e-02, 1.2125e-01, 3.2695e-02, 1.6276e-01],\n",
      "        [1.3542e-01, 5.8264e-02, 2.6579e-01, 8.5871e-02, 5.3742e-02, 3.3002e-02,\n",
      "         8.8295e-02, 1.2147e-01, 2.1367e-02, 1.3677e-01],\n",
      "        [6.1418e-04, 6.4898e-03, 2.9452e-01, 1.7554e-01, 5.1815e-02, 4.5826e-02,\n",
      "         6.3904e-02, 2.1000e-01, 2.9268e-02, 1.2202e-01],\n",
      "        [1.7575e-03, 5.2541e-03, 3.0422e-01, 7.1333e-02, 7.6980e-02, 6.3815e-02,\n",
      "         7.9821e-02, 1.5996e-01, 2.8407e-02, 2.0846e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.45, Train Loss: 0.00, Val Loss: 4.87, Train BLEU: 0.00, Val BLEU: 7.80, Minutes Elapsed: 427.61\n",
      "Sampling from val predictions...\n",
      "Source: 她 也 开始 始自 自己 亲手 清洗 <UNK> 照片 直到\n",
      "Reference: she had started <UNK> them herself and stopped when\n",
      "Model: <SOS> she she a go the , the the and\n",
      "Attention Weights: tensor([[0.9512, 0.0008, 0.0006, 0.0005, 0.0042, 0.0027, 0.0022, 0.0015, 0.0015,\n",
      "         0.0347],\n",
      "        [0.9347, 0.0087, 0.0138, 0.0071, 0.0120, 0.0022, 0.0015, 0.0019, 0.0009,\n",
      "         0.0173],\n",
      "        [0.0812, 0.0613, 0.0950, 0.0503, 0.0439, 0.0202, 0.0126, 0.0240, 0.0074,\n",
      "         0.6042],\n",
      "        [0.0365, 0.0283, 0.1915, 0.1819, 0.1229, 0.1082, 0.0485, 0.0260, 0.0089,\n",
      "         0.2473],\n",
      "        [0.0061, 0.0041, 0.0373, 0.1152, 0.2630, 0.0894, 0.0759, 0.0248, 0.0121,\n",
      "         0.3721],\n",
      "        [0.0259, 0.0091, 0.0292, 0.1354, 0.3446, 0.1125, 0.0725, 0.0189, 0.0128,\n",
      "         0.2391],\n",
      "        [0.0037, 0.0016, 0.0034, 0.0317, 0.0686, 0.1882, 0.1297, 0.0618, 0.0567,\n",
      "         0.4546],\n",
      "        [0.0009, 0.0002, 0.0007, 0.0064, 0.0216, 0.0323, 0.0888, 0.0266, 0.0241,\n",
      "         0.7984],\n",
      "        [0.0012, 0.0004, 0.0015, 0.0106, 0.0332, 0.0460, 0.1282, 0.0401, 0.0631,\n",
      "         0.6759]])\n",
      "\n",
      "Source: 而且 它 的 意思 是 一个 <UNK> 的 政客 <EOS>\n",
      "Reference: and what <UNK> means is &quot; a dishonest politician\n",
      "Model: <SOS> and it &apos;s it is it &apos;s , of\n",
      "Attention Weights: tensor([[0.0012, 0.9770, 0.0018, 0.0165, 0.0009, 0.0005, 0.0001, 0.0006, 0.0012,\n",
      "         0.0002],\n",
      "        [0.0243, 0.8455, 0.0065, 0.1202, 0.0008, 0.0008, 0.0003, 0.0003, 0.0009,\n",
      "         0.0004],\n",
      "        [0.0556, 0.5242, 0.0412, 0.1425, 0.0396, 0.0459, 0.0300, 0.0158, 0.0372,\n",
      "         0.0680],\n",
      "        [0.0364, 0.3792, 0.0805, 0.2571, 0.0946, 0.0596, 0.0201, 0.0131, 0.0284,\n",
      "         0.0310],\n",
      "        [0.0325, 0.4686, 0.0143, 0.2289, 0.0383, 0.0615, 0.0236, 0.0116, 0.0615,\n",
      "         0.0593],\n",
      "        [0.0084, 0.2489, 0.0077, 0.1979, 0.0869, 0.0706, 0.0818, 0.0643, 0.1180,\n",
      "         0.1156],\n",
      "        [0.0056, 0.0736, 0.0071, 0.1570, 0.0747, 0.2899, 0.0782, 0.0993, 0.1343,\n",
      "         0.0802],\n",
      "        [0.0030, 0.0234, 0.0030, 0.2007, 0.0349, 0.3644, 0.0803, 0.0823, 0.1304,\n",
      "         0.0775],\n",
      "        [0.0088, 0.0679, 0.0047, 0.4108, 0.0608, 0.2618, 0.0356, 0.0367, 0.0837,\n",
      "         0.0291]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.60, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.73, Minutes Elapsed: 436.39\n",
      "Sampling from val predictions...\n",
      "Source: 因为 我 的 家人 不 懂 中文 我 害怕 他们\n",
      "Reference: since my family couldn &apos;t understand chinese , i\n",
      "Model: <SOS> because my father , , understand me , i\n",
      "Attention Weights: tensor([[7.3416e-03, 9.4337e-01, 1.3882e-04, 4.6017e-05, 1.4581e-05, 4.3600e-05,\n",
      "         6.4701e-05, 4.7747e-02, 1.1394e-04, 1.1223e-03],\n",
      "        [1.1056e-02, 9.8551e-01, 8.8422e-04, 6.7173e-04, 9.1215e-06, 4.8849e-05,\n",
      "         7.8906e-05, 1.4353e-03, 3.3893e-05, 2.6899e-04],\n",
      "        [2.0348e-01, 4.2992e-01, 8.2773e-02, 1.5901e-01, 8.5908e-03, 8.3164e-03,\n",
      "         1.2517e-02, 1.6628e-02, 4.5572e-03, 7.4197e-02],\n",
      "        [1.3940e-01, 3.3594e-01, 1.5140e-01, 2.1646e-01, 2.5608e-02, 1.8038e-02,\n",
      "         3.3321e-02, 1.9679e-02, 4.1623e-03, 5.5986e-02],\n",
      "        [4.5669e-02, 1.8815e-02, 2.1929e-02, 4.5002e-02, 1.4027e-01, 1.2727e-01,\n",
      "         3.8887e-01, 1.6617e-01, 1.5085e-02, 3.0919e-02],\n",
      "        [1.5360e-02, 4.5052e-02, 1.1267e-02, 2.8834e-02, 1.3906e-01, 1.7038e-01,\n",
      "         3.1335e-01, 1.8698e-01, 1.6544e-02, 7.3188e-02],\n",
      "        [9.8280e-04, 1.5089e-02, 2.2289e-03, 6.4398e-03, 1.5082e-02, 6.9087e-02,\n",
      "         1.5651e-01, 5.6142e-01, 7.2219e-02, 1.0095e-01],\n",
      "        [1.4756e-03, 1.3102e-02, 9.9688e-04, 5.2208e-03, 1.6174e-02, 9.0376e-02,\n",
      "         1.8377e-01, 5.1351e-01, 5.0412e-02, 1.2497e-01],\n",
      "        [4.3201e-04, 2.2874e-03, 1.5602e-04, 7.7900e-04, 6.3548e-03, 1.9876e-02,\n",
      "         9.2471e-02, 6.6864e-01, 8.4715e-02, 1.2428e-01]])\n",
      "\n",
      "Source: 但是 如果 我们 不 改变 土壤 的 成分 我们 永远\n",
      "Reference: but if we don &apos;t change the composition of\n",
      "Model: <SOS> but if we don &apos;t have the , ,\n",
      "Attention Weights: tensor([[0.0276, 0.0041, 0.5109, 0.0018, 0.0008, 0.0018, 0.0015, 0.0087, 0.3900,\n",
      "         0.0527],\n",
      "        [0.0782, 0.0389, 0.8632, 0.0004, 0.0011, 0.0005, 0.0006, 0.0017, 0.0088,\n",
      "         0.0066],\n",
      "        [0.0271, 0.0609, 0.7532, 0.0191, 0.0128, 0.0060, 0.0093, 0.0048, 0.0075,\n",
      "         0.0993],\n",
      "        [0.0217, 0.0311, 0.5987, 0.0693, 0.0459, 0.0229, 0.0209, 0.0182, 0.0263,\n",
      "         0.1450],\n",
      "        [0.0017, 0.0016, 0.7038, 0.1372, 0.0642, 0.0245, 0.0069, 0.0066, 0.0062,\n",
      "         0.0474],\n",
      "        [0.0002, 0.0003, 0.0628, 0.0739, 0.2321, 0.4032, 0.0390, 0.1214, 0.0536,\n",
      "         0.0135],\n",
      "        [0.0001, 0.0000, 0.0174, 0.0783, 0.1526, 0.4033, 0.1088, 0.0764, 0.0632,\n",
      "         0.0999],\n",
      "        [0.0002, 0.0002, 0.0229, 0.0893, 0.0715, 0.4844, 0.0759, 0.1020, 0.0828,\n",
      "         0.0707],\n",
      "        [0.0005, 0.0005, 0.0406, 0.0172, 0.0158, 0.0558, 0.0688, 0.1210, 0.5978,\n",
      "         0.0821]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.75, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 8.02, Minutes Elapsed: 445.21\n",
      "Sampling from val predictions...\n",
      "Source: 谁 能 发明 这种 科技 来 实现 绿色 绿色革命 革命\n",
      "Reference: who is going to invent the technology for the\n",
      "Model: <SOS> and , the to be this ? ? the\n",
      "Attention Weights: tensor([[0.0139, 0.0101, 0.0037, 0.3326, 0.0048, 0.0210, 0.0159, 0.0172, 0.0361,\n",
      "         0.5446],\n",
      "        [0.2550, 0.2619, 0.0769, 0.2750, 0.0023, 0.0062, 0.0038, 0.0067, 0.0106,\n",
      "         0.1015],\n",
      "        [0.0770, 0.1494, 0.1138, 0.2735, 0.0151, 0.0316, 0.0100, 0.0134, 0.0124,\n",
      "         0.3039],\n",
      "        [0.0083, 0.0137, 0.0411, 0.5810, 0.0354, 0.0308, 0.0191, 0.0203, 0.0166,\n",
      "         0.2338],\n",
      "        [0.0041, 0.0047, 0.0383, 0.5695, 0.0627, 0.0653, 0.0628, 0.0363, 0.0226,\n",
      "         0.1338],\n",
      "        [0.0008, 0.0008, 0.0056, 0.5216, 0.0302, 0.0383, 0.0893, 0.0676, 0.0277,\n",
      "         0.2182],\n",
      "        [0.0002, 0.0002, 0.0028, 0.3284, 0.0473, 0.0630, 0.3195, 0.1035, 0.0332,\n",
      "         0.1020],\n",
      "        [0.0000, 0.0000, 0.0001, 0.0052, 0.0112, 0.0387, 0.3507, 0.2986, 0.1159,\n",
      "         0.1795],\n",
      "        [0.0001, 0.0000, 0.0001, 0.0103, 0.0200, 0.0962, 0.5394, 0.1780, 0.0862,\n",
      "         0.0697]])\n",
      "\n",
      "Source: 企业 企业家 们 从不 不出 出席 他们 绝不 绝不会 不会\n",
      "Reference: entrepreneurs never come , and they never tell you\n",
      "Model: <SOS> so , to to the they they to to\n",
      "Attention Weights: tensor([[2.5283e-02, 9.9735e-02, 4.7226e-02, 3.2112e-03, 1.6266e-03, 7.1217e-02,\n",
      "         4.1713e-01, 9.2511e-03, 1.4157e-02, 3.1116e-01],\n",
      "        [1.0155e-02, 3.0888e-01, 6.0953e-01, 1.6205e-02, 1.4330e-03, 1.1375e-02,\n",
      "         2.7620e-02, 4.7685e-04, 6.3770e-04, 1.3684e-02],\n",
      "        [3.4136e-02, 1.7330e-01, 4.5964e-01, 1.0642e-01, 1.7973e-02, 3.3080e-02,\n",
      "         3.0449e-02, 8.7258e-03, 7.9720e-03, 1.2830e-01],\n",
      "        [3.0795e-04, 6.5714e-04, 1.0193e-02, 9.0760e-02, 7.6316e-02, 5.4633e-01,\n",
      "         1.6067e-01, 2.5191e-02, 1.0873e-02, 7.8711e-02],\n",
      "        [7.7249e-05, 3.0354e-04, 2.5037e-03, 1.6136e-02, 6.0382e-02, 4.2191e-01,\n",
      "         2.0564e-01, 7.2074e-02, 4.0922e-02, 1.8006e-01],\n",
      "        [3.3188e-05, 7.6526e-05, 3.9846e-04, 2.5349e-03, 1.9306e-02, 1.3581e-01,\n",
      "         3.0758e-01, 1.6821e-01, 1.4468e-01, 2.2138e-01],\n",
      "        [1.4351e-04, 2.7444e-04, 2.3611e-03, 1.7073e-03, 3.1827e-03, 8.4690e-02,\n",
      "         4.1715e-01, 8.6236e-02, 7.4634e-02, 3.2962e-01],\n",
      "        [1.6229e-04, 3.2166e-04, 6.0279e-03, 9.6563e-03, 2.1400e-02, 9.9739e-02,\n",
      "         2.2561e-01, 2.0089e-01, 8.4081e-02, 3.5212e-01],\n",
      "        [4.9346e-06, 1.2121e-05, 2.0902e-04, 1.1794e-03, 2.8700e-03, 7.5210e-02,\n",
      "         1.9716e-01, 6.9118e-02, 1.0121e-01, 5.5302e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.90, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.97, Minutes Elapsed: 454.04\n",
      "Sampling from val predictions...\n",
      "Source: 我 想 改变 这种 看法 我 想 改变 大家 对\n",
      "Reference: i want to change that perception . i want\n",
      "Model: <SOS> i i to talk this word , and want\n",
      "Attention Weights: tensor([[9.9945e-01, 3.2947e-06, 7.9024e-07, 1.2771e-04, 4.6168e-07, 3.9058e-04,\n",
      "         4.0271e-07, 6.2327e-07, 4.2067e-07, 2.1013e-05],\n",
      "        [9.9061e-01, 2.5670e-03, 7.9402e-04, 5.1684e-03, 1.7744e-05, 5.4644e-04,\n",
      "         9.4965e-06, 8.5968e-06, 5.8920e-06, 2.7460e-04],\n",
      "        [3.4469e-01, 2.2852e-01, 1.1965e-01, 1.9355e-01, 3.7060e-03, 1.5539e-02,\n",
      "         2.8152e-03, 1.8374e-03, 1.7835e-03, 8.7901e-02],\n",
      "        [4.5300e-02, 1.2883e-01, 1.7956e-01, 4.8030e-01, 2.3769e-02, 3.7010e-02,\n",
      "         7.9876e-03, 3.4875e-03, 2.8387e-03, 9.0923e-02],\n",
      "        [2.8322e-02, 7.5893e-03, 2.6824e-02, 6.4207e-01, 9.3915e-03, 9.3137e-02,\n",
      "         2.5029e-03, 1.0908e-03, 2.3999e-04, 1.8883e-01],\n",
      "        [5.2628e-03, 2.1119e-03, 1.3696e-02, 7.1835e-01, 3.1982e-02, 1.1847e-01,\n",
      "         1.0940e-02, 5.0296e-03, 1.3388e-03, 9.2813e-02],\n",
      "        [3.8835e-03, 1.0420e-04, 1.6885e-04, 2.7756e-02, 1.6159e-02, 8.0877e-01,\n",
      "         1.5775e-02, 1.4715e-02, 5.0013e-03, 1.0767e-01],\n",
      "        [2.6412e-03, 3.0863e-04, 2.4615e-04, 9.7132e-03, 2.0847e-02, 6.8284e-01,\n",
      "         9.8928e-02, 7.7516e-02, 2.1066e-02, 8.5898e-02],\n",
      "        [1.4199e-03, 3.1386e-05, 2.7360e-05, 2.4517e-03, 2.1334e-03, 7.6068e-01,\n",
      "         6.4119e-02, 3.4425e-02, 2.1103e-02, 1.1361e-01]])\n",
      "\n",
      "Source: 虐待 只能 活 在 沉默 中 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: abuse thrives only in silence . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> it is be . . . <EOS> <EOS> .\n",
      "Attention Weights: tensor([[0.1653, 0.1445, 0.1583, 0.1336, 0.0333, 0.2788, 0.0861, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3351, 0.4741, 0.0708, 0.0270, 0.0102, 0.0601, 0.0227, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0561, 0.1761, 0.1622, 0.1128, 0.0443, 0.3001, 0.1485, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0174, 0.0423, 0.2747, 0.2696, 0.1508, 0.1967, 0.0486, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0174, 0.0300, 0.1022, 0.2045, 0.2418, 0.3181, 0.0860, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0434, 0.0515, 0.0633, 0.1890, 0.2163, 0.3224, 0.1140, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0408, 0.0380, 0.0565, 0.1401, 0.2265, 0.3695, 0.1286, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0704, 0.0605, 0.0687, 0.1252, 0.1599, 0.3807, 0.1345, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0332, 0.0571, 0.0697, 0.1312, 0.1726, 0.3585, 0.1777, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.00, Train Loss: 0.00, Val Loss: 4.91, Train BLEU: 0.00, Val BLEU: 7.93, Minutes Elapsed: 459.98\n",
      "Sampling from val predictions...\n",
      "Source: 这 并 不是 说 母亲 们 对于 我们 的 成功\n",
      "Reference: it &apos;s not to say that our mothers aren\n",
      "Model: <SOS> it &apos;s not just the the our &apos;s are\n",
      "Attention Weights: tensor([[9.9715e-01, 2.7351e-05, 5.8295e-06, 1.5419e-05, 5.0148e-05, 5.8004e-04,\n",
      "         3.6362e-05, 9.8801e-04, 6.1111e-06, 1.1378e-03],\n",
      "        [9.1841e-01, 6.0510e-02, 5.0819e-03, 5.3584e-03, 5.1716e-04, 4.6151e-03,\n",
      "         1.1413e-04, 1.2335e-03, 5.2912e-05, 4.1115e-03],\n",
      "        [6.4487e-01, 1.7407e-01, 3.2341e-02, 4.3432e-02, 8.7941e-03, 2.4689e-02,\n",
      "         3.1456e-03, 1.0704e-02, 5.2164e-03, 5.2742e-02],\n",
      "        [3.2327e-03, 1.4036e-02, 5.1625e-02, 5.0490e-01, 9.0487e-02, 1.8992e-01,\n",
      "         1.2929e-02, 8.1723e-02, 5.3069e-03, 4.5844e-02],\n",
      "        [6.0517e-03, 5.7176e-03, 1.6284e-02, 3.5581e-01, 1.3443e-01, 3.5119e-01,\n",
      "         2.4614e-02, 8.6444e-02, 3.3074e-03, 1.6153e-02],\n",
      "        [8.0142e-03, 1.8965e-03, 5.0166e-03, 2.3901e-01, 9.5365e-02, 3.7392e-01,\n",
      "         2.0520e-02, 1.9871e-01, 4.4380e-03, 5.3121e-02],\n",
      "        [1.0275e-03, 7.4935e-04, 1.4413e-03, 7.1814e-02, 1.2811e-01, 2.4037e-01,\n",
      "         7.2978e-02, 4.2552e-01, 7.0723e-03, 5.0906e-02],\n",
      "        [1.0508e-03, 6.8691e-04, 1.5239e-03, 2.0004e-02, 6.1531e-02, 3.1722e-01,\n",
      "         6.0502e-02, 3.8302e-01, 2.2280e-02, 1.3219e-01],\n",
      "        [1.7626e-04, 1.0411e-04, 2.3063e-04, 1.0902e-02, 1.2354e-02, 1.5752e-01,\n",
      "         3.3941e-02, 4.9708e-01, 2.4688e-02, 2.6300e-01]])\n",
      "\n",
      "Source: 所以 以为 为什么 什么 说 <UNK> 重要 呢 首先 <UNK>\n",
      "Reference: so why are <UNK> important ? first of all\n",
      "Model: <SOS> so , does it doing ? and ? all\n",
      "Attention Weights: tensor([[0.0320, 0.0180, 0.2486, 0.0064, 0.1002, 0.0299, 0.0824, 0.0707, 0.3269,\n",
      "         0.0852],\n",
      "        [0.0185, 0.0718, 0.7856, 0.0212, 0.0229, 0.0227, 0.0094, 0.0034, 0.0266,\n",
      "         0.0179],\n",
      "        [0.0370, 0.1327, 0.2977, 0.0492, 0.1184, 0.0850, 0.0304, 0.0297, 0.0988,\n",
      "         0.1211],\n",
      "        [0.0250, 0.1029, 0.3350, 0.0744, 0.1239, 0.0812, 0.0358, 0.0267, 0.0980,\n",
      "         0.0970],\n",
      "        [0.0115, 0.0236, 0.1282, 0.0851, 0.1585, 0.1454, 0.1374, 0.0538, 0.1858,\n",
      "         0.0707],\n",
      "        [0.0037, 0.0010, 0.0170, 0.0144, 0.1863, 0.0878, 0.3556, 0.1290, 0.1884,\n",
      "         0.0168],\n",
      "        [0.0031, 0.0006, 0.0102, 0.0024, 0.0216, 0.0347, 0.5173, 0.1093, 0.2681,\n",
      "         0.0325],\n",
      "        [0.0020, 0.0007, 0.0065, 0.0029, 0.0421, 0.0417, 0.3431, 0.1840, 0.2693,\n",
      "         0.1077],\n",
      "        [0.0016, 0.0017, 0.0047, 0.0023, 0.0293, 0.0318, 0.2612, 0.1324, 0.4294,\n",
      "         0.1056]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.15, Train Loss: 0.00, Val Loss: 4.96, Train BLEU: 0.00, Val BLEU: 7.59, Minutes Elapsed: 468.76\n",
      "Sampling from val predictions...\n",
      "Source: 这 也 就是 为什么 什么 在 非洲 白人 被称作 称作\n",
      "Reference: that &apos;s why the white people in africa are\n",
      "Model: <SOS> and that why the &apos;s is is in room\n",
      "Attention Weights: tensor([[9.9889e-01, 5.6256e-04, 3.5427e-05, 7.2516e-05, 4.7569e-06, 3.5170e-06,\n",
      "         6.7613e-06, 1.1048e-05, 4.8037e-06, 4.1268e-04],\n",
      "        [7.9321e-01, 1.0842e-01, 2.3368e-02, 5.8468e-02, 1.0052e-03, 3.1411e-04,\n",
      "         7.9936e-04, 2.3695e-04, 2.1370e-04, 1.3964e-02],\n",
      "        [1.7940e-01, 1.6871e-01, 1.2489e-01, 3.7866e-01, 2.1045e-02, 1.6897e-02,\n",
      "         8.7792e-03, 6.9963e-03, 3.8408e-03, 9.0778e-02],\n",
      "        [5.2966e-02, 4.3419e-02, 8.1804e-02, 3.4974e-01, 7.3814e-02, 7.6386e-02,\n",
      "         8.0220e-02, 4.0556e-02, 2.6769e-03, 1.9842e-01],\n",
      "        [1.3497e-02, 7.3973e-03, 1.1844e-02, 9.4480e-02, 7.8823e-02, 1.3595e-01,\n",
      "         2.5217e-01, 4.1219e-02, 7.0907e-03, 3.5753e-01],\n",
      "        [1.1817e-02, 6.3311e-03, 2.4192e-02, 6.2725e-02, 2.4553e-02, 6.6360e-02,\n",
      "         1.5857e-01, 6.5852e-02, 3.6675e-03, 5.7593e-01],\n",
      "        [2.0747e-03, 2.1278e-03, 8.9092e-03, 4.9892e-02, 5.5190e-02, 2.1103e-01,\n",
      "         2.5861e-01, 1.2490e-01, 1.4044e-02, 2.7323e-01],\n",
      "        [4.0433e-04, 1.7001e-04, 1.7343e-03, 7.4390e-03, 1.5797e-02, 2.6549e-01,\n",
      "         1.4039e-01, 8.8028e-02, 1.1956e-02, 4.6859e-01],\n",
      "        [1.0158e-02, 2.2062e-03, 1.8962e-03, 6.2836e-03, 2.1417e-03, 4.2840e-02,\n",
      "         4.2782e-01, 2.3576e-01, 1.5253e-02, 2.5564e-01]])\n",
      "\n",
      "Source: 由于 那里 里面 的 <UNK> 和 尘土 我 的 相机\n",
      "Reference: so pervasive was the heat and the dust that\n",
      "Model: <SOS> and , &apos;s a , of the , ,\n",
      "Attention Weights: tensor([[0.0013, 0.0004, 0.0011, 0.0044, 0.0053, 0.0049, 0.0104, 0.8210, 0.0156,\n",
      "         0.1356],\n",
      "        [0.1001, 0.0465, 0.0633, 0.0419, 0.0406, 0.0176, 0.0448, 0.4520, 0.0298,\n",
      "         0.1634],\n",
      "        [0.0465, 0.0393, 0.0695, 0.0377, 0.0783, 0.0136, 0.0210, 0.1543, 0.0415,\n",
      "         0.4982],\n",
      "        [0.0326, 0.0503, 0.1417, 0.0584, 0.0674, 0.0228, 0.0253, 0.1143, 0.0402,\n",
      "         0.4470],\n",
      "        [0.0006, 0.0004, 0.0025, 0.0037, 0.0155, 0.0255, 0.0520, 0.7501, 0.0095,\n",
      "         0.1402],\n",
      "        [0.0007, 0.0013, 0.0070, 0.0073, 0.0312, 0.0462, 0.0449, 0.0824, 0.0336,\n",
      "         0.7454],\n",
      "        [0.0019, 0.0032, 0.0131, 0.0099, 0.0293, 0.1217, 0.1971, 0.2700, 0.0825,\n",
      "         0.2713],\n",
      "        [0.0002, 0.0001, 0.0006, 0.0003, 0.0025, 0.0060, 0.0644, 0.6102, 0.0239,\n",
      "         0.2918],\n",
      "        [0.0003, 0.0006, 0.0028, 0.0030, 0.0083, 0.0225, 0.1054, 0.3474, 0.1151,\n",
      "         0.3947]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.30, Train Loss: 0.00, Val Loss: 4.93, Train BLEU: 0.00, Val BLEU: 8.01, Minutes Elapsed: 477.58\n",
      "Sampling from val predictions...\n",
      "Source: 他们 不断 地 发现 <UNK> 照片 <UNK> 相机 和 <UNK>\n",
      "Reference: they &apos;d been finding photos and photo albums and\n",
      "Model: <SOS> they are to to to the the . <EOS>\n",
      "Attention Weights: tensor([[0.9978, 0.0001, 0.0000, 0.0000, 0.0000, 0.0002, 0.0001, 0.0004, 0.0012,\n",
      "         0.0002],\n",
      "        [0.9552, 0.0168, 0.0029, 0.0011, 0.0004, 0.0015, 0.0009, 0.0021, 0.0164,\n",
      "         0.0029],\n",
      "        [0.3102, 0.1936, 0.1408, 0.0420, 0.0207, 0.0119, 0.0230, 0.0275, 0.1702,\n",
      "         0.0602],\n",
      "        [0.0146, 0.0170, 0.1171, 0.2463, 0.0724, 0.0505, 0.1102, 0.1122, 0.1977,\n",
      "         0.0620],\n",
      "        [0.0067, 0.0155, 0.0827, 0.3271, 0.0617, 0.0707, 0.0759, 0.1320, 0.1549,\n",
      "         0.0727],\n",
      "        [0.0164, 0.0129, 0.0158, 0.0826, 0.1550, 0.1102, 0.2186, 0.1962, 0.1084,\n",
      "         0.0839],\n",
      "        [0.0125, 0.0256, 0.0291, 0.1399, 0.1579, 0.1974, 0.1368, 0.1141, 0.1249,\n",
      "         0.0617],\n",
      "        [0.0490, 0.0227, 0.0299, 0.0866, 0.0984, 0.1071, 0.1167, 0.1112, 0.2462,\n",
      "         0.1322],\n",
      "        [0.0281, 0.0227, 0.0215, 0.0898, 0.1153, 0.1560, 0.1327, 0.1411, 0.1531,\n",
      "         0.1397]])\n",
      "\n",
      "Source: 但是 如果 我们 不 改变 土壤 的 成分 我们 永远\n",
      "Reference: but if we don &apos;t change the composition of\n",
      "Model: <SOS> but if we don &apos;t have the , ,\n",
      "Attention Weights: tensor([[9.5777e-03, 5.2185e-03, 8.9761e-01, 4.7025e-04, 4.1425e-04, 5.3341e-04,\n",
      "         1.3679e-03, 4.6658e-03, 5.4281e-02, 2.5862e-02],\n",
      "        [1.5164e-02, 1.5044e-02, 9.6509e-01, 9.0318e-06, 9.6864e-05, 6.0022e-05,\n",
      "         5.4332e-05, 2.7669e-04, 2.3348e-03, 1.8665e-03],\n",
      "        [2.6827e-02, 5.7486e-02, 5.9635e-01, 2.7835e-02, 2.0548e-02, 1.8479e-02,\n",
      "         2.7740e-02, 1.8855e-02, 3.2151e-02, 1.7373e-01],\n",
      "        [1.1947e-02, 1.7518e-02, 6.0662e-01, 6.1176e-02, 3.0357e-02, 1.6404e-02,\n",
      "         2.3004e-02, 2.1580e-02, 2.7002e-02, 1.8439e-01],\n",
      "        [2.5326e-03, 2.9704e-03, 6.9523e-01, 8.5803e-02, 5.6412e-02, 2.1268e-02,\n",
      "         1.6038e-02, 2.2069e-02, 1.6615e-02, 8.1063e-02],\n",
      "        [5.8286e-05, 1.4335e-04, 2.0308e-02, 8.1289e-02, 2.3329e-01, 3.6885e-01,\n",
      "         5.5091e-02, 1.5254e-01, 6.5604e-02, 2.2820e-02],\n",
      "        [4.1445e-05, 7.5679e-05, 1.7350e-02, 1.0287e-01, 1.5981e-01, 2.2670e-01,\n",
      "         1.2530e-01, 1.7159e-01, 1.0875e-01, 8.7513e-02],\n",
      "        [5.3345e-05, 6.3991e-05, 4.1076e-03, 8.0458e-02, 5.4526e-02, 2.6774e-01,\n",
      "         8.7913e-02, 1.6393e-01, 2.5984e-01, 8.1364e-02],\n",
      "        [1.7429e-04, 2.0572e-04, 1.2251e-02, 2.6504e-02, 6.8574e-03, 2.2764e-02,\n",
      "         3.2108e-02, 2.2031e-01, 4.1814e-01, 2.6069e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.45, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 7.94, Minutes Elapsed: 486.47\n",
      "Sampling from val predictions...\n",
      "Source: 因此 我 做 的 就是 在 我 的 房子 前\n",
      "Reference: so what i did , i planted a food\n",
      "Model: <SOS> so so i did is i was my my\n",
      "Attention Weights: tensor([[6.1403e-05, 9.9549e-01, 1.3759e-06, 3.6468e-05, 2.0487e-04, 1.7490e-04,\n",
      "         3.8797e-03, 4.6914e-06, 4.5227e-05, 1.0227e-04],\n",
      "        [2.0553e-03, 9.9652e-01, 1.1260e-04, 6.3902e-04, 2.0850e-04, 1.0379e-05,\n",
      "         2.6421e-04, 9.8334e-06, 4.7596e-05, 1.3669e-04],\n",
      "        [4.1940e-02, 4.7017e-01, 3.5304e-02, 1.5221e-01, 1.2917e-01, 1.8295e-02,\n",
      "         2.6079e-02, 8.8730e-03, 2.1522e-02, 9.6440e-02],\n",
      "        [3.1062e-02, 2.2141e-01, 3.3091e-02, 1.9357e-01, 3.4444e-01, 3.1424e-02,\n",
      "         5.0421e-02, 6.3635e-03, 1.6609e-02, 7.1603e-02],\n",
      "        [1.6283e-02, 4.1023e-01, 6.7747e-03, 5.4486e-02, 3.0030e-01, 3.4272e-02,\n",
      "         4.5436e-02, 4.2180e-03, 2.3201e-02, 1.0480e-01],\n",
      "        [5.0305e-03, 1.9873e-02, 6.0592e-03, 9.5983e-02, 4.0993e-01, 1.2802e-01,\n",
      "         2.7948e-01, 6.2707e-03, 1.6824e-02, 3.2524e-02],\n",
      "        [2.1971e-03, 1.4589e-01, 2.7268e-03, 4.2154e-02, 1.1883e-01, 1.1709e-01,\n",
      "         2.6912e-01, 1.1093e-02, 8.6663e-02, 2.0424e-01],\n",
      "        [5.4123e-04, 6.9734e-02, 2.7035e-03, 3.2642e-02, 1.2722e-01, 2.0589e-01,\n",
      "         3.0386e-01, 2.8864e-02, 8.7986e-02, 1.4056e-01],\n",
      "        [5.1026e-05, 9.0758e-04, 4.4875e-04, 5.5596e-03, 1.3266e-02, 1.2098e-01,\n",
      "         5.9314e-01, 2.9964e-02, 1.2714e-01, 1.0854e-01]])\n",
      "\n",
      "Source: 我们 都 知道 自己 正 冒 着 生命 的 危险\n",
      "Reference: we all knew we were risking our lives --\n",
      "Model: <SOS> we all know that the the to . ,\n",
      "Attention Weights: tensor([[9.9998e-01, 5.1855e-06, 2.2188e-07, 9.7358e-07, 4.3185e-07, 5.4421e-07,\n",
      "         2.2380e-07, 1.6479e-06, 5.4531e-07, 8.2133e-06],\n",
      "        [9.9226e-01, 3.9611e-03, 1.0322e-03, 1.2521e-03, 1.8981e-04, 8.7629e-05,\n",
      "         1.6505e-05, 7.6806e-05, 3.6314e-05, 1.0915e-03],\n",
      "        [2.7752e-01, 2.0946e-01, 1.3377e-01, 7.2972e-02, 3.8036e-02, 2.7849e-02,\n",
      "         1.6837e-02, 1.8544e-02, 2.2138e-02, 1.8287e-01],\n",
      "        [2.5044e-02, 2.5122e-02, 8.9869e-02, 2.4841e-01, 2.0051e-01, 1.1280e-01,\n",
      "         6.6005e-02, 6.8665e-02, 3.5333e-02, 1.2824e-01],\n",
      "        [2.3846e-02, 2.1197e-02, 6.0136e-02, 1.3478e-01, 1.2008e-01, 1.2700e-01,\n",
      "         7.1110e-02, 8.6907e-02, 3.6673e-02, 3.1827e-01],\n",
      "        [6.4343e-03, 4.6673e-03, 1.2823e-02, 1.2810e-01, 9.4074e-02, 1.6894e-01,\n",
      "         1.0905e-01, 1.4602e-01, 5.8916e-02, 2.7097e-01],\n",
      "        [7.4920e-05, 8.1589e-04, 1.6408e-03, 1.5943e-02, 5.6301e-02, 1.3390e-01,\n",
      "         7.4588e-02, 1.2683e-01, 4.7294e-02, 5.4262e-01],\n",
      "        [1.3281e-04, 3.3302e-04, 2.8329e-04, 1.3162e-02, 2.0825e-02, 7.5838e-02,\n",
      "         3.6750e-02, 1.1699e-01, 6.8478e-02, 6.6720e-01],\n",
      "        [2.9426e-03, 1.2344e-03, 1.4687e-03, 3.3814e-02, 3.4717e-02, 1.5449e-01,\n",
      "         9.8797e-02, 3.1329e-01, 6.8873e-02, 2.9037e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n"
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=500, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=True, print_attn=True, inspect_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_single_learning_curve(experiment_results[0]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_val_bleu</th>\n",
       "      <th>runtime</th>\n",
       "      <th>total_params</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>dt_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zh-rnn-additive-attn-2018-12-14 02:14:28</td>\n",
       "      <td>4.846652</td>\n",
       "      <td>8.334325</td>\n",
       "      <td>578.66527</td>\n",
       "      <td>71591344</td>\n",
       "      <td>53591344</td>\n",
       "      <td>2018-12-14 11:53:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model_name  best_val_loss  best_val_bleu  \\\n",
       "0  zh-rnn-additive-attn-2018-12-14 02:14:28       4.846652       8.334325   \n",
       "\n",
       "     runtime  total_params  trainable_params           dt_created  \n",
       "0  578.66527      71591344          53591344  2018-12-14 11:53:32  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(experiment_results)[['model_name', 'best_val_loss', 'best_val_bleu', 'runtime', \n",
    "                                       'total_params', 'trainable_params', 'dt_created']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model \n",
    "MODEL_NAME_TO_RELOAD = 'zh-rnn-additive-attn-2018-12-14 02:14:28'\n",
    "checkpoint = torch.load('model_checkpoints/{}.pth.tar'.format(MODEL_NAME_TO_RELOAD), map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU: 8.33 | Validation Loss: 4.92\n"
     ]
    }
   ],
   "source": [
    "# check performance on validation set \n",
    "val_loss, val_bleu, val_hyp_idxs, val_ref_idxs, val_source_idxs, val_hyp_tokens, val_ref_tokens, val_source_tokens,\\\n",
    "val_attn = evaluate(model=model, loader=loaders_full['dev'], \n",
    "                    src_id2token=vocab[SRC_LANG]['id2token'], targ_id2token=vocab[TARG_LANG]['id2token'])\n",
    "print(\"Validation BLEU: {:.2f} | Validation Loss: {:.2f}\".format(val_bleu, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU: 9.45 | Test Loss: 4.79\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set \n",
    "test_loss, test_bleu, test_hyp_idxs, test_ref_idxs, test_source_idxs, test_hyp_tokens, test_ref_tokens, test_source_tokens,\\\n",
    "test_attn = evaluate(model=model, loader=loaders_full['test'], \n",
    "                     src_id2token=vocab[SRC_LANG]['id2token'], targ_id2token=vocab[TARG_LANG]['id2token'])\n",
    "print(\"Test BLEU: {:.2f} | Test Loss: {:.2f}\".format(test_bleu, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
