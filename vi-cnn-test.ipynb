{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import train_and_eval, count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params required for generating data loaders \n",
    "\n",
    "SRC_LANG = 'vi'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a long time to process, save to pickle for reimport in future \n",
    "#vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "#vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "#pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture params \n",
    "NETWORK_TYPE = 'cnn'\n",
    "RNN_CELL_TYPE = 'NA'\n",
    "NUM_LAYERS = 1\n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 \n",
    "DEC_DROPOUT = 0.2  \n",
    "ATTENTION_TYPE = 'without'\n",
    "\n",
    "# training params  \n",
    "NUM_EPOCHS = 15 \n",
    "LR = 0.0001 \n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = False\n",
    "\n",
    "# name the model and experiment \n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    EXPERIMENT_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    EXPERIMENT_NAME = '{}-cnn'.format(SRC_LANG)\n",
    "MODEL_NAME = '{}-{}'.format(EXPERIMENT_NAME, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n",
    "          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n",
    "          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n",
    "          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n",
    "          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n",
    "          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_word2vec, src_max_sentence_len=10, enc_hidden_dim=512, dropout=0.1):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.enc_embed_dim = 300\n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True).to(device)\n",
    "        self.conv1_a = nn.Conv1d(300, enc_hidden_dim, kernel_size=3, padding=1).to(device)\n",
    "        self.conv2_a = nn.Conv1d(enc_hidden_dim, enc_hidden_dim, kernel_size=3, padding=1).to(device)\n",
    "        self.dropout_val = dropout\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.linearout = nn.Linear(enc_hidden_dim,300)\n",
    "        self.linear_for_hidden = nn.Linear(300 * 10, self.enc_hidden_dim)\n",
    " \n",
    "\n",
    "        \n",
    "    def forward(self, enc_input, enc_input_lens):\n",
    "        enc_input = enc_input.to(device)\n",
    "        enc_input_lens = enc_input_lens.to(device)\n",
    "        batch_size = enc_input.size()[0]\n",
    "        embedded = self.embedding(enc_input)\n",
    "        embedded = F.dropout(embedded, self.dropout_val)\n",
    "        \n",
    "        # 1st net\n",
    "        hidden_1_a = self.conv1_a(embedded.transpose(1,2)).transpose(1,2)\n",
    "        #print(hidden_1_a.shape)\n",
    "        #hidden_1_a.contiguous().view(-1, hidden_1_a.size(-1))\n",
    "        hidden_1_a = torch.tanh(hidden_1_a.contiguous()).view(batch_size, -1, hidden_1_a.size(-1))\n",
    "        hidden_2_a = self.conv2_a(hidden_1_a.transpose(1,2)).transpose(1,2)\n",
    "        hidden_2_a = torch.tanh(hidden_2_a.contiguous().view(\n",
    "                                                    batch_size, -1, hidden_2_a.size(-1)))\n",
    "        #print(hidden_2_a.transpose(1,2).shape)\n",
    "        hidden_2_a = self.linearout(hidden_2_a)\n",
    "        #print(hidden_2_a.shape)\n",
    "        dim_1_hidden = self.linear_for_hidden(hidden_2_a.view(batch_size,1, -1))\n",
    "        \n",
    "        return hidden_2_a, dim_1_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_word2vec, src_max_sentence_len=10, enc_hidden_dim=512, dropout=0.1):\n",
    "        super(EncoderCNN2, self).__init__()\n",
    "        self.enc_embed_dim = 300\n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True).to(device)\n",
    "        self.conv1_a = nn.Conv1d(300*SRC_MAX_SENTENCE_LEN, enc_hidden_dim, kernel_size=3, padding=1, stride=300).to(device)\n",
    "        self.conv2_a = nn.Conv1d(enc_hidden_dim, enc_hidden_dim, kernel_size=3, padding=1, stride=300).to(device)\n",
    "        self.dropout_val = dropout\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.linearout = nn.Linear(enc_hidden_dim,3000)\n",
    "        self.linear_for_hidden = nn.Linear(3000, self.enc_hidden_dim)\n",
    " \n",
    "\n",
    "        \n",
    "    def forward(self, enc_input, enc_input_lens):\n",
    "        enc_input = enc_input.to(device)\n",
    "        enc_input_lens = enc_input_lens.to(device)\n",
    "        batch_size = enc_input.size()[0]\n",
    "        embedded = self.embedding(enc_input)\n",
    "        embedded = F.dropout(embedded, self.dropout_val)\n",
    "        embedded = embedded.view(batch_size, -1, 1)\n",
    "        \n",
    "        # 1st net\n",
    "        hidden_1_a = self.conv1_a(embedded)\n",
    "        #print(hidden_1_a.shape)\n",
    "        #hidden_1_a.contiguous().view(-1, hidden_1_a.size(-1))\n",
    "        hidden_1_a = torch.tanh(hidden_1_a.contiguous()).view(batch_size, -1, hidden_1_a.size(-1))\n",
    "        hidden_2_a = self.conv2_a(hidden_1_a)\n",
    "        hidden_2_a = torch.tanh(hidden_2_a.contiguous().view(\n",
    "                                                    batch_size, -1, hidden_2_a.size(-1)))\n",
    "        #print(hidden_2_a.transpose(1,2).shape)\n",
    "        #print(hidden_2_a.shape)\n",
    "        hidden_2_a = self.linearout(hidden_2_a.transpose(1,2)).view(batch_size, -1,self.enc_embed_dim)\n",
    "        #print(hidden_2_a.shape)\n",
    "        dim_1_hidden = self.linear_for_hidden(hidden_2_a.view(batch_size,1, -1))\n",
    "        #print(dim_1_hidden.shape)\n",
    "        #print('output {}'.format(hidden_2_a.shape))\n",
    "        #print('hidden {}'.format(dim_1_hidden.shape))\n",
    "        \n",
    "        return hidden_2_a, dim_1_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_RNN_from_CNN(nn.Module):\n",
    "    \"\"\" Vanilla decoder without attention, but final layer from encoder is repeatedly passed as input to each time step. \n",
    "        Handles output from EncoderRNN, which concats bidirectional output. \n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self, dec_hidden_dim, enc_hidden_dim, num_layers, targ_vocab_size, targ_max_sentence_len, pretrained_word2vec, batch_size):\n",
    "        super(Decoder_RNN_from_CNN, self).__init__()\n",
    "        self.dec_embed_dim = 300\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.targ_vocab_size = targ_vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True) \n",
    "        self.gru = nn.GRU(300 + self.enc_hidden_dim, self.dec_hidden_dim, num_layers=self.num_layers) \n",
    "        self.out = nn.Linear(dec_hidden_dim, self.targ_vocab_size) \n",
    "        self.softmax = nn.LogSoftmax(dim=1) \n",
    "\n",
    "    def forward(self, dec_input, context, dec_hidden, enc_outputs):  \n",
    "        \n",
    "        batch_size = dec_input.size()[0]\n",
    "        dec_hidden = dec_hidden.view(1, batch_size, -1)\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)   \n",
    "        #print(embedded.shape)\n",
    "        #context = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n",
    "        #                     enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n",
    "        context = context.view(1, batch_size, -1) \n",
    "        #print(context.shape)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))  \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_RNN_EncoderDecoder(nn.Module): \n",
    "\n",
    "    \"\"\" Encoder-Decoder without attention \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, decoder_token2id): \n",
    "        super(CNN_RNN_EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.targ_vocab_size = self.decoder.targ_vocab_size\n",
    "        self.src_max_sentence_len = self.encoder.src_max_sentence_len \n",
    "        self.targ_max_sentence_len = self.decoder.targ_max_sentence_len\n",
    "\n",
    "    def forward(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio): \n",
    "        \n",
    "        batch_size = src_idx.size()[0]\n",
    "        enc_outputs, enc_hidden = self.encoder(src_idx, src_lens)\n",
    "        dec_hidden = enc_hidden \n",
    "        dec_outputs = Variable(torch.zeros(self.targ_max_sentence_len, batch_size, self.targ_vocab_size))\n",
    "        hypotheses = Variable(torch.zeros(self.targ_max_sentence_len, batch_size))\n",
    "        dec_output = targ_idx[:, 0] \n",
    "\n",
    "        for di in range(1, self.targ_max_sentence_len): \n",
    "            dec_output, dec_hidden = self.decoder(dec_output, dec_hidden, dec_hidden, enc_outputs)\n",
    "            dec_outputs[di] = dec_output \n",
    "            teacher_labels = targ_idx[:, di-1] \n",
    "            greedy_labels = dec_output.data.max(1)[1]\n",
    "            dec_output = teacher_labels if random.random() < teacher_forcing_ratio else greedy_labels \n",
    "            hypotheses[di] = greedy_labels\n",
    "\n",
    "        attn_placeholder = Variable(torch.zeros(batch_size, self.targ_max_sentence_len, self.src_max_sentence_len))\n",
    "\n",
    "        return dec_outputs, hypotheses.transpose(0,1), attn_placeholder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "encoder = EncoderCNN2(pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "if ATTENTION_TYPE == 'without': \n",
    "    # without attention \n",
    "    decoder =  Decoder_RNN_from_CNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                         targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, batch_size=BATCH_SIZE, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "    model = CNN_RNN_EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.30, Train BLEU: 0.00, Val BLEU: 0.01, Minutes Elapsed: 0.08\n",
      "Sampling from val predictions...\n",
      "Source: bây_giờ tôi muốn giới_thiệu các bạn với những người em_trai\n",
      "Reference: now i &apos;d like to introduce you to my\n",
      "Model: <SOS> thump thump crimes camels ellipses ellipses citizenship citizenship citizenship\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=100, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=True, print_attn=False, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_learning_curve(experiment_results[0]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(experiment_results)[['best_val_loss', 'best_val_bleu', 'runtime', \n",
    "                                       'total_params', 'trainable_params', 'dt_created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model and test \n",
    "checkpoint = torch.load('model_checkpoints/{}.pth.tar'.format(MODEL_NAME), map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
