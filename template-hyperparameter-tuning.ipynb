{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import train_and_eval, count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params required for generating data loaders \n",
    "\n",
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture params \n",
    "NETWORK_TYPE = 'rnn'\n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0 #0.2 \n",
    "DEC_DROPOUT = 0 #0.2 \n",
    "ATTENTION_TYPE = 'additive'\n",
    "\n",
    "# training params  \n",
    "NUM_EPOCHS = 1 #5\n",
    "LR = 0.0003 # 0.0005\n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = True\n",
    "\n",
    "# name the model and experiment \n",
    "EXPERIMENT_NAME = 'hyperparameter_tuning_dropout'\n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    MODEL_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    MODEL_NAME = '{}-cnn'.format(SRC_LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n",
    "          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n",
    "          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n",
    "          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n",
    "          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n",
    "          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with dropout = 0\n",
      "Epoch: 0.00, Train Loss: 9.97, Val Loss: 10.12, Train BLEU: 0.17, Val BLEU: 0.14, Minutes Elapsed: 0.18\n",
      "Sampling from training predictions...\n",
      "Source: 它们 有 的 会 贴近 潜水 潜水艇 它们 的 眼睛\n",
      "Reference: they come right up to the submarine -- they\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.1539, 0.1332, 0.1155, 0.1023, 0.0942, 0.0897, 0.0889, 0.0859, 0.0741,\n",
      "         0.0624],\n",
      "        [0.1544, 0.1334, 0.1155, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0623],\n",
      "        [0.1544, 0.1334, 0.1155, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0623],\n",
      "        [0.1544, 0.1334, 0.1155, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0623],\n",
      "        [0.1543, 0.1334, 0.1156, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0623],\n",
      "        [0.1543, 0.1334, 0.1156, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0622],\n",
      "        [0.1543, 0.1334, 0.1156, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0622],\n",
      "        [0.1543, 0.1334, 0.1156, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0622],\n",
      "        [0.1543, 0.1334, 0.1156, 0.1022, 0.0941, 0.0896, 0.0888, 0.0858, 0.0740,\n",
      "         0.0622]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 父亲 在 用 他 的 灰色 小 收音\n",
      "Reference: my father was listening to bbc news on his\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.1529, 0.1305, 0.1187, 0.1099, 0.1007, 0.0932, 0.0855, 0.0792, 0.0696,\n",
      "         0.0599],\n",
      "        [0.1535, 0.1308, 0.1188, 0.1099, 0.1005, 0.0930, 0.0853, 0.0790, 0.0695,\n",
      "         0.0597],\n",
      "        [0.1536, 0.1309, 0.1188, 0.1099, 0.1005, 0.0930, 0.0853, 0.0790, 0.0695,\n",
      "         0.0596],\n",
      "        [0.1536, 0.1309, 0.1188, 0.1099, 0.1005, 0.0930, 0.0853, 0.0790, 0.0695,\n",
      "         0.0596],\n",
      "        [0.1535, 0.1309, 0.1188, 0.1099, 0.1005, 0.0930, 0.0853, 0.0790, 0.0695,\n",
      "         0.0596],\n",
      "        [0.1535, 0.1309, 0.1188, 0.1099, 0.1006, 0.0930, 0.0853, 0.0790, 0.0694,\n",
      "         0.0596],\n",
      "        [0.1535, 0.1309, 0.1188, 0.1099, 0.1006, 0.0930, 0.0853, 0.0790, 0.0694,\n",
      "         0.0596],\n",
      "        [0.1535, 0.1309, 0.1188, 0.1099, 0.1006, 0.0930, 0.0853, 0.0790, 0.0694,\n",
      "         0.0596],\n",
      "        [0.1535, 0.1309, 0.1188, 0.1099, 0.1006, 0.0930, 0.0853, 0.0790, 0.0694,\n",
      "         0.0596]])\n",
      "\n",
      "Experiment completed in 0 minutes with 10.12 best validation loss and 0.14 best validation BLEU.\n",
      "Training with dropout = 0.2\n",
      "Epoch: 0.00, Train Loss: 9.98, Val Loss: 10.13, Train BLEU: 0.17, Val BLEU: 0.14, Minutes Elapsed: 0.18\n",
      "Sampling from training predictions...\n",
      "Source: 我们 得用 非常 特殊 的 仪器 才能 能到 到达 那个\n",
      "Reference: we have to have a very special technology to\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.1279, 0.1238, 0.1200, 0.1152, 0.1084, 0.0980, 0.0910, 0.0831, 0.0732,\n",
      "         0.0596],\n",
      "        [0.1281, 0.1240, 0.1202, 0.1153, 0.1085, 0.0980, 0.0909, 0.0829, 0.0729,\n",
      "         0.0593],\n",
      "        [0.1280, 0.1240, 0.1202, 0.1154, 0.1086, 0.0980, 0.0909, 0.0829, 0.0729,\n",
      "         0.0592],\n",
      "        [0.1280, 0.1240, 0.1202, 0.1154, 0.1086, 0.0980, 0.0909, 0.0829, 0.0729,\n",
      "         0.0591],\n",
      "        [0.1279, 0.1240, 0.1202, 0.1154, 0.1086, 0.0981, 0.0909, 0.0829, 0.0729,\n",
      "         0.0591],\n",
      "        [0.1279, 0.1239, 0.1202, 0.1154, 0.1086, 0.0981, 0.0909, 0.0829, 0.0729,\n",
      "         0.0591],\n",
      "        [0.1279, 0.1239, 0.1202, 0.1154, 0.1086, 0.0981, 0.0909, 0.0830, 0.0729,\n",
      "         0.0592],\n",
      "        [0.1278, 0.1239, 0.1202, 0.1154, 0.1086, 0.0981, 0.0910, 0.0830, 0.0729,\n",
      "         0.0592],\n",
      "        [0.1278, 0.1239, 0.1202, 0.1154, 0.1086, 0.0981, 0.0910, 0.0830, 0.0729,\n",
      "         0.0592]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 掌声 阿富汗 与 美国 国有 很大 不同 <EOS> <PAD> <PAD>\n",
      "Reference: afghanistan looks so different from here in america .\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.1816, 0.1729, 0.1627, 0.1395, 0.1214, 0.0995, 0.0752, 0.0471, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1821, 0.1734, 0.1630, 0.1396, 0.1212, 0.0992, 0.0748, 0.0467, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1822, 0.1734, 0.1631, 0.1396, 0.1212, 0.0991, 0.0747, 0.0466, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1821, 0.1734, 0.1632, 0.1397, 0.1212, 0.0991, 0.0747, 0.0466, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1821, 0.1734, 0.1632, 0.1397, 0.1213, 0.0991, 0.0746, 0.0466, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1820, 0.1734, 0.1632, 0.1397, 0.1213, 0.0991, 0.0746, 0.0466, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1820, 0.1734, 0.1632, 0.1397, 0.1213, 0.0992, 0.0746, 0.0465, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1820, 0.1734, 0.1632, 0.1397, 0.1213, 0.0992, 0.0746, 0.0465, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1820, 0.1734, 0.1632, 0.1397, 0.1213, 0.0992, 0.0746, 0.0465, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Experiment completed in 0 minutes with 10.13 best validation loss and 0.14 best validation BLEU.\n",
      "Training with dropout = 0.5\n",
      "Epoch: 0.00, Train Loss: 9.99, Val Loss: 10.15, Train BLEU: 0.17, Val BLEU: 0.14, Minutes Elapsed: 0.18\n",
      "Sampling from training predictions...\n",
      "Source: 它们 大小 不同 形状 各异 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: they come in all different sizes and shapes .\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.1266, 0.1141, 0.1177, 0.1376, 0.1883, 0.3156, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1262, 0.1136, 0.1172, 0.1372, 0.1883, 0.3176, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1261, 0.1134, 0.1171, 0.1371, 0.1883, 0.3181, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1260, 0.1134, 0.1170, 0.1370, 0.1883, 0.3183, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1260, 0.1133, 0.1170, 0.1370, 0.1883, 0.3184, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1260, 0.1133, 0.1170, 0.1370, 0.1883, 0.3184, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1260, 0.1133, 0.1170, 0.1370, 0.1883, 0.3184, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1260, 0.1133, 0.1170, 0.1370, 0.1883, 0.3184, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1260, 0.1133, 0.1170, 0.1370, 0.1883, 0.3184, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 总是 担心 会 被 塔利 塔利班 发现 <EOS> <PAD>\n",
      "Reference: we always wondered what they knew about us .\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0950, 0.0799, 0.0727, 0.0686, 0.0733, 0.0887, 0.1119, 0.1516, 0.2583,\n",
      "         0.0000],\n",
      "        [0.0947, 0.0794, 0.0722, 0.0681, 0.0728, 0.0883, 0.1117, 0.1519, 0.2608,\n",
      "         0.0000],\n",
      "        [0.0946, 0.0793, 0.0721, 0.0679, 0.0726, 0.0882, 0.1117, 0.1520, 0.2615,\n",
      "         0.0000],\n",
      "        [0.0946, 0.0793, 0.0720, 0.0679, 0.0726, 0.0881, 0.1117, 0.1521, 0.2618,\n",
      "         0.0000],\n",
      "        [0.0946, 0.0793, 0.0720, 0.0678, 0.0725, 0.0881, 0.1117, 0.1521, 0.2619,\n",
      "         0.0000],\n",
      "        [0.0946, 0.0792, 0.0720, 0.0678, 0.0725, 0.0881, 0.1117, 0.1521, 0.2619,\n",
      "         0.0000],\n",
      "        [0.0946, 0.0792, 0.0720, 0.0678, 0.0725, 0.0881, 0.1117, 0.1521, 0.2620,\n",
      "         0.0000],\n",
      "        [0.0946, 0.0792, 0.0720, 0.0678, 0.0725, 0.0881, 0.1117, 0.1521, 0.2620,\n",
      "         0.0000],\n",
      "        [0.0946, 0.0792, 0.0720, 0.0678, 0.0725, 0.0881, 0.1117, 0.1521, 0.2619,\n",
      "         0.0000]])\n",
      "\n",
      "Experiment completed in 0 minutes with 10.15 best validation loss and 0.14 best validation BLEU.\n"
     ]
    }
   ],
   "source": [
    "for candidate in [0, 0.2, 0.5]: \n",
    "    \n",
    "    print(\"Training with dropout = {}\".format(candidate))\n",
    "    \n",
    "    # overwrite relevant key-value in params \n",
    "    params['enc_dropout'] = candidate \n",
    "    params['dec_dropout'] = candidate\n",
    "    params['model_name'] = '{}-rnn-{}-attn-{}-dropout'.format(SRC_LANG, ATTENTION_TYPE, candidate)\n",
    "    \n",
    "    # instantiate model \n",
    "    encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                         src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=candidate, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "    decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                             num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, \n",
    "                             src_max_sentence_len=SRC_MAX_SENTENCE_LEN, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                             dec_dropout=candidate, attention_type=ATTENTION_TYPE,\n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                    vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) \n",
    "    \n",
    "    # train and eval \n",
    "    model, results = train_and_eval(\n",
    "        model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "        params=params, vocab=vocab, print_intermediate=100, save_checkpoint=True, save_to_log=True, \n",
    "        lazy_eval=False, print_attn=True, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_dropout</th>\n",
       "      <th>dec_dropout</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_val_bleu</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.148959</td>\n",
       "      <td>0.141454</td>\n",
       "      <td>0.185415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.134708</td>\n",
       "      <td>0.142060</td>\n",
       "      <td>0.187911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.119202</td>\n",
       "      <td>0.144099</td>\n",
       "      <td>0.188242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enc_dropout  dec_dropout  best_val_loss  best_val_bleu   runtime\n",
       "2          0.5          0.5      10.148959       0.141454  0.185415\n",
       "1          0.2          0.2      10.134708       0.142060  0.187911\n",
       "0          0.0          0.0      10.119202       0.144099  0.188242"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)\n",
    "summarize_results(experiment_results)[['enc_dropout', 'dec_dropout', 'best_val_loss', 'best_val_bleu', 'runtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
