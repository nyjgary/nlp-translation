{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import train_and_eval, count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model identification\n",
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "# data processing params  \n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "# model architecture params \n",
    "NETWORK_TYPE = 'rnn'\n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 \n",
    "DEC_DROPOUT = 0.2 \n",
    "ATTENTION_TYPE = 'additive'\n",
    "\n",
    "# training params  \n",
    "BATCH_SIZE = 64 #32\n",
    "NUM_EPOCHS = 20\n",
    "LR = 0.0003 # 0.0005\n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = True\n",
    "\n",
    "# name the model \n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    MODEL_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    MODEL_NAME = '{}-cnn'.format(SRC_LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, 'rnn_cell_type': RNN_CELL_TYPE, \n",
    "          'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, \n",
    "          'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "          'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, 'dec_hidden_dim': DEC_HIDDEN_DIM,\n",
    "          'teacher_forcing_ratio': TEACHER_FORCING_RATIO, 'clip_grad_max_norm': CLIP_GRAD_MAX_NORM,\n",
    "          'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, 'attention_type': ATTENTION_TYPE, \n",
    "          'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, 'learning_rate': LR, 'optimizer': OPTIMIZER, \n",
    "          'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "# # without attention \n",
    "# decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "#                      targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)\n",
    "\n",
    "# with additive attention \n",
    "decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                         num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "                         targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) \n",
    "\n",
    "# # with multiplicative attention \n",
    "# decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "#                          num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "#                          targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "#                          pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 9.98, Val Loss: 10.15, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 0.18\n",
      "Sampling from training predictions...\n",
      "Source: 它 的 耳朵 在 上下 <UNK> 摆动 还 很 优雅\n",
      "Reference: and here he is , flapping with his ears\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0705, 0.0676, 0.0740, 0.0840, 0.1162, 0.2136, 0.1077, 0.0859, 0.0837,\n",
      "         0.0968],\n",
      "        [0.0703, 0.0674, 0.0738, 0.0839, 0.1162, 0.2148, 0.1078, 0.0858, 0.0835,\n",
      "         0.0966],\n",
      "        [0.0704, 0.0674, 0.0738, 0.0839, 0.1162, 0.2149, 0.1078, 0.0857, 0.0835,\n",
      "         0.0965],\n",
      "        [0.0704, 0.0674, 0.0738, 0.0838, 0.1161, 0.2150, 0.1078, 0.0857, 0.0834,\n",
      "         0.0965],\n",
      "        [0.0704, 0.0674, 0.0737, 0.0838, 0.1162, 0.2151, 0.1078, 0.0857, 0.0834,\n",
      "         0.0964],\n",
      "        [0.0704, 0.0674, 0.0737, 0.0838, 0.1162, 0.2152, 0.1078, 0.0857, 0.0834,\n",
      "         0.0964],\n",
      "        [0.0704, 0.0674, 0.0737, 0.0838, 0.1162, 0.2152, 0.1078, 0.0857, 0.0834,\n",
      "         0.0964],\n",
      "        [0.0705, 0.0674, 0.0737, 0.0838, 0.1162, 0.2152, 0.1078, 0.0857, 0.0833,\n",
      "         0.0964],\n",
      "        [0.0705, 0.0674, 0.0737, 0.0838, 0.1162, 0.2152, 0.1078, 0.0857, 0.0833,\n",
      "         0.0964]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 所以 在 那 之后 5 年 我 <UNK> <UNK> 陪\n",
      "Reference: so for the next five years , i dressed\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0519, 0.0448, 0.0446, 0.0480, 0.0504, 0.0666, 0.0998, 0.2281, 0.2422,\n",
      "         0.1236],\n",
      "        [0.0515, 0.0444, 0.0441, 0.0475, 0.0499, 0.0661, 0.0993, 0.2294, 0.2441,\n",
      "         0.1237],\n",
      "        [0.0515, 0.0443, 0.0441, 0.0474, 0.0498, 0.0660, 0.0992, 0.2294, 0.2445,\n",
      "         0.1238],\n",
      "        [0.0515, 0.0443, 0.0440, 0.0474, 0.0498, 0.0659, 0.0991, 0.2295, 0.2447,\n",
      "         0.1237],\n",
      "        [0.0515, 0.0443, 0.0440, 0.0474, 0.0497, 0.0659, 0.0991, 0.2296, 0.2448,\n",
      "         0.1237],\n",
      "        [0.0515, 0.0443, 0.0440, 0.0473, 0.0497, 0.0659, 0.0991, 0.2297, 0.2449,\n",
      "         0.1237],\n",
      "        [0.0515, 0.0443, 0.0440, 0.0473, 0.0497, 0.0659, 0.0991, 0.2297, 0.2449,\n",
      "         0.1237],\n",
      "        [0.0515, 0.0443, 0.0440, 0.0473, 0.0497, 0.0659, 0.0991, 0.2297, 0.2449,\n",
      "         0.1237],\n",
      "        [0.0515, 0.0443, 0.0440, 0.0473, 0.0497, 0.0659, 0.0991, 0.2297, 0.2449,\n",
      "         0.1237]])\n",
      "\n",
      "Epoch: 1.00, Train Loss: 9.49, Val Loss: 9.91, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 0.36\n",
      "Sampling from training predictions...\n",
      "Source: 它们 有 的 会 贴近 潜水 潜水艇 它们 的 眼睛\n",
      "Reference: they come right up to the submarine -- they\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0770, 0.0976, 0.1110, 0.1166, 0.1145, 0.1113, 0.1022, 0.0985, 0.0930,\n",
      "         0.0784],\n",
      "        [0.0768, 0.0977, 0.1113, 0.1169, 0.1146, 0.1113, 0.1021, 0.0984, 0.0928,\n",
      "         0.0779],\n",
      "        [0.0769, 0.0978, 0.1114, 0.1170, 0.1146, 0.1113, 0.1021, 0.0984, 0.0928,\n",
      "         0.0778],\n",
      "        [0.0771, 0.0979, 0.1114, 0.1169, 0.1145, 0.1112, 0.1020, 0.0984, 0.0928,\n",
      "         0.0778],\n",
      "        [0.0773, 0.0980, 0.1114, 0.1168, 0.1144, 0.1111, 0.1020, 0.0984, 0.0928,\n",
      "         0.0779],\n",
      "        [0.0774, 0.0981, 0.1114, 0.1168, 0.1144, 0.1111, 0.1020, 0.0984, 0.0928,\n",
      "         0.0779],\n",
      "        [0.0774, 0.0981, 0.1114, 0.1167, 0.1143, 0.1110, 0.1020, 0.0983, 0.0927,\n",
      "         0.0779],\n",
      "        [0.0775, 0.0981, 0.1114, 0.1167, 0.1143, 0.1110, 0.1020, 0.0983, 0.0927,\n",
      "         0.0780],\n",
      "        [0.0775, 0.0981, 0.1114, 0.1167, 0.1143, 0.1110, 0.1020, 0.0983, 0.0927,\n",
      "         0.0780]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 梦想 很大 但 他们 对 我 的 期待\n",
      "Reference: i dream big , but my family dreams even\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0753, 0.0930, 0.1024, 0.1082, 0.1140, 0.1131, 0.1131, 0.1075, 0.0958,\n",
      "         0.0777],\n",
      "        [0.0751, 0.0931, 0.1025, 0.1083, 0.1143, 0.1132, 0.1133, 0.1075, 0.0956,\n",
      "         0.0772],\n",
      "        [0.0753, 0.0932, 0.1025, 0.1083, 0.1143, 0.1132, 0.1132, 0.1074, 0.0955,\n",
      "         0.0770],\n",
      "        [0.0754, 0.0933, 0.1025, 0.1083, 0.1142, 0.1132, 0.1132, 0.1074, 0.0955,\n",
      "         0.0770],\n",
      "        [0.0756, 0.0934, 0.1025, 0.1083, 0.1142, 0.1131, 0.1131, 0.1073, 0.0954,\n",
      "         0.0770],\n",
      "        [0.0757, 0.0934, 0.1026, 0.1083, 0.1141, 0.1130, 0.1130, 0.1073, 0.0954,\n",
      "         0.0771],\n",
      "        [0.0758, 0.0935, 0.1026, 0.1083, 0.1141, 0.1130, 0.1130, 0.1072, 0.0954,\n",
      "         0.0771],\n",
      "        [0.0759, 0.0935, 0.1026, 0.1083, 0.1141, 0.1130, 0.1129, 0.1072, 0.0954,\n",
      "         0.0772],\n",
      "        [0.0759, 0.0935, 0.1026, 0.1083, 0.1140, 0.1130, 0.1129, 0.1072, 0.0954,\n",
      "         0.0772]])\n",
      "\n",
      "Epoch: 2.00, Train Loss: 8.80, Val Loss: 9.55, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 0.56\n",
      "Sampling from training predictions...\n",
      "Source: 它们 有 的 会 贴近 潜水 潜水艇 它们 的 眼睛\n",
      "Reference: they come right up to the submarine -- they\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0400, 0.0929, 0.1371, 0.1565, 0.1430, 0.1281, 0.0996, 0.0942, 0.0739,\n",
      "         0.0346],\n",
      "        [0.0392, 0.0927, 0.1379, 0.1579, 0.1437, 0.1285, 0.0993, 0.0939, 0.0732,\n",
      "         0.0337],\n",
      "        [0.0392, 0.0927, 0.1379, 0.1579, 0.1437, 0.1285, 0.0993, 0.0939, 0.0733,\n",
      "         0.0337],\n",
      "        [0.0393, 0.0927, 0.1378, 0.1577, 0.1435, 0.1284, 0.0993, 0.0940, 0.0734,\n",
      "         0.0338],\n",
      "        [0.0394, 0.0928, 0.1378, 0.1575, 0.1434, 0.1283, 0.0993, 0.0940, 0.0735,\n",
      "         0.0339],\n",
      "        [0.0395, 0.0929, 0.1377, 0.1574, 0.1433, 0.1282, 0.0993, 0.0941, 0.0735,\n",
      "         0.0340],\n",
      "        [0.0396, 0.0930, 0.1377, 0.1573, 0.1432, 0.1282, 0.0994, 0.0941, 0.0736,\n",
      "         0.0341],\n",
      "        [0.0396, 0.0930, 0.1377, 0.1572, 0.1432, 0.1282, 0.0993, 0.0941, 0.0736,\n",
      "         0.0341],\n",
      "        [0.0397, 0.0930, 0.1377, 0.1572, 0.1431, 0.1281, 0.0994, 0.0941, 0.0736,\n",
      "         0.0341]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 但是 我 那 受过 教育 的 母亲 成为 为了 一名\n",
      "Reference: but my educated mother became a teacher . <EOS>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0382, 0.0875, 0.1275, 0.1354, 0.1439, 0.1510, 0.1244, 0.0965, 0.0636,\n",
      "         0.0320],\n",
      "        [0.0374, 0.0871, 0.1279, 0.1359, 0.1447, 0.1521, 0.1247, 0.0962, 0.0628,\n",
      "         0.0311],\n",
      "        [0.0374, 0.0871, 0.1279, 0.1359, 0.1447, 0.1521, 0.1247, 0.0963, 0.0629,\n",
      "         0.0311],\n",
      "        [0.0376, 0.0872, 0.1278, 0.1357, 0.1446, 0.1520, 0.1247, 0.0963, 0.0630,\n",
      "         0.0312],\n",
      "        [0.0377, 0.0873, 0.1278, 0.1356, 0.1444, 0.1518, 0.1246, 0.0964, 0.0631,\n",
      "         0.0313],\n",
      "        [0.0378, 0.0874, 0.1278, 0.1356, 0.1443, 0.1517, 0.1246, 0.0964, 0.0631,\n",
      "         0.0314],\n",
      "        [0.0379, 0.0874, 0.1278, 0.1355, 0.1443, 0.1516, 0.1245, 0.0964, 0.0632,\n",
      "         0.0314],\n",
      "        [0.0379, 0.0875, 0.1278, 0.1355, 0.1442, 0.1515, 0.1245, 0.0964, 0.0632,\n",
      "         0.0314],\n",
      "        [0.0380, 0.0875, 0.1278, 0.1355, 0.1442, 0.1515, 0.1245, 0.0964, 0.0632,\n",
      "         0.0315]])\n",
      "\n",
      "Epoch: 3.00, Train Loss: 8.09, Val Loss: 9.21, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 0.82\n",
      "Sampling from training predictions...\n",
      "Source: 其实 实地 地球 上 最长 的 山脉 都 在 海洋\n",
      "Reference: and in the oceans , there are the longest\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0124, 0.0459, 0.0940, 0.1519, 0.1638, 0.1818, 0.1506, 0.1222, 0.0629,\n",
      "         0.0144],\n",
      "        [0.0113, 0.0439, 0.0926, 0.1530, 0.1655, 0.1849, 0.1520, 0.1224, 0.0612,\n",
      "         0.0133],\n",
      "        [0.0112, 0.0436, 0.0923, 0.1531, 0.1657, 0.1852, 0.1521, 0.1225, 0.0611,\n",
      "         0.0132],\n",
      "        [0.0112, 0.0436, 0.0923, 0.1530, 0.1656, 0.1852, 0.1521, 0.1226, 0.0612,\n",
      "         0.0133],\n",
      "        [0.0112, 0.0436, 0.0923, 0.1529, 0.1655, 0.1850, 0.1521, 0.1226, 0.0613,\n",
      "         0.0133],\n",
      "        [0.0112, 0.0437, 0.0923, 0.1528, 0.1654, 0.1849, 0.1521, 0.1227, 0.0614,\n",
      "         0.0134],\n",
      "        [0.0113, 0.0437, 0.0923, 0.1528, 0.1654, 0.1849, 0.1521, 0.1227, 0.0615,\n",
      "         0.0134],\n",
      "        [0.0113, 0.0437, 0.0923, 0.1528, 0.1653, 0.1848, 0.1521, 0.1227, 0.0616,\n",
      "         0.0134],\n",
      "        [0.0113, 0.0438, 0.0923, 0.1527, 0.1653, 0.1848, 0.1520, 0.1227, 0.0616,\n",
      "         0.0135]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 不到 一个 一个月 月前 他 和 他 女儿 在 从\n",
      "Reference: less than a month ago , he and his\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0112, 0.0378, 0.0606, 0.1121, 0.1747, 0.1987, 0.1829, 0.1253, 0.0753,\n",
      "         0.0214],\n",
      "        [0.0101, 0.0358, 0.0584, 0.1112, 0.1770, 0.2026, 0.1858, 0.1253, 0.0737,\n",
      "         0.0200],\n",
      "        [0.0100, 0.0355, 0.0581, 0.1111, 0.1773, 0.2031, 0.1861, 0.1253, 0.0736,\n",
      "         0.0199],\n",
      "        [0.0100, 0.0355, 0.0581, 0.1110, 0.1772, 0.2030, 0.1861, 0.1253, 0.0737,\n",
      "         0.0199],\n",
      "        [0.0101, 0.0356, 0.0581, 0.1110, 0.1771, 0.2028, 0.1861, 0.1254, 0.0738,\n",
      "         0.0200],\n",
      "        [0.0101, 0.0356, 0.0582, 0.1110, 0.1769, 0.2027, 0.1860, 0.1255, 0.0739,\n",
      "         0.0201],\n",
      "        [0.0101, 0.0357, 0.0582, 0.1110, 0.1769, 0.2026, 0.1860, 0.1255, 0.0740,\n",
      "         0.0201],\n",
      "        [0.0102, 0.0357, 0.0582, 0.1110, 0.1768, 0.2025, 0.1859, 0.1255, 0.0741,\n",
      "         0.0202],\n",
      "        [0.0102, 0.0357, 0.0583, 0.1110, 0.1767, 0.2025, 0.1859, 0.1255, 0.0741,\n",
      "         0.0202]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.00, Train Loss: 7.49, Val Loss: 8.93, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 1.36\n",
      "Sampling from training predictions...\n",
      "Source: 我们 能 在 不同 深度 甚至 最深 最深处 深处 看到\n",
      "Reference: we see those at all depths and even at\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0091, 0.0685, 0.1523, 0.1802, 0.1871, 0.1700, 0.1287, 0.0637, 0.0342,\n",
      "         0.0063],\n",
      "        [0.0075, 0.0645, 0.1530, 0.1839, 0.1920, 0.1734, 0.1287, 0.0605, 0.0312,\n",
      "         0.0053],\n",
      "        [0.0073, 0.0638, 0.1529, 0.1844, 0.1928, 0.1740, 0.1287, 0.0601, 0.0309,\n",
      "         0.0052],\n",
      "        [0.0072, 0.0636, 0.1529, 0.1844, 0.1929, 0.1741, 0.1288, 0.0601, 0.0309,\n",
      "         0.0052],\n",
      "        [0.0072, 0.0636, 0.1528, 0.1844, 0.1928, 0.1741, 0.1289, 0.0601, 0.0309,\n",
      "         0.0052],\n",
      "        [0.0072, 0.0636, 0.1527, 0.1843, 0.1928, 0.1741, 0.1289, 0.0602, 0.0310,\n",
      "         0.0052],\n",
      "        [0.0072, 0.0636, 0.1527, 0.1843, 0.1927, 0.1741, 0.1289, 0.0602, 0.0310,\n",
      "         0.0052],\n",
      "        [0.0072, 0.0636, 0.1527, 0.1842, 0.1927, 0.1741, 0.1289, 0.0603, 0.0310,\n",
      "         0.0052],\n",
      "        [0.0073, 0.0636, 0.1527, 0.1842, 0.1926, 0.1741, 0.1289, 0.0603, 0.0311,\n",
      "         0.0052]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 这 并 不是 说 母亲 们 对于 我们 的 成功\n",
      "Reference: it &apos;s not to say that our mothers aren\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0105, 0.0600, 0.1134, 0.1691, 0.1736, 0.1833, 0.1407, 0.0961, 0.0469,\n",
      "         0.0065],\n",
      "        [0.0086, 0.0560, 0.1115, 0.1723, 0.1776, 0.1889, 0.1420, 0.0943, 0.0436,\n",
      "         0.0054],\n",
      "        [0.0084, 0.0553, 0.1111, 0.1728, 0.1782, 0.1897, 0.1421, 0.0940, 0.0432,\n",
      "         0.0053],\n",
      "        [0.0083, 0.0551, 0.1109, 0.1728, 0.1782, 0.1899, 0.1422, 0.0940, 0.0432,\n",
      "         0.0053],\n",
      "        [0.0083, 0.0551, 0.1109, 0.1727, 0.1782, 0.1899, 0.1423, 0.0941, 0.0433,\n",
      "         0.0053],\n",
      "        [0.0084, 0.0551, 0.1109, 0.1727, 0.1781, 0.1898, 0.1423, 0.0942, 0.0434,\n",
      "         0.0053],\n",
      "        [0.0084, 0.0551, 0.1109, 0.1726, 0.1780, 0.1898, 0.1423, 0.0942, 0.0434,\n",
      "         0.0053],\n",
      "        [0.0084, 0.0552, 0.1109, 0.1726, 0.1780, 0.1897, 0.1423, 0.0943, 0.0435,\n",
      "         0.0053],\n",
      "        [0.0084, 0.0552, 0.1109, 0.1725, 0.1779, 0.1897, 0.1423, 0.0943, 0.0435,\n",
      "         0.0053]])\n",
      "\n",
      "Epoch: 5.00, Train Loss: 6.95, Val Loss: 8.69, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 1.55\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 通过 潜水 潜水艇 拍下 的 影片 把 我们\n",
      "Reference: with vibrant video clips captured by submarines , david\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0032, 0.0082, 0.0545, 0.1176, 0.1399, 0.2036, 0.2305, 0.1610, 0.0742,\n",
      "         0.0072],\n",
      "        [0.0022, 0.0063, 0.0491, 0.1146, 0.1386, 0.2112, 0.2428, 0.1617, 0.0681,\n",
      "         0.0053],\n",
      "        [0.0020, 0.0061, 0.0481, 0.1139, 0.1382, 0.2124, 0.2450, 0.1619, 0.0672,\n",
      "         0.0051],\n",
      "        [0.0020, 0.0061, 0.0479, 0.1137, 0.1380, 0.2125, 0.2454, 0.1620, 0.0671,\n",
      "         0.0051],\n",
      "        [0.0020, 0.0061, 0.0479, 0.1137, 0.1380, 0.2125, 0.2455, 0.1621, 0.0672,\n",
      "         0.0051],\n",
      "        [0.0020, 0.0061, 0.0479, 0.1137, 0.1379, 0.2124, 0.2454, 0.1621, 0.0673,\n",
      "         0.0051],\n",
      "        [0.0020, 0.0061, 0.0480, 0.1137, 0.1379, 0.2123, 0.2453, 0.1622, 0.0673,\n",
      "         0.0052],\n",
      "        [0.0020, 0.0061, 0.0480, 0.1137, 0.1379, 0.2122, 0.2453, 0.1622, 0.0674,\n",
      "         0.0052],\n",
      "        [0.0020, 0.0061, 0.0480, 0.1138, 0.1379, 0.2122, 0.2452, 0.1622, 0.0674,\n",
      "         0.0052]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 这 就是 我 成为 <UNK> 的 全球 大使 的 原因\n",
      "Reference: that &apos;s why i am a global ambassador for\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0100, 0.0670, 0.1543, 0.1571, 0.0706, 0.1655, 0.1695, 0.1343, 0.0652,\n",
      "         0.0065],\n",
      "        [0.0073, 0.0600, 0.1549, 0.1613, 0.0695, 0.1730, 0.1753, 0.1341, 0.0598,\n",
      "         0.0048],\n",
      "        [0.0070, 0.0588, 0.1547, 0.1616, 0.0693, 0.1742, 0.1763, 0.1342, 0.0591,\n",
      "         0.0047],\n",
      "        [0.0069, 0.0585, 0.1546, 0.1617, 0.0694, 0.1744, 0.1765, 0.1343, 0.0591,\n",
      "         0.0047],\n",
      "        [0.0069, 0.0585, 0.1545, 0.1616, 0.0694, 0.1744, 0.1765, 0.1344, 0.0592,\n",
      "         0.0047],\n",
      "        [0.0069, 0.0585, 0.1545, 0.1616, 0.0694, 0.1744, 0.1765, 0.1344, 0.0592,\n",
      "         0.0047],\n",
      "        [0.0069, 0.0585, 0.1544, 0.1615, 0.0694, 0.1744, 0.1765, 0.1344, 0.0593,\n",
      "         0.0047],\n",
      "        [0.0069, 0.0585, 0.1544, 0.1615, 0.0695, 0.1743, 0.1764, 0.1345, 0.0594,\n",
      "         0.0047],\n",
      "        [0.0070, 0.0585, 0.1544, 0.1614, 0.0695, 0.1743, 0.1764, 0.1345, 0.0594,\n",
      "         0.0047]])\n",
      "\n",
      "Epoch: 6.00, Train Loss: 6.46, Val Loss: 8.45, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 1.76\n",
      "Sampling from training predictions...\n",
      "Source: 你们 看到 的 这些 基本 基本上 都 是 科学 数据\n",
      "Reference: this is basically scientific data that you &apos;re looking\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0032, 0.0404, 0.1188, 0.1574, 0.1729, 0.1634, 0.1718, 0.1249, 0.0435,\n",
      "         0.0036],\n",
      "        [0.0018, 0.0323, 0.1142, 0.1601, 0.1799, 0.1690, 0.1798, 0.1238, 0.0366,\n",
      "         0.0023],\n",
      "        [0.0017, 0.0312, 0.1133, 0.1603, 0.1808, 0.1698, 0.1811, 0.1238, 0.0359,\n",
      "         0.0022],\n",
      "        [0.0017, 0.0310, 0.1131, 0.1603, 0.1809, 0.1698, 0.1813, 0.1239, 0.0358,\n",
      "         0.0022],\n",
      "        [0.0017, 0.0310, 0.1130, 0.1602, 0.1808, 0.1698, 0.1814, 0.1240, 0.0359,\n",
      "         0.0022],\n",
      "        [0.0017, 0.0310, 0.1130, 0.1602, 0.1808, 0.1697, 0.1814, 0.1241, 0.0360,\n",
      "         0.0022],\n",
      "        [0.0017, 0.0310, 0.1130, 0.1601, 0.1807, 0.1697, 0.1813, 0.1241, 0.0360,\n",
      "         0.0022],\n",
      "        [0.0017, 0.0311, 0.1131, 0.1601, 0.1806, 0.1697, 0.1813, 0.1242, 0.0361,\n",
      "         0.0022],\n",
      "        [0.0017, 0.0311, 0.1131, 0.1601, 0.1806, 0.1696, 0.1813, 0.1242, 0.0361,\n",
      "         0.0022]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 他 骄傲 的 不仅 不仅仅 仅仅 是因为 因为 我 的\n",
      "Reference: he not only <UNK> about my college degree ,\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0064, 0.0563, 0.1385, 0.1609, 0.1369, 0.1657, 0.1407, 0.1232, 0.0646,\n",
      "         0.0069],\n",
      "        [0.0040, 0.0480, 0.1384, 0.1664, 0.1386, 0.1738, 0.1439, 0.1235, 0.0586,\n",
      "         0.0047],\n",
      "        [0.0038, 0.0469, 0.1382, 0.1670, 0.1387, 0.1749, 0.1443, 0.1237, 0.0580,\n",
      "         0.0046],\n",
      "        [0.0038, 0.0467, 0.1381, 0.1671, 0.1387, 0.1750, 0.1444, 0.1238, 0.0580,\n",
      "         0.0046],\n",
      "        [0.0038, 0.0466, 0.1380, 0.1670, 0.1386, 0.1750, 0.1444, 0.1238, 0.0581,\n",
      "         0.0046],\n",
      "        [0.0038, 0.0467, 0.1380, 0.1670, 0.1386, 0.1749, 0.1444, 0.1239, 0.0582,\n",
      "         0.0046],\n",
      "        [0.0038, 0.0467, 0.1380, 0.1669, 0.1385, 0.1748, 0.1444, 0.1239, 0.0583,\n",
      "         0.0046],\n",
      "        [0.0038, 0.0467, 0.1380, 0.1669, 0.1385, 0.1748, 0.1444, 0.1239, 0.0584,\n",
      "         0.0046],\n",
      "        [0.0038, 0.0468, 0.1380, 0.1668, 0.1385, 0.1747, 0.1444, 0.1240, 0.0584,\n",
      "         0.0046]])\n",
      "\n",
      "Epoch: 7.00, Train Loss: 6.04, Val Loss: 8.26, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 1.97\n",
      "Sampling from training predictions...\n",
      "Source: 我们 现在 <UNK> 右转 转过 一个 悬崖 <EOS> <PAD> <PAD>\n",
      "Reference: we &apos;re coming around a cliff here on the\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0086, 0.0918, 0.0689, 0.2365, 0.2862, 0.2386, 0.0686, 0.0009, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0049, 0.0763, 0.0607, 0.2454, 0.3056, 0.2476, 0.0589, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0046, 0.0741, 0.0596, 0.2458, 0.3078, 0.2493, 0.0583, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0045, 0.0737, 0.0594, 0.2458, 0.3081, 0.2497, 0.0583, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0045, 0.0736, 0.0594, 0.2457, 0.3081, 0.2498, 0.0584, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0045, 0.0736, 0.0594, 0.2456, 0.3080, 0.2499, 0.0585, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0045, 0.0736, 0.0595, 0.2456, 0.3079, 0.2498, 0.0586, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0045, 0.0736, 0.0596, 0.2455, 0.3078, 0.2498, 0.0586, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0045, 0.0737, 0.0596, 0.2455, 0.3077, 0.2498, 0.0587, 0.0005, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 她 两年 年前 退休 了 结果 却 把 我家 变成\n",
      "Reference: she retired two years ago , only to turn\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0045, 0.0461, 0.1089, 0.1515, 0.1810, 0.1754, 0.1664, 0.1201, 0.0431,\n",
      "         0.0031],\n",
      "        [0.0024, 0.0357, 0.1020, 0.1537, 0.1922, 0.1857, 0.1745, 0.1174, 0.0345,\n",
      "         0.0017],\n",
      "        [0.0022, 0.0345, 0.1008, 0.1537, 0.1934, 0.1870, 0.1757, 0.1173, 0.0338,\n",
      "         0.0016],\n",
      "        [0.0022, 0.0343, 0.1006, 0.1536, 0.1935, 0.1871, 0.1759, 0.1174, 0.0337,\n",
      "         0.0016],\n",
      "        [0.0022, 0.0343, 0.1006, 0.1536, 0.1935, 0.1870, 0.1759, 0.1175, 0.0338,\n",
      "         0.0016],\n",
      "        [0.0022, 0.0344, 0.1006, 0.1535, 0.1934, 0.1870, 0.1759, 0.1175, 0.0339,\n",
      "         0.0016],\n",
      "        [0.0022, 0.0344, 0.1007, 0.1535, 0.1933, 0.1869, 0.1758, 0.1176, 0.0340,\n",
      "         0.0016],\n",
      "        [0.0022, 0.0344, 0.1007, 0.1535, 0.1932, 0.1868, 0.1758, 0.1177, 0.0340,\n",
      "         0.0016],\n",
      "        [0.0022, 0.0345, 0.1007, 0.1534, 0.1932, 0.1868, 0.1758, 0.1177, 0.0341,\n",
      "         0.0016]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8.00, Train Loss: 5.68, Val Loss: 8.15, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 2.15\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 这位 是 比尔 <UNK> 我 是 大卫 <UNK>\n",
      "Reference: this is bill lange . i &apos;m dave gallo\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0033, 0.0126, 0.1041, 0.2043, 0.1901, 0.0832, 0.1944, 0.1630, 0.0445,\n",
      "         0.0004],\n",
      "        [0.0015, 0.0079, 0.0932, 0.2133, 0.1993, 0.0783, 0.2067, 0.1647, 0.0349,\n",
      "         0.0002],\n",
      "        [0.0014, 0.0075, 0.0915, 0.2137, 0.1999, 0.0777, 0.2083, 0.1656, 0.0343,\n",
      "         0.0002],\n",
      "        [0.0014, 0.0074, 0.0912, 0.2137, 0.1999, 0.0775, 0.2085, 0.1658, 0.0343,\n",
      "         0.0002],\n",
      "        [0.0014, 0.0074, 0.0912, 0.2136, 0.1999, 0.0776, 0.2085, 0.1659, 0.0344,\n",
      "         0.0002],\n",
      "        [0.0014, 0.0074, 0.0912, 0.2135, 0.1998, 0.0776, 0.2085, 0.1659, 0.0344,\n",
      "         0.0002],\n",
      "        [0.0014, 0.0074, 0.0913, 0.2134, 0.1997, 0.0777, 0.2084, 0.1659, 0.0345,\n",
      "         0.0002],\n",
      "        [0.0014, 0.0074, 0.0913, 0.2134, 0.1997, 0.0778, 0.2084, 0.1659, 0.0345,\n",
      "         0.0002],\n",
      "        [0.0014, 0.0075, 0.0913, 0.2133, 0.1996, 0.0778, 0.2083, 0.1659, 0.0346,\n",
      "         0.0002]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 很 幸运 幸运地 成长 在 一个 珍视 教育 也\n",
      "Reference: i was very lucky to grow up in a\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0050, 0.0606, 0.1184, 0.1403, 0.1675, 0.1790, 0.1554, 0.1155, 0.0534,\n",
      "         0.0050],\n",
      "        [0.0024, 0.0484, 0.1132, 0.1418, 0.1783, 0.1945, 0.1629, 0.1123, 0.0434,\n",
      "         0.0027],\n",
      "        [0.0022, 0.0470, 0.1123, 0.1417, 0.1792, 0.1963, 0.1639, 0.1122, 0.0426,\n",
      "         0.0026],\n",
      "        [0.0022, 0.0468, 0.1122, 0.1416, 0.1793, 0.1965, 0.1640, 0.1123, 0.0426,\n",
      "         0.0026],\n",
      "        [0.0022, 0.0468, 0.1122, 0.1416, 0.1793, 0.1964, 0.1640, 0.1123, 0.0427,\n",
      "         0.0026],\n",
      "        [0.0022, 0.0469, 0.1122, 0.1416, 0.1792, 0.1963, 0.1640, 0.1124, 0.0427,\n",
      "         0.0026],\n",
      "        [0.0022, 0.0469, 0.1122, 0.1416, 0.1791, 0.1962, 0.1639, 0.1124, 0.0428,\n",
      "         0.0026],\n",
      "        [0.0022, 0.0470, 0.1123, 0.1415, 0.1791, 0.1961, 0.1639, 0.1125, 0.0429,\n",
      "         0.0026],\n",
      "        [0.0022, 0.0470, 0.1123, 0.1415, 0.1790, 0.1960, 0.1639, 0.1125, 0.0429,\n",
      "         0.0026]])\n",
      "\n",
      "Epoch: 9.00, Train Loss: 5.39, Val Loss: 8.11, Train BLEU: 0.25, Val BLEU: 0.19, Minutes Elapsed: 2.44\n",
      "Sampling from training predictions...\n",
      "Source: 我们 能 在 不同 深度 甚至 最深 最深处 深处 看到\n",
      "Reference: we see those at all depths and even at\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0035, 0.0626, 0.1360, 0.1640, 0.1734, 0.1685, 0.1469, 0.0956, 0.0463,\n",
      "         0.0031],\n",
      "        [0.0014, 0.0486, 0.1340, 0.1729, 0.1871, 0.1809, 0.1515, 0.0873, 0.0349,\n",
      "         0.0014],\n",
      "        [0.0013, 0.0469, 0.1333, 0.1735, 0.1885, 0.1823, 0.1522, 0.0866, 0.0340,\n",
      "         0.0013],\n",
      "        [0.0013, 0.0467, 0.1332, 0.1736, 0.1887, 0.1825, 0.1523, 0.0865, 0.0339,\n",
      "         0.0013],\n",
      "        [0.0013, 0.0468, 0.1332, 0.1736, 0.1886, 0.1824, 0.1523, 0.0866, 0.0340,\n",
      "         0.0013],\n",
      "        [0.0013, 0.0468, 0.1332, 0.1735, 0.1885, 0.1824, 0.1523, 0.0867, 0.0341,\n",
      "         0.0013],\n",
      "        [0.0013, 0.0469, 0.1332, 0.1735, 0.1884, 0.1823, 0.1522, 0.0867, 0.0342,\n",
      "         0.0013],\n",
      "        [0.0013, 0.0469, 0.1332, 0.1734, 0.1884, 0.1822, 0.1522, 0.0868, 0.0342,\n",
      "         0.0013],\n",
      "        [0.0013, 0.0470, 0.1332, 0.1734, 0.1883, 0.1822, 0.1522, 0.0868, 0.0343,\n",
      "         0.0013]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 祖父 在 他 的 年代 是 位非 非凡\n",
      "Reference: my grandfather was an extraordinary man for his time\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0050, 0.0620, 0.1210, 0.1584, 0.1715, 0.1691, 0.1479, 0.1157, 0.0463,\n",
      "         0.0031],\n",
      "        [0.0021, 0.0481, 0.1158, 0.1660, 0.1851, 0.1824, 0.1532, 0.1112, 0.0348,\n",
      "         0.0014],\n",
      "        [0.0020, 0.0465, 0.1147, 0.1665, 0.1864, 0.1838, 0.1539, 0.1111, 0.0339,\n",
      "         0.0013],\n",
      "        [0.0019, 0.0463, 0.1146, 0.1665, 0.1865, 0.1840, 0.1540, 0.1111, 0.0338,\n",
      "         0.0013],\n",
      "        [0.0019, 0.0463, 0.1146, 0.1665, 0.1865, 0.1839, 0.1540, 0.1111, 0.0339,\n",
      "         0.0013],\n",
      "        [0.0019, 0.0464, 0.1146, 0.1664, 0.1864, 0.1839, 0.1539, 0.1112, 0.0340,\n",
      "         0.0013],\n",
      "        [0.0019, 0.0465, 0.1146, 0.1664, 0.1863, 0.1838, 0.1539, 0.1112, 0.0340,\n",
      "         0.0013],\n",
      "        [0.0020, 0.0465, 0.1147, 0.1664, 0.1862, 0.1837, 0.1539, 0.1113, 0.0341,\n",
      "         0.0013],\n",
      "        [0.0020, 0.0466, 0.1147, 0.1663, 0.1862, 0.1837, 0.1539, 0.1113, 0.0341,\n",
      "         0.0013]])\n",
      "\n",
      "Epoch: 10.00, Train Loss: 5.15, Val Loss: 8.14, Train BLEU: 0.29, Val BLEU: 0.22, Minutes Elapsed: 2.61\n",
      "Sampling from training predictions...\n",
      "Source: 它们 还 没有 被 研究 透 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: they &apos;re very little understood . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0084, 0.1381, 0.2611, 0.2956, 0.2174, 0.0788, 0.0005, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0035, 0.1148, 0.2711, 0.3256, 0.2223, 0.0625, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.1114, 0.2711, 0.3292, 0.2235, 0.0614, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.1109, 0.2711, 0.3297, 0.2238, 0.0613, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.1109, 0.2710, 0.3297, 0.2239, 0.0614, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.1109, 0.2709, 0.3295, 0.2239, 0.0615, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.1110, 0.2708, 0.3293, 0.2239, 0.0616, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.1111, 0.2708, 0.3292, 0.2239, 0.0617, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.1112, 0.2707, 0.3291, 0.2239, 0.0618, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 总是 担心 会 被 塔利 塔利班 发现 <EOS> <PAD>\n",
      "Reference: we always wondered what they knew about us .\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0046, 0.0752, 0.1625, 0.2096, 0.2139, 0.1791, 0.1179, 0.0370, 0.0002,\n",
      "         0.0000],\n",
      "        [0.0018, 0.0566, 0.1587, 0.2255, 0.2342, 0.1871, 0.1101, 0.0260, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0543, 0.1573, 0.2269, 0.2366, 0.1884, 0.1097, 0.0252, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0539, 0.1571, 0.2271, 0.2369, 0.1886, 0.1097, 0.0251, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0539, 0.1571, 0.2270, 0.2369, 0.1886, 0.1097, 0.0251, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0540, 0.1571, 0.2269, 0.2367, 0.1886, 0.1098, 0.0252, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0541, 0.1571, 0.2268, 0.2366, 0.1885, 0.1099, 0.0253, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0541, 0.1571, 0.2267, 0.2365, 0.1885, 0.1100, 0.0253, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0542, 0.1571, 0.2267, 0.2365, 0.1885, 0.1100, 0.0254, 0.0001,\n",
      "         0.0000]])\n",
      "\n",
      "Epoch: 11.00, Train Loss: 4.98, Val Loss: 8.22, Train BLEU: 0.29, Val BLEU: 0.22, Minutes Elapsed: 2.80\n",
      "Sampling from training predictions...\n",
      "Source: 还有 这些 摇晃 着 旋转 转着 的 触角 <EOS> <PAD>\n",
      "Reference: it &apos;s got tentacles dangling , swirling around like\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0052, 0.0797, 0.1610, 0.2008, 0.1991, 0.1782, 0.1337, 0.0420, 0.0003,\n",
      "         0.0000],\n",
      "        [0.0018, 0.0599, 0.1582, 0.2170, 0.2167, 0.1881, 0.1291, 0.0291, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0572, 0.1569, 0.2185, 0.2189, 0.1897, 0.1291, 0.0281, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0567, 0.1566, 0.2188, 0.2192, 0.1899, 0.1291, 0.0279, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0567, 0.1566, 0.2188, 0.2192, 0.1899, 0.1292, 0.0280, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0568, 0.1566, 0.2187, 0.2191, 0.1899, 0.1293, 0.0280, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0569, 0.1566, 0.2186, 0.2190, 0.1898, 0.1293, 0.0281, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0569, 0.1566, 0.2185, 0.2189, 0.1898, 0.1293, 0.0282, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0570, 0.1567, 0.2184, 0.2189, 0.1898, 0.1294, 0.0282, 0.0001,\n",
      "         0.0000]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 如今 我 22 岁 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: today i am 22 . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0224, 0.3134, 0.4650, 0.1979, 0.0013, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0098, 0.2949, 0.5190, 0.1757, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0090, 0.2905, 0.5258, 0.1742, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0088, 0.2897, 0.5269, 0.1740, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0088, 0.2895, 0.5270, 0.1742, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0088, 0.2895, 0.5267, 0.1744, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0089, 0.2896, 0.5265, 0.1746, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0089, 0.2896, 0.5263, 0.1748, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0089, 0.2896, 0.5261, 0.1749, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12.00, Train Loss: 4.85, Val Loss: 8.33, Train BLEU: 0.29, Val BLEU: 0.22, Minutes Elapsed: 2.98\n",
      "Sampling from training predictions...\n",
      "Source: 我们 现在 <UNK> 右转 转过 一个 悬崖 <EOS> <PAD> <PAD>\n",
      "Reference: we &apos;re coming around a cliff here on the\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0093, 0.1119, 0.0754, 0.2286, 0.2661, 0.2258, 0.0824, 0.0005, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0034, 0.0880, 0.0596, 0.2462, 0.2999, 0.2395, 0.0631, 0.0002, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0030, 0.0844, 0.0573, 0.2472, 0.3045, 0.2421, 0.0614, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0029, 0.0837, 0.0569, 0.2474, 0.3052, 0.2426, 0.0611, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0029, 0.0836, 0.0568, 0.2474, 0.3053, 0.2427, 0.0612, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0029, 0.0837, 0.0569, 0.2473, 0.3051, 0.2427, 0.0613, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0029, 0.0837, 0.0570, 0.2473, 0.3050, 0.2426, 0.0614, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0029, 0.0838, 0.0570, 0.2472, 0.3048, 0.2426, 0.0615, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0029, 0.0839, 0.0571, 0.2472, 0.3047, 0.2426, 0.0616, 0.0001, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 比如 艾哈迈 艾哈迈德 哈迈德 迈德 这 不是 真名 我 也\n",
      "Reference: like ahmed . that &apos;s not his real name\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0044, 0.0593, 0.1145, 0.1432, 0.1567, 0.1633, 0.1511, 0.1265, 0.0738,\n",
      "         0.0073],\n",
      "        [0.0013, 0.0419, 0.1075, 0.1495, 0.1708, 0.1809, 0.1617, 0.1250, 0.0586,\n",
      "         0.0028],\n",
      "        [0.0011, 0.0394, 0.1058, 0.1497, 0.1725, 0.1834, 0.1634, 0.1251, 0.0570,\n",
      "         0.0025],\n",
      "        [0.0011, 0.0390, 0.1055, 0.1498, 0.1728, 0.1839, 0.1637, 0.1250, 0.0567,\n",
      "         0.0025],\n",
      "        [0.0011, 0.0390, 0.1055, 0.1498, 0.1728, 0.1839, 0.1637, 0.1251, 0.0567,\n",
      "         0.0025],\n",
      "        [0.0011, 0.0390, 0.1055, 0.1497, 0.1727, 0.1838, 0.1637, 0.1251, 0.0568,\n",
      "         0.0025],\n",
      "        [0.0011, 0.0391, 0.1055, 0.1497, 0.1726, 0.1837, 0.1636, 0.1251, 0.0569,\n",
      "         0.0025],\n",
      "        [0.0011, 0.0392, 0.1056, 0.1497, 0.1726, 0.1836, 0.1636, 0.1252, 0.0570,\n",
      "         0.0025],\n",
      "        [0.0011, 0.0392, 0.1056, 0.1497, 0.1725, 0.1836, 0.1635, 0.1252, 0.0571,\n",
      "         0.0025]])\n",
      "\n",
      "Epoch: 13.00, Train Loss: 4.75, Val Loss: 8.46, Train BLEU: 0.29, Val BLEU: 0.22, Minutes Elapsed: 3.15\n",
      "Sampling from training predictions...\n",
      "Source: 大多 大多数 多数 地震 和 火山 喷发 也 都 发生\n",
      "Reference: most of the earthquakes and volcanoes are in the\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0048, 0.0558, 0.1184, 0.1469, 0.1603, 0.1574, 0.1494, 0.1290, 0.0722,\n",
      "         0.0058],\n",
      "        [0.0013, 0.0373, 0.1124, 0.1555, 0.1772, 0.1724, 0.1592, 0.1276, 0.0552,\n",
      "         0.0019],\n",
      "        [0.0011, 0.0346, 0.1105, 0.1562, 0.1796, 0.1748, 0.1608, 0.1276, 0.0531,\n",
      "         0.0017],\n",
      "        [0.0011, 0.0341, 0.1102, 0.1563, 0.1800, 0.1752, 0.1612, 0.1276, 0.0527,\n",
      "         0.0016],\n",
      "        [0.0011, 0.0340, 0.1101, 0.1563, 0.1801, 0.1752, 0.1612, 0.1277, 0.0528,\n",
      "         0.0016],\n",
      "        [0.0011, 0.0341, 0.1101, 0.1562, 0.1800, 0.1752, 0.1612, 0.1277, 0.0529,\n",
      "         0.0016],\n",
      "        [0.0011, 0.0341, 0.1102, 0.1562, 0.1799, 0.1751, 0.1611, 0.1277, 0.0530,\n",
      "         0.0016],\n",
      "        [0.0011, 0.0342, 0.1102, 0.1561, 0.1798, 0.1750, 0.1611, 0.1278, 0.0530,\n",
      "         0.0016],\n",
      "        [0.0011, 0.0343, 0.1102, 0.1561, 0.1797, 0.1750, 0.1610, 0.1278, 0.0531,\n",
      "         0.0016]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 父亲 那 就是 他 他 是 他家 家族\n",
      "Reference: and my father -- that &apos;s him -- he\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0080, 0.0765, 0.1243, 0.1473, 0.1519, 0.1545, 0.1463, 0.1218, 0.0642,\n",
      "         0.0053],\n",
      "        [0.0026, 0.0592, 0.1220, 0.1578, 0.1656, 0.1695, 0.1560, 0.1186, 0.0470,\n",
      "         0.0017],\n",
      "        [0.0022, 0.0561, 0.1208, 0.1589, 0.1675, 0.1719, 0.1577, 0.1184, 0.0450,\n",
      "         0.0015],\n",
      "        [0.0021, 0.0556, 0.1206, 0.1591, 0.1679, 0.1724, 0.1580, 0.1183, 0.0446,\n",
      "         0.0014],\n",
      "        [0.0021, 0.0555, 0.1205, 0.1591, 0.1679, 0.1724, 0.1580, 0.1183, 0.0446,\n",
      "         0.0014],\n",
      "        [0.0021, 0.0555, 0.1205, 0.1591, 0.1679, 0.1724, 0.1580, 0.1184, 0.0447,\n",
      "         0.0014],\n",
      "        [0.0021, 0.0556, 0.1206, 0.1591, 0.1678, 0.1723, 0.1580, 0.1184, 0.0448,\n",
      "         0.0014],\n",
      "        [0.0021, 0.0557, 0.1206, 0.1590, 0.1677, 0.1722, 0.1579, 0.1184, 0.0448,\n",
      "         0.0014],\n",
      "        [0.0021, 0.0557, 0.1206, 0.1590, 0.1677, 0.1722, 0.1579, 0.1185, 0.0449,\n",
      "         0.0015]])\n",
      "\n",
      "Epoch: 14.00, Train Loss: 4.68, Val Loss: 8.59, Train BLEU: 0.29, Val BLEU: 0.22, Minutes Elapsed: 3.32\n",
      "Sampling from training predictions...\n",
      "Source: 他们 不会 会用 <UNK> 来 捕 这些 生物 <EOS> <PAD>\n",
      "Reference: they don &apos;t catch them in a net .\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0097, 0.1046, 0.1596, 0.0826, 0.1964, 0.2123, 0.1670, 0.0674, 0.0005,\n",
      "         0.0000],\n",
      "        [0.0030, 0.0831, 0.1605, 0.0689, 0.2206, 0.2438, 0.1719, 0.0480, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0025, 0.0791, 0.1597, 0.0666, 0.2238, 0.2493, 0.1731, 0.0457, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0025, 0.0782, 0.1594, 0.0660, 0.2245, 0.2506, 0.1734, 0.0454, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0780, 0.1593, 0.0660, 0.2246, 0.2507, 0.1735, 0.0454, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0781, 0.1592, 0.0660, 0.2245, 0.2506, 0.1735, 0.0454, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0025, 0.0781, 0.1592, 0.0661, 0.2244, 0.2505, 0.1735, 0.0455, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0025, 0.0782, 0.1592, 0.0662, 0.2243, 0.2504, 0.1735, 0.0456, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0025, 0.0782, 0.1593, 0.0663, 0.2243, 0.2503, 0.1735, 0.0456, 0.0001,\n",
      "         0.0000]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 那 就是 她 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: there she is . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0794, 0.5330, 0.3851, 0.0026, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0392, 0.5773, 0.3827, 0.0008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0355, 0.5804, 0.3834, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0349, 0.5805, 0.3839, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0348, 0.5803, 0.3842, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0348, 0.5800, 0.3844, 0.0008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0348, 0.5798, 0.3846, 0.0008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0349, 0.5797, 0.3847, 0.0008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0349, 0.5796, 0.3848, 0.0008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Epoch: 15.00, Train Loss: 4.63, Val Loss: 8.73, Train BLEU: 0.29, Val BLEU: 0.22, Minutes Elapsed: 3.49\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 这 也 是 其中 之一 是 我们 另 一个\n",
      "Reference: this is one of them , another one of\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0078, 0.0787, 0.1263, 0.1437, 0.1475, 0.1469, 0.1423, 0.1220, 0.0769,\n",
      "         0.0079],\n",
      "        [0.0021, 0.0602, 0.1266, 0.1557, 0.1622, 0.1606, 0.1523, 0.1192, 0.0586,\n",
      "         0.0024],\n",
      "        [0.0017, 0.0565, 0.1256, 0.1573, 0.1647, 0.1632, 0.1543, 0.1189, 0.0559,\n",
      "         0.0020],\n",
      "        [0.0016, 0.0556, 0.1253, 0.1577, 0.1652, 0.1637, 0.1548, 0.1188, 0.0553,\n",
      "         0.0019],\n",
      "        [0.0016, 0.0555, 0.1252, 0.1577, 0.1653, 0.1638, 0.1548, 0.1189, 0.0553,\n",
      "         0.0019],\n",
      "        [0.0016, 0.0555, 0.1252, 0.1576, 0.1652, 0.1638, 0.1548, 0.1189, 0.0554,\n",
      "         0.0019],\n",
      "        [0.0016, 0.0555, 0.1252, 0.1576, 0.1652, 0.1637, 0.1548, 0.1189, 0.0555,\n",
      "         0.0019],\n",
      "        [0.0016, 0.0556, 0.1252, 0.1575, 0.1651, 0.1637, 0.1548, 0.1190, 0.0556,\n",
      "         0.0019],\n",
      "        [0.0016, 0.0557, 0.1252, 0.1575, 0.1651, 0.1636, 0.1547, 0.1190, 0.0556,\n",
      "         0.0019]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 想 过 要 放弃 但 我 的 父亲 这时\n",
      "Reference: i would want to quit , but my father\n",
      "Model: <SOS> we the the the the the the the the\n",
      "Attention Weights: tensor([[0.0105, 0.0819, 0.1256, 0.1424, 0.1465, 0.1484, 0.1419, 0.1225, 0.0725,\n",
      "         0.0077],\n",
      "        [0.0032, 0.0642, 0.1261, 0.1540, 0.1607, 0.1635, 0.1520, 0.1203, 0.0538,\n",
      "         0.0023],\n",
      "        [0.0026, 0.0605, 0.1251, 0.1555, 0.1631, 0.1664, 0.1539, 0.1200, 0.0510,\n",
      "         0.0019],\n",
      "        [0.0024, 0.0596, 0.1248, 0.1558, 0.1637, 0.1670, 0.1544, 0.1200, 0.0504,\n",
      "         0.0018],\n",
      "        [0.0024, 0.0595, 0.1248, 0.1558, 0.1637, 0.1671, 0.1544, 0.1200, 0.0504,\n",
      "         0.0018],\n",
      "        [0.0024, 0.0595, 0.1248, 0.1558, 0.1637, 0.1670, 0.1544, 0.1201, 0.0505,\n",
      "         0.0018],\n",
      "        [0.0024, 0.0596, 0.1248, 0.1557, 0.1636, 0.1670, 0.1544, 0.1201, 0.0506,\n",
      "         0.0018],\n",
      "        [0.0024, 0.0596, 0.1248, 0.1557, 0.1635, 0.1669, 0.1544, 0.1201, 0.0507,\n",
      "         0.0019],\n",
      "        [0.0025, 0.0597, 0.1248, 0.1557, 0.1635, 0.1669, 0.1543, 0.1201, 0.0507,\n",
      "         0.0019]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16.00, Train Loss: 4.60, Val Loss: 8.87, Train BLEU: 0.34, Val BLEU: 0.21, Minutes Elapsed: 3.63\n",
      "Sampling from training predictions...\n",
      "Source: 还有 前面 的 这个 是 推进 引擎 它 一会 一会儿\n",
      "Reference: and it &apos;s got these jet thrusters up in\n",
      "Model: <SOS> and the the the the the the the the\n",
      "Attention Weights: tensor([[0.0089, 0.0796, 0.1274, 0.1435, 0.1509, 0.1490, 0.1422, 0.1241, 0.0683,\n",
      "         0.0062],\n",
      "        [0.0023, 0.0602, 0.1279, 0.1552, 0.1682, 0.1641, 0.1515, 0.1214, 0.0477,\n",
      "         0.0016],\n",
      "        [0.0018, 0.0560, 0.1268, 0.1568, 0.1713, 0.1671, 0.1534, 0.1209, 0.0446,\n",
      "         0.0013],\n",
      "        [0.0017, 0.0550, 0.1264, 0.1571, 0.1720, 0.1678, 0.1539, 0.1209, 0.0440,\n",
      "         0.0012],\n",
      "        [0.0017, 0.0548, 0.1263, 0.1571, 0.1721, 0.1679, 0.1540, 0.1209, 0.0439,\n",
      "         0.0012],\n",
      "        [0.0017, 0.0548, 0.1263, 0.1571, 0.1721, 0.1679, 0.1540, 0.1210, 0.0440,\n",
      "         0.0012],\n",
      "        [0.0017, 0.0549, 0.1263, 0.1570, 0.1720, 0.1678, 0.1539, 0.1210, 0.0441,\n",
      "         0.0012],\n",
      "        [0.0017, 0.0549, 0.1264, 0.1570, 0.1719, 0.1677, 0.1539, 0.1210, 0.0442,\n",
      "         0.0012],\n",
      "        [0.0017, 0.0550, 0.1264, 0.1570, 0.1719, 0.1677, 0.1539, 0.1210, 0.0442,\n",
      "         0.0012]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 父亲 在 用 他 的 灰色 小 收音\n",
      "Reference: my father was listening to bbc news on his\n",
      "Model: <SOS> and the the the the the the the the\n",
      "Attention Weights: tensor([[0.0117, 0.0829, 0.1226, 0.1400, 0.1460, 0.1461, 0.1400, 0.1214, 0.0799,\n",
      "         0.0094],\n",
      "        [0.0034, 0.0652, 0.1223, 0.1517, 0.1621, 0.1616, 0.1504, 0.1192, 0.0614,\n",
      "         0.0027],\n",
      "        [0.0027, 0.0612, 0.1212, 0.1533, 0.1650, 0.1646, 0.1525, 0.1188, 0.0584,\n",
      "         0.0022],\n",
      "        [0.0026, 0.0602, 0.1208, 0.1537, 0.1657, 0.1654, 0.1530, 0.1187, 0.0578,\n",
      "         0.0022],\n",
      "        [0.0026, 0.0600, 0.1207, 0.1537, 0.1658, 0.1655, 0.1531, 0.1187, 0.0577,\n",
      "         0.0022],\n",
      "        [0.0026, 0.0600, 0.1207, 0.1537, 0.1657, 0.1655, 0.1531, 0.1188, 0.0578,\n",
      "         0.0022],\n",
      "        [0.0026, 0.0601, 0.1207, 0.1536, 0.1657, 0.1654, 0.1531, 0.1188, 0.0579,\n",
      "         0.0022],\n",
      "        [0.0026, 0.0601, 0.1208, 0.1536, 0.1656, 0.1653, 0.1530, 0.1188, 0.0580,\n",
      "         0.0022],\n",
      "        [0.0026, 0.0602, 0.1208, 0.1535, 0.1655, 0.1653, 0.1530, 0.1189, 0.0580,\n",
      "         0.0022]])\n",
      "\n",
      "Epoch: 17.00, Train Loss: 4.57, Val Loss: 9.00, Train BLEU: 0.34, Val BLEU: 0.21, Minutes Elapsed: 3.77\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 这位 是 比尔 <UNK> 我 是 大卫 <UNK>\n",
      "Reference: this is bill lange . i &apos;m dave gallo\n",
      "Model: <SOS> it the the the the the the the the\n",
      "Attention Weights: tensor([[0.0090, 0.0225, 0.1243, 0.1775, 0.1717, 0.0811, 0.1730, 0.1623, 0.0778,\n",
      "         0.0006],\n",
      "        [0.0023, 0.0096, 0.1147, 0.1994, 0.1912, 0.0642, 0.1910, 0.1715, 0.0561,\n",
      "         0.0001],\n",
      "        [0.0018, 0.0081, 0.1110, 0.2025, 0.1948, 0.0612, 0.1944, 0.1734, 0.0527,\n",
      "         0.0001],\n",
      "        [0.0017, 0.0078, 0.1100, 0.2031, 0.1956, 0.0606, 0.1951, 0.1739, 0.0520,\n",
      "         0.0001],\n",
      "        [0.0017, 0.0078, 0.1098, 0.2032, 0.1957, 0.0605, 0.1953, 0.1740, 0.0520,\n",
      "         0.0001],\n",
      "        [0.0017, 0.0078, 0.1099, 0.2032, 0.1956, 0.0606, 0.1952, 0.1740, 0.0520,\n",
      "         0.0001],\n",
      "        [0.0017, 0.0078, 0.1099, 0.2031, 0.1956, 0.0607, 0.1951, 0.1740, 0.0521,\n",
      "         0.0001],\n",
      "        [0.0017, 0.0078, 0.1100, 0.2030, 0.1955, 0.0607, 0.1951, 0.1739, 0.0522,\n",
      "         0.0001],\n",
      "        [0.0017, 0.0078, 0.1100, 0.2030, 0.1955, 0.0608, 0.1950, 0.1739, 0.0522,\n",
      "         0.0001]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 很 幸运 幸运地 成长 在 一个 珍视 教育 也\n",
      "Reference: i was very lucky to grow up in a\n",
      "Model: <SOS> and the the the the the the the the\n",
      "Attention Weights: tensor([[0.0130, 0.0849, 0.1220, 0.1342, 0.1425, 0.1446, 0.1380, 0.1226, 0.0840,\n",
      "         0.0143],\n",
      "        [0.0038, 0.0677, 0.1230, 0.1438, 0.1582, 0.1616, 0.1489, 0.1221, 0.0663,\n",
      "         0.0047],\n",
      "        [0.0030, 0.0635, 0.1220, 0.1451, 0.1612, 0.1651, 0.1512, 0.1220, 0.0632,\n",
      "         0.0038],\n",
      "        [0.0028, 0.0624, 0.1216, 0.1453, 0.1619, 0.1660, 0.1518, 0.1220, 0.0624,\n",
      "         0.0037],\n",
      "        [0.0028, 0.0622, 0.1215, 0.1454, 0.1620, 0.1661, 0.1519, 0.1220, 0.0624,\n",
      "         0.0036],\n",
      "        [0.0028, 0.0622, 0.1215, 0.1454, 0.1620, 0.1661, 0.1519, 0.1220, 0.0624,\n",
      "         0.0037],\n",
      "        [0.0028, 0.0623, 0.1216, 0.1453, 0.1619, 0.1660, 0.1518, 0.1220, 0.0625,\n",
      "         0.0037],\n",
      "        [0.0028, 0.0624, 0.1216, 0.1453, 0.1619, 0.1659, 0.1518, 0.1221, 0.0626,\n",
      "         0.0037],\n",
      "        [0.0028, 0.0624, 0.1216, 0.1453, 0.1618, 0.1659, 0.1517, 0.1221, 0.0627,\n",
      "         0.0037]])\n",
      "\n",
      "Epoch: 18.00, Train Loss: 4.54, Val Loss: 9.12, Train BLEU: 0.34, Val BLEU: 0.21, Minutes Elapsed: 3.91\n",
      "Sampling from training predictions...\n",
      "Source: 大部 大部分 部分 的 动物 也 都 生活 在 海洋\n",
      "Reference: most of the animals are in the oceans .\n",
      "Model: <SOS> and the the the the the the the the\n",
      "Attention Weights: tensor([[0.0106, 0.0779, 0.1204, 0.1379, 0.1429, 0.1449, 0.1400, 0.1250, 0.0875,\n",
      "         0.0128],\n",
      "        [0.0026, 0.0576, 0.1194, 0.1499, 0.1586, 0.1615, 0.1520, 0.1251, 0.0695,\n",
      "         0.0038],\n",
      "        [0.0020, 0.0529, 0.1178, 0.1516, 0.1617, 0.1652, 0.1547, 0.1251, 0.0661,\n",
      "         0.0030],\n",
      "        [0.0019, 0.0518, 0.1173, 0.1520, 0.1624, 0.1661, 0.1553, 0.1250, 0.0653,\n",
      "         0.0029],\n",
      "        [0.0018, 0.0516, 0.1172, 0.1521, 0.1625, 0.1662, 0.1555, 0.1251, 0.0652,\n",
      "         0.0028],\n",
      "        [0.0018, 0.0516, 0.1172, 0.1520, 0.1625, 0.1662, 0.1554, 0.1251, 0.0652,\n",
      "         0.0029],\n",
      "        [0.0019, 0.0517, 0.1172, 0.1520, 0.1624, 0.1661, 0.1554, 0.1251, 0.0654,\n",
      "         0.0029],\n",
      "        [0.0019, 0.0517, 0.1173, 0.1519, 0.1623, 0.1660, 0.1554, 0.1251, 0.0654,\n",
      "         0.0029],\n",
      "        [0.0019, 0.0518, 0.1173, 0.1519, 0.1623, 0.1660, 0.1553, 0.1252, 0.0655,\n",
      "         0.0029]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 塔利 塔利班 走 了 父亲 大声 叫 着 <EOS> <PAD>\n",
      "Reference: &quot; the taliban are gone ! &quot; my father\n",
      "Model: <SOS> it the the the the the the the the\n",
      "Attention Weights: tensor([[0.0129, 0.1004, 0.1554, 0.1734, 0.1738, 0.1652, 0.1421, 0.0761, 0.0006,\n",
      "         0.0000],\n",
      "        [0.0034, 0.0780, 0.1604, 0.1924, 0.1934, 0.1784, 0.1406, 0.0534, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0026, 0.0723, 0.1598, 0.1959, 0.1977, 0.1815, 0.1405, 0.0496, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0708, 0.1595, 0.1967, 0.1988, 0.1823, 0.1405, 0.0488, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0706, 0.1594, 0.1969, 0.1989, 0.1825, 0.1405, 0.0487, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0706, 0.1594, 0.1968, 0.1989, 0.1824, 0.1405, 0.0488, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0707, 0.1594, 0.1968, 0.1988, 0.1824, 0.1406, 0.0489, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0707, 0.1595, 0.1967, 0.1988, 0.1824, 0.1406, 0.0489, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0024, 0.0708, 0.1595, 0.1967, 0.1987, 0.1823, 0.1406, 0.0490, 0.0001,\n",
      "         0.0000]])\n",
      "\n",
      "Epoch: 19.00, Train Loss: 4.52, Val Loss: 9.24, Train BLEU: 0.34, Val BLEU: 0.21, Minutes Elapsed: 4.04\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 这 也 是 其中 之一 是 我们 另 一个\n",
      "Reference: this is one of them , another one of\n",
      "Model: <SOS> and the the the the the the the the\n",
      "Attention Weights: tensor([[0.0122, 0.0842, 0.1219, 0.1358, 0.1401, 0.1406, 0.1376, 0.1240, 0.0891,\n",
      "         0.0145],\n",
      "        [0.0031, 0.0656, 0.1232, 0.1480, 0.1554, 0.1554, 0.1492, 0.1243, 0.0715,\n",
      "         0.0042],\n",
      "        [0.0023, 0.0607, 0.1221, 0.1499, 0.1586, 0.1588, 0.1519, 0.1244, 0.0679,\n",
      "         0.0034],\n",
      "        [0.0022, 0.0596, 0.1218, 0.1504, 0.1593, 0.1596, 0.1526, 0.1244, 0.0671,\n",
      "         0.0032],\n",
      "        [0.0021, 0.0593, 0.1217, 0.1505, 0.1595, 0.1597, 0.1527, 0.1244, 0.0669,\n",
      "         0.0032],\n",
      "        [0.0021, 0.0594, 0.1217, 0.1504, 0.1594, 0.1597, 0.1526, 0.1244, 0.0670,\n",
      "         0.0032],\n",
      "        [0.0021, 0.0594, 0.1217, 0.1504, 0.1594, 0.1596, 0.1526, 0.1244, 0.0671,\n",
      "         0.0032],\n",
      "        [0.0021, 0.0595, 0.1217, 0.1504, 0.1593, 0.1596, 0.1525, 0.1244, 0.0672,\n",
      "         0.0032],\n",
      "        [0.0022, 0.0596, 0.1218, 0.1503, 0.1592, 0.1595, 0.1525, 0.1245, 0.0673,\n",
      "         0.0032]])\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 如今 我 22 岁 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: today i am 22 . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it the the the the the the the the\n",
      "Attention Weights: tensor([[0.0460, 0.3127, 0.3977, 0.2420, 0.0017, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0154, 0.3068, 0.4638, 0.2137, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0124, 0.3014, 0.4775, 0.2085, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0119, 0.2998, 0.4805, 0.2076, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0118, 0.2995, 0.4809, 0.2075, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0118, 0.2995, 0.4808, 0.2076, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0118, 0.2996, 0.4806, 0.2078, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0118, 0.2996, 0.4805, 0.2079, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0118, 0.2996, 0.4803, 0.2080, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n",
      "Experiment completed in 4 minutes with 8.11 best validation loss and 0.22 best validation BLEU.\n"
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=100, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=False, print_attn=True, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_single_learning_curve(load_experiment_log(experiment_name=MODEL_NAME)[0]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_val_bleu</th>\n",
       "      <th>runtime</th>\n",
       "      <th>dt_created</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>enc_hidden_dim</th>\n",
       "      <th>dec_hidden_dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zh-rnn-additive-attn</td>\n",
       "      <td>8.111922</td>\n",
       "      <td>0.185347</td>\n",
       "      <td>4.042134</td>\n",
       "      <td>2018-12-09 02:35:54</td>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zh-rnn-additive-attn</td>\n",
       "      <td>7.979359</td>\n",
       "      <td>0.185347</td>\n",
       "      <td>3.184948</td>\n",
       "      <td>2018-12-09 01:48:42</td>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name  best_val_loss  best_val_bleu   runtime  \\\n",
       "1  zh-rnn-additive-attn       8.111922       0.185347  4.042134   \n",
       "0  zh-rnn-additive-attn       7.979359       0.185347  3.184948   \n",
       "\n",
       "            dt_created  num_layers  enc_hidden_dim  dec_hidden_dim  \n",
       "1  2018-12-09 02:35:54           2             512            1024  \n",
       "0  2018-12-09 01:48:42           2             512            1024  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(load_experiment_log(experiment_name=MODEL_NAME))[[\n",
    "    'model_name', 'best_val_loss', 'best_val_bleu', 'runtime', 'dt_created', \n",
    "    'num_layers', 'enc_hidden_dim', 'dec_hidden_dim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model and test \n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "# # without attention \n",
    "# decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "#                      targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device)\n",
    "\n",
    "# with additive attention \n",
    "decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                         num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "                         targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id'])\n",
    "\n",
    "# # with multiplicative attention \n",
    "# decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "#                          num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "#                          targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, attention_type=ATTENTION_TYPE,\n",
    "#                          pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model_checkpoints/{}.pth.tar'.format(MODEL_NAME), map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
