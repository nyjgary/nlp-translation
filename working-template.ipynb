{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from data_processing import generate_vocab, process_data, create_dataloaders \n",
    "from model import get_pretrained_emb, EncoderDecoder, EncoderRNN, DecoderRNN, Attention, DecoderAttnRNN\n",
    "from train_eval import train_and_eval, inspect_model, count_parameters, summarize_results, \\\n",
    "    plot_single_learning_curve, load_experiment_log\n",
    "import importlib\n",
    "import pickle as pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and hyperparameters \n",
    "MODEL_NAME = 'test_model'\n",
    "SRC_LANG = 'vi'\n",
    "TARG_LANG = 'en'\n",
    "SRC_VOCAB_SIZE = 30000\n",
    "TARG_VOCAB_SIZE = 30000\n",
    "BATCH_SIZE = 32\n",
    "SRC_MAX_SENTENCE_LEN = 10 \n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "NUM_EPOCHS = 5\n",
    "LR = 0.0005\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 300 \n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = True \n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "CLIP_GRAD_MAX_NORM = 10\n",
    "\n",
    "# to actually implement\n",
    "ENC_DROPOUT = 0 \n",
    "DEC_DROPOUT = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'model_name': MODEL_NAME, 'num_epochs': NUM_EPOCHS, 'learning_rate': LR, 'enc_hidden_dim': ENC_HIDDEN_DIM,\n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'num_layers': NUM_LAYERS, 'optimizer': OPTIMIZER, 'enc_dropout': ENC_DROPOUT, \n",
    "          'dec_dropout': DEC_DROPOUT, 'batch_size': BATCH_SIZE, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "          'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, \n",
    "          'teacher_forcing_ratio': TEACHER_FORCING_RATIO, 'clip_grad_max_norm': CLIP_GRAD_MAX_NORM,\n",
    "          'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, vocab)\n",
    "limited_data = process_data(SRC_LANG, TARG_LANG, vocab, sample_limit=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "full_loaders = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "fast_loaders = create_dataloaders(limited_data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, src_max_sentence_len=SRC_MAX_SENTENCE_LEN,\n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# decoder = DecoderAttnRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "#                          targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "#                          targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "#                          pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.22, Train BLEU: 0.00, Val BLEU: 0.24\n",
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: rachel pike : the science behind a climate headline in 4 minutes , atmospheric chemist rachel pike provides i &apos;d like to talk to you today about headlines that look like this when they have to they are both two branches of the same field\n",
      "MODEL TRANSLATION: websites is is is is is is is is sleeping the the the the the the the the laces shoah it it it it it it it eukaryotic the the the the the the the the compensated the the the the the the the the\n",
      "Inspecting model on validation data...\n",
      "REFERENCE TRANSLATION: when i was little , i thought my country and i was very proud . in school , we spent a lot of time although i often wondered about the outside world , when i was seven years old , i saw\n",
      "MODEL TRANSLATION: repetition the the the the the the the the websites is is is is is is is is retail the the the the the the the the uranus the the the the the the the the laces the the the the the the the the\n",
      "Epoch: 1.00, Train Loss: 0.00, Val Loss: 10.09, Train BLEU: 0.00, Val BLEU: 0.39\n",
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: rachel pike : the science behind a climate headline in 4 minutes , atmospheric chemist rachel pike provides i &apos;d like to talk to you today about headlines that look like this when they have to they are both two branches of the same field\n",
      "MODEL TRANSLATION: when is is is euphore euphore euphore euphore euphore the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Inspecting model on validation data...\n",
      "REFERENCE TRANSLATION: when i was little , i thought my country and i was very proud . in school , we spent a lot of time although i often wondered about the outside world , when i was seven years old , i saw\n",
      "MODEL TRANSLATION: the the the the the the the the the over is is is is is is is is the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch: 2.00, Train Loss: 0.00, Val Loss: 9.87, Train BLEU: 0.00, Val BLEU: 0.39\n",
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: rachel pike : the science behind a climate headline in 4 minutes , atmospheric chemist rachel pike provides i &apos;d like to talk to you today about headlines that look like this when they have to they are both two branches of the same field\n",
      "MODEL TRANSLATION: is is is is is is is is is the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Inspecting model on validation data...\n",
      "REFERENCE TRANSLATION: when i was little , i thought my country and i was very proud . in school , we spent a lot of time although i often wondered about the outside world , when i was seven years old , i saw\n",
      "MODEL TRANSLATION: the the the the the the the the the when is is is is is is is is the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch: 3.00, Train Loss: 0.00, Val Loss: 9.56, Train BLEU: 0.00, Val BLEU: 0.39\n",
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: rachel pike : the science behind a climate headline in 4 minutes , atmospheric chemist rachel pike provides i &apos;d like to talk to you today about headlines that look like this when they have to they are both two branches of the same field\n",
      "MODEL TRANSLATION: is is is is is is is is is the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Inspecting model on validation data...\n",
      "REFERENCE TRANSLATION: when i was little , i thought my country and i was very proud . in school , we spent a lot of time although i often wondered about the outside world , when i was seven years old , i saw\n",
      "MODEL TRANSLATION: the the the the the the the the the of of of of of is is is is the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch: 4.00, Train Loss: 0.00, Val Loss: 9.23, Train BLEU: 0.00, Val BLEU: 0.22\n",
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: rachel pike : the science behind a climate headline in 4 minutes , atmospheric chemist rachel pike provides i &apos;d like to talk to you today about headlines that look like this when they have to they are both two branches of the same field\n",
      "MODEL TRANSLATION: the of of of of of of of of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Inspecting model on validation data...\n",
      "REFERENCE TRANSLATION: when i was little , i thought my country and i was very proud . in school , we spent a lot of time although i often wondered about the outside world , when i was seven years old , i saw\n",
      "MODEL TRANSLATION: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Experiment completed in 0 minutes with 9.23 best validation loss and 0.39 best validation BLEU.\n"
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, full_loaders=full_loaders, fast_loaders=fast_loaders, params=params, vocab=vocab, \n",
    "    print_intermediate=True, save_checkpoint=True, lazy_eval=True, inspect=True, save_to_log=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_created</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>results</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model_type</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>enc_hidden_dim</th>\n",
       "      <th>dec_hidden_dim</th>\n",
       "      <th>...</th>\n",
       "      <th>targ_lang</th>\n",
       "      <th>src_vocab_size</th>\n",
       "      <th>targ_vocab_size</th>\n",
       "      <th>src_max_sentence_len</th>\n",
       "      <th>targ_max_sentence_len</th>\n",
       "      <th>model_name</th>\n",
       "      <th>teacher_forcing_ratio</th>\n",
       "      <th>lazy_train</th>\n",
       "      <th>clip_grad_max_norm</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-26 23:17:44</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epoc...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 8.783560848236084,...</td>\n",
       "      <td>53.518373</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.433290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-27 00:43:43</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epoc...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 8.989323210716247,...</td>\n",
       "      <td>52.872075</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.436468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-27 02:28:19</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'without_attention', 'num_epoch...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 8.887566566467285,...</td>\n",
       "      <td>1.302775</td>\n",
       "      <td>without_attention</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.335167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-27 02:30:44</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'without_attention', 'num_epoch...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 8.855473518371582,...</td>\n",
       "      <td>1.179587</td>\n",
       "      <td>without_attention</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.390460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-27 05:10:18</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epoc...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 8.919134140014648,...</td>\n",
       "      <td>155.873592</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.484231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-11-28 00:43:37</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epoc...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 10.040691375732422...</td>\n",
       "      <td>31.569452</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.985807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-11-28 13:52:27</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epoc...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 10.062241554260254...</td>\n",
       "      <td>673.474022</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.088548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-11-28 17:55:24</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epoc...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 10.21630573272705,...</td>\n",
       "      <td>6.029234</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.507013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-12-01 02:29:11</td>\n",
       "      <td>test_model</td>\n",
       "      <td>{'model_name': 'test_model', 'num_epochs': 5, ...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 10.221494674682617...</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>test_model</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.228798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-12-01 02:22:55</td>\n",
       "      <td>test_model</td>\n",
       "      <td>{'model_name': 'test_model', 'num_epochs': 5, ...</td>\n",
       "      <td>[{'epoch': 0.0, 'val_loss': 10.231721878051758...</td>\n",
       "      <td>0.381173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>test_model</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.330558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt_created experiment_name  \\\n",
       "0  2018-11-26 23:17:44        test_run   \n",
       "1  2018-11-27 00:43:43        test_run   \n",
       "2  2018-11-27 02:28:19        test_run   \n",
       "3  2018-11-27 02:30:44        test_run   \n",
       "4  2018-11-27 05:10:18        test_run   \n",
       "5  2018-11-28 00:43:37        test_run   \n",
       "6  2018-11-28 13:52:27        test_run   \n",
       "7  2018-11-28 17:55:24        test_run   \n",
       "9  2018-12-01 02:29:11      test_model   \n",
       "8  2018-12-01 02:22:55      test_model   \n",
       "\n",
       "                                         hyperparams  \\\n",
       "0  {'model_type': 'attention_bahdanau', 'num_epoc...   \n",
       "1  {'model_type': 'attention_bahdanau', 'num_epoc...   \n",
       "2  {'model_type': 'without_attention', 'num_epoch...   \n",
       "3  {'model_type': 'without_attention', 'num_epoch...   \n",
       "4  {'model_type': 'attention_bahdanau', 'num_epoc...   \n",
       "5  {'model_type': 'attention_bahdanau', 'num_epoc...   \n",
       "6  {'model_type': 'attention_bahdanau', 'num_epoc...   \n",
       "7  {'model_type': 'attention_bahdanau', 'num_epoc...   \n",
       "9  {'model_name': 'test_model', 'num_epochs': 5, ...   \n",
       "8  {'model_name': 'test_model', 'num_epochs': 5, ...   \n",
       "\n",
       "                                             results     runtime  \\\n",
       "0  [{'epoch': 0.0, 'val_loss': 8.783560848236084,...   53.518373   \n",
       "1  [{'epoch': 0.0, 'val_loss': 8.989323210716247,...   52.872075   \n",
       "2  [{'epoch': 0.0, 'val_loss': 8.887566566467285,...    1.302775   \n",
       "3  [{'epoch': 0.0, 'val_loss': 8.855473518371582,...    1.179587   \n",
       "4  [{'epoch': 0.0, 'val_loss': 8.919134140014648,...  155.873592   \n",
       "5  [{'epoch': 0.0, 'val_loss': 10.040691375732422...   31.569452   \n",
       "6  [{'epoch': 0.0, 'val_loss': 10.062241554260254...  673.474022   \n",
       "7  [{'epoch': 0.0, 'val_loss': 10.21630573272705,...    6.029234   \n",
       "9  [{'epoch': 0.0, 'val_loss': 10.221494674682617...    0.312100   \n",
       "8  [{'epoch': 0.0, 'val_loss': 10.231721878051758...    0.381173   \n",
       "\n",
       "           model_type  num_epochs  learning_rate  enc_hidden_dim  \\\n",
       "0  attention_bahdanau          10         0.0005             300   \n",
       "1  attention_bahdanau          10         0.0005             300   \n",
       "2   without_attention          10         0.0005             300   \n",
       "3   without_attention          10         0.0005             300   \n",
       "4  attention_bahdanau        1000         0.0005             300   \n",
       "5  attention_bahdanau         100         0.0005             300   \n",
       "6  attention_bahdanau        2000         0.0005             300   \n",
       "7  attention_bahdanau         100         0.0005             300   \n",
       "9                 NaN           5         0.0005             300   \n",
       "8                 NaN           5         0.0005             300   \n",
       "\n",
       "   dec_hidden_dim    ...     targ_lang src_vocab_size  targ_vocab_size  \\\n",
       "0             600    ...            en          10000            10000   \n",
       "1             600    ...            en          10000            10000   \n",
       "2             600    ...            en          10000            10000   \n",
       "3             600    ...            en          10000            10000   \n",
       "4             600    ...            en          10000            10000   \n",
       "5             600    ...            en          30000            30000   \n",
       "6             600    ...            en          30000            30000   \n",
       "7             600    ...            en          30000            30000   \n",
       "9             600    ...            en          30000            30000   \n",
       "8             600    ...            en          30000            30000   \n",
       "\n",
       "   src_max_sentence_len  targ_max_sentence_len  model_name  \\\n",
       "0                    40                     40         NaN   \n",
       "1                    40                     40         NaN   \n",
       "2                    40                     40         NaN   \n",
       "3                    40                     40         NaN   \n",
       "4                    40                     40         NaN   \n",
       "5                    40                     40         NaN   \n",
       "6                    40                     40         NaN   \n",
       "7                    10                     10         NaN   \n",
       "9                    10                     10  test_model   \n",
       "8                    10                     10  test_model   \n",
       "\n",
       "  teacher_forcing_ratio  lazy_train  clip_grad_max_norm  val_loss  \n",
       "0                   NaN         NaN                 NaN  5.433290  \n",
       "1                   NaN         NaN                 NaN  5.436468  \n",
       "2                   NaN         NaN                 NaN  7.335167  \n",
       "3                   NaN         NaN                 NaN  7.390460  \n",
       "4                   NaN         NaN                 NaN  7.484231  \n",
       "5                   NaN         NaN                 NaN  7.985807  \n",
       "6                   NaN         NaN                 NaN  8.088548  \n",
       "7                   NaN         NaN                 NaN  8.507013  \n",
       "9                   0.5        True                10.0  9.228798  \n",
       "8                   0.5        True                 NaN  9.330558  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(load_experiment_log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
