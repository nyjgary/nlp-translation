{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from data_processing import generate_vocab, process_data, create_dataloaders, text2tokens\n",
    "from model import get_pretrained_emb, EncoderDecoder, EncoderRNN, DecoderRNN, EncoderDecoderAttn, DecoderAttnRNN\n",
    "from train_eval import count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "from train_eval import train_and_eval\n",
    "import importlib\n",
    "import pickle as pkl \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model identification\n",
    "MODEL_NAME = 'zh-seq2seq-rnn-vanilla'\n",
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "# data processing params  \n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 #30000\n",
    "TARG_VOCAB_SIZE = 30000 #30000\n",
    "\n",
    "# model architecture params \n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 #2 \n",
    "ENC_HIDDEN_DIM = 256 #512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM #2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 # to actually implement\n",
    "DEC_DROPOUT = 0.2 # to actually implement\n",
    "USE_ATTN = True\n",
    "\n",
    "# training params  \n",
    "BATCH_SIZE = 32 #32\n",
    "NUM_EPOCHS = 200\n",
    "LR = 0.0003 # 0.0005\n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, 'rnn_cell_type': RNN_CELL_TYPE, \n",
    "          'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, \n",
    "          'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "          'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, 'dec_hidden_dim': DEC_HIDDEN_DIM,\n",
    "          'teacher_forcing_ratio': TEACHER_FORCING_RATIO, 'clip_grad_max_norm': CLIP_GRAD_MAX_NORM,\n",
    "          'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, 'use_attn': USE_ATTN, \n",
    "          'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, 'learning_rate': LR, 'optimizer': OPTIMIZER, \n",
    "          'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "# without attention \n",
    "# decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "#                      targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id'])\n",
    "\n",
    "# with additive attention \n",
    "# decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "#                          num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "#                          targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, attention_type='additive',\n",
    "#                          pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']) \n",
    "\n",
    "# with multiplicative attention \n",
    "decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                         num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "                         targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, attention_type='multiplicative',\n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished gradient updates at 1.7702600955963135\n",
      "Evaluated on validation set at 2.1121270656585693 seconds\n",
      "Evaluated on training set at 2.4443230628967285 seconds\n",
      "Appended results at 2.444643259048462 seconds\n",
      "Epoch: 0.00, Train Loss: 10.18, Val Loss: 10.26, Train BLEU: 2.23, Val BLEU: 0.17, Minutes Elapsed: 0.04\n",
      "Sampling from training predictions...\n",
      "Source: 我们 将 用 一些 影片 来讲 讲述 一些 深海 海里\n",
      "Reference: and we &apos;re going to tell you some stories\n",
      "Model: <SOS> &apos;s the the the the the the the the\n",
      "Attention Weights: tensor([[2.0814e-04, 4.1357e-02, 2.9225e-01, 1.8755e-01, 2.5040e-01, 1.6599e-01,\n",
      "         5.5885e-02, 5.9584e-03, 3.8446e-04, 4.9128e-06],\n",
      "        [5.4272e-03, 7.4556e-02, 2.1089e-01, 1.8136e-01, 2.2277e-01, 1.7409e-01,\n",
      "         9.2658e-02, 2.9833e-02, 7.5984e-03, 8.1563e-04],\n",
      "        [2.2758e-02, 9.1142e-02, 1.6631e-01, 1.6324e-01, 1.9013e-01, 1.6013e-01,\n",
      "         1.0955e-01, 6.0266e-02, 2.8474e-02, 8.0026e-03],\n",
      "        [5.1483e-02, 9.9991e-02, 1.3745e-01, 1.4180e-01, 1.5429e-01, 1.3841e-01,\n",
      "         1.1408e-01, 8.2572e-02, 5.3666e-02, 2.6262e-02],\n",
      "        [7.9697e-02, 1.0250e-01, 1.1870e-01, 1.2392e-01, 1.2772e-01, 1.1943e-01,\n",
      "         1.1082e-01, 9.4177e-02, 7.3970e-02, 4.9069e-02],\n",
      "        [8.7525e-02, 9.8489e-02, 1.0844e-01, 1.1542e-01, 1.1912e-01, 1.1155e-01,\n",
      "         1.0799e-01, 9.7740e-02, 8.5632e-02, 6.8087e-02],\n",
      "        [9.2797e-02, 9.8177e-02, 1.0582e-01, 1.1083e-01, 1.1093e-01, 1.0530e-01,\n",
      "         1.0517e-01, 9.9285e-02, 9.2776e-02, 7.8908e-02],\n",
      "        [9.9233e-02, 9.7955e-02, 1.0246e-01, 1.0671e-01, 1.0460e-01, 1.0095e-01,\n",
      "         1.0342e-01, 1.0059e-01, 9.7651e-02, 8.6428e-02],\n",
      "        [1.0554e-01, 9.8957e-02, 1.0109e-01, 1.0404e-01, 1.0040e-01, 9.8439e-02,\n",
      "         1.0165e-01, 1.0083e-01, 9.9302e-02, 8.9757e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 一个 真正 的 学校 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: a real school . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> &apos;s &apos;s the the the the the the the\n",
      "Attention Weights: tensor([[5.0166e-04, 1.1458e-02, 6.9835e-02, 4.3211e-01, 3.5211e-01, 8.7728e-02,\n",
      "         3.7293e-02, 8.3841e-03, 5.7101e-04, 8.5864e-06],\n",
      "        [9.9850e-03, 4.4565e-02, 1.0706e-01, 2.8515e-01, 2.6432e-01, 1.3364e-01,\n",
      "         9.3253e-02, 4.6785e-02, 1.3651e-02, 1.5911e-03],\n",
      "        [3.1537e-02, 6.7015e-02, 1.0877e-01, 1.9859e-01, 1.9809e-01, 1.3997e-01,\n",
      "         1.2066e-01, 8.1129e-02, 4.3664e-02, 1.0566e-02],\n",
      "        [6.2967e-02, 8.7258e-02, 1.1189e-01, 1.5056e-01, 1.5111e-01, 1.2873e-01,\n",
      "         1.1752e-01, 9.5657e-02, 6.8435e-02, 2.5872e-02],\n",
      "        [9.1382e-02, 9.9930e-02, 1.1310e-01, 1.2324e-01, 1.2233e-01, 1.1694e-01,\n",
      "         1.0793e-01, 9.8881e-02, 8.3588e-02, 4.2682e-02],\n",
      "        [1.0836e-01, 1.0576e-01, 1.1189e-01, 1.0787e-01, 1.0702e-01, 1.0948e-01,\n",
      "         1.0095e-01, 9.9048e-02, 9.2344e-02, 5.7269e-02],\n",
      "        [1.1097e-01, 1.0634e-01, 1.1118e-01, 1.0148e-01, 1.0139e-01, 1.0792e-01,\n",
      "         9.8548e-02, 9.9863e-02, 9.7270e-02, 6.5024e-02],\n",
      "        [1.1246e-01, 1.0543e-01, 1.1020e-01, 9.6336e-02, 9.6229e-02, 1.0652e-01,\n",
      "         9.8038e-02, 1.0165e-01, 1.0142e-01, 7.1722e-02],\n",
      "        [1.1098e-01, 1.0055e-01, 1.0428e-01, 8.8108e-02, 8.9068e-02, 1.0550e-01,\n",
      "         1.0053e-01, 1.0635e-01, 1.0945e-01, 8.5187e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 2.4491331577301025 seconds\n",
      "Saved checkpoint at 2.7915451526641846 seconds\n",
      "Finished gradient updates at 4.676432132720947\n",
      "Evaluated on validation set at 4.994889974594116 seconds\n",
      "Evaluated on training set at 5.34032416343689 seconds\n",
      "Appended results at 5.340415954589844 seconds\n",
      "Epoch: 1.00, Train Loss: 10.01, Val Loss: 10.20, Train BLEU: 0.94, Val BLEU: 0.19, Minutes Elapsed: 0.09\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 这位 是 比尔 <UNK> 我 是 大卫 <UNK>\n",
      "Reference: this is bill lange . i &apos;m dave gallo\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0002, 0.0063, 0.0270, 0.0624, 0.0064, 0.1529, 0.5129, 0.2177, 0.0137,\n",
      "         0.0004],\n",
      "        [0.0048, 0.0246, 0.0574, 0.0751, 0.0192, 0.1634, 0.3759, 0.2291, 0.0448,\n",
      "         0.0059],\n",
      "        [0.0176, 0.0393, 0.0709, 0.0738, 0.0270, 0.1504, 0.3062, 0.2273, 0.0707,\n",
      "         0.0169],\n",
      "        [0.0354, 0.0589, 0.0823, 0.0812, 0.0401, 0.1581, 0.2360, 0.1922, 0.0840,\n",
      "         0.0317],\n",
      "        [0.0494, 0.0774, 0.0864, 0.0883, 0.0550, 0.1715, 0.1827, 0.1575, 0.0863,\n",
      "         0.0454],\n",
      "        [0.0591, 0.0921, 0.0901, 0.0897, 0.0649, 0.1781, 0.1564, 0.1328, 0.0813,\n",
      "         0.0555],\n",
      "        [0.0645, 0.0986, 0.0893, 0.0888, 0.0730, 0.1812, 0.1440, 0.1204, 0.0779,\n",
      "         0.0622],\n",
      "        [0.0686, 0.1126, 0.0887, 0.0913, 0.0818, 0.1927, 0.1221, 0.1022, 0.0717,\n",
      "         0.0683],\n",
      "        [0.0710, 0.1236, 0.0858, 0.0924, 0.0907, 0.2032, 0.1056, 0.0893, 0.0668,\n",
      "         0.0716]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 冬天 很 舒服 但 夏天 却 <UNK> <EOS> <PAD> <PAD>\n",
      "Reference: it was cozy in winter but extremely hot in\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[9.2486e-06, 1.3916e-02, 8.1811e-02, 4.9293e-01, 9.4125e-02, 2.9473e-03,\n",
      "         2.3119e-01, 5.9574e-02, 2.3397e-02, 9.3904e-05],\n",
      "        [1.0702e-03, 4.4028e-02, 1.1134e-01, 2.8571e-01, 1.2706e-01, 2.6193e-02,\n",
      "         2.2737e-01, 1.1085e-01, 6.2821e-02, 3.5610e-03],\n",
      "        [8.3142e-03, 6.6316e-02, 1.1528e-01, 1.9834e-01, 1.2507e-01, 5.8138e-02,\n",
      "         1.9558e-01, 1.2800e-01, 8.8818e-02, 1.6141e-02],\n",
      "        [2.3373e-02, 7.2620e-02, 1.0201e-01, 1.4380e-01, 1.1613e-01, 9.3754e-02,\n",
      "         1.8386e-01, 1.3218e-01, 9.8704e-02, 3.3574e-02],\n",
      "        [4.2750e-02, 7.4989e-02, 8.9570e-02, 1.1054e-01, 1.0140e-01, 1.1627e-01,\n",
      "         1.7898e-01, 1.3463e-01, 1.0202e-01, 4.8838e-02],\n",
      "        [5.5299e-02, 7.2815e-02, 8.0906e-02, 9.3653e-02, 9.2992e-02, 1.2886e-01,\n",
      "         1.7618e-01, 1.3579e-01, 1.0341e-01, 6.0097e-02],\n",
      "        [6.2747e-02, 7.1175e-02, 7.5655e-02, 8.3515e-02, 8.5914e-02, 1.3336e-01,\n",
      "         1.7940e-01, 1.3884e-01, 1.0404e-01, 6.5353e-02],\n",
      "        [7.4219e-02, 8.1987e-02, 8.2907e-02, 8.8599e-02, 8.3445e-02, 1.0298e-01,\n",
      "         1.5546e-01, 1.3607e-01, 1.1588e-01, 7.8453e-02],\n",
      "        [7.5095e-02, 7.9800e-02, 8.1288e-02, 8.5235e-02, 8.1502e-02, 9.8614e-02,\n",
      "         1.5328e-01, 1.4006e-01, 1.1927e-01, 8.5852e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 5.343867301940918 seconds\n",
      "Saved checkpoint at 5.563115119934082 seconds\n",
      "Finished gradient updates at 7.375587224960327\n",
      "Evaluated on validation set at 7.689645051956177 seconds\n",
      "Evaluated on training set at 8.112900257110596 seconds\n",
      "Appended results at 8.113304138183594 seconds\n",
      "Epoch: 2.00, Train Loss: 9.81, Val Loss: 10.12, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.14\n",
      "Sampling from training predictions...\n",
      "Source: 我们 用 的 是 深海 潜水 潜水艇 <UNK> 号 和\n",
      "Reference: we use the submarine alvin and we use cameras\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[4.8948e-06, 3.0962e-02, 5.0219e-01, 4.5500e-01, 1.1644e-02, 1.9124e-04,\n",
      "         1.4738e-07, 7.0301e-08, 6.0192e-06, 1.8777e-08],\n",
      "        [7.3708e-04, 8.4933e-02, 4.1861e-01, 4.2197e-01, 6.6354e-02, 6.5882e-03,\n",
      "         1.0868e-04, 7.5911e-05, 5.9893e-04, 2.1575e-05],\n",
      "        [5.0885e-03, 1.1004e-01, 3.4825e-01, 3.7727e-01, 1.2560e-01, 2.6868e-02,\n",
      "         1.4803e-03, 1.2451e-03, 3.7588e-03, 4.0033e-04],\n",
      "        [1.3211e-02, 1.1988e-01, 2.9938e-01, 3.3237e-01, 1.5607e-01, 5.1425e-02,\n",
      "         6.2121e-03, 6.8752e-03, 1.2138e-02, 2.4397e-03],\n",
      "        [2.1508e-02, 1.2026e-01, 2.6209e-01, 2.9713e-01, 1.6690e-01, 7.0154e-02,\n",
      "         1.3014e-02, 1.8615e-02, 2.3576e-02, 6.7612e-03],\n",
      "        [2.8254e-02, 1.1946e-01, 2.3802e-01, 2.7049e-01, 1.6698e-01, 7.9611e-02,\n",
      "         1.8377e-02, 3.2213e-02, 3.4451e-02, 1.2138e-02],\n",
      "        [3.6355e-02, 1.2412e-01, 2.2488e-01, 2.4733e-01, 1.5993e-01, 8.1731e-02,\n",
      "         2.0944e-02, 4.3012e-02, 4.4319e-02, 1.7379e-02],\n",
      "        [4.1136e-02, 1.2516e-01, 2.1479e-01, 2.3233e-01, 1.5336e-01, 8.1346e-02,\n",
      "         2.2383e-02, 5.2099e-02, 5.3996e-02, 2.3404e-02],\n",
      "        [4.1120e-02, 1.1925e-01, 2.0491e-01, 2.2683e-01, 1.5625e-01, 8.5448e-02,\n",
      "         2.4448e-02, 5.7785e-02, 5.7293e-02, 2.6655e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 父亲 那 就是 他 他 是 他家 家族\n",
      "Reference: and my father -- that &apos;s him -- he\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[3.3598e-03, 1.3651e-01, 7.9136e-01, 6.8618e-02, 7.1525e-05, 6.8059e-05,\n",
      "         4.5009e-07, 8.0894e-06, 3.3386e-06, 4.3259e-08],\n",
      "        [2.7905e-02, 2.1901e-01, 5.9558e-01, 1.4943e-01, 3.3789e-03, 3.3722e-03,\n",
      "         2.0283e-04, 7.5658e-04, 3.4872e-04, 2.2230e-05],\n",
      "        [5.3735e-02, 2.3159e-01, 4.9063e-01, 1.8612e-01, 1.4221e-02, 1.4806e-02,\n",
      "         2.1212e-03, 4.3884e-03, 2.1513e-03, 2.3549e-04],\n",
      "        [6.6819e-02, 2.1369e-01, 4.2337e-01, 2.1226e-01, 3.2332e-02, 2.8692e-02,\n",
      "         5.8961e-03, 1.0451e-02, 5.6394e-03, 8.4398e-04],\n",
      "        [8.0571e-02, 2.2097e-01, 3.9471e-01, 2.0002e-01, 3.9186e-02, 3.3074e-02,\n",
      "         7.4523e-03, 1.3714e-02, 8.6549e-03, 1.6431e-03],\n",
      "        [8.6163e-02, 2.0947e-01, 3.6264e-01, 2.0472e-01, 5.3373e-02, 4.0906e-02,\n",
      "         9.8922e-03, 1.8088e-02, 1.2033e-02, 2.7182e-03],\n",
      "        [9.1370e-02, 1.9975e-01, 3.3250e-01, 2.0342e-01, 6.6519e-02, 4.9163e-02,\n",
      "         1.3412e-02, 2.3515e-02, 1.6007e-02, 4.3415e-03],\n",
      "        [9.3682e-02, 1.9086e-01, 3.0894e-01, 1.9922e-01, 7.8340e-02, 5.7773e-02,\n",
      "         1.6696e-02, 2.8346e-02, 2.0155e-02, 5.9836e-03],\n",
      "        [8.3477e-02, 1.6637e-01, 2.8795e-01, 2.0738e-01, 1.0325e-01, 6.9542e-02,\n",
      "         2.0859e-02, 3.0609e-02, 2.3143e-02, 7.4263e-03]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 8.11641812324524 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at 8.485828161239624 seconds\n",
      "Finished gradient updates at 10.203670024871826\n",
      "Evaluated on validation set at 10.530593156814575 seconds\n",
      "Evaluated on training set at 10.861161947250366 seconds\n",
      "Appended results at 10.861316204071045 seconds\n",
      "Epoch: 3.00, Train Loss: 9.55, Val Loss: 10.02, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.18\n",
      "Sampling from training predictions...\n",
      "Source: 我们 得用 非常 特殊 的 仪器 才能 能到 到达 那个\n",
      "Reference: we have to have a very special technology to\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[1.5474e-08, 7.3774e-08, 5.1423e-04, 5.8693e-02, 8.4970e-01, 8.9045e-02,\n",
      "         2.0430e-03, 9.7378e-07, 7.2937e-06, 2.7249e-08],\n",
      "        [2.0058e-05, 3.9815e-05, 8.2459e-03, 1.4830e-01, 6.6451e-01, 1.5972e-01,\n",
      "         1.8413e-02, 1.9056e-04, 5.4368e-04, 1.4602e-05],\n",
      "        [3.0665e-04, 4.2790e-04, 2.1055e-02, 1.9301e-01, 5.6003e-01, 1.8187e-01,\n",
      "         3.9506e-02, 1.2290e-03, 2.4223e-03, 1.4258e-04],\n",
      "        [1.3132e-03, 1.9201e-03, 3.2184e-02, 2.0053e-01, 4.9706e-01, 1.9720e-01,\n",
      "         6.1280e-02, 3.3684e-03, 4.6922e-03, 4.5398e-04],\n",
      "        [3.0245e-03, 4.7648e-03, 3.9972e-02, 1.9806e-01, 4.5447e-01, 2.0694e-01,\n",
      "         7.8621e-02, 6.2376e-03, 6.9692e-03, 9.3461e-04],\n",
      "        [4.9853e-03, 8.4300e-03, 4.4407e-02, 1.9358e-01, 4.2865e-01, 2.1130e-01,\n",
      "         8.9614e-02, 8.8287e-03, 8.7465e-03, 1.4556e-03],\n",
      "        [6.8739e-03, 1.1934e-02, 4.8545e-02, 1.9390e-01, 4.1093e-01, 2.1083e-01,\n",
      "         9.4323e-02, 1.0704e-02, 1.0059e-02, 1.9113e-03],\n",
      "        [8.2826e-03, 1.4891e-02, 4.9773e-02, 1.9070e-01, 3.9970e-01, 2.1145e-01,\n",
      "         9.8896e-02, 1.2524e-02, 1.1388e-02, 2.3878e-03],\n",
      "        [9.1567e-03, 1.6555e-02, 5.1780e-02, 1.9354e-01, 3.9518e-01, 2.0914e-01,\n",
      "         9.7502e-02, 1.2745e-02, 1.1800e-02, 2.5896e-03]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 不知 知道 那 意味 意味着 什么 但是 我 能\n",
      "Reference: i didn &apos;t know what it meant , but\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[3.4716e-06, 2.9824e-02, 6.3221e-01, 2.5060e-01, 8.2510e-02, 4.8481e-03,\n",
      "         1.3859e-06, 3.7199e-11, 7.5482e-13, 1.4905e-13],\n",
      "        [2.7830e-04, 5.8126e-02, 4.2036e-01, 3.0989e-01, 1.7302e-01, 3.7997e-02,\n",
      "         3.3772e-04, 3.8301e-07, 2.1297e-08, 7.3027e-09],\n",
      "        [9.9972e-04, 5.5851e-02, 2.9936e-01, 3.1340e-01, 2.2995e-01, 9.6709e-02,\n",
      "         3.7108e-03, 1.6529e-05, 1.2421e-06, 5.2762e-07],\n",
      "        [1.5297e-03, 4.9668e-02, 2.4403e-01, 3.0157e-01, 2.4573e-01, 1.4528e-01,\n",
      "         1.2058e-02, 1.2097e-04, 1.0984e-05, 5.7917e-06],\n",
      "        [2.2026e-03, 5.1799e-02, 2.3358e-01, 2.9047e-01, 2.3426e-01, 1.6603e-01,\n",
      "         2.1163e-02, 4.2348e-04, 4.4510e-05, 2.5632e-05],\n",
      "        [2.3521e-03, 4.8093e-02, 2.1636e-01, 2.7986e-01, 2.3345e-01, 1.8700e-01,\n",
      "         3.1794e-02, 9.1288e-04, 1.0745e-04, 6.6568e-05],\n",
      "        [2.6120e-03, 4.7464e-02, 2.0650e-01, 2.6884e-01, 2.3094e-01, 2.0008e-01,\n",
      "         4.1724e-02, 1.5355e-03, 1.8696e-04, 1.2014e-04],\n",
      "        [2.7064e-03, 4.6585e-02, 2.0116e-01, 2.6326e-01, 2.2982e-01, 2.0613e-01,\n",
      "         4.7934e-02, 1.9928e-03, 2.4718e-04, 1.6559e-04],\n",
      "        [2.8822e-03, 4.7630e-02, 2.0101e-01, 2.5939e-01, 2.2759e-01, 2.0751e-01,\n",
      "         5.1214e-02, 2.2863e-03, 2.8997e-04, 2.0082e-04]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 10.865350246429443 seconds\n",
      "Saved checkpoint at 11.043683052062988 seconds\n",
      "Finished gradient updates at 12.772943258285522\n",
      "Evaluated on validation set at 13.133645057678223 seconds\n",
      "Evaluated on training set at 13.455817937850952 seconds\n",
      "Appended results at 13.456298112869263 seconds\n",
      "Epoch: 4.00, Train Loss: 9.23, Val Loss: 9.89, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.22\n",
      "Sampling from training predictions...\n",
      "Source: 大家 想想 海洋 占 了 地球 球面 面积 的 75\n",
      "Reference: when you think about it , the oceans are\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[1.7591e-11, 1.3283e-06, 1.3833e-03, 5.1632e-01, 4.7698e-01, 5.2403e-03,\n",
      "         7.5345e-05, 5.7862e-08, 2.0203e-12, 1.4743e-21],\n",
      "        [9.5814e-08, 1.0985e-04, 9.9943e-03, 4.6356e-01, 4.9217e-01, 3.1892e-02,\n",
      "         2.2411e-03, 2.7760e-05, 6.4671e-08, 1.0219e-13],\n",
      "        [1.5213e-06, 4.1316e-04, 1.7777e-02, 4.0754e-01, 4.9820e-01, 6.7405e-02,\n",
      "         8.3790e-03, 2.8021e-04, 3.0036e-06, 4.9431e-11],\n",
      "        [4.0669e-06, 6.5015e-04, 2.1331e-02, 3.7021e-01, 4.9920e-01, 9.3860e-02,\n",
      "         1.3990e-02, 7.4510e-04, 1.6485e-05, 6.7009e-10],\n",
      "        [6.4439e-06, 8.0733e-04, 2.2990e-02, 3.4531e-01, 4.9770e-01, 1.1341e-01,\n",
      "         1.8438e-02, 1.2897e-03, 4.4769e-05, 3.1939e-09],\n",
      "        [9.7345e-06, 1.0069e-03, 2.4985e-02, 3.3180e-01, 4.9121e-01, 1.2708e-01,\n",
      "         2.1976e-02, 1.8474e-03, 8.6655e-05, 9.7779e-09],\n",
      "        [1.2812e-05, 1.1739e-03, 2.6510e-02, 3.2290e-01, 4.8500e-01, 1.3713e-01,\n",
      "         2.4787e-02, 2.3549e-03, 1.3381e-04, 2.0693e-08],\n",
      "        [1.4789e-05, 1.2737e-03, 2.7345e-02, 3.1889e-01, 4.8092e-01, 1.4234e-01,\n",
      "         2.6372e-02, 2.6775e-03, 1.6931e-04, 3.2107e-08],\n",
      "        [1.5415e-05, 1.2932e-03, 2.7072e-02, 3.1149e-01, 4.7861e-01, 1.4943e-01,\n",
      "         2.8814e-02, 3.0652e-03, 2.0983e-04, 4.8445e-08]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 在 塔利 塔利班 控制 阿富汗 的 那些 年 我 记得\n",
      "Reference: during taliban years , i remember there were times\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[6.0345e-12, 1.0561e-06, 1.5358e-02, 4.3033e-01, 5.4280e-01, 1.1311e-02,\n",
      "         1.9867e-04, 4.5037e-09, 1.7879e-14, 3.5792e-23],\n",
      "        [4.1317e-08, 8.9311e-05, 4.0359e-02, 3.6121e-01, 5.3337e-01, 5.9605e-02,\n",
      "         5.3505e-03, 8.0219e-06, 3.8011e-09, 1.6492e-14],\n",
      "        [7.9341e-07, 3.7442e-04, 5.1460e-02, 3.1047e-01, 5.0524e-01, 1.1205e-01,\n",
      "         2.0251e-02, 1.5181e-04, 4.5276e-07, 3.8646e-11],\n",
      "        [2.5696e-06, 6.3717e-04, 5.5166e-02, 2.8325e-01, 4.7831e-01, 1.4535e-01,\n",
      "         3.6679e-02, 6.0338e-04, 4.5015e-06, 1.5618e-09],\n",
      "        [4.2400e-06, 7.7766e-04, 5.4633e-02, 2.6536e-01, 4.6106e-01, 1.6705e-01,\n",
      "         4.9822e-02, 1.2777e-03, 1.6273e-05, 1.2575e-08],\n",
      "        [5.9724e-06, 8.9674e-04, 5.4109e-02, 2.5360e-01, 4.4786e-01, 1.8224e-01,\n",
      "         5.9242e-02, 2.0066e-03, 3.5660e-05, 4.5832e-08],\n",
      "        [8.2004e-06, 1.0447e-03, 5.5328e-02, 2.4862e-01, 4.3747e-01, 1.9042e-01,\n",
      "         6.4481e-02, 2.5717e-03, 5.6393e-05, 1.0072e-07],\n",
      "        [1.1334e-05, 1.2552e-03, 5.8455e-02, 2.4841e-01, 4.2781e-01, 1.9376e-01,\n",
      "         6.7232e-02, 2.9902e-03, 7.6558e-05, 1.7190e-07],\n",
      "        [1.4055e-05, 1.3962e-03, 5.9553e-02, 2.4603e-01, 4.2099e-01, 1.9796e-01,\n",
      "         7.0518e-02, 3.4401e-03, 1.0012e-04, 2.6773e-07]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 13.45968222618103 seconds\n",
      "Saved checkpoint at 13.703287124633789 seconds\n",
      "Finished gradient updates at 15.62187910079956\n",
      "Evaluated on validation set at 16.00798511505127 seconds\n",
      "Evaluated on training set at 16.41967010498047 seconds\n",
      "Appended results at 16.419795274734497 seconds\n",
      "Epoch: 5.00, Train Loss: 8.88, Val Loss: 9.75, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.27\n",
      "Sampling from training predictions...\n",
      "Source: 但 我 想 告诉 你 的 是 当 你 站\n",
      "Reference: but when you &apos;re standing at the beach ,\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[1.9472e-21, 6.1085e-11, 2.5253e-05, 1.7538e-03, 5.4085e-01, 4.2714e-01,\n",
      "         3.0187e-02, 4.1263e-05, 3.8051e-11, 3.0383e-22],\n",
      "        [3.8052e-13, 4.3384e-07, 1.1658e-03, 1.6701e-02, 4.5426e-01, 4.5103e-01,\n",
      "         7.5609e-02, 1.2277e-03, 2.8190e-07, 1.1595e-13],\n",
      "        [3.0775e-10, 8.9692e-06, 3.8612e-03, 3.1612e-02, 3.8802e-01, 4.6154e-01,\n",
      "         1.1034e-01, 4.6138e-03, 9.0056e-06, 2.3448e-10],\n",
      "        [3.8363e-09, 2.7809e-05, 5.9054e-03, 3.9134e-02, 3.4916e-01, 4.6271e-01,\n",
      "         1.3396e-01, 9.0498e-03, 5.0608e-05, 9.7562e-09],\n",
      "        [1.2922e-08, 4.8578e-05, 7.3241e-03, 4.3699e-02, 3.2761e-01, 4.5816e-01,\n",
      "         1.4958e-01, 1.3437e-02, 1.3488e-04, 7.8498e-08],\n",
      "        [2.5418e-08, 6.6439e-05, 8.2561e-03, 4.6498e-02, 3.1444e-01, 4.5269e-01,\n",
      "         1.6040e-01, 1.7397e-02, 2.5379e-04, 2.9441e-07],\n",
      "        [4.7560e-08, 8.7833e-05, 9.1366e-03, 4.9423e-02, 3.0554e-01, 4.4579e-01,\n",
      "         1.6814e-01, 2.1446e-02, 4.3103e-04, 8.8112e-07],\n",
      "        [7.0463e-08, 1.0676e-04, 9.8707e-03, 5.0731e-02, 2.9949e-01, 4.3984e-01,\n",
      "         1.7475e-01, 2.4627e-02, 5.9158e-04, 1.6195e-06],\n",
      "        [8.4704e-08, 1.1734e-04, 1.0262e-02, 5.1492e-02, 2.9641e-01, 4.3674e-01,\n",
      "         1.7790e-01, 2.6378e-02, 6.9676e-04, 2.2706e-06]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 她 两年 年前 退休 了 结果 却 把 我家 变成\n",
      "Reference: she retired two years ago , only to turn\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.6744, 0.3250, 0.0006, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0004, 0.5596, 0.4304, 0.0095, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0011, 0.4836, 0.4863, 0.0290, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0011, 0.3994, 0.5442, 0.0551, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.3474, 0.5727, 0.0784, 0.0006, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0009, 0.3205, 0.5839, 0.0937, 0.0011, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.3071, 0.5867, 0.1039, 0.0015, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0009, 0.2923, 0.5920, 0.1130, 0.0019, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0010, 0.2904, 0.5892, 0.1172, 0.0022, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 16.423641204833984 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at 16.701471090316772 seconds\n",
      "Finished gradient updates at 18.49996519088745\n",
      "Evaluated on validation set at 18.802311182022095 seconds\n",
      "Evaluated on training set at 19.13669204711914 seconds\n",
      "Appended results at 19.13707208633423 seconds\n",
      "Epoch: 6.00, Train Loss: 8.52, Val Loss: 9.61, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.32\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 通过 潜水 潜水艇 拍下 的 影片 把 我们\n",
      "Reference: with vibrant video clips captured by submarines , david\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[1.0324e-25, 6.1997e-23, 3.3924e-13, 4.4032e-09, 2.6853e-10, 1.0525e-03,\n",
      "         9.6070e-01, 3.8198e-02, 4.8329e-05, 4.6713e-15],\n",
      "        [3.8198e-16, 1.6979e-14, 1.6273e-08, 5.9481e-06, 1.6523e-06, 1.7634e-02,\n",
      "         8.8756e-01, 9.3879e-02, 9.2386e-04, 5.4236e-10],\n",
      "        [4.6926e-13, 9.9877e-12, 4.9523e-07, 5.4630e-05, 2.5450e-05, 4.1052e-02,\n",
      "         8.3186e-01, 1.2454e-01, 2.4637e-03, 2.6227e-08],\n",
      "        [7.0926e-12, 1.3654e-10, 1.9283e-06, 1.3494e-04, 7.0104e-05, 5.1599e-02,\n",
      "         7.9752e-01, 1.4616e-01, 4.5132e-03, 1.8018e-07],\n",
      "        [2.9759e-11, 5.5488e-10, 4.1434e-06, 2.2851e-04, 1.1483e-04, 5.6194e-02,\n",
      "         7.7282e-01, 1.6382e-01, 6.8116e-03, 5.9283e-07],\n",
      "        [7.0760e-11, 1.2542e-09, 6.6016e-06, 3.1608e-04, 1.4841e-04, 5.7446e-02,\n",
      "         7.5398e-01, 1.7891e-01, 9.1932e-03, 1.3438e-06],\n",
      "        [1.2060e-10, 2.0274e-09, 8.7406e-06, 3.8712e-04, 1.7224e-04, 5.7870e-02,\n",
      "         7.4118e-01, 1.8918e-01, 1.1202e-02, 2.2918e-06],\n",
      "        [1.7394e-10, 2.8303e-09, 1.0805e-05, 4.4876e-04, 1.8871e-04, 5.8335e-02,\n",
      "         7.3314e-01, 1.9529e-01, 1.2580e-02, 3.1122e-06],\n",
      "        [1.9993e-10, 3.1699e-09, 1.1158e-05, 4.5578e-04, 1.8755e-04, 5.6676e-02,\n",
      "         7.2525e-01, 2.0317e-01, 1.4245e-02, 4.1532e-06]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人\n",
      "Reference: the school was in a house , more than\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[1.2624e-28, 8.7587e-26, 2.4799e-13, 1.1289e-06, 1.6235e-01, 4.1120e-01,\n",
      "         3.2405e-01, 1.0239e-01, 1.1435e-06, 8.6538e-18],\n",
      "        [7.7212e-18, 1.0364e-15, 2.1751e-08, 1.7232e-04, 2.1818e-01, 4.2859e-01,\n",
      "         2.7307e-01, 7.9937e-02, 5.0628e-05, 5.8545e-12],\n",
      "        [3.3588e-14, 2.7312e-12, 8.5994e-07, 8.4599e-04, 2.2532e-01, 4.3461e-01,\n",
      "         2.5816e-01, 8.0844e-02, 2.1872e-04, 6.7442e-10],\n",
      "        [8.2518e-13, 5.8297e-11, 2.9943e-06, 1.3759e-03, 2.1928e-01, 4.3382e-01,\n",
      "         2.5243e-01, 9.2555e-02, 5.4356e-04, 7.3424e-09],\n",
      "        [3.6870e-12, 2.5366e-10, 4.9651e-06, 1.6142e-03, 2.0859e-01, 4.3058e-01,\n",
      "         2.5309e-01, 1.0511e-01, 1.0057e-03, 3.2587e-08],\n",
      "        [8.4529e-12, 5.7661e-10, 6.5554e-06, 1.7475e-03, 2.0090e-01, 4.2683e-01,\n",
      "         2.5520e-01, 1.1387e-01, 1.4405e-03, 7.7108e-08],\n",
      "        [1.4524e-11, 9.7494e-10, 8.0065e-06, 1.8574e-03, 1.9615e-01, 4.2195e-01,\n",
      "         2.5759e-01, 1.2064e-01, 1.8018e-03, 1.2976e-07],\n",
      "        [1.9799e-11, 1.3277e-09, 8.7874e-06, 1.8869e-03, 1.9102e-01, 4.1892e-01,\n",
      "         2.5980e-01, 1.2622e-01, 2.1396e-03, 1.9107e-07],\n",
      "        [2.6603e-11, 1.7393e-09, 9.8606e-06, 1.9662e-03, 1.8851e-01, 4.1583e-01,\n",
      "         2.6159e-01, 1.2972e-01, 2.3822e-03, 2.4786e-07]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 19.142105102539062 seconds\n",
      "Saved checkpoint at 19.46993112564087 seconds\n",
      "Finished gradient updates at 21.16629719734192\n",
      "Evaluated on validation set at 21.493285179138184 seconds\n",
      "Evaluated on training set at 21.795814275741577 seconds\n",
      "Appended results at 21.79595112800598 seconds\n",
      "Epoch: 7.00, Train Loss: 8.17, Val Loss: 9.46, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.36\n",
      "Sampling from training predictions...\n",
      "Source: 海洋 里 生物 的 多样 多样性 和 密度 要 比\n",
      "Reference: the biodiversity and the <UNK> in the ocean is\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[5.5363e-25, 1.0594e-09, 3.2292e-04, 9.1249e-01, 9.8181e-03, 5.5673e-09,\n",
      "         7.6889e-02, 4.8212e-04, 5.5488e-08, 1.7882e-20],\n",
      "        [1.7727e-15, 1.1072e-06, 3.7502e-03, 6.4359e-01, 1.0087e-01, 6.5563e-05,\n",
      "         2.4532e-01, 6.3803e-03, 1.3810e-05, 6.1841e-13],\n",
      "        [1.0864e-12, 6.4450e-06, 6.1384e-03, 4.5944e-01, 1.8257e-01, 1.5025e-03,\n",
      "         3.3558e-01, 1.4675e-02, 9.4171e-05, 2.5150e-10],\n",
      "        [8.1090e-12, 9.5021e-06, 6.3151e-03, 3.7008e-01, 2.2438e-01, 5.1963e-03,\n",
      "         3.7240e-01, 2.1376e-02, 2.4027e-04, 4.5541e-09],\n",
      "        [2.1624e-11, 1.1330e-05, 6.2639e-03, 3.2566e-01, 2.4409e-01, 9.4241e-03,\n",
      "         3.8736e-01, 2.6783e-02, 4.0869e-04, 2.2266e-08],\n",
      "        [3.3961e-11, 1.1872e-05, 6.0834e-03, 3.0254e-01, 2.5881e-01, 1.2812e-02,\n",
      "         3.8820e-01, 3.0967e-02, 5.7715e-04, 6.0162e-08],\n",
      "        [4.5067e-11, 1.2360e-05, 6.0073e-03, 2.9044e-01, 2.6732e-01, 1.5074e-02,\n",
      "         3.8645e-01, 3.3979e-02, 7.1544e-04, 1.0963e-07],\n",
      "        [6.3780e-11, 1.4897e-05, 6.4783e-03, 2.9422e-01, 2.6541e-01, 1.5163e-02,\n",
      "         3.8197e-01, 3.5916e-02, 8.2656e-04, 1.4889e-07],\n",
      "        [6.7462e-11, 1.4478e-05, 6.3440e-03, 2.8895e-01, 2.7287e-01, 1.5832e-02,\n",
      "         3.7666e-01, 3.8361e-02, 9.6333e-04, 2.1940e-07]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 塔利 塔利班 走 了 父亲 大声 叫 着 <EOS> <PAD>\n",
      "Reference: &quot; the taliban are gone ! &quot; my father\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[4.5251e-24, 2.2726e-09, 1.1963e-02, 1.5202e-01, 6.9141e-01, 1.4399e-01,\n",
      "         6.0878e-04, 9.2847e-09, 9.8494e-17, 3.2691e-30],\n",
      "        [1.7123e-15, 9.3800e-07, 1.8492e-02, 1.9459e-01, 6.1255e-01, 1.6945e-01,\n",
      "         4.9079e-03, 5.7038e-06, 7.7401e-11, 6.9501e-19],\n",
      "        [5.7646e-13, 4.6999e-06, 1.8698e-02, 1.9990e-01, 5.8671e-01, 1.8338e-01,\n",
      "         1.1253e-02, 5.8059e-05, 9.3373e-09, 6.8434e-15],\n",
      "        [3.7368e-12, 7.1756e-06, 1.8319e-02, 2.0297e-01, 5.7041e-01, 1.9153e-01,\n",
      "         1.6597e-02, 1.6635e-04, 8.5436e-08, 5.1921e-13],\n",
      "        [8.9968e-12, 8.5859e-06, 1.8038e-02, 2.0303e-01, 5.5891e-01, 1.9888e-01,\n",
      "         2.0836e-02, 2.9889e-04, 2.9745e-07, 5.9927e-12],\n",
      "        [1.7119e-11, 1.0188e-05, 1.8205e-02, 2.0301e-01, 5.4962e-01, 2.0469e-01,\n",
      "         2.4033e-02, 4.3199e-04, 6.4833e-07, 2.7013e-11],\n",
      "        [2.4163e-11, 1.1100e-05, 1.8224e-02, 2.0171e-01, 5.4357e-01, 2.0927e-01,\n",
      "         2.6671e-02, 5.4945e-04, 1.0633e-06, 6.7425e-11],\n",
      "        [2.7995e-11, 1.1315e-05, 1.8054e-02, 2.0082e-01, 5.3982e-01, 2.1220e-01,\n",
      "         2.8454e-02, 6.3839e-04, 1.4486e-06, 1.2030e-10],\n",
      "        [3.1979e-11, 1.1810e-05, 1.8223e-02, 2.0109e-01, 5.3754e-01, 2.1312e-01,\n",
      "         2.9325e-02, 6.9186e-04, 1.7198e-06, 1.6746e-10]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 21.799520015716553 seconds\n",
      "Saved checkpoint at 21.952616214752197 seconds\n",
      "Finished gradient updates at 23.70594096183777\n",
      "Evaluated on validation set at 24.21224021911621 seconds\n",
      "Evaluated on training set at 24.759721040725708 seconds\n",
      "Appended results at 24.760091304779053 seconds\n",
      "Epoch: 8.00, Train Loss: 7.82, Val Loss: 9.33, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.41\n",
      "Sampling from training predictions...\n",
      "Source: 底下 这些 都 是 <UNK> 它们 上上 上上下下 上下 下下\n",
      "Reference: it &apos;s got these fishing <UNK> on the bottom\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[8.1246e-15, 1.7743e-05, 9.9430e-01, 5.6802e-03, 3.9685e-13, 1.5333e-13,\n",
      "         3.4419e-25, 7.1146e-23, 1.6145e-27, 4.7644e-43],\n",
      "        [3.1263e-10, 4.8285e-04, 9.2824e-01, 7.1273e-02, 5.9769e-08, 2.6670e-08,\n",
      "         1.9483e-15, 1.0322e-14, 6.6967e-18, 9.9758e-28],\n",
      "        [1.9456e-09, 6.5126e-04, 8.1084e-01, 1.8850e-01, 2.1270e-06, 9.0989e-07,\n",
      "         1.2441e-12, 2.9748e-12, 8.0661e-15, 6.4779e-23],\n",
      "        [1.8272e-09, 4.6002e-04, 6.8608e-01, 3.1345e-01, 9.1574e-06, 3.9094e-06,\n",
      "         1.4704e-11, 2.7282e-11, 1.6758e-13, 8.6137e-21],\n",
      "        [1.7253e-09, 3.7514e-04, 6.1425e-01, 3.8535e-01, 1.8053e-05, 8.3232e-06,\n",
      "         5.2497e-11, 8.4266e-11, 7.8290e-13, 9.5544e-20],\n",
      "        [1.5843e-09, 3.1846e-04, 5.5757e-01, 4.4207e-01, 2.8380e-05, 1.4042e-05,\n",
      "         1.2544e-10, 1.7806e-10, 2.1888e-12, 5.0739e-19],\n",
      "        [1.4476e-09, 2.8377e-04, 5.2340e-01, 4.7626e-01, 3.6190e-05, 1.9131e-05,\n",
      "         2.1013e-10, 2.7397e-10, 3.9162e-12, 1.2614e-18],\n",
      "        [1.3342e-09, 2.6274e-04, 5.0338e-01, 4.9629e-01, 4.1476e-05, 2.2825e-05,\n",
      "         2.8076e-10, 3.4863e-10, 5.4523e-12, 2.0626e-18],\n",
      "        [1.3022e-09, 2.5725e-04, 4.9958e-01, 5.0010e-01, 4.3339e-05, 2.3958e-05,\n",
      "         3.1170e-10, 3.7808e-10, 6.0655e-12, 2.3643e-18]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 冬天 很 舒服 但 夏天 却 <UNK> <EOS> <PAD> <PAD>\n",
      "Reference: it was cozy in winter but extremely hot in\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[1.0747e-28, 4.7167e-11, 1.3373e-04, 9.2375e-01, 1.0403e-02, 1.7231e-09,\n",
      "         6.5498e-02, 2.2048e-04, 3.5771e-09, 9.2658e-24],\n",
      "        [1.3882e-17, 1.4288e-07, 2.0630e-03, 6.3830e-01, 1.3699e-01, 7.1893e-05,\n",
      "         2.1940e-01, 3.1777e-03, 1.7711e-06, 6.6149e-15],\n",
      "        [1.5115e-14, 9.7466e-07, 3.4234e-03, 4.4896e-01, 2.5103e-01, 1.8544e-03,\n",
      "         2.8755e-01, 7.1637e-03, 1.4039e-05, 5.5021e-12],\n",
      "        [1.4139e-13, 1.5975e-06, 3.6258e-03, 3.6579e-01, 3.0598e-01, 6.1451e-03,\n",
      "         3.0807e-01, 1.0352e-02, 3.8223e-05, 1.3051e-10],\n",
      "        [4.1726e-13, 2.0115e-06, 3.6640e-03, 3.2581e-01, 3.3166e-01, 1.0703e-02,\n",
      "         3.1512e-01, 1.2978e-02, 6.7076e-05, 7.0569e-10],\n",
      "        [6.8711e-13, 2.1646e-06, 3.5847e-03, 3.0381e-01, 3.4890e-01, 1.4177e-02,\n",
      "         3.1441e-01, 1.5021e-02, 9.6240e-05, 1.9858e-09],\n",
      "        [9.2161e-13, 2.2702e-06, 3.5476e-03, 2.9255e-01, 3.5909e-01, 1.6405e-02,\n",
      "         3.1183e-01, 1.6461e-02, 1.2021e-04, 3.6833e-09],\n",
      "        [1.2459e-12, 2.6421e-06, 3.7728e-03, 2.9538e-01, 3.5824e-01, 1.6640e-02,\n",
      "         3.0851e-01, 1.7319e-02, 1.3705e-04, 4.9570e-09],\n",
      "        [1.3159e-12, 2.5788e-06, 3.6918e-03, 2.9008e-01, 3.6635e-01, 1.7289e-02,\n",
      "         3.0407e-01, 1.8361e-02, 1.5872e-04, 7.2474e-09]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 24.76462721824646 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at 25.02724814414978 seconds\n",
      "Finished gradient updates at 27.732924222946167\n",
      "Evaluated on validation set at 28.113031148910522 seconds\n",
      "Evaluated on training set at 28.61577820777893 seconds\n",
      "Appended results at 28.61588406562805 seconds\n",
      "Epoch: 9.00, Train Loss: 7.49, Val Loss: 9.20, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.48\n",
      "Sampling from training predictions...\n",
      "Source: 我 真 喜欢 这些 东西 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: i love that kind of stuff . <EOS> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0428, 0.9572, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0762, 0.9214, 0.0024, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0667, 0.9224, 0.0110, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0552, 0.9228, 0.0221, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0493, 0.9195, 0.0312, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0456, 0.9167, 0.0376, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0433, 0.9146, 0.0421, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0412, 0.9130, 0.0458, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0405, 0.9118, 0.0477, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 那 就是 她 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: there she is . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.2935, 0.7054, 0.0010, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0007, 0.3909, 0.6018, 0.0066, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.3978, 0.5879, 0.0125, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0022, 0.3842, 0.5961, 0.0174, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0024, 0.3706, 0.6053, 0.0216, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0024, 0.3603, 0.6122, 0.0247, 0.0003, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0025, 0.3541, 0.6164, 0.0267, 0.0003, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0025, 0.3500, 0.6190, 0.0281, 0.0004, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0025, 0.3473, 0.6209, 0.0289, 0.0004, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 28.618993282318115 seconds\n",
      "Saved checkpoint at 29.043105125427246 seconds\n",
      "Finished gradient updates at 30.911912202835083\n",
      "Evaluated on validation set at 31.277937173843384 seconds\n",
      "Evaluated on training set at 31.604294300079346 seconds\n",
      "Appended results at 31.60457706451416 seconds\n",
      "Epoch: 10.00, Train Loss: 7.17, Val Loss: 9.07, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.53\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 这位 是 比尔 <UNK> 我 是 大卫 <UNK>\n",
      "Reference: this is bill lange . i &apos;m dave gallo\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[7.3023e-29, 1.3200e-22, 4.2252e-09, 9.3105e-05, 1.4241e-13, 5.3139e-09,\n",
      "         9.9924e-01, 6.6990e-04, 1.9551e-16, 3.3404e-39],\n",
      "        [2.4101e-18, 1.6011e-14, 2.6415e-06, 2.2322e-03, 9.3704e-08, 6.8968e-05,\n",
      "         9.9221e-01, 5.4815e-03, 1.6014e-10, 4.7167e-24],\n",
      "        [8.6767e-16, 1.4042e-12, 9.6828e-06, 4.6415e-03, 2.9129e-06, 6.3141e-04,\n",
      "         9.8002e-01, 1.4695e-02, 1.3094e-08, 1.2671e-19],\n",
      "        [7.0787e-15, 7.2492e-12, 1.5727e-05, 6.8665e-03, 1.0908e-05, 1.2120e-03,\n",
      "         9.6653e-01, 2.5369e-02, 9.4266e-08, 9.6168e-18],\n",
      "        [1.8677e-14, 1.5568e-11, 2.0061e-05, 8.7815e-03, 2.1654e-05, 1.5755e-03,\n",
      "         9.5508e-01, 3.4516e-02, 2.6539e-07, 9.1423e-17],\n",
      "        [3.1307e-14, 2.3435e-11, 2.3106e-05, 1.0078e-02, 3.1016e-05, 1.8047e-03,\n",
      "         9.4795e-01, 4.0109e-02, 4.4099e-07, 2.8297e-16],\n",
      "        [4.1114e-14, 2.9051e-11, 2.5020e-05, 1.1004e-02, 3.8073e-05, 1.9253e-03,\n",
      "         9.4332e-01, 4.3686e-02, 5.7735e-07, 5.0536e-16],\n",
      "        [4.9245e-14, 3.3085e-11, 2.6320e-05, 1.1932e-02, 4.3120e-05, 1.9371e-03,\n",
      "         9.3829e-01, 4.7775e-02, 7.3538e-07, 8.0038e-16],\n",
      "        [5.5841e-14, 3.6207e-11, 2.7459e-05, 1.2629e-02, 4.6757e-05, 1.9306e-03,\n",
      "         9.3496e-01, 5.0409e-02, 8.4687e-07, 1.0504e-15]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 在 被 跟踪 吗 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: were we being followed ? <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[9.2264e-32, 2.2453e-11, 4.7114e-03, 9.9388e-01, 1.4106e-03, 6.6581e-09,\n",
      "         6.9429e-21, 4.0474e-24, 1.5924e-20, 4.9205e-36],\n",
      "        [4.1915e-19, 1.3211e-07, 2.2203e-02, 9.2274e-01, 5.4967e-02, 9.2633e-05,\n",
      "         7.2441e-11, 3.1537e-13, 5.5301e-13, 9.4356e-22],\n",
      "        [3.7370e-16, 8.3107e-07, 2.7512e-02, 8.4792e-01, 1.2359e-01, 9.7479e-04,\n",
      "         1.8859e-08, 1.3585e-10, 5.5079e-11, 1.2410e-17],\n",
      "        [3.3550e-15, 1.3945e-06, 2.7589e-02, 8.0983e-01, 1.6037e-01, 2.2065e-03,\n",
      "         1.1722e-07, 1.0895e-09, 3.3947e-10, 7.5978e-16],\n",
      "        [9.1420e-15, 1.7082e-06, 2.6760e-02, 7.8466e-01, 1.8518e-01, 3.3961e-03,\n",
      "         2.8884e-07, 3.1214e-09, 9.0250e-10, 6.4038e-15],\n",
      "        [1.4857e-14, 1.8420e-06, 2.5842e-02, 7.6742e-01, 2.0236e-01, 4.3830e-03,\n",
      "         4.7835e-07, 5.5230e-09, 1.5511e-09, 2.0546e-14],\n",
      "        [1.9436e-14, 1.9201e-06, 2.5326e-02, 7.5659e-01, 2.1300e-01, 5.0797e-03,\n",
      "         6.3606e-07, 7.6569e-09, 2.1064e-09, 3.9316e-14],\n",
      "        [2.2693e-14, 1.9581e-06, 2.4952e-02, 7.4936e-01, 2.2010e-01, 5.5904e-03,\n",
      "         7.6669e-07, 9.4305e-09, 2.5530e-09, 5.9524e-14],\n",
      "        [2.4174e-14, 1.9427e-06, 2.4485e-02, 7.4375e-01, 2.2578e-01, 5.9816e-03,\n",
      "         8.6638e-07, 1.0720e-08, 2.8875e-09, 7.8757e-14]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 31.60842227935791 seconds\n",
      "Saved checkpoint at 31.854284048080444 seconds\n",
      "Finished gradient updates at 33.79288411140442\n",
      "Evaluated on validation set at 34.182056188583374 seconds\n",
      "Evaluated on training set at 34.50780534744263 seconds\n",
      "Appended results at 34.508061170578 seconds\n",
      "Epoch: 11.00, Train Loss: 6.87, Val Loss: 8.96, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.58\n",
      "Sampling from training predictions...\n",
      "Source: 地球 的 大部 大部分 部分 都 是 海水 <EOS> <PAD>\n",
      "Reference: most of the planet is ocean water . <EOS>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.0044, 0.6722, 0.3234, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0005, 0.0761, 0.7898, 0.1335, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0013, 0.1185, 0.7391, 0.1411, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.1299, 0.7011, 0.1672, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0019, 0.1321, 0.6782, 0.1876, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0020, 0.1330, 0.6657, 0.1991, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0021, 0.1339, 0.6591, 0.2046, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0021, 0.1336, 0.6534, 0.2106, 0.0003, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0022, 0.1343, 0.6509, 0.2123, 0.0003, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人\n",
      "Reference: the school was in a house , more than\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.1091, 0.6791, 0.2100, 0.0018, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0001, 0.1827, 0.6707, 0.1450, 0.0015, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0003, 0.2022, 0.6596, 0.1359, 0.0020, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0005, 0.2069, 0.6523, 0.1376, 0.0027, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0006, 0.2043, 0.6491, 0.1426, 0.0034, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0007, 0.2011, 0.6480, 0.1464, 0.0038, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0007, 0.1991, 0.6477, 0.1484, 0.0041, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0007, 0.1972, 0.6478, 0.1500, 0.0042, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0007, 0.1961, 0.6478, 0.1510, 0.0044, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 34.511861085891724 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at 34.82652020454407 seconds\n",
      "Finished gradient updates at 36.64821124076843\n",
      "Evaluated on validation set at 36.97070908546448 seconds\n",
      "Evaluated on training set at 37.315118074417114 seconds\n",
      "Appended results at 37.31521916389465 seconds\n",
      "Epoch: 12.00, Train Loss: 6.57, Val Loss: 8.85, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.62\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 这位 是 比尔 <UNK> 我 是 大卫 <UNK>\n",
      "Reference: this is bill lange . i &apos;m dave gallo\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.0002, 0.0000, 0.0000, 0.9996, 0.0002, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0071, 0.0000, 0.0006, 0.9906, 0.0018, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0158, 0.0000, 0.0043, 0.9746, 0.0052, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0237, 0.0002, 0.0079, 0.9591, 0.0091, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0298, 0.0003, 0.0103, 0.9473, 0.0123, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0337, 0.0004, 0.0119, 0.9397, 0.0142, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0364, 0.0005, 0.0129, 0.9348, 0.0153, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0387, 0.0006, 0.0132, 0.9309, 0.0166, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0403, 0.0006, 0.0133, 0.9284, 0.0173, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 把 书 放在 食品 杂货 袋中 这样 别人 就\n",
      "Reference: we would cover our books in grocery bags so\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[4.2039e-45, 3.0610e-25, 5.5689e-12, 1.3896e-04, 1.8769e-02, 9.4381e-01,\n",
      "         3.7281e-02, 8.4195e-08, 1.5553e-17, 2.5948e-41],\n",
      "        [8.9557e-26, 1.9267e-14, 6.7239e-07, 9.2392e-03, 2.0473e-01, 7.3603e-01,\n",
      "         4.9983e-02, 1.4941e-05, 1.1883e-11, 4.7803e-24],\n",
      "        [1.2087e-21, 3.2948e-12, 6.2369e-06, 1.7949e-02, 2.6203e-01, 6.5697e-01,\n",
      "         6.2991e-02, 6.2251e-05, 4.8272e-10, 1.0611e-19],\n",
      "        [3.4360e-20, 1.8339e-11, 1.2000e-05, 2.0710e-02, 2.6090e-01, 6.4020e-01,\n",
      "         7.8046e-02, 1.2794e-04, 2.6953e-09, 7.8281e-18],\n",
      "        [1.5344e-19, 3.8423e-11, 1.5442e-05, 2.1348e-02, 2.5502e-01, 6.3398e-01,\n",
      "         8.9448e-02, 1.9413e-04, 6.8391e-09, 7.1938e-17],\n",
      "        [3.3632e-19, 5.6667e-11, 1.7601e-05, 2.1632e-02, 2.5224e-01, 6.2965e-01,\n",
      "         9.6216e-02, 2.4419e-04, 1.1246e-08, 2.2788e-16],\n",
      "        [5.3800e-19, 7.2210e-11, 1.9254e-05, 2.1970e-02, 2.5221e-01, 6.2594e-01,\n",
      "         9.9580e-02, 2.7590e-04, 1.4652e-08, 4.1758e-16],\n",
      "        [6.9944e-19, 8.1824e-11, 1.9978e-05, 2.1976e-02, 2.5067e-01, 6.2447e-01,\n",
      "         1.0257e-01, 3.0068e-04, 1.7580e-08, 6.2095e-16],\n",
      "        [8.3008e-19, 8.8952e-11, 2.0501e-05, 2.2003e-02, 2.4994e-01, 6.2318e-01,\n",
      "         1.0454e-01, 3.1896e-04, 2.0007e-08, 8.2688e-16]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 37.31876611709595 seconds\n",
      "Saved checkpoint at 37.755805253982544 seconds\n",
      "Finished gradient updates at 39.53007102012634\n",
      "Evaluated on validation set at 39.85591411590576 seconds\n",
      "Evaluated on training set at 40.24402403831482 seconds\n",
      "Appended results at 40.24411606788635 seconds\n",
      "Epoch: 13.00, Train Loss: 6.30, Val Loss: 8.76, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.67\n",
      "Sampling from training predictions...\n",
      "Source: 我们 将 用 一些 影片 来讲 讲述 一些 深海 海里\n",
      "Reference: and we &apos;re going to tell you some stories\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[3.1799e-39, 4.1933e-15, 5.1087e-05, 3.6923e-02, 6.4871e-01, 3.1231e-01,\n",
      "         2.0059e-03, 8.7418e-09, 3.2231e-20, 1.0089e-43],\n",
      "        [2.0421e-23, 1.4251e-10, 2.8297e-04, 8.9116e-02, 6.7834e-01, 2.2717e-01,\n",
      "         5.0936e-03, 2.0348e-06, 2.9328e-13, 1.3360e-25],\n",
      "        [2.9975e-20, 1.4118e-09, 5.0458e-04, 1.0946e-01, 6.6704e-01, 2.1548e-01,\n",
      "         7.5045e-03, 9.9989e-06, 2.0788e-11, 3.9547e-21],\n",
      "        [3.9779e-19, 3.4175e-09, 6.7076e-04, 1.1767e-01, 6.5527e-01, 2.1695e-01,\n",
      "         9.4238e-03, 2.1275e-05, 1.4026e-10, 3.4887e-19],\n",
      "        [1.2291e-18, 4.9170e-09, 7.4456e-04, 1.1947e-01, 6.4747e-01, 2.2141e-01,\n",
      "         1.0873e-02, 3.2177e-05, 3.7876e-10, 3.3622e-18],\n",
      "        [2.1536e-18, 5.8806e-09, 7.7903e-04, 1.2003e-01, 6.4316e-01, 2.2422e-01,\n",
      "         1.1767e-02, 4.0132e-05, 6.3329e-10, 1.0451e-17],\n",
      "        [3.0843e-18, 6.5214e-09, 7.9584e-04, 1.2029e-01, 6.4084e-01, 2.2575e-01,\n",
      "         1.2284e-02, 4.5439e-05, 8.5119e-10, 2.0398e-17],\n",
      "        [3.8567e-18, 6.9943e-09, 8.1100e-04, 1.2069e-01, 6.3920e-01, 2.2663e-01,\n",
      "         1.2619e-02, 4.9171e-05, 1.0265e-09, 3.0896e-17],\n",
      "        [4.5291e-18, 7.3642e-09, 8.2119e-04, 1.2094e-01, 6.3825e-01, 2.2712e-01,\n",
      "         1.2820e-02, 5.1525e-05, 1.1452e-09, 3.9268e-17]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 冬天 很 舒服 但 夏天 却 <UNK> <EOS> <PAD> <PAD>\n",
      "Reference: it was cozy in winter but extremely hot in\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[1.3539e-39, 1.3087e-14, 2.5059e-05, 7.7392e-01, 1.1163e-01, 5.3435e-08,\n",
      "         1.1428e-01, 1.3569e-04, 5.3572e-12, 1.1125e-33],\n",
      "        [2.9121e-23, 2.4305e-10, 2.3505e-04, 1.8978e-01, 7.1393e-01, 1.2479e-02,\n",
      "         8.3305e-02, 2.6945e-04, 1.6432e-09, 5.6184e-21],\n",
      "        [4.0676e-20, 1.6277e-09, 3.1166e-04, 1.1775e-01, 7.4678e-01, 7.3483e-02,\n",
      "         6.1336e-02, 3.4496e-04, 1.0345e-08, 9.1008e-18],\n",
      "        [4.6016e-19, 3.1515e-09, 3.3765e-04, 9.8783e-02, 7.2417e-01, 1.2091e-01,\n",
      "         5.5390e-02, 4.1694e-04, 2.6377e-08, 2.4214e-16],\n",
      "        [1.3274e-18, 4.0976e-09, 3.4163e-04, 8.9797e-02, 7.0735e-01, 1.4882e-01,\n",
      "         5.3210e-02, 4.7538e-04, 4.3532e-08, 1.2444e-15],\n",
      "        [2.2456e-18, 4.6280e-09, 3.4149e-04, 8.5342e-02, 6.9840e-01, 1.6349e-01,\n",
      "         5.1915e-02, 5.1397e-04, 5.7737e-08, 3.0243e-15],\n",
      "        [3.0869e-18, 5.0062e-09, 3.4341e-04, 8.3146e-02, 6.9371e-01, 1.7128e-01,\n",
      "         5.0987e-02, 5.3852e-04, 6.8035e-08, 4.9955e-15],\n",
      "        [4.0306e-18, 5.5295e-09, 3.5586e-04, 8.2898e-02, 6.9101e-01, 1.7477e-01,\n",
      "         5.0417e-02, 5.5128e-04, 7.3581e-08, 6.3544e-15],\n",
      "        [4.5394e-18, 5.6806e-09, 3.5590e-04, 8.2263e-02, 6.9117e-01, 1.7584e-01,\n",
      "         4.9809e-02, 5.6338e-04, 7.9850e-08, 8.0272e-15]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 40.24823021888733 seconds\n",
      "Saved checkpoint at 40.47383499145508 seconds\n",
      "Finished gradient updates at 42.577473163604736\n",
      "Evaluated on validation set at 42.92119812965393 seconds\n",
      "Evaluated on training set at 43.258440256118774 seconds\n",
      "Appended results at 43.258546352386475 seconds\n",
      "Epoch: 14.00, Train Loss: 6.04, Val Loss: 8.69, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.72\n",
      "Sampling from training predictions...\n",
      "Source: 还有 前面 的 这个 是 推进 引擎 它 一会 一会儿\n",
      "Reference: and it &apos;s got these jet thrusters up in\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0001, 0.0853, 0.9122, 0.0023, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0010, 0.2197, 0.7546, 0.0247, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0015, 0.2232, 0.7320, 0.0433, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0016, 0.2139, 0.7300, 0.0545, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0016, 0.2061, 0.7300, 0.0621, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0016, 0.2022, 0.7291, 0.0669, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.2018, 0.7270, 0.0694, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.2013, 0.7259, 0.0710, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.2014, 0.7248, 0.0719, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 不知 知道 那 意味 意味着 什么 但是 我 能\n",
      "Reference: i didn &apos;t know what it meant , but\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0034, 0.4920, 0.5035, 0.0011, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0038, 0.4723, 0.5121, 0.0119, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0040, 0.4381, 0.5317, 0.0262, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0042, 0.4142, 0.5435, 0.0382, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0042, 0.3983, 0.5515, 0.0460, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0041, 0.3862, 0.5580, 0.0517, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0040, 0.3800, 0.5609, 0.0549, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0040, 0.3770, 0.5620, 0.0569, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0040, 0.3757, 0.5623, 0.0579, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 43.26272630691528 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at 43.54687428474426 seconds\n",
      "Finished gradient updates at 45.90603423118591\n",
      "Evaluated on validation set at 46.297362327575684 seconds\n",
      "Evaluated on training set at 46.702704191207886 seconds\n",
      "Appended results at 46.70283222198486 seconds\n",
      "Epoch: 15.00, Train Loss: 5.79, Val Loss: 8.63, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.78\n",
      "Sampling from training predictions...\n",
      "Source: 我们 将 用 一些 影片 来讲 讲述 一些 深海 海里\n",
      "Reference: and we &apos;re going to tell you some stories\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[6.4151e-41, 1.4794e-15, 2.5837e-05, 3.0944e-02, 5.9068e-01, 3.7360e-01,\n",
      "         4.7494e-03, 5.7849e-08, 3.0484e-19, 2.8026e-44],\n",
      "        [4.4546e-24, 6.6034e-11, 1.7011e-04, 8.0866e-02, 6.5132e-01, 2.5907e-01,\n",
      "         8.5676e-03, 5.8062e-06, 9.0310e-13, 1.0399e-25],\n",
      "        [4.2198e-21, 6.2620e-10, 3.3910e-04, 1.0224e-01, 6.4416e-01, 2.4187e-01,\n",
      "         1.1370e-02, 2.1857e-05, 3.9990e-11, 1.7156e-21],\n",
      "        [4.3660e-20, 1.4553e-09, 4.6532e-04, 1.1094e-01, 6.3491e-01, 2.4031e-01,\n",
      "         1.3337e-02, 3.9484e-05, 2.0223e-10, 1.0357e-19],\n",
      "        [1.1359e-19, 1.9995e-09, 5.1830e-04, 1.1302e-01, 6.2883e-01, 2.4285e-01,\n",
      "         1.4727e-02, 5.3884e-05, 4.5415e-10, 7.6818e-19],\n",
      "        [1.8605e-19, 2.3420e-09, 5.4297e-04, 1.1377e-01, 6.2545e-01, 2.4463e-01,\n",
      "         1.5550e-02, 6.3479e-05, 6.8852e-10, 2.0950e-18],\n",
      "        [2.6101e-19, 2.5961e-09, 5.5835e-04, 1.1441e-01, 6.2376e-01, 2.4523e-01,\n",
      "         1.5973e-02, 6.9402e-05, 8.7088e-10, 3.7550e-18],\n",
      "        [3.2661e-19, 2.7953e-09, 5.7158e-04, 1.1499e-01, 6.2257e-01, 2.4555e-01,\n",
      "         1.6248e-02, 7.3456e-05, 1.0107e-09, 5.3882e-18],\n",
      "        [3.8583e-19, 2.9510e-09, 5.8017e-04, 1.1534e-01, 6.2183e-01, 2.4575e-01,\n",
      "         1.6419e-02, 7.6035e-05, 1.1048e-09, 6.6700e-18]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 11 岁 那年 记得 得有 一天 早晨 醒来 听见\n",
      "Reference: when i was 11 , i remember waking up\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.9852, 0.0148, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.8239, 0.1761, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.7021, 0.2979, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.6257, 0.3743, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.5771, 0.4228, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.5504, 0.4495, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.5361, 0.4637, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.5286, 0.4712, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.5265, 0.4734, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 46.70411014556885 seconds\n",
      "Saved checkpoint at 47.21126914024353 seconds\n",
      "Finished gradient updates at 48.955201148986816\n",
      "Evaluated on validation set at 49.33175230026245 seconds\n",
      "Evaluated on training set at 49.6522490978241 seconds\n",
      "Appended results at 49.652475118637085 seconds\n",
      "Epoch: 16.00, Train Loss: 5.57, Val Loss: 8.59, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.83\n",
      "Sampling from training predictions...\n",
      "Source: 我们 这 有 不少 精彩 的 泰坦 泰坦尼克 坦尼 尼克\n",
      "Reference: we &apos;ve got some of the most incredible video\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0011, 0.3387, 0.6556, 0.0046, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.3584, 0.6088, 0.0312, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0022, 0.3505, 0.5933, 0.0539, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0025, 0.3404, 0.5880, 0.0688, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0026, 0.3306, 0.5884, 0.0781, 0.0003, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0025, 0.3233, 0.5896, 0.0842, 0.0004, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0025, 0.3194, 0.5899, 0.0876, 0.0005, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0026, 0.3177, 0.5895, 0.0897, 0.0005, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0026, 0.3170, 0.5890, 0.0908, 0.0006, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 他 面带 <UNK> <UNK> 笑容 这 很少 少见 因为 大部\n",
      "Reference: there was a big smile on his face which\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.0026, 0.0000, 0.0000, 0.9973, 0.0001, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0820, 0.0025, 0.0924, 0.8223, 0.0007, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0001, 0.1423, 0.0205, 0.2349, 0.6005, 0.0016, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0001, 0.1768, 0.0442, 0.2790, 0.4975, 0.0024, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0001, 0.1904, 0.0634, 0.2946, 0.4487, 0.0028, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0001, 0.1948, 0.0756, 0.3044, 0.4221, 0.0030, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0001, 0.1974, 0.0828, 0.3088, 0.4077, 0.0031, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0001, 0.2018, 0.0868, 0.3070, 0.4010, 0.0032, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0001, 0.2047, 0.0892, 0.3054, 0.3973, 0.0033, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 49.65599012374878 seconds\n",
      "Saved checkpoint at 49.910163164138794 seconds\n",
      "Finished gradient updates at 51.694520235061646\n",
      "Evaluated on validation set at 52.09888219833374 seconds\n",
      "Evaluated on training set at 52.46815323829651 seconds\n",
      "Appended results at 52.4682502746582 seconds\n",
      "Epoch: 17.00, Train Loss: 5.37, Val Loss: 8.56, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.87\n",
      "Sampling from training predictions...\n",
      "Source: <UNK> 塞尔 <UNK> <UNK> 斯特 说 过 真正 的 探索\n",
      "Reference: marcel proust said , &quot; the true voyage of\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1910, 0.8090, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.5861, 0.4138, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0005, 0.6296, 0.3699, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0006, 0.6295, 0.3698, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0006, 0.6210, 0.3783, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0007, 0.6147, 0.3846, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0007, 0.6102, 0.3890, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0007, 0.6096, 0.3896, 0.0001,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0007, 0.6095, 0.3897, 0.0001,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 那 就是 她 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: there she is . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "Attention Weights: tensor([[0.0000, 0.0000, 0.0000, 0.1066, 0.8700, 0.0233, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0005, 0.2277, 0.7219, 0.0499, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0011, 0.2723, 0.6734, 0.0531, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0015, 0.2863, 0.6568, 0.0553, 0.0001, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0016, 0.2880, 0.6521, 0.0581, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0016, 0.2878, 0.6503, 0.0601, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.2876, 0.6492, 0.0613, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.2876, 0.6484, 0.0621, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0017, 0.2881, 0.6475, 0.0625, 0.0002, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Inspect samples at 52.47285509109497 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0ef1b5bd176f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_full\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_minibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_minibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_minitrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_minitrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     lazy_eval=False, print_attn=True, inspect_samples=1)\n\u001b[0m",
      "\u001b[0;32m~/Documents/data-science-coursework/nyu-nlp/project/train_eval.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, loaders_full, loaders_minibatch, loaders_minitrain, params, vocab, lazy_eval, print_intermediate, save_checkpoint, save_to_log, inspect_samples, print_attn)\u001b[0m\n\u001b[1;32m    247\u001b[0m                         \u001b[0mcheckpoint_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_checkpoints/{}.pth.tar'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0mcheck_dir_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved checkpoint at {} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=10000, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=False, print_attn=True, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(load_experiment_log())[['dt_created', 'num_epochs', 'learning_rate', 'clip_grad_max_norm', 'val_loss']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch: 199.00, Train Loss: 0.32, Val Loss: 13.19, Train BLEU: 98.94, Val BLEU: 0.27\n",
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention energies = v_broadcast.bmm(torch.tanh(self.attn(concat)).transpose(1, 2)) # switched order  \n",
    "# Epoch: 199.00, Train Loss: 0.63, Val Loss: 12.82, Train BLEU: 92.05, Val BLEU: 0.38\n",
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(vocab[SRC_LANG]['id2token']): \n",
    "    if i < 20: \n",
    "        print(\"{}: {}\".format(i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(vocab[TARG_LANG]['id2token']): \n",
    "    if i < 20: \n",
    "        print(\"{}: {}\".format(i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.arange(0, 3*5*10).view(3, 5, 10)\n",
    "print(x)\n",
    "y = x[1:, :, :]\n",
    "print(y)\n",
    "z = y.view(-1, 10)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(0, 2*5).view(5, 2)\n",
    "print(t)\n",
    "u = t.contiguous().view(-1)\n",
    "print(u)\n",
    "v = t.permute(1, 0)\n",
    "print(v)\n",
    "w = v.contiguous().view(-1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(0, 2*1*300)\n",
    "print(a)\n",
    "b = a.view(-1, 1, 300)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loaders_full['train']):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     print(targ_idxs.size())\n",
    "#     print(targ_idxs)\n",
    "#     print(targ_lens)\n",
    "    id2token = vocab[SRC_LANG]['id2token']\n",
    "    test_tensor = src_idxs\n",
    "    list_of_lists = test_tensor.numpy().astype(int).tolist()\n",
    "    to_token = lambda l: ' '.join([id2token[idx] for idx in l])\n",
    "    list_of_lists_tokens = [to_token(l) for l in list_of_lists] \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "test_tensor.\n",
    "\n",
    "test_tensor.data.masked_fill_(test_tensor == 2, float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
