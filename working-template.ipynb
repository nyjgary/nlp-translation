{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from data_processing import generate_vocab, process_data, create_dataloaders \n",
    "from model import get_pretrained_emb, EncoderDecoder, EncoderRNN, DecoderRNN, EncoderDecoderAttn, DecoderAttnRNN\n",
    "from train_eval import count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "from train_eval import train_and_eval\n",
    "import importlib\n",
    "import pickle as pkl \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model identification\n",
    "MODEL_NAME = 'zh-seq2seq-rnn-vanilla'\n",
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "# data processing params  \n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 #30000\n",
    "TARG_VOCAB_SIZE = 30000 #30000\n",
    "\n",
    "# model architecture params \n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 #2 \n",
    "ENC_HIDDEN_DIM = 256 #512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM #2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 # to actually implement\n",
    "DEC_DROPOUT = 0.2 # to actually implement\n",
    "USE_ATTN = False\n",
    "\n",
    "# training params  \n",
    "BATCH_SIZE = 32 #32\n",
    "NUM_EPOCHS = 200\n",
    "LR = 0.0005 # 0.0005\n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, 'rnn_cell_type': RNN_CELL_TYPE, \n",
    "          'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, \n",
    "          'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "          'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, 'dec_hidden_dim': DEC_HIDDEN_DIM,\n",
    "          'teacher_forcing_ratio': TEACHER_FORCING_RATIO, 'clip_grad_max_norm': CLIP_GRAD_MAX_NORM,\n",
    "          'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, 'use_attn': USE_ATTN, \n",
    "          'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, 'learning_rate': LR, 'optimizer': OPTIMIZER, \n",
    "          'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, vocab)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, vocab, sample_limit=BATCH_SIZE) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, vocab, sample_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab-fake.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_filename = \"{}-{}-vocab-fake.p\".format(SRC_LANG, TARG_LANG)\n",
    "# vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "# data = process_data(SRC_LANG, TARG_LANG, vocab)\n",
    "# limited_data = process_data(SRC_LANG, TARG_LANG, vocab, sample_limit=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "# without attention \n",
    "decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "                     targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id'])\n",
    "\n",
    "# with attention \n",
    "# decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "#                          num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "#                          targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, \n",
    "#                          pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 10.02, Val Loss: 10.21, Train BLEU: 0.46, Val BLEU: 0.24, Minutes Elapsed: 0.05\n",
      "Sampling from training predictions...\n",
      "Source: 但 我 想 告诉 你 的 是 当 你 站\n",
      "Reference: but when you &apos;re standing at the beach ,\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 祖父 在 他 的 年代 是 位非 非凡\n",
      "Reference: my grandfather was an extraordinary man for his time\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Epoch: 1.00, Train Loss: 9.67, Val Loss: 10.07, Train BLEU: 0.32, Val BLEU: 0.22, Minutes Elapsed: 0.10\n",
      "Sampling from training predictions...\n",
      "Source: 其实 实地 地球 上 最长 的 山脉 都 在 海洋\n",
      "Reference: and in the oceans , there are the longest\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 不知 知道 那 意味 意味着 什么 但是 我 能\n",
      "Reference: i didn &apos;t know what it meant , but\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 2.00, Train Loss: 9.23, Val Loss: 9.89, Train BLEU: 0.32, Val BLEU: 0.22, Minutes Elapsed: 0.15\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 通过 潜水 潜水艇 拍下 的 影片 把 我们\n",
      "Reference: with vibrant video clips captured by submarines , david\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 这 是 我们 俩 人 唯一 的 受教 教育 方式\n",
      "Reference: it was the only way we both could be\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 3.00, Train Loss: 8.69, Val Loss: 9.68, Train BLEU: 0.32, Val BLEU: 0.22, Minutes Elapsed: 0.20\n",
      "Sampling from training predictions...\n",
      "Source: 地球 的 大部 大部分 部分 都 是 海水 <EOS> <PAD>\n",
      "Reference: most of the planet is ocean water . <EOS>\n",
      "Model: <SOS> the the the the the . . . .\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 祖父 在 他 的 年代 是 位非 非凡\n",
      "Reference: my grandfather was an extraordinary man for his time\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 4.00, Train Loss: 8.12, Val Loss: 9.45, Train BLEU: 0.28, Val BLEU: 0.21, Minutes Elapsed: 0.25\n",
      "Sampling from training predictions...\n",
      "Source: 地球 的 大部 大部分 部分 都 是 海水 <EOS> <PAD>\n",
      "Reference: most of the planet is ocean water . <EOS>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 祖父 在 他 的 年代 是 位非 非凡\n",
      "Reference: my grandfather was an extraordinary man for his time\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 5.00, Train Loss: 7.58, Val Loss: 9.24, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.31\n",
      "Sampling from training predictions...\n",
      "Source: 底下 这些 都 是 <UNK> 它们 上上 上上下下 上下 下下\n",
      "Reference: it &apos;s got these fishing <UNK> on the bottom\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 你 现在 可以 去 个 真正 的 学校 念书 了\n",
      "Reference: &quot; you can go to a real school now\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 6.00, Train Loss: 7.08, Val Loss: 9.05, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.37\n",
      "Sampling from training predictions...\n",
      "Source: 地球 的 大部 大部分 部分 都 是 海水 <EOS> <PAD>\n",
      "Reference: most of the planet is ocean water . <EOS>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 他们 知道 我们 的 住处 吗 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: do they know where we live ? <EOS> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 7.00, Train Loss: 6.62, Val Loss: 8.90, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.41\n",
      "Sampling from training predictions...\n",
      "Source: 我们 这 有 不少 精彩 的 泰坦 泰坦尼克 坦尼 尼克\n",
      "Reference: we &apos;ve got some of the most incredible video\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 一个 真正 的 学校 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: a real school . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 8.00, Train Loss: 6.20, Val Loss: 8.77, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.47\n",
      "Sampling from training predictions...\n",
      "Source: 原因 在于 我们 一直 没 把 海洋 当回事 回事 回事儿\n",
      "Reference: and the problem , i think , is that\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 不知 知道 那 意味 意味着 什么 但是 我 能\n",
      "Reference: i didn &apos;t know what it meant , but\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 9.00, Train Loss: 5.82, Val Loss: 8.68, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.52\n",
      "Sampling from training predictions...\n",
      "Source: 深海 海中 的 生命 大卫 <UNK> <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: life in the deep oceans <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 11 岁 那年 记得 得有 一天 早晨 醒来 听见\n",
      "Reference: when i was 11 , i remember waking up\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 10.00, Train Loss: 5.49, Val Loss: 8.62, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.57\n",
      "Sampling from training predictions...\n",
      "Source: <UNK> 塞尔 <UNK> <UNK> 斯特 说 过 真正 的 探索\n",
      "Reference: marcel proust said , &quot; the true voyage of\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 在 塔利 塔利班 控制 阿富汗 的 那些 年 我 记得\n",
      "Reference: during taliban years , i remember there were times\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 11.00, Train Loss: 5.20, Val Loss: 8.61, Train BLEU: 0.28, Val BLEU: 0.19, Minutes Elapsed: 0.62\n",
      "Sampling from training predictions...\n",
      "Source: 这 是 一种 种群 栖 动物 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: it &apos;s a colonial animal . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 不知 知道 那 意味 意味着 什么 但是 我 能\n",
      "Reference: i didn &apos;t know what it meant , but\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 12.00, Train Loss: 4.95, Val Loss: 8.63, Train BLEU: 0.28, Val BLEU: 0.20, Minutes Elapsed: 0.67\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 这位 是 比尔 <UNK> 我 是 大卫 <UNK>\n",
      "Reference: this is bill lange . i &apos;m dave gallo\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人\n",
      "Reference: the school was in a house , more than\n",
      "Model: <SOS> the the the the the the the the the\n",
      "\n",
      "Epoch: 13.00, Train Loss: 4.75, Val Loss: 8.68, Train BLEU: 0.35, Val BLEU: 0.24, Minutes Elapsed: 0.71\n",
      "Sampling from training predictions...\n",
      "Source: 海洋 的 平均 深度 是 两英里 英里 <EOS> <PAD> <PAD>\n",
      "Reference: the average depth is about two miles . <EOS>\n",
      "Model: <SOS> it the the the . . . . .\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 想 过 要 放弃 但 我 的 父亲 这时\n",
      "Reference: i would want to quit , but my father\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 14.00, Train Loss: 4.59, Val Loss: 8.77, Train BLEU: 0.36, Val BLEU: 0.24, Minutes Elapsed: 0.76\n",
      "Sampling from training predictions...\n",
      "Source: 大多 大多数 多数 地震 和 火山 喷发 也 都 发生\n",
      "Reference: most of the earthquakes and volcanoes are in the\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 11 岁 那年 记得 得有 一天 早晨 醒来 听见\n",
      "Reference: when i was 11 , i remember waking up\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 15.00, Train Loss: 4.45, Val Loss: 8.87, Train BLEU: 0.36, Val BLEU: 0.24, Minutes Elapsed: 0.80\n",
      "Sampling from training predictions...\n",
      "Source: 大部 大部分 部分 的 动物 也 都 生活 在 海洋\n",
      "Reference: most of the animals are in the oceans .\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 想 过 要 放弃 但 我 的 父亲 这时\n",
      "Reference: i would want to quit , but my father\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 16.00, Train Loss: 4.34, Val Loss: 8.99, Train BLEU: 0.34, Val BLEU: 0.24, Minutes Elapsed: 0.85\n",
      "Sampling from training predictions...\n",
      "Source: 我们 将 用 一些 影片 来讲 讲述 一些 深海 海里\n",
      "Reference: and we &apos;re going to tell you some stories\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 但是 我 那 受过 教育 的 母亲 成为 为了 一名\n",
      "Reference: but my educated mother became a teacher . <EOS>\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 17.00, Train Loss: 4.26, Val Loss: 9.12, Train BLEU: 0.37, Val BLEU: 0.32, Minutes Elapsed: 0.89\n",
      "Sampling from training predictions...\n",
      "Source: 还有 前面 的 这个 是 推进 引擎 它 一会 一会儿\n",
      "Reference: and it &apos;s got these jet thrusters up in\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 11 岁 那年 记得 得有 一天 早晨 醒来 听见\n",
      "Reference: when i was 11 , i remember waking up\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18.00, Train Loss: 4.19, Val Loss: 9.26, Train BLEU: 0.35, Val BLEU: 0.26, Minutes Elapsed: 0.93\n",
      "Sampling from training predictions...\n",
      "Source: 这 是 一只 水母 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: here &apos;s a jelly . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it it the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 那 就是 她 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: there she is . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it it . <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Epoch: 19.00, Train Loss: 4.14, Val Loss: 9.40, Train BLEU: 0.35, Val BLEU: 0.21, Minutes Elapsed: 0.98\n",
      "Sampling from training predictions...\n",
      "Source: 我们 用 的 是 深海 潜水 潜水艇 <UNK> 号 和\n",
      "Reference: we use the submarine alvin and we use cameras\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 总是 担心 会 被 塔利 塔利班 发现 <EOS> <PAD>\n",
      "Reference: we always wondered what they knew about us .\n",
      "Model: <SOS> it it the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Epoch: 20.00, Train Loss: 4.10, Val Loss: 9.54, Train BLEU: 3.77, Val BLEU: 0.22, Minutes Elapsed: 1.02\n",
      "Sampling from training predictions...\n",
      "Source: 大部 大部分 部分 的 动物 也 都 生活 在 海洋\n",
      "Reference: most of the animals are in the oceans .\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 他 出身 阿富汗 边远 <UNK> 地区 有着 与 他人 不同\n",
      "Reference: a total maverick from a remote province of afghanistan\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 21.00, Train Loss: 4.06, Val Loss: 9.67, Train BLEU: 3.77, Val BLEU: 0.22, Minutes Elapsed: 1.07\n",
      "Sampling from training predictions...\n",
      "Source: 我们 得用 非常 特殊 的 仪器 才能 能到 到达 那个\n",
      "Reference: we have to have a very special technology to\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 在 被 跟踪 吗 <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: were we being followed ? <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it &apos;s the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Epoch: 22.00, Train Loss: 4.03, Val Loss: 9.80, Train BLEU: 3.77, Val BLEU: 0.22, Minutes Elapsed: 1.11\n",
      "Sampling from training predictions...\n",
      "Source: 海洋 的 平均 深度 是 两英里 英里 <EOS> <PAD> <PAD>\n",
      "Reference: the average depth is about two miles . <EOS>\n",
      "Model: <SOS> it &apos;s the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 都 知道 自己 正 冒 着 生命 的 危险\n",
      "Reference: we all knew we were risking our lives --\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 23.00, Train Loss: 4.00, Val Loss: 9.92, Train BLEU: 3.77, Val BLEU: 0.22, Minutes Elapsed: 1.16\n",
      "Sampling from training predictions...\n",
      "Source: <UNK> 塞尔 <UNK> <UNK> 斯特 说 过 真正 的 探索\n",
      "Reference: marcel proust said , &quot; the true voyage of\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 很 幸运 幸运地 成长 在 一个 珍视 教育 也\n",
      "Reference: i was very lucky to grow up in a\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 24.00, Train Loss: 3.97, Val Loss: 10.03, Train BLEU: 3.97, Val BLEU: 0.26, Minutes Elapsed: 1.21\n",
      "Sampling from training predictions...\n",
      "Source: 当 你 站 在 海滩 上 或是 当 你 看到\n",
      "Reference: part of the problem , i think , is\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 父亲 那 就是 他 他 是 他家 家族\n",
      "Reference: and my father -- that &apos;s him -- he\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 25.00, Train Loss: 3.95, Val Loss: 10.13, Train BLEU: 3.94, Val BLEU: 0.25, Minutes Elapsed: 1.25\n",
      "Sampling from training predictions...\n",
      "Source: 这 是 一只 水母 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: here &apos;s a jelly . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it &apos;s the . . <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 祖父 在 他 的 年代 是 位非 非凡\n",
      "Reference: my grandfather was an extraordinary man for his time\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 26.00, Train Loss: 3.92, Val Loss: 10.23, Train BLEU: 3.31, Val BLEU: 0.23, Minutes Elapsed: 1.30\n",
      "Sampling from training predictions...\n",
      "Source: 这儿 基本 基本上 都 没有 被 开发 发过 但是 像\n",
      "Reference: it &apos;s mostly unexplored , and yet there are\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 不知 知道 那 意味 意味着 什么 但是 我 能\n",
      "Reference: i didn &apos;t know what it meant , but\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 27.00, Train Loss: 3.90, Val Loss: 10.33, Train BLEU: 3.02, Val BLEU: 0.22, Minutes Elapsed: 1.34\n",
      "Sampling from training predictions...\n",
      "Source: 和 我们 合作 的 人们 帮 我们 找到 了 新\n",
      "Reference: people that have partnered with us have given us\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 6 岁 那年 塔利 塔利班 占领 阿富汗 并 规定\n",
      "Reference: you see , i was six when the taliban\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 28.00, Train Loss: 3.88, Val Loss: 10.42, Train BLEU: 3.02, Val BLEU: 0.22, Minutes Elapsed: 1.38\n",
      "Sampling from training predictions...\n",
      "Source: 其实 它们 都 是 由 单独 的 动物 结合 合在\n",
      "Reference: these are all individual animals banding together to make\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 把 书 放在 食品 杂货 袋中 这样 别人 就\n",
      "Reference: we would cover our books in grocery bags so\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 29.00, Train Loss: 3.86, Val Loss: 10.50, Train BLEU: 3.02, Val BLEU: 0.22, Minutes Elapsed: 1.43\n",
      "Sampling from training predictions...\n",
      "Source: 它 可以 伸展 <UNK> 150 英尺 长 <EOS> <PAD> <PAD>\n",
      "Reference: it gets up to about 150 feet long .\n",
      "Model: <SOS> it &apos;s the . . . . . .\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人\n",
      "Reference: the school was in a house , more than\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 30.00, Train Loss: 3.84, Val Loss: 10.58, Train BLEU: 3.02, Val BLEU: 0.22, Minutes Elapsed: 1.47\n",
      "Sampling from training predictions...\n",
      "Source: 其实 它们 都 是 由 单独 的 动物 结合 合在\n",
      "Reference: these are all individual animals banding together to make\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 的 祖父 在 他 的 年代 是 位非 非凡\n",
      "Reference: my grandfather was an extraordinary man for his time\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 31.00, Train Loss: 3.82, Val Loss: 10.66, Train BLEU: 3.02, Val BLEU: 0.22, Minutes Elapsed: 1.51\n",
      "Sampling from training predictions...\n",
      "Source: 原因 在于 我们 一直 没 把 海洋 当回事 回事 回事儿\n",
      "Reference: and the problem , i think , is that\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 很 害怕 但是 我们 仍然 渴望 望去 学校 <EOS>\n",
      "Reference: we were scared , but still , school was\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 32.00, Train Loss: 3.80, Val Loss: 10.73, Train BLEU: 3.02, Val BLEU: 0.22, Minutes Elapsed: 1.56\n",
      "Sampling from training predictions...\n",
      "Source: 这儿 基本 基本上 都 没有 被 开发 发过 但是 像\n",
      "Reference: it &apos;s mostly unexplored , and yet there are\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 每天 要 走 不同 的 路线 这样 才 没有\n",
      "Reference: each day , we took a different route so\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 33.00, Train Loss: 3.78, Val Loss: 10.79, Train BLEU: 3.02, Val BLEU: 0.22, Minutes Elapsed: 1.60\n",
      "Sampling from training predictions...\n",
      "Source: 看到 这些 在 动 的 东西 了 吗 <EOS> <PAD>\n",
      "Reference: but see all those different working things ? <EOS>\n",
      "Model: <SOS> it &apos;s the . . . . . .\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 因此 毫无 毫无疑问 无疑 疑问 他 的 孩子 应当 受到\n",
      "Reference: there was no question that his children would receive\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 34.00, Train Loss: 3.76, Val Loss: 10.85, Train BLEU: 3.03, Val BLEU: 0.22, Minutes Elapsed: 1.64\n",
      "Sampling from training predictions...\n",
      "Source: 它 可以 伸展 <UNK> 150 英尺 长 <EOS> <PAD> <PAD>\n",
      "Reference: it gets up to about 150 feet long .\n",
      "Model: <SOS> it &apos;s the . . . . . .\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人\n",
      "Reference: the school was in a house , more than\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 35.00, Train Loss: 3.74, Val Loss: 10.91, Train BLEU: 3.96, Val BLEU: 0.24, Minutes Elapsed: 1.69\n",
      "Sampling from training predictions...\n",
      "Source: 大卫 <UNK> 这位 是 比尔 <UNK> 我 是 大卫 <UNK>\n",
      "Reference: this is bill lange . i &apos;m dave gallo\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 在 塔利 塔利班 控制 阿富汗 的 那些 年 我 记得\n",
      "Reference: during taliban years , i remember there were times\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36.00, Train Loss: 3.72, Val Loss: 10.96, Train BLEU: 6.78, Val BLEU: 0.25, Minutes Elapsed: 1.80\n",
      "Sampling from training predictions...\n",
      "Source: 我们 用 的 是 深海 潜水 潜水艇 <UNK> 号 和\n",
      "Reference: we use the submarine alvin and we use cameras\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 在 塔利 塔利班 控制 阿富汗 的 那些 年 我 记得\n",
      "Reference: during taliban years , i remember there were times\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Epoch: 37.00, Train Loss: 3.70, Val Loss: 11.01, Train BLEU: 6.88, Val BLEU: 0.25, Minutes Elapsed: 1.89\n",
      "Sampling from training predictions...\n",
      "Source: 还有 这些 摇晃 着 旋转 转着 的 触角 <EOS> <PAD>\n",
      "Reference: it &apos;s got tentacles dangling , swirling around like\n",
      "Model: <SOS> it &apos;s the . <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 但是 我 那 受过 教育 的 母亲 成为 为了 一名\n",
      "Reference: but my educated mother became a teacher . <EOS>\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Epoch: 38.00, Train Loss: 3.69, Val Loss: 11.06, Train BLEU: 6.89, Val BLEU: 0.26, Minutes Elapsed: 1.95\n",
      "Sampling from training predictions...\n",
      "Source: 当 你 站 在 海滩 上 或是 当 你 看到\n",
      "Reference: part of the problem , i think , is\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我 想 过 要 放弃 但 我 的 父亲 这时\n",
      "Reference: i would want to quit , but my father\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Epoch: 39.00, Train Loss: 3.67, Val Loss: 11.10, Train BLEU: 6.89, Val BLEU: 0.26, Minutes Elapsed: 2.01\n",
      "Sampling from training predictions...\n",
      "Source: 大家 想想 海洋 占 了 地球 球面 面积 的 75\n",
      "Reference: when you think about it , the oceans are\n",
      "Model: <SOS> it the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 每天 要 走 不同 的 路线 这样 才 没有\n",
      "Reference: each day , we took a different route so\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Epoch: 40.00, Train Loss: 3.65, Val Loss: 11.15, Train BLEU: 6.89, Val BLEU: 0.26, Minutes Elapsed: 2.06\n",
      "Sampling from training predictions...\n",
      "Source: 这 是 一只 水母 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: here &apos;s a jelly . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it &apos;s the . <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 对 他 来说 孩子 不 接受 受教 教育 <UNK> 是\n",
      "Reference: to him , there was greater risk in not\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Epoch: 41.00, Train Loss: 3.63, Val Loss: 11.19, Train BLEU: 6.94, Val BLEU: 0.26, Minutes Elapsed: 2.12\n",
      "Sampling from training predictions...\n",
      "Source: 我们 将 用 一些 影片 来讲 讲述 一些 深海 海里\n",
      "Reference: and we &apos;re going to tell you some stories\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 他 面带 <UNK> <UNK> 笑容 这 很少 少见 因为 大部\n",
      "Reference: there was a big smile on his face which\n",
      "Model: <SOS> and the the the the the the , ,\n",
      "\n",
      "Epoch: 42.00, Train Loss: 3.61, Val Loss: 11.22, Train BLEU: 6.94, Val BLEU: 0.26, Minutes Elapsed: 2.25\n",
      "Sampling from training predictions...\n",
      "Source: 原因 在于 我们 一直 没 把 海洋 当回事 回事 回事儿\n",
      "Reference: and the problem , i think , is that\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 冬天 很 舒服 但 夏天 却 <UNK> <EOS> <PAD> <PAD>\n",
      "Reference: it was cozy in winter but extremely hot in\n",
      "Model: <SOS> it &apos;s the . <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Epoch: 43.00, Train Loss: 3.59, Val Loss: 11.25, Train BLEU: 6.95, Val BLEU: 0.26, Minutes Elapsed: 2.38\n",
      "Sampling from training predictions...\n",
      "Source: 和 我们 合作 的 人们 帮 我们 找到 了 新\n",
      "Reference: people that have partnered with us have given us\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 把 书 放在 食品 杂货 袋中 这样 别人 就\n",
      "Reference: we would cover our books in grocery bags so\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Epoch: 44.00, Train Loss: 3.58, Val Loss: 11.28, Train BLEU: 6.95, Val BLEU: 0.30, Minutes Elapsed: 2.46\n",
      "Sampling from training predictions...\n",
      "Source: 它 可以 伸展 <UNK> 150 英尺 长 <EOS> <PAD> <PAD>\n",
      "Reference: it gets up to about 150 feet long .\n",
      "Model: <SOS> it &apos;s the . . <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 学校 在 一个 房屋 房屋里 屋里 我们 100 多 人\n",
      "Reference: the school was in a house , more than\n",
      "Model: <SOS> and the the the the the the the the\n",
      "\n",
      "Epoch: 45.00, Train Loss: 3.56, Val Loss: 11.31, Train BLEU: 6.95, Val BLEU: 0.33, Minutes Elapsed: 2.54\n",
      "Sampling from training predictions...\n",
      "Source: 其实 它们 都 是 由 单独 的 动物 结合 合在\n",
      "Reference: these are all individual animals banding together to make\n",
      "Model: <SOS> and the the the the , , , ,\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 她 两年 年前 退休 了 结果 却 把 我家 变成\n",
      "Reference: she retired two years ago , only to turn\n",
      "Model: <SOS> and the the the the the the the ,\n",
      "\n",
      "Epoch: 46.00, Train Loss: 3.54, Val Loss: 11.33, Train BLEU: 6.96, Val BLEU: 0.33, Minutes Elapsed: 2.61\n",
      "Sampling from training predictions...\n",
      "Source: 泰坦 泰坦尼克 泰坦尼克号 坦尼 尼克 号 是 拿 了 不少\n",
      "Reference: the truth of the matter is that the titanic\n",
      "Model: <SOS> and the the the the the the , ,\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 我们 总是 担心 会 被 塔利 塔利班 发现 <EOS> <PAD>\n",
      "Reference: we always wondered what they knew about us .\n",
      "Model: <SOS> it &apos;s the . . <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Epoch: 47.00, Train Loss: 3.52, Val Loss: 11.35, Train BLEU: 6.96, Val BLEU: 0.33, Minutes Elapsed: 2.67\n",
      "Sampling from training predictions...\n",
      "Source: 我们 将 用 一些 影片 来讲 讲述 一些 深海 海里\n",
      "Reference: and we &apos;re going to tell you some stories\n",
      "Model: <SOS> and the the the the the the , ,\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 因此 毫无 毫无疑问 无疑 疑问 他 的 孩子 应当 受到\n",
      "Reference: there was no question that his children would receive\n",
      "Model: <SOS> and the the the the the the , ,\n",
      "\n",
      "Epoch: 48.00, Train Loss: 3.50, Val Loss: 11.37, Train BLEU: 6.96, Val BLEU: 0.33, Minutes Elapsed: 2.72\n",
      "Sampling from training predictions...\n",
      "Source: 当 你 站 在 海滩 上 或是 当 你 看到\n",
      "Reference: part of the problem , i think , is\n",
      "Model: <SOS> and the the the the the the , ,\n",
      "\n",
      "Sampling from val predictions...\n",
      "Source: 所以 在 那 之后 5 年 我 <UNK> <UNK> 陪\n",
      "Reference: so for the next five years , i dressed\n",
      "Model: <SOS> and the the the the the the , ,\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0946aa5c63e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_full\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_minibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_minibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_minitrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_minitrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     lazy_eval=False, print_attn=False, inspect_samples=1)\n\u001b[0m",
      "\u001b[0;32m~/Documents/data-science-coursework/nyu-nlp/project/train_eval.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, loaders_full, loaders_minibatch, loaders_minitrain, params, vocab, lazy_eval, print_intermediate, save_checkpoint, save_to_log, inspect_samples, print_attn)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# calculate metrics on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_bleu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hyp_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ref_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_source_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hyp_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ref_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_source_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_id2token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_id2token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# calculate metrics on train set (or proxy thereof) only if lazy_eval not set to True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlazy_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/data-science-coursework/nyu-nlp/project/train_eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, src_id2token, targ_id2token, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         outputs, hypotheses, attn_weights = model(src_idxs, targ_idxs, src_lens, targ_lens, \n\u001b[0;32m--> 106\u001b[0;31m             teacher_forcing_ratio=teacher_forcing_ratio)\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/data-science-coursework/nyu-nlp/project/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarg_max_sentence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                         \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                         \u001b[0mdec_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                         \u001b[0mteacher_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/data-science-coursework/nyu-nlp/project/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dec_input, dec_hidden, enc_outputs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=True, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=False, print_attn=False, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(load_experiment_log())[['dt_created', 'num_epochs', 'learning_rate', 'clip_grad_max_norm', 'val_loss']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch: 199.00, Train Loss: 0.32, Val Loss: 13.19, Train BLEU: 98.94, Val BLEU: 0.27\n",
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention energies = v_broadcast.bmm(torch.tanh(self.attn(concat)).transpose(1, 2)) # switched order  \n",
    "# Epoch: 199.00, Train Loss: 0.63, Val Loss: 12.82, Train BLEU: 92.05, Val BLEU: 0.38\n",
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(vocab[SRC_LANG]['id2token']): \n",
    "    if i < 20: \n",
    "        print(\"{}: {}\".format(i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(vocab[TARG_LANG]['id2token']): \n",
    "    if i < 20: \n",
    "        print(\"{}: {}\".format(i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.arange(0, 3*5*10).view(3, 5, 10)\n",
    "print(x)\n",
    "y = x[1:, :, :]\n",
    "print(y)\n",
    "z = y.view(-1, 10)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(0, 2*5).view(5, 2)\n",
    "print(t)\n",
    "u = t.contiguous().view(-1)\n",
    "print(u)\n",
    "v = t.permute(1, 0)\n",
    "print(v)\n",
    "w = v.contiguous().view(-1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(0, 2*1*300)\n",
    "print(a)\n",
    "b = a.view(-1, 1, 300)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(full_loaders['train']):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     print(targ_idxs.size())\n",
    "#     print(targ_idxs)\n",
    "#     print(targ_lens)\n",
    "    id2token = vocab[SRC_LANG]['id2token']\n",
    "    test_tensor = src_idxs\n",
    "    list_of_lists = test_tensor.numpy().astype(int).tolist()\n",
    "    to_token = lambda l: ' '.join([id2token[idx] for idx in l])\n",
    "    list_of_lists_tokens = [to_token(l) for l in list_of_lists] \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
