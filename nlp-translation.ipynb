{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "BATCH_SIZE = 32\n",
    "SRC_MAX_SENTENCE_LEN = 20 \n",
    "TARG_MAX_SENTENCE_LEN = 20\n",
    "SRC_VOCAB_SIZE = 1000\n",
    "TARG_VOCAB_SIZE = 1000\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_en = load_word2vec('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = load_word2vec('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'test': 2, 'hi': 1})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = ['test', 'test', 'hi', 'bye']\n",
    "best_words = ['test', 'hi']\n",
    "counter = Counter(test_words)\n",
    "Counter({token: counter[token] for token in counter if token in best_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海', '海中', '的', '生命', '大卫']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = data['train']['source']['tokens'][0] \n",
    "[w for w in test_words if w in word2vec_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]       \n",
    "        tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(len(RESERVED_TOKENS), len(RESERVED_TOKENS)+len(vocab))))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + id2token \n",
    "    for t in RESERVED_TOKENS: \n",
    "        token2id[t] = RESERVED_TOKENS[t]\n",
    "    return token2id, id2token \n",
    "\n",
    "# def build_vocab(token_lists, max_vocab_size): \n",
    "#     \"\"\" Takes lists of tokens (representing sentences of words) and max_vocab_size and returns: \n",
    "#         - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "#         - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "#     \"\"\"\n",
    "#     all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "#     token_counter = Counter(all_tokens)\n",
    "#     vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "#     id2token = list(vocab)\n",
    "#     token2id = dict(zip(vocab, range(len(RESERVED_TOKENS), len(RESERVED_TOKENS)+len(vocab))))\n",
    "#     id2token = list(RESERVED_TOKENS.keys()) + id2token \n",
    "#     for t in RESERVED_TOKENS: \n",
    "#         token2id[t] = RESERVED_TOKENS[t]\n",
    "#     return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "    \n",
    "# def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "#     \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "#         and returns as a nested dictionary containing: \n",
    "#         - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "#         - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "#         - source language's token2id and id2token \n",
    "#         - target language's token2id and id2token\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # get filepaths \n",
    "#     data = get_filepaths(src_lang, targ_lang)\n",
    "#     data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "#     data['train']['target']['max_vocab_size'] = targ_max_vocab_size    \n",
    "    \n",
    "#     # loop through each file, read in text, convert to tokens, then to indices \n",
    "#     for split in ['train', 'dev', 'test']: \n",
    "#         for lang_type in ['source', 'target']: \n",
    "            \n",
    "#             # read in tokens \n",
    "#             data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'])\n",
    "            \n",
    "#             # build vocab from training data\n",
    "#             if split == 'train': \n",
    "#                 data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "#                     data['train'][lang_type]['tokens'], data['train'][lang_type]['max_vocab_size']) \n",
    "                \n",
    "#             # convert tokens to indices \n",
    "#             data[split][lang_type]['indices'] = tokens2indices(\n",
    "#                 data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "#     return data\n",
    "\n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # attach vocab sizes and word2vec models \n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size \n",
    "    data['train']['source']['word2vec'] = load_word2vec(src_lang) \n",
    "    data['train']['target']['word2vec'] = load_word2vec(targ_lang) \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'])\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    token_lists = data['train'][lang_type]['tokens'], \n",
    "                    max_vocab_size = data['train'][lang_type]['max_vocab_size'], \n",
    "                    word2vec = data['train'][lang_type]['word2vec']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海', '海中', '的', '生命', '大卫', '盖罗', '<EOS>']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values= 0)\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values= 0)\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 20])\n",
      "tensor([[  3,   3,   4, 204,   3,   3,   1,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  3,   3, 124,   3,   3,   3,   4,   3,  39,   7,   3,   9, 290, 118,\n",
      "           3, 118,   3, 198,  23,   3],\n",
      "        [  3,   3,   3,   6,   3,   3,   5,   6,   3,   3,   1,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  7,  52,  84,  67,   3,   3, 791,  67,   3,   3,   4, 146,   1,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  7,  11,  15,   3,   3,   4,   3,   3,   3,   3,   4,   3,   3, 928,\n",
      "         195,   3, 457,   1,   0,   0],\n",
      "        [  3,   3,   3,   3,   3,   3,   6, 597,   9,   3,   3,   3,  36, 142,\n",
      "           3,  16,  71,  55, 144,   3],\n",
      "        [282, 470,   7, 227, 384,  39, 585,   3,   3,   3,   3,   1,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [116, 852, 585,   3,   9, 290,   3,   3,   4,   3,   1,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [290,   4, 583, 596, 134,  22,   6,   3,   1,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [585,   4, 876,   3,   6,   3, 821,   1,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [ 56,  10, 651,   8,   3,  37, 528,  56,  10,  72, 585,  78,   4, 897,\n",
      "          56,  10,   3, 202,   3,   3],\n",
      "        [242,   3, 290,  37,   3,   4,   3,  22,   8, 585,  78,   1,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [583, 596, 134,   4, 248,  23,  22, 114,   8, 585,  78,   1,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [414,   3, 381,   3,  14,   3,   3,  23,  22, 106,   3, 585,  78,   8,\n",
      "         585,   4, 118,   3,   1,   0],\n",
      "        [585,  78, 192,   4,   3,   3,  14,   3,  51, 185,   3,   3,  82, 339,\n",
      "           1,   0,   0,   0,   0,   0],\n",
      "        [702, 325, 745,  22,  38,  45, 710,   3,  53,  81,  24, 951,   4,   3,\n",
      "          16, 924,   3,   7,  71,  45],\n",
      "        [ 36,   5,  49, 131,  10,   4,   6,  56,  10, 651,   8,   3,  91,  10,\n",
      "         667,   4,   6,  12, 206,   3],\n",
      "        [  7,   3,  64,   3,   4,   3, 406,   3, 929, 172,   3,   4,  60,   1,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  7,  84,   4,   6,   3,   3,   3,   3,   3,  14,   3,   3,   3,   3,\n",
      "           6,   3,   3,  14,   3, 693],\n",
      "        [  3,   3,   3,   3,   3,  26,  96, 209,   4, 672,   3,  55, 188, 156,\n",
      "           4, 100, 308, 188, 237, 156],\n",
      "        [ 14,   7, 386,   4,  76, 916,   7, 237,   9, 156,   4,   3,   7, 299,\n",
      "          72,   9,   3,  88, 208,   4],\n",
      "        [ 11,   6, 570,   3,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  6,   5, 118, 196,   4,  31,  16, 845,  22,   3,   1,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  3,  16,   6, 585,   3,   3,   4, 192,   1,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [ 16,  21,   3,   3,   3, 942, 451,   1,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [ 72,  24,   8,   3,   4,  87,   9, 121,   1,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  5,   3, 196,  24,  87,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  3,  24,  22,   6,   3,  65,   3,   3,   3,   3,   4,   3,   1,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [119,  24,   3,  79,   3,   3,   4,   3,   1,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [ 11,   6,  86,   3,   3, 248,   1,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [242,  65,  22,   6, 243,   3,   4, 248, 965,   3,   3, 153,  17, 574,\n",
      "           9,  46, 178,   4,  12, 192],\n",
      "        [119,   3,   4,  18,   6,   3,   3,  16,   3,   3,   3,   3,   3,  16,\n",
      "         119,  67, 891,   1,   0,   0]])\n",
      "tensor([ 7, 20, 11, 13, 18, 20, 12, 11,  9,  8, 20, 12, 12, 19, 15, 20, 20, 14,\n",
      "        20, 20, 20,  5, 11,  9,  8,  9,  6, 13,  9,  7, 20, 18])\n"
     ]
    }
   ],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#        embedded = self.embedding(input).view(1, 1, -1)\n",
    "#        print(input)\n",
    "#         print(input.size())\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input).view(SRC_MAX_SENTENCE_LEN, batch_size, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         print(\"Before embedding, targ[i-1] size is: {}\".format(input.size()))\n",
    "#         print(\"Before embedding, targ[i-1] is: {}\".format(input))     \n",
    "        batch_size = input.size()[0]\n",
    "        output = self.embedding(input).view(1, batch_size, -1)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "        output = F.relu(output)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "#         print(\"hidden size is: {}\".format(hidden.size()))        \n",
    "#         print(\"hidden is: {}\".format(hidden))        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))       \n",
    "#         output = output.squeeze(0) # B x N\n",
    "#         output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)\n",
    "    \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_size = self.decoder.output_size\n",
    "        \n",
    "    def forward(self, src, targ): \n",
    "        batch_size = src.size()[0]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_output, encoder_hidden = self.encoder(src, encoder_hidden)\n",
    "        decoder_hidden = encoder_hidden \n",
    "#        print(\"Encoder Hidden size: {}\".format(encoder_hidden.size()))\n",
    "        final_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size, self.output_size))\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "#            print(\"targ[di-1] Size is {}\".format(targ[:, di-1].size()))\n",
    "            decoder_outputs, decoder_hidden = self.decoder(targ[:, di-1], decoder_hidden)\n",
    "#            print(\"Final Outputs {} is {}\".format(di, decoder_outputs))\n",
    "            top1 = decoder_outputs.data.max(1)[1]\n",
    "#            print(\"Top 1 is {}\".format(top1))\n",
    "            final_outputs[di] = decoder_outputs\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for src_idxs, targ_idxs, src_lens, targ_lens in loader: \n",
    "        input_len = src_idxs.size()[0]\n",
    "        final_outputs = model(src_idxs, targ_idxs) \n",
    "        loss = criterion(final_outputs.view(input_len, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n",
    "                         targ_idxs.view(input_len, TARG_MAX_SENTENCE_LEN))\n",
    "        total_loss += loss.item()  \n",
    "    \n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, dev_loader, num_epochs=3, learning_rate=0.1, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs = model(src_idxs, targ_idxs) \n",
    "            loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n",
    "                             targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 10 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "#                result['train_loss'] = evaluate(model, train_loader) \n",
    "                result['val_loss'] = evaluate(model, dev_loader)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Validation Loss: {:.2f}'.format(\n",
    "                        result['epoch'], result['val_loss']))\n",
    "            \n",
    "        print(\"Epoch 1 {}: Total loss is {}\".format(i, total_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Validation Loss: 267.89\n",
      "Epoch: 0.00, Validation Loss: 377.74\n",
      "Epoch: 0.00, Validation Loss: 422.17\n",
      "Epoch: 0.00, Validation Loss: 432.80\n",
      "Epoch: 0.01, Validation Loss: 438.60\n",
      "Epoch: 0.01, Validation Loss: 433.59\n",
      "Epoch: 0.01, Validation Loss: 413.90\n",
      "Epoch: 0.01, Validation Loss: 388.71\n",
      "Epoch: 0.01, Validation Loss: 378.67\n",
      "Epoch: 0.01, Validation Loss: 372.65\n",
      "Epoch: 0.01, Validation Loss: 372.56\n",
      "Epoch: 0.02, Validation Loss: 367.50\n",
      "Epoch: 0.02, Validation Loss: 380.68\n",
      "Epoch: 0.02, Validation Loss: 363.58\n",
      "Epoch: 0.02, Validation Loss: 353.12\n",
      "Epoch: 0.02, Validation Loss: 349.28\n",
      "Epoch: 0.02, Validation Loss: 339.12\n",
      "Epoch: 0.03, Validation Loss: 343.51\n",
      "Epoch: 0.03, Validation Loss: 344.31\n",
      "Epoch: 0.03, Validation Loss: 338.64\n",
      "Epoch: 0.03, Validation Loss: 332.84\n",
      "Epoch: 0.03, Validation Loss: 330.53\n",
      "Epoch: 0.03, Validation Loss: 330.64\n",
      "Epoch: 0.03, Validation Loss: 332.15\n",
      "Epoch: 0.04, Validation Loss: 352.82\n",
      "Epoch: 0.04, Validation Loss: 343.39\n",
      "Epoch: 0.04, Validation Loss: 331.66\n",
      "Epoch: 0.04, Validation Loss: 324.85\n",
      "Epoch: 0.04, Validation Loss: 329.36\n",
      "Epoch: 0.04, Validation Loss: 329.57\n",
      "Epoch: 0.04, Validation Loss: 332.48\n",
      "Epoch: 0.05, Validation Loss: 319.70\n",
      "Epoch: 0.05, Validation Loss: 314.31\n",
      "Epoch: 0.05, Validation Loss: 307.35\n",
      "Epoch: 0.05, Validation Loss: 303.50\n",
      "Epoch: 0.05, Validation Loss: 303.15\n",
      "Epoch: 0.05, Validation Loss: 302.93\n",
      "Epoch: 0.06, Validation Loss: 303.24\n",
      "Epoch: 0.06, Validation Loss: 311.94\n",
      "Epoch: 0.06, Validation Loss: 304.51\n",
      "Epoch: 0.06, Validation Loss: 303.76\n",
      "Epoch: 0.06, Validation Loss: 304.24\n",
      "Epoch: 0.06, Validation Loss: 309.83\n",
      "Epoch: 0.06, Validation Loss: 308.27\n",
      "Epoch: 0.07, Validation Loss: 306.84\n",
      "Epoch: 0.07, Validation Loss: 308.13\n",
      "Epoch: 0.07, Validation Loss: 303.29\n",
      "Epoch: 0.07, Validation Loss: 306.45\n",
      "Epoch: 0.07, Validation Loss: 310.26\n",
      "Epoch: 0.07, Validation Loss: 300.77\n",
      "Epoch: 0.07, Validation Loss: 303.04\n",
      "Epoch: 0.08, Validation Loss: 296.93\n",
      "Epoch: 0.08, Validation Loss: 296.71\n",
      "Epoch: 0.08, Validation Loss: 299.32\n",
      "Epoch: 0.08, Validation Loss: 303.73\n",
      "Epoch: 0.08, Validation Loss: 300.24\n",
      "Epoch: 0.08, Validation Loss: 297.70\n",
      "Epoch: 0.09, Validation Loss: 301.34\n",
      "Epoch: 0.09, Validation Loss: 301.29\n",
      "Epoch: 0.09, Validation Loss: 302.90\n",
      "Epoch: 0.09, Validation Loss: 297.61\n",
      "Epoch: 0.09, Validation Loss: 297.75\n",
      "Epoch: 0.09, Validation Loss: 292.26\n",
      "Epoch: 0.09, Validation Loss: 292.58\n",
      "Epoch: 0.10, Validation Loss: 286.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-7811c2c30ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARG_VOCAB_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-6aedf4678dc5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader, num_epochs, learning_rate, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#                result['train_loss'] = evaluate(model, train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-ab4286042283>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfinal_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         loss = criterion(final_outputs.view(input_len, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n\u001b[1;32m     15\u001b[0m                          targ_idxs.view(input_len, TARG_MAX_SENTENCE_LEN))\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-9664fe8d3bab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, targ)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARG_MAX_SENTENCE_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#            print(\"targ[di-1] Size is {}\".format(targ[:, di-1].size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;31m#            print(\"Final Outputs {} is {}\".format(di, decoder_outputs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-9664fe8d3bab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#         print(\"hidden size is: {}\".format(hidden.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#         print(\"hidden is: {}\".format(hidden))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#         output = output.squeeze(0) # B x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCudnnRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# Hack for the tracer that allows us to represent RNNs as single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mAutogradRNN\u001b[0;34m(mode, input_size, hidden_size, num_layers, batch_first, dropout, train, bidirectional, variable_length, dropout_state, flat_weight)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrec_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrec_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     func = StackedRNN(layer,\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mRecurrent\u001b[0;34m(inner, reverse)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mRecurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE+4, hidden_size=HIDDEN_SIZE)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE+4, hidden_size=HIDDEN_SIZE)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train(model, train_loader, dev_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=10)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=10)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(\"Targ shape is {}\".format(targ_idxs.size()))\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)    \n",
    "    final_outputs = model(src_idxs, targ_idxs)\n",
    "    print(final_outputs.size())\n",
    "    print(final_outputs)\n",
    "    print(targ_idxs.size())\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(input_size=10000, hidden_size=10)\n",
    "# decoder = DecoderRNN(hidden_size=10, output_size=10000)\n",
    "# encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     output, hidden = encoder(src_idxs, encoder_hidden)\n",
    "#     print(\"Output:::\")\n",
    "#     print(output.size())\n",
    "#     print(output)\n",
    "#     print(\"Hidden:::\")\n",
    "#     print(hidden.size())\n",
    "#     print(hidden)\n",
    "#     dec_output, dec_hidden = decoder(targ_idxs, hidden) \n",
    "#     print(\"Decoder Output:::\")\n",
    "#     print(dec_output.size())\n",
    "#     print(dec_output)    \n",
    "#     print(\"Decoder Hidden:::\")\n",
    "#     print(dec_hidden.size())\n",
    "#     print(dec_hidden)\n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
