{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "BATCH_SIZE = 1\n",
    "SRC_MAX_SENTENCE_LEN = 20 \n",
    "TARG_MAX_SENTENCE_LEN = 20\n",
    "SRC_VOCAB_SIZE = 1000\n",
    "TARG_VOCAB_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]       \n",
    "        tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words) and max_vocab_size and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "    \"\"\"\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(len(RESERVED_TOKENS), len(RESERVED_TOKENS)+len(vocab))))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + id2token \n",
    "    for t in RESERVED_TOKENS: \n",
    "        token2id[t] = RESERVED_TOKENS[t]\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "    \n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size    \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'])\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    data['train'][lang_type]['tokens'], data['train'][lang_type]['max_vocab_size']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海', '海中', '的', '生命', '大卫', '盖罗', '<EOS>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD9CAYAAABX0LttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFelJREFUeJzt3X+sXPV55/H3ExwSNy2xScKVZVtrolhtaCgErsARq+oWusZAVfNHkEBobSJLV4pIm0pIrdmVFjVpJPLHlgQpRbWCi6myJSxtFgucuJZhtKqUgJ1CAOMQ3xA3XNnFTW0ot1GTdXj2j/lecrjfub5zr3/MHPx+SaM55znfc+b5wuAP58yZcWQmkiQ1vWvQDUiSho/hIEmqGA6SpIrhIEmqGA6SpIrhIEmq9BUOEbEkIh6JiO9HxP6I+EREnB8RuyLiQHleWsZGRNwbERMR8VxEXNY4zsYy/kBEbGzUL4+I58s+90ZEnPqpSpL61e+Zw5eBb2XmbwCXAPuBzcDuzFwN7C7rANcBq8tjHLgPICLOB+4CrgSuAO6aDpQyZryx37qTm5Yk6WTMGQ4RcR7w28D9AJn588x8DVgPbCvDtgE3luX1wIPZ9R1gSUQsA64FdmXm0cw8BuwC1pVt52Xmt7P7jbwHG8eSJA1AP2cOHwb+BfiriHgmIr4aEe8DRjLzMEB5vqCMXw680th/stROVJ/sUZckDciiPsdcBvxBZj4VEV/ml5eQeun1eUEuoF4fOGKc7uUnFi9efPnKlStP1HdPb775Ju96V7s/h2/7HNreP7R/Dm3vH9o/h0H0/4Mf/OAnmfmhfsb2Ew6TwGRmPlXWH6EbDq9GxLLMPFwuDR1pjG/+qb0COFTqYzPqnVJf0WN8JTO3AFsARkdHc+/evX20/3adToexsbE5xw2zts+h7f1D++fQ9v6h/XMYRP8R8U/9jp0ztjLzn4FXIuLXS+ka4EVgOzB9x9FG4NGyvB3YUO5aWgO8Xi477QTWRsTS8kH0WmBn2fZGRKwpdyltaBxLkjQA/Zw5APwB8LWIOBd4GfgU3WB5OCI2AT8GbipjdwDXAxPAT8tYMvNoRHwe2FPGfS4zj5blTwMPAIuBb5aHJGlA+gqHzHwWGO2x6ZoeYxO4fZbjbAW29qjvBT7WTy+SpNOvvZ/mSJJOG8NBklQxHCRJFcNBklQxHCRJFcNBklTp93sO71irNj/+1vLBu28YYCeSNDw8c5AkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVfoKh4g4GBHPR8SzEbG31M6PiF0RcaA8Ly31iIh7I2IiIp6LiMsax9lYxh+IiI2N+uXl+BNl3zjVE5Uk9W8+Zw6/k5mXZuZoWd8M7M7M1cDusg5wHbC6PMaB+6AbJsBdwJXAFcBd04FSxow39lu34BlJkk7ayVxWWg9sK8vbgBsb9Qez6zvAkohYBlwL7MrMo5l5DNgFrCvbzsvMb2dmAg82jiVJGoBFfY5L4O8jIoG/zMwtwEhmHgbIzMMRcUEZuxx4pbHvZKmdqD7Zo16JiHG6ZxiMjIzQ6XT6bP+Xpqam3rbfHRcff2t5IccbhJlzaJu29w/tn0Pb+4f2z2HY++83HK7KzEMlAHZFxPdPMLbX5wW5gHpd7IbSFoDR0dEcGxs7YdO9dDodmvvdtvnxt5YP3jr/4w3CzDm0Tdv7h/bPoe39Q/vnMOz993VZKTMPlecjwDfofmbwarkkRHk+UoZPAisbu68ADs1RX9GjLkkakDnDISLeFxG/Nr0MrAVeALYD03ccbQQeLcvbgQ3lrqU1wOvl8tNOYG1ELC0fRK8FdpZtb0TEmnKX0obGsSRJA9DPZaUR4Bvl7tJFwP/KzG9FxB7g4YjYBPwYuKmM3wFcD0wAPwU+BZCZRyPi88CeMu5zmXm0LH8aeABYDHyzPCRJAzJnOGTmy8AlPer/ClzTo57A7bMcayuwtUd9L/CxPvqVJJ0BfkNaklQxHCRJFcNBklQxHCRJFcNBklTp9xvSZ4VVzW9L333DADuRpMHyzEGSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVOk7HCLinIh4JiIeK+sXRsRTEXEgIr4eEeeW+nvK+kTZvqpxjDtL/aWIuLZRX1dqExGx+dRNT5K0EPM5c/gssL+x/kXgnsxcDRwDNpX6JuBYZn4EuKeMIyIuAm4GfhNYB/xFCZxzgK8A1wEXAbeUsZKkAekrHCJiBXAD8NWyHsDVwCNlyDbgxrK8vqxTtl9Txq8HHsrMn2Xmj4AJ4IrymMjMlzPz58BDZawkaUD6PXP4EvDHwJtl/QPAa5l5vKxPAsvL8nLgFYCy/fUy/q36jH1mq0uSBmTRXAMi4veAI5n53YgYmy73GJpzbJut3iugskeNiBgHxgFGRkbodDqzNz6Lqampt+13x8XHe45byLHPlJlzaJu29w/tn0Pb+4f2z2HY+58zHICrgN+PiOuB9wLn0T2TWBIRi8rZwQrgUBk/CawEJiNiEfB+4GijPq25z2z1t8nMLcAWgNHR0RwbG+uj/bfrdDo097tt8+M9xx28df7HPlNmzqFt2t4/tH8Obe8f2j+HYe9/zstKmXlnZq7IzFV0P1B+IjNvBZ4EPlmGbQQeLcvbyzpl+xOZmaV+c7mb6UJgNfA0sAdYXe5+Ore8xvZTMjtJ0oL0c+Ywmz8BHoqIPwOeAe4v9fuBv46ICbpnDDcDZOa+iHgYeBE4Dtyemb8AiIjPADuBc4CtmbnvJPqSJJ2keYVDZnaATll+me6dRjPH/Adw0yz7fwH4Qo/6DmDHfHqRJJ0+fkNaklQxHCRJFcNBklQxHCRJFcNBklQxHCRJlZP5nsM72qrGN6cP3n3DADuRpDPPMwdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRV5gyHiHhvRDwdEd+LiH0R8aelfmFEPBURByLi6xFxbqm/p6xPlO2rGse6s9RfiohrG/V1pTYREZtP/TQlSfPRz5nDz4CrM/MS4FJgXUSsAb4I3JOZq4FjwKYyfhNwLDM/AtxTxhERFwE3A78JrAP+IiLOiYhzgK8A1wEXAbeUsZKkAZkzHLJrqqy+uzwSuBp4pNS3ATeW5fVlnbL9moiIUn8oM3+WmT8CJoArymMiM1/OzJ8DD5WxkqQB6eszh/J/+M8CR4BdwA+B1zLzeBkyCSwvy8uBVwDK9teBDzTrM/aZrS5JGpBF/QzKzF8Al0bEEuAbwEd7DSvPMcu22eq9Aip71IiIcWAcYGRkhE6nc+LGe5iamnrbfndcfHz2wcVCXud0mjmHtml7/9D+ObS9f2j/HIa9/77CYVpmvhYRHWANsCQiFpWzgxXAoTJsElgJTEbEIuD9wNFGfVpzn9nqM19/C7AFYHR0NMfGxubTPtD9g765322bH59zn4O3zv91TqeZc2ibtvcP7Z9D2/uH9s9h2Pvv526lD5UzBiJiMfC7wH7gSeCTZdhG4NGyvL2sU7Y/kZlZ6jeXu5kuBFYDTwN7gNXl7qdz6X5ovf1UTE6StDD9nDksA7aVu4reBTycmY9FxIvAQxHxZ8AzwP1l/P3AX0fEBN0zhpsBMnNfRDwMvAgcB24vl6uIiM8AO4FzgK2Zue+UzVCSNG9zhkNmPgd8vEf9Zbp3Gs2s/wdw0yzH+gLwhR71HcCOPvqVJJ0BfkNaklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklSZMxwiYmVEPBkR+yNiX0R8ttTPj4hdEXGgPC8t9YiIeyNiIiKei4jLGsfaWMYfiIiNjfrlEfF82efeiIjTMVlJUn/6OXM4DtyRmR8F1gC3R8RFwGZgd2auBnaXdYDrgNXlMQ7cB90wAe4CrgSuAO6aDpQyZryx37qTn5okaaHmDIfMPJyZ/1iW3wD2A8uB9cC2MmwbcGNZXg88mF3fAZZExDLgWmBXZh7NzGPALmBd2XZeZn47MxN4sHEsSdIALJrP4IhYBXwceAoYyczD0A2QiLigDFsOvNLYbbLUTlSf7FEfGqs2P/7W8sG7bxhgJ5J0ZvQdDhHxq8DfAn+Umf92go8Fem3IBdR79TBO9/ITIyMjdDqdObquTU1NvW2/Oy4+Pq/9F/Kap9rMObRN2/uH9s+h7f1D++cw7P33FQ4R8W66wfC1zPy7Un41IpaVs4ZlwJFSnwRWNnZfARwq9bEZ9U6pr+gxvpKZW4AtAKOjozk2NtZr2Al1Oh2a+93WOCvox8Fb5/+ap9rMObRN2/uH9s+h7f1D++cw7P33c7dSAPcD+zPzzxubtgPTdxxtBB5t1DeUu5bWAK+Xy087gbURsbR8EL0W2Fm2vRERa8prbWgcS5I0AP2cOVwF/Ffg+Yh4ttT+G3A38HBEbAJ+DNxUtu0ArgcmgJ8CnwLIzKMR8XlgTxn3ucw8WpY/DTwALAa+WR6SpAGZMxwy8x/o/bkAwDU9xidw+yzH2gps7VHfC3xsrl4kSWeG35CWJFUMB0lSxXCQJFUMB0lSxXCQJFUMB0lSxXCQJFUMB0lSxXCQJFUMB0lSZV5/n8M7xap5/hKrJJ1tPHOQJFUMB0lSxXCQJFUMB0lSxXCQJFUMB0lSxXCQJFUMB0lSxXCQJFXOym9In4zmt6sP3n3DADuRpNPHMwdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUmXOcIiIrRFxJCJeaNTOj4hdEXGgPC8t9YiIeyNiIiKei4jLGvtsLOMPRMTGRv3yiHi+7HNvRMSpnqQkaX76OXN4AFg3o7YZ2J2Zq4HdZR3gOmB1eYwD90E3TIC7gCuBK4C7pgOljBlv7DfztSRJZ9ic4ZCZ/xc4OqO8HthWlrcBNzbqD2bXd4AlEbEMuBbYlZlHM/MYsAtYV7adl5nfzswEHmwcS5I0IAv9+YyRzDwMkJmHI+KCUl8OvNIYN1lqJ6pP9qj3FBHjdM8yGBkZodPpzLvxqakp7rj4F/Per5eFvP6pMDU1NbDXPhXa3j+0fw5t7x/aP4dh7/9U/7ZSr88LcgH1njJzC7AFYHR0NMfGxubdYKfT4X/+w7/Pe79eDt46/9c/FTqdDguZ+7Boe//Q/jm0vX9o/xyGvf+F3q30arkkRHk+UuqTwMrGuBXAoTnqK3rUJUkDtNBw2A5M33G0EXi0Ud9Q7lpaA7xeLj/tBNZGxNLyQfRaYGfZ9kZErCl3KW1oHEuSNCBzXlaKiL8BxoAPRsQk3buO7gYejohNwI+Bm8rwHcD1wATwU+BTAJl5NCI+D+wp4z6XmdMfcn+a7h1Ri4FvlockaYDmDIfMvGWWTdf0GJvA7bMcZyuwtUd9L/CxufqQJJ05fkNaklTxb4I7Cf6tcJLeqTxzkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRV/J7DKeJ3HiS9k3jmIEmqGA6SpIrhIEmqGA6SpIrhIEmqGA6SpIrhIEmqGA6SpIrhIEmqGA6SpIo/n3Ea+FMaktrOMwdJUsVwkCRVDAdJUsXPHE4zP3+Q1EaeOUiSKp45nEGeRUhqi6E5c4iIdRHxUkRMRMTmQfcjSWezoThziIhzgK8A/wWYBPZExPbMfHGwnZ0+nkVIGmZDEQ7AFcBEZr4MEBEPAeuBd2w4NBkUkobNsITDcuCVxvokcOWAehmoZlDM5o6Lj3ObgSLpNBqWcIgetawGRYwD42V1KiJeWsBrfRD4yQL2Gxp/OGMO8cUBNrMwrf93QPvn0Pb+of1zGET//6nfgcMSDpPAysb6CuDQzEGZuQXYcjIvFBF7M3P0ZI4xaG2fQ9v7h/bPoe39Q/vnMOz9D8vdSnuA1RFxYUScC9wMbB9wT5J01hqKM4fMPB4RnwF2AucAWzNz34DbkqSz1lCEA0Bm7gB2nIGXOqnLUkOi7XNoe//Q/jm0vX9o/xyGuv/IrD73lSSd5YblMwdJ0hA5q8KhDT/RERFbI+JIRLzQqJ0fEbsi4kB5XlrqERH3lvk8FxGXDa7zX4qIlRHxZETsj4h9EfHZUm/FPCLivRHxdER8r/T/p6V+YUQ8Vfr/erl5goh4T1mfKNtXDbL/aRFxTkQ8ExGPlfW29X8wIp6PiGcjYm+pteI9VHpaEhGPRMT3y38Ln2hT/2dNODR+ouM64CLgloi4aLBd9fQAsG5GbTOwOzNXA7vLOnTnsro8xoH7zlCPczkO3JGZHwXWALeXf9ZtmcfPgKsz8xLgUmBdRKwBvgjcU/o/Bmwq4zcBxzLzI8A9Zdww+Cywv7Hetv4BficzL23c8tmW9xDAl4FvZeZvAJfQ/XfRnv4z86x4AJ8AdjbW7wTuHHRfs/S6Cnihsf4SsKwsLwNeKst/CdzSa9wwPYBH6f5uVuvmAfwK8I90v7H/E2DRzPcT3bvsPlGWF5VxMeC+V9D9w+dq4DG6XzRtTf+ll4PAB2fUWvEeAs4DfjTzn2Nb+s/Ms+fMgd4/0bF8QL3M10hmHgYozxeU+tDPqVyi+DjwFC2aR7kk8yxwBNgF/BB4LTOPlyHNHt/qv2x/HfjAme248iXgj4E3y/oHaFf/0P2VhL+PiO+WX0eA9ryHPgz8C/BX5dLeVyPifbSn/7MqHPr6iY6WGeo5RcSvAn8L/FFm/tuJhvaoDXQemfmLzLyU7v+BXwF8tNew8jxU/UfE7wFHMvO7zXKPoUPZf8NVmXkZ3Usut0fEb59g7LDNYRFwGXBfZn4c+Hd+eQmpl2Hr/6wKh75+omNIvRoRywDK85FSH9o5RcS76QbD1zLz70q5dfPIzNeADt3PTpZExPR3g5o9vtV/2f5+4OiZ7fRtrgJ+PyIOAg/RvbT0JdrTPwCZeag8HwG+QTek2/IemgQmM/Opsv4I3bBoS/9nVTi0+Sc6tgMby/JGutfwp+sbyp0Oa4DXp09ZBykiArgf2J+Zf97Y1Ip5RMSHImJJWV4M/C7dDxOfBD5Zhs3sf3penwSeyHLheBAy887MXJGZq+i+z5/IzFtpSf8AEfG+iPi16WVgLfACLXkPZeY/A69ExK+X0jV0/wqCVvQPnD0fSJf3+vXAD+heP/7vg+5nlh7/BjgM/D+6/zexie71393AgfJ8fhkbdO/A+iHwPDA66P5LX/+Z7inxc8Cz5XF9W+YB/BbwTOn/BeB/lPqHgaeBCeB/A+8p9feW9Ymy/cOD/nfQmMsY8Fjb+i+9fq889k3/99qW91Dp6VJgb3kf/R9gaZv69xvSkqTK2XRZSZLUJ8NBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklT5/2mrBqszf+RyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values= 0)\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values= 0)\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 20])\n",
      "tensor([[  3,   3,   4, 206,   3,   3,   1,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]])\n",
      "tensor([7])\n"
     ]
    }
   ],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#        embedded = self.embedding(input).view(1, 1, -1)\n",
    "#        print(input)\n",
    "        embedded = self.embedding(input).view(SRC_MAX_SENTENCE_LEN, BATCH_SIZE, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         print(\"Before embedding, targ[i-1] size is: {}\".format(input.size()))\n",
    "#         print(\"Before embedding, targ[i-1] is: {}\".format(input))        \n",
    "        output = self.embedding(input).view(1, BATCH_SIZE, -1)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "        output = F.relu(output)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "#         print(\"hidden size is: {}\".format(hidden.size()))        \n",
    "#         print(\"hidden is: {}\".format(hidden))        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))       \n",
    "#         output = output.squeeze(0) # B x N\n",
    "#         output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)\n",
    "    \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_size = self.decoder.output_size\n",
    "        \n",
    "    def forward(self, src, targ): \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_output, encoder_hidden = self.encoder(src, encoder_hidden)\n",
    "        decoder_hidden = encoder_hidden \n",
    "#        print(\"Encoder Hidden size: {}\".format(encoder_hidden.size()))\n",
    "        final_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, BATCH_SIZE, self.output_size))\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "#            print(\"targ[di-1] Size is {}\".format(targ[:, di-1].size()))\n",
    "            decoder_outputs, decoder_hidden = self.decoder(targ[:, di-1], decoder_hidden)\n",
    "#            print(\"Final Outputs {} is {}\".format(di, decoder_outputs))\n",
    "            top1 = decoder_outputs.data.max(1)[1]\n",
    "#            print(\"Top 1 is {}\".format(top1))\n",
    "            final_outputs[di] = decoder_outputs\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader=None, num_epochs=3, learning_rate=0.1, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for e in range(1, num_epochs+1): \n",
    "        total_loss = 0 \n",
    "        for b, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs = model(src_idxs, targ_idxs) \n",
    "            loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n",
    "                             targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(\"Epoch 1 {}: Total loss is {}\".format(i, total_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6922, grad_fn=<NllLoss2DBackward>)\n",
      "0.6921676397323608\n",
      "tensor(2.4437, grad_fn=<NllLoss2DBackward>)\n",
      "2.443692445755005\n",
      "tensor(0.3489, grad_fn=<NllLoss2DBackward>)\n",
      "0.34891510009765625\n",
      "tensor(2.8439, grad_fn=<NllLoss2DBackward>)\n",
      "2.8439202308654785\n",
      "tensor(3.1875, grad_fn=<NllLoss2DBackward>)\n",
      "3.187467098236084\n",
      "tensor(2.1745, grad_fn=<NllLoss2DBackward>)\n",
      "2.174468517303467\n",
      "tensor(1.3445, grad_fn=<NllLoss2DBackward>)\n",
      "1.3445372581481934\n",
      "tensor(1.4070, grad_fn=<NllLoss2DBackward>)\n",
      "1.4070336818695068\n",
      "tensor(1.4114, grad_fn=<NllLoss2DBackward>)\n",
      "1.4114328622817993\n",
      "tensor(1.0175, grad_fn=<NllLoss2DBackward>)\n",
      "1.0175015926361084\n",
      "tensor(2.5292, grad_fn=<NllLoss2DBackward>)\n",
      "2.5292041301727295\n",
      "tensor(0.2403, grad_fn=<NllLoss2DBackward>)\n",
      "0.24032649397850037\n",
      "tensor(0.6148, grad_fn=<NllLoss2DBackward>)\n",
      "0.6148163676261902\n",
      "tensor(1.3278, grad_fn=<NllLoss2DBackward>)\n",
      "1.3277889490127563\n",
      "tensor(1.4347, grad_fn=<NllLoss2DBackward>)\n",
      "1.4346622228622437\n",
      "tensor(2.2452, grad_fn=<NllLoss2DBackward>)\n",
      "2.2451672554016113\n",
      "tensor(1.0046, grad_fn=<NllLoss2DBackward>)\n",
      "1.0045979022979736\n",
      "tensor(2.1013, grad_fn=<NllLoss2DBackward>)\n",
      "2.101306676864624\n",
      "tensor(1.8614, grad_fn=<NllLoss2DBackward>)\n",
      "1.8613532781600952\n",
      "tensor(1.5139, grad_fn=<NllLoss2DBackward>)\n",
      "1.5138744115829468\n",
      "tensor(2.5215, grad_fn=<NllLoss2DBackward>)\n",
      "2.5215206146240234\n",
      "tensor(0.3501, grad_fn=<NllLoss2DBackward>)\n",
      "0.3501114845275879\n",
      "tensor(1.5257, grad_fn=<NllLoss2DBackward>)\n",
      "1.5257070064544678\n",
      "tensor(0.7279, grad_fn=<NllLoss2DBackward>)\n",
      "0.7278758883476257\n",
      "tensor(1.4717, grad_fn=<NllLoss2DBackward>)\n",
      "1.4717457294464111\n",
      "tensor(1.9858, grad_fn=<NllLoss2DBackward>)\n",
      "1.9857597351074219\n",
      "tensor(1.2121, grad_fn=<NllLoss2DBackward>)\n",
      "1.2120596170425415\n",
      "tensor(2.2853, grad_fn=<NllLoss2DBackward>)\n",
      "2.2853169441223145\n",
      "tensor(0.8213, grad_fn=<NllLoss2DBackward>)\n",
      "0.8213404417037964\n",
      "tensor(0.3525, grad_fn=<NllLoss2DBackward>)\n",
      "0.35245245695114136\n",
      "tensor(2.0361, grad_fn=<NllLoss2DBackward>)\n",
      "2.0360825061798096\n",
      "tensor(2.4324, grad_fn=<NllLoss2DBackward>)\n",
      "2.43241810798645\n",
      "tensor(2.6332, grad_fn=<NllLoss2DBackward>)\n",
      "2.6332361698150635\n",
      "tensor(1.5704, grad_fn=<NllLoss2DBackward>)\n",
      "1.5703502893447876\n",
      "tensor(0.7376, grad_fn=<NllLoss2DBackward>)\n",
      "0.7375825047492981\n",
      "tensor(0.3470, grad_fn=<NllLoss2DBackward>)\n",
      "0.34703850746154785\n",
      "tensor(2.5938, grad_fn=<NllLoss2DBackward>)\n",
      "2.5938315391540527\n",
      "tensor(0.7482, grad_fn=<NllLoss2DBackward>)\n",
      "0.7482209205627441\n",
      "tensor(2.2029, grad_fn=<NllLoss2DBackward>)\n",
      "2.2029166221618652\n",
      "tensor(2.2699, grad_fn=<NllLoss2DBackward>)\n",
      "2.2699027061462402\n",
      "tensor(2.6861, grad_fn=<NllLoss2DBackward>)\n",
      "2.686089277267456\n",
      "tensor(0.8175, grad_fn=<NllLoss2DBackward>)\n",
      "0.8175126910209656\n",
      "tensor(2.3221, grad_fn=<NllLoss2DBackward>)\n",
      "2.322117328643799\n",
      "tensor(2.1822, grad_fn=<NllLoss2DBackward>)\n",
      "2.182241678237915\n",
      "tensor(2.4132, grad_fn=<NllLoss2DBackward>)\n",
      "2.4131617546081543\n",
      "tensor(1.1046, grad_fn=<NllLoss2DBackward>)\n",
      "1.1045607328414917\n",
      "tensor(1.6811, grad_fn=<NllLoss2DBackward>)\n",
      "1.6810615062713623\n",
      "tensor(3.9175, grad_fn=<NllLoss2DBackward>)\n",
      "3.9175400733947754\n",
      "tensor(2.5869, grad_fn=<NllLoss2DBackward>)\n",
      "2.586860179901123\n",
      "tensor(1.7794, grad_fn=<NllLoss2DBackward>)\n",
      "1.7794443368911743\n",
      "tensor(0.9501, grad_fn=<NllLoss2DBackward>)\n",
      "0.9500996470451355\n",
      "tensor(2.1582, grad_fn=<NllLoss2DBackward>)\n",
      "2.1581740379333496\n",
      "tensor(1.2627, grad_fn=<NllLoss2DBackward>)\n",
      "1.262681484222412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2026a4b92138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARG_VOCAB_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-069536fcc9df>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m             loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n\u001b[1;32m     16\u001b[0m                              targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE+4, hidden_size=10)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE+4, hidden_size=10)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train(model, train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=10)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=10)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(\"Targ shape is {}\".format(targ_idxs.size()))\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)    \n",
    "    final_outputs = model(src_idxs, targ_idxs)\n",
    "    print(final_outputs.size())\n",
    "    print(final_outputs)\n",
    "    print(targ_idxs.size())\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(input_size=10000, hidden_size=10)\n",
    "# decoder = DecoderRNN(hidden_size=10, output_size=10000)\n",
    "# encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     output, hidden = encoder(src_idxs, encoder_hidden)\n",
    "#     print(\"Output:::\")\n",
    "#     print(output.size())\n",
    "#     print(output)\n",
    "#     print(\"Hidden:::\")\n",
    "#     print(hidden.size())\n",
    "#     print(hidden)\n",
    "#     dec_output, dec_hidden = decoder(targ_idxs, hidden) \n",
    "#     print(\"Decoder Output:::\")\n",
    "#     print(dec_output.size())\n",
    "#     print(dec_output)    \n",
    "#     print(\"Decoder Hidden:::\")\n",
    "#     print(dec_hidden.size())\n",
    "#     print(dec_hidden)\n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
