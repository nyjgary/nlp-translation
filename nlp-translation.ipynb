{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "import sacrebleu\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle as pkl\n",
    "import string\n",
    "import os\n",
    "from os import listdir \n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "\n",
    "pd.set_option('max_colwidth',100)\n",
    "mpl.style.use('bmh')\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp, lang_type): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]\n",
    "        if lang_type == 'source': \n",
    "            tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "        elif lang_type == 'target': \n",
    "            tokens_data = [['<SOS>'] + datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(num_vocab))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + list(vocab)\n",
    "    token2id = dict(zip(id2token, range(max_vocab_size)))\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    \n",
    "    # store language names \n",
    "    fps['languages'] = {} \n",
    "    fps['languages']['source'] = src_lang\n",
    "    fps['languages']['target'] = targ_lang \n",
    "    \n",
    "    # store filepaths \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "            \n",
    "    return fps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = 10000\n",
    "TARG_VOCAB_SIZE = 10000\n",
    "# ENC_EMBED_DIM = 300 \n",
    "# DEC_EMBED_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate language dictionaries from train sets \n",
    "\n",
    "def generate_vocab(src_lang, targ_lang, src_vocab_size, targ_vocab_size):\n",
    "    \"\"\" Outputs a nested dictionary containing token2id, id2token, and word embeddings \n",
    "    for source and target lang's vocab \"\"\"\n",
    "    \n",
    "    vocab = {} \n",
    "    for lang, vocab_size in zip([src_lang, targ_lang], [src_vocab_size, targ_vocab_size]): \n",
    "        \n",
    "        # load train data \n",
    "        train_data_fp = get_filepath(split='train', src_lang=SRC_LANG, targ_lang=TARG_LANG, \n",
    "                                     lang_type='target' if lang == 'en' else 'source')\n",
    "        with open(train_data_fp) as f:\n",
    "            train_tokens = [line.lower().split() for line in f.readlines()]        \n",
    "        \n",
    "        # load word embeddings, generate token2id and id2token \n",
    "        word2vec_full = load_word2vec(lang)\n",
    "        token2id, id2token = build_vocab(train_tokens, vocab_size, word2vec_full) \n",
    "        word2vec_reduced = {word: word2vec_full[word] for word in token2id if word not in RESERVED_TOKENS} \n",
    "        \n",
    "        # store token2id, id2token, and word embeddings as a dict in nested dict lang \n",
    "        vocab[lang] = {'token2id': token2id, 'id2token': id2token, 'word2vec': word2vec_reduced}\n",
    "        \n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new process_data that uses separate generate_vocab function \n",
    "\n",
    "# def process_data(src_lang, targ_lang): \n",
    "#     \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "#         and returns as a nested dictionary containing: \n",
    "#         - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "#         - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "#         - source language's token2id and id2token \n",
    "#         - target language's token2id and id2token\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # get filepaths \n",
    "#     data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "#     # loop through each file, read in text, convert to tokens, then to indices \n",
    "#     for split in ['train', 'dev', 'test']: \n",
    "#         for lang_type in ['source', 'target']: \n",
    "#             # read in tokens \n",
    "#             data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'], lang_type)\n",
    "#             # convert tokens to indices \n",
    "#             data[split][lang_type]['indices'] = tokens2indices(\n",
    "#                 data[split][lang_type]['tokens'], vocab[data['languages'][lang_type]]['token2id'])\n",
    "        \n",
    "#     return data\n",
    "\n",
    "def process_data(src_lang, targ_lang, sample_limit=None): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            # read in tokens \n",
    "            tokens = text2tokens(data[split][lang_type]['filepath'], lang_type)\n",
    "            if sample_limit is not None: \n",
    "                tokens = tokens[:sample_limit]\n",
    "            # convert tokens to indices \n",
    "            indices = tokens2indices(tokens, vocab[data['languages'][lang_type]]['token2id'])\n",
    "            # save to dictionary \n",
    "            data[split][lang_type]['tokens'] = tokens\n",
    "            data[split][lang_type]['indices'] = indices\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data(SRC_LANG, TARG_LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Source: ['泰坦', '泰坦尼克', '泰坦尼克号', '坦尼', '尼克', '号', '是', '拿', '了', '不少', '票房', '冠军', '但', '事实', '事实上', '它', '并', '不是', '关于', '于海洋', '海洋', '的', '最', '刺激', '的', '故事', '<EOS>']\n",
      "Example Target: ['<SOS>', 'the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'titanic', '--', 'even', 'though', 'it', '&apos;s', 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', 'it', '&apos;s', 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# print example for sanity check  \n",
    "print(\"Example Source: {}\".format(data['train']['source']['tokens'][5]))\n",
    "print(\"Example Target: {}\".format(data['train']['target']['tokens'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of source sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of target sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len, batch_size): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(processed_data[split]['source']['indices'], processed_data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SRC_MAX_SENTENCE_LEN = 40 \n",
    "TARG_MAX_SENTENCE_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, dev_loader, test_loader = create_dataloaders(\n",
    "    data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "train_loader_limited, dev_loader_limited, test_loader_limited = create_dataloaders(\n",
    "    limited_data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 40])\n",
      "tensor([[3318, 3071,    4,  ...,    2,    2,    2],\n",
      "        [1606,    3,  124,  ...,    3,    3, 1460],\n",
      "        [1606,    3, 1106,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [  11,    6,   86,  ...,    2,    2,    2],\n",
      "        [ 242,   65,   22,  ...,    2,    2,    2],\n",
      "        [ 119, 1540,    4,  ...,    2,    2,    2]])\n",
      "tensor([ 7, 40, 11, 13, 18, 27, 12, 11,  9,  8, 40, 12, 12, 19, 15, 24, 23, 14,\n",
      "        23, 23, 35,  5, 11,  9,  8,  9,  6, 13,  9,  7, 21, 18])\n",
      "torch.Size([32, 40])\n",
      "tensor([[   0,  118,   13,  ...,    2,    2,    2],\n",
      "        [   0,   28, 7488,  ..., 6098,    4,   86],\n",
      "        [   0,   18,   17,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,   14,    3,  ...,    2,    2,    2],\n",
      "        [   0,   48,   23,  ...,    2,    2,    2],\n",
      "        [   0,    7,   14,  ...,    2,    2,    2]])\n",
      "tensor([ 7, 40, 12, 18, 31, 35, 18, 17, 10, 10, 40, 17, 11, 20, 23, 26, 29, 17,\n",
      "        28, 27, 40,  7, 19, 14, 11, 10,  9, 19, 13,  8, 15, 25])\n"
     ]
    }
   ],
   "source": [
    "# check that loader works \n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    print(targ_idxs.size())\n",
    "    print(targ_idxs)\n",
    "    print(targ_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_emb(word2vec, token2id): \n",
    "    \"\"\" Given word2vec model and the vocab's token2id, extract pretrained word embeddings \"\"\"\n",
    "    pretrained_emb = np.zeros((len(token2id), 300)) \n",
    "    for token in token2id: \n",
    "        try: \n",
    "            pretrained_emb[token2id[token]] = word2vec[token]\n",
    "        except: \n",
    "            pretrained_emb[token2id[token]] = np.random.normal(size=(300,))\n",
    "    return torch.from_numpy(pretrained_emb.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_dim, num_layers, pretrained_word2vec): \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.enc_embed_dim = 300\n",
    "        self.enc_hidden_dim = enc_hidden_dim \n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(input_size=self.enc_embed_dim, hidden_size=self.enc_hidden_dim, num_layers=self.num_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, enc_input, enc_input_lens):\n",
    "        batch_size = enc_input.size()[0]\n",
    "        _, idx_sort = torch.sort(enc_input_lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        enc_input, enc_input_lens = enc_input.index_select(0, idx_sort), enc_input_lens.index_select(0, idx_sort)\n",
    "        embedded = self.embedding(enc_input)\n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, enc_input_lens, batch_first=True)\n",
    "        hidden = self.initHidden(batch_size).to(device)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n",
    "                                                           total_length=SRC_MAX_SENTENCE_LEN,\n",
    "                                                           padding_value=RESERVED_TOKENS['<PAD>'])\n",
    "        output = output.index_select(0, idx_unsort)\n",
    "        hidden = hidden.index_select(1, idx_unsort).transpose(0, 1).contiguous().view(self.num_layers, batch_size, -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2*self.num_layers, batch_size, self.enc_hidden_dim, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, dec_hidden_dim, enc_hidden_dim, num_layers, pretrained_word2vec):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.dec_embed_dim = 300\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(self.dec_embed_dim + 2 * self.enc_hidden_dim, self.dec_hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(dec_hidden_dim, TARG_VOCAB_SIZE)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "        context = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n",
    "                             enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden\n",
    "        \n",
    "class EncoderDecoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, encoder, decoder, decoder_token2id): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "\n",
    "    def forward(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio): \n",
    "        batch_size = src_idx.size()[0]\n",
    "        enc_outputs, enc_hidden = self.encoder(src_idx, src_lens)\n",
    "        dec_hidden = enc_hidden \n",
    "        dec_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size, TARG_VOCAB_SIZE))\n",
    "        hypotheses = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size))\n",
    "        dec_output = targ_idx[:, 0] # initialize with <SOS>\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "            dec_output, dec_hidden = self.decoder(dec_output, dec_hidden, enc_outputs)\n",
    "            dec_outputs[di] = dec_output \n",
    "            teacher_labels = targ_idx[:, di-1] \n",
    "            greedy_labels = dec_output.data.max(1)[1]\n",
    "            dec_output = teacher_labels if random.random() < teacher_forcing_ratio else greedy_labels \n",
    "            hypotheses[di] = greedy_labels\n",
    "\n",
    "        return dec_outputs, hypotheses.transpose(0,1)\n",
    "    \n",
    "class Attention(nn.Module): \n",
    "    \n",
    "    \"\"\" Implements the attention mechanism by Bahdanau et al. (2015) \"\"\"\n",
    "    \n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim, num_annotations, num_layers): \n",
    "        super(Attention, self).__init__() \n",
    "        self.num_annotations = num_annotations\n",
    "        self.input_dim = enc_hidden_dim * 2 + dec_hidden_dim\n",
    "        self.attn = nn.Linear(self.input_dim, self.num_annotations)\n",
    "        self.v = nn.Parameter(torch.rand(self.num_annotations))\n",
    "        self.num_layers = num_layers \n",
    "        nn.init.normal_(self.v)\n",
    "        \n",
    "    def forward(self, encoder_outputs, last_dec_hidden): \n",
    "        batch_size = encoder_outputs.size()[0]\n",
    "        last_dec_hidden = last_dec_hidden.transpose(0, 1)[:, -1, :].unsqueeze(1) \n",
    "        hidden_broadcast = last_dec_hidden.repeat(1, self.num_annotations, 1)\n",
    "        v_broadcast = self.v.repeat(batch_size, 1, 1)\n",
    "        concat = torch.cat([encoder_outputs, hidden_broadcast], dim=2)\n",
    "        energies = v_broadcast.bmm(torch.tanh(self.attn(concat)))\n",
    "        attn_weights = F.softmax(energies, dim=2).squeeze(1)\n",
    "        return attn_weights\n",
    "\n",
    "class DecoderAttnRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dec_hidden_dim, enc_hidden_dim, num_layers, pretrained_word2vec):\n",
    "        super(DecoderAttnRNN, self).__init__()\n",
    "        self.dec_embed_dim = 300\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.num_layers = num_layers \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.attn = Attention(self.enc_hidden_dim, self.dec_hidden_dim, \n",
    "                              num_annotations=SRC_MAX_SENTENCE_LEN, num_layers=self.num_layers)\n",
    "        self.gru = nn.GRU(self.dec_embed_dim + 2 * self.enc_hidden_dim, self.dec_hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.dec_hidden_dim, TARG_VOCAB_SIZE)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "        attn_weights = self.attn(encoder_outputs=enc_outputs, last_dec_hidden=dec_hidden).unsqueeze(1)\n",
    "        context = attn_weights.bmm(enc_outputs).transpose(0, 1)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2corpus(tensor, id2token): \n",
    "    \"\"\" Takes a tensor (num_sentences x max_sentence_length) representing the corpus, \n",
    "        returns its string equivalent \n",
    "    \"\"\"\n",
    "    tensor = tensor.view(-1)\n",
    "    ignored_idx = [RESERVED_TOKENS[token] for token in ['<SOS>', '<EOS>', '<PAD>']] \n",
    "    filtered_list = [id2token[idx] for idx in tensor.numpy().astype(int).tolist() if idx not in ignored_idx] \n",
    "    corpus = ' '.join(filtered_list)\n",
    "    return corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, id2token, teacher_forcing_ratio=0.0): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "    reference_corpus = []\n",
    "    hypothesis_corpus = [] \n",
    "    \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader): \n",
    "        batch_size = src_idxs.size()[0]        \n",
    "        outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, \n",
    "                                    teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        outputs = outputs[1:].view(-1, TARG_VOCAB_SIZE)\n",
    "        targets = targ_idxs[:,1:]\n",
    "        hypothesis_corpus.append(hypotheses)\n",
    "        reference_corpus.append(targets)\n",
    " \n",
    "        loss = F.nll_loss(outputs.view(-1, TARG_VOCAB_SIZE), targets.contiguous().view(-1), \n",
    "                          ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "        total_loss += loss.item()  \n",
    "\n",
    "    # reconstruct corpus and compute bleu score \n",
    "    hypothesis_corpus = torch.cat(hypothesis_corpus, dim=0) \n",
    "    reference_corpus = torch.cat(reference_corpus, dim=0)\n",
    "    hypothesis_corpus = tensor2corpus(hypothesis_corpus, id2token)\n",
    "    reference_corpus = tensor2corpus(reference_corpus, id2token)\n",
    "    bleu_score = sacrebleu.corpus_bleu(hypothesis_corpus, reference_corpus).score\n",
    "    \n",
    "    return total_loss / len(loader), bleu_score, hypothesis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to save results to and load results from a pkl logfile \n",
    "\n",
    "RESULTS_LOG = 'experiment_results/experiment_results_log.pkl'\n",
    "\n",
    "def check_dir_exists(filename): \n",
    "    \"\"\" Helper function to check that the directory of filename exists, otherwise creates it \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    else: \n",
    "        pass \n",
    "        \n",
    "def append_to_log(hyperparams, results, runtime, experiment_name, dt_created, filename=RESULTS_LOG): \n",
    "    \"\"\" Appends results and details of a single experiment to a log file \"\"\"\n",
    "    \n",
    "    # check directory exists, else creates it \n",
    "    check_dir_exists(filename)\n",
    "        \n",
    "    # store experiment details in a dictionary \n",
    "    new_result = {'experiment_name': experiment_name, 'hyperparams': hyperparams, 'results': results, \n",
    "                  'runtime': runtime, 'dt_created': dt_created}\n",
    "    \n",
    "    # if log already exists, append to log \n",
    "    try: \n",
    "        results_log = pkl.load(open(filename, \"rb\"))\n",
    "        results_log.append(new_result)\n",
    "\n",
    "    # if log doesn't exists, initialize first result as the log \n",
    "    except (OSError, IOError) as e:\n",
    "        results_log = [new_result]\n",
    "    \n",
    "    # save to pickle \n",
    "    pkl.dump(results_log, open(filename, \"wb\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_log(experiment_name=None, filename=RESULTS_LOG): \n",
    "    \"\"\" Loads experiment log, with option to filter for a specific experiment_name \"\"\"\n",
    "    \n",
    "    results_log = pkl.load(open(filename, \"rb\"))\n",
    "    \n",
    "    if experiment_name is not None: \n",
    "        results_log = [r for r in results_log if r['experiment_name'] == experiment_name]\n",
    "        \n",
    "    return results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model(model, data_split, train_loader_, dev_loader_, batch=0, num_samples=5): \n",
    "    \"\"\" Use the model and output translates for first num_samples in chosen batch in chosen loader \"\"\"\n",
    "    \n",
    "    # set loader based on data_split choice \n",
    "    if data_split == 'train': \n",
    "        loader = train_loader_ \n",
    "    elif data_split == 'val': \n",
    "        loader = dev_loader_ \n",
    "        \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader):\n",
    "        if i == batch: \n",
    "            src_idxs = src_idxs[:num_samples, :]\n",
    "            targ_idxs = targ_idxs[:num_samples, :]\n",
    "            src_lens = src_lens[:num_samples]\n",
    "            targ_lens = targ_lens[:num_samples]              \n",
    "            output, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0)\n",
    "            \n",
    "            if data_split == 'train': \n",
    "                print(\"Inspecting model on training data...\")\n",
    "            elif data_split == 'val': \n",
    "                print(\"Inspecting model on validation data...\")\n",
    "                \n",
    "            print(\"REFERENCE TRANSLATION: {}\".format(tensor2corpus(targ_idxs, vocab[TARG_LANG]['id2token'])))\n",
    "            print(\"MODEL TRANSLATION: {}\".format(tensor2corpus(torch.cat([hypotheses], dim=0), vocab[TARG_LANG]['id2token'])))\n",
    "            break \n",
    "        else: \n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_eval(model, train_loader, dev_loader, id2token, learning_rate, num_epochs, \n",
    "#                    print_intermediate, save_checkpoint, model_name, lazy_eval, inspect): \n",
    "    \n",
    "#     # initialize optimizer and criterion \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "#     results = [] \n",
    "    \n",
    "#     # loop through train data in batches and train \n",
    "#     for epoch in range(num_epochs): \n",
    "#         train_loss = 0 \n",
    "#         for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#             model.train()\n",
    "#             optimizer.zero_grad()\n",
    "#             final_outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.5) \n",
    "#             loss = criterion(final_outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1))\n",
    "#             loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             if batch % 100 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "#                 result = {} \n",
    "#                 result['epoch'] = epoch + batch / len(train_loader) \n",
    "#                 result['val_loss'], result['val_bleu'], val_hypotheses = evaluate(\n",
    "#                     model, dev_loader, id2token, teacher_forcing_ratio=1)\n",
    "#                 if lazy_eval: \n",
    "#                     # eval on full train set is very expensive \n",
    "#                     result['train_loss'], result['train_bleu'], train_hypotheses = 0, 0, None\n",
    "#                 else: \n",
    "#                     result['train_loss'], result['train_bleu'], train_hypotheses = evaluate(\n",
    "#                         model, train_loader, id2token, teacher_forcing_ratio=1)\n",
    "                \n",
    "#                 results.append(result)\n",
    "                \n",
    "#                 if print_intermediate: \n",
    "#                     print('Epoch: {:.2f}, Train Loss: {:.2f}, Val Loss: {:.2f}, Train BLEU: {:.2f}, Val BLEU: {:.2f}'\\\n",
    "#                           .format(result['epoch'], result['train_loss'], result['val_loss'], \n",
    "#                                   result['train_bleu'], result['val_bleu']))\n",
    "                    \n",
    "#                 if inspect: \n",
    "#                     inspect_model(model, data_split='train')\n",
    "#                     inspect_model(model, data_split='val')\n",
    "                    \n",
    "#                 if save_checkpoint: \n",
    "#                     if result['val_loss'] == pd.DataFrame.from_dict(results)['val_loss'].min(): \n",
    "#                         checkpoint_fp = 'model_checkpoints/{}.pth.tar'.format(model_name)\n",
    "#                         check_dir_exists(filename=checkpoint_fp)\n",
    "#                         torch.save(model.state_dict(), checkpoint_fp)\n",
    "                \n",
    "#     return results \n",
    "\n",
    "def train_and_eval(model, id2token, learning_rate, num_epochs, \n",
    "                   print_intermediate, save_checkpoint, model_name, lazy_eval, lazy_train, inspect): \n",
    "    \n",
    "    if lazy_train: \n",
    "        train_loader_ = train_loader_limited \n",
    "        dev_loader_ = dev_loader_limited \n",
    "    else: \n",
    "        train_loader_ = train_loader\n",
    "        dev_loader_ = dev_loader      \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader_):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.5) \n",
    "            loss = criterion(final_outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 100 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader_)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader_) \n",
    "                result['val_loss'], result['val_bleu'], val_hypotheses = evaluate(\n",
    "                    model, dev_loader_, id2token, teacher_forcing_ratio=1)\n",
    "                if lazy_eval: \n",
    "                    # eval on full train set is very expensive \n",
    "                    result['train_loss'], result['train_bleu'], train_hypotheses = 0, 0, None\n",
    "                else: \n",
    "                    result['train_loss'], result['train_bleu'], train_hypotheses = evaluate(\n",
    "                        model, train_loader_, id2token, teacher_forcing_ratio=1)\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Loss: {:.2f}, Val Loss: {:.2f}, Train BLEU: {:.2f}, Val BLEU: {:.2f}'\\\n",
    "                          .format(result['epoch'], result['train_loss'], result['val_loss'], \n",
    "                                  result['train_bleu'], result['val_bleu']))\n",
    "                    \n",
    "                if inspect: \n",
    "                    inspect_model(model, 'train', train_loader_, dev_loader_)\n",
    "                    inspect_model(model, 'val', train_loader_, dev_loader_)\n",
    "                    \n",
    "                if save_checkpoint: \n",
    "                    if result['val_loss'] == pd.DataFrame.from_dict(results)['val_loss'].min(): \n",
    "                        checkpoint_fp = 'model_checkpoints/{}.pth.tar'.format(model_name)\n",
    "                        check_dir_exists(filename=checkpoint_fp)\n",
    "                        torch.save(model.state_dict(), checkpoint_fp)\n",
    "                \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_experiment(train_loader, dev_loader, model_type, num_epochs=10, learning_rate=0.0005, num_layers=2,\n",
    "#                    enc_hidden_dim=300, dec_hidden_dim=2*300, experiment_name='NA', model_name='NA', inspect=True, \n",
    "#                    lazy_eval=True, save_to_log=True, save_checkpoint=False, print_summary=True, print_intermediate=True):  \n",
    "    \n",
    "#     \"\"\" Wraps all processing, training and evaluation steps in a function to facilitate hyperparam tuning. \n",
    "#         Note that the function takes as input tokenized data rather than raw data since there's significant \n",
    "#         lag time in generating tokens.  \n",
    "#     \"\"\"\n",
    "    \n",
    "#     start_time = time.time() \n",
    "    \n",
    "#     # TODO: try dropout and optimization algorithms. for now use as default: \n",
    "#     optimizer = 'Adam' \n",
    "#     enc_dropout = 0 \n",
    "#     dec_dropout = 0 \n",
    "    \n",
    "#     # instantiate model and optimizer \n",
    "#     if model_type == 'without_attention': \n",
    "#         encoder = EncoderRNN(enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "#                              pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "#         decoder = DecoderRNN(dec_hidden_dim=dec_hidden_dim, enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "#                              pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "#         model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']) \n",
    "        \n",
    "#     elif model_type == 'attention_bahdanau': \n",
    "#         encoder = EncoderRNN(enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "#                              pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "#         decoder = DecoderAttnRNN(dec_hidden_dim=dec_hidden_dim, enc_hidden_dim=enc_hidden_dim, num_layers=num_layers,\n",
    "#                                  pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "#         model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id'])\n",
    "        \n",
    "#     else: \n",
    "#         raise ValueError(\"Invalid model_type. Must be either 'without_attention' or 'attention_bahdanau'\")\n",
    "        \n",
    "#     # train and evaluate \n",
    "#     results = train_and_eval(model, train_loader, dev_loader, id2token=vocab[TARG_LANG]['id2token'], \n",
    "#                              learning_rate=learning_rate, num_epochs=num_epochs, \n",
    "#                              print_intermediate=print_intermediate, save_checkpoint=save_checkpoint, \n",
    "#                              model_name=model_name, lazy_eval=lazy_eval, inspect=inspect)\n",
    "    \n",
    "#     # store, print, and save results \n",
    "#     runtime = (time.time() - start_time) / 60 \n",
    "#     dt_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     hyperparams = {'model_type': model_type, 'num_epochs': num_epochs, 'learning_rate': learning_rate, \n",
    "#                    'enc_hidden_dim': enc_hidden_dim, 'dec_hidden_dim': dec_hidden_dim, 'num_layers': num_layers, \n",
    "#                    'optimizer': optimizer, 'enc_dropout': enc_dropout, 'dec_dropout': dec_dropout, \n",
    "#                    'batch_size': BATCH_SIZE, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "#                    'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "#                    'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN}  \n",
    "        \n",
    "#     if save_to_log: \n",
    "#         append_to_log(hyperparams, results, runtime, experiment_name, dt_created)\n",
    "#     if print_summary: \n",
    "#         print(\"Experiment completed in {} minutes with {:.2f} validation loss and {:.2f} validation BLEU.\".format(\n",
    "#             int(runtime), pd.DataFrame.from_dict(results)['val_loss'].min(), \n",
    "#             pd.DataFrame.from_dict(results)['val_bleu'].max()))\n",
    "        \n",
    "#     return results, hyperparams, runtime, model, train_loader, dev_loader\n",
    "\n",
    "def run_experiment(model_type, num_epochs=10, learning_rate=0.0005, num_layers=2, enc_hidden_dim=300, \n",
    "                   dec_hidden_dim=2*300, experiment_name='NA', model_name='NA', inspect=True, lazy_eval=True, \n",
    "                   lazy_train=False, save_to_log=True, save_checkpoint=False, print_summary=True, print_intermediate=True):  \n",
    "    \n",
    "    \"\"\" Wraps all processing, training and evaluation steps in a function to facilitate hyperparam tuning. \n",
    "        Note that the function takes as input tokenized data rather than raw data since there's significant \n",
    "        lag time in generating tokens.  \n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time() \n",
    "    \n",
    "    # TODO: try dropout and optimization algorithms. for now use as default: \n",
    "    optimizer = 'Adam' \n",
    "    enc_dropout = 0 \n",
    "    dec_dropout = 0 \n",
    "    \n",
    "    # instantiate model and optimizer \n",
    "    if model_type == 'without_attention': \n",
    "        encoder = EncoderRNN(enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "        decoder = DecoderRNN(dec_hidden_dim=dec_hidden_dim, enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "        model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']) \n",
    "        \n",
    "    elif model_type == 'attention_bahdanau': \n",
    "        encoder = EncoderRNN(enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "        decoder = DecoderAttnRNN(dec_hidden_dim=dec_hidden_dim, enc_hidden_dim=enc_hidden_dim, num_layers=num_layers,\n",
    "                                 pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "        model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id'])\n",
    "        \n",
    "    else: \n",
    "        raise ValueError(\"Invalid model_type. Must be either 'without_attention' or 'attention_bahdanau'\")\n",
    "        \n",
    "    # train and evaluate \n",
    "    results = train_and_eval(model, id2token=vocab[TARG_LANG]['id2token'], \n",
    "                             learning_rate=learning_rate, num_epochs=num_epochs, \n",
    "                             print_intermediate=print_intermediate, save_checkpoint=save_checkpoint, \n",
    "                             model_name=model_name, lazy_eval=lazy_eval, lazy_train=lazy_train, inspect=inspect)\n",
    "    \n",
    "    # store, print, and save results \n",
    "    runtime = (time.time() - start_time) / 60 \n",
    "    dt_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    hyperparams = {'model_type': model_type, 'num_epochs': num_epochs, 'learning_rate': learning_rate, \n",
    "                   'enc_hidden_dim': enc_hidden_dim, 'dec_hidden_dim': dec_hidden_dim, 'num_layers': num_layers, \n",
    "                   'optimizer': optimizer, 'enc_dropout': enc_dropout, 'dec_dropout': dec_dropout, \n",
    "                   'batch_size': BATCH_SIZE, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "                   'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "                   'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN}  \n",
    "        \n",
    "    if save_to_log: \n",
    "        append_to_log(hyperparams, results, runtime, experiment_name, dt_created)\n",
    "    if print_summary: \n",
    "        print(\"Experiment completed in {} minutes with {:.2f} validation loss and {:.2f} validation BLEU.\".format(\n",
    "            int(runtime), pd.DataFrame.from_dict(results)['val_loss'].min(), \n",
    "            pd.DataFrame.from_dict(results)['val_bleu'].max()))\n",
    "        \n",
    "    return results, hyperparams, runtime, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to summarize, evaluate, and plot results \n",
    "\n",
    "def summarize_results(results_log): \n",
    "    \"\"\" Summarizes results_log (list) into a dataframe, splitting hyperparameters string into columns, and reducing \n",
    "        the val_acc dict into the best validation accuracy obtained amongst all the epochs logged \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results_log)\n",
    "    results_df = pd.concat([results_df, results_df['hyperparams'].apply(pd.Series)], axis=1)\n",
    "    results_df['val_loss'] = results_df['results'].apply(lambda d: pd.DataFrame.from_dict(d)['val_loss'].min())\n",
    "    return results_df.sort_values(by='val_loss', ascending=True) \n",
    "\n",
    "def plot_multiple_learning_curves(results_df, plot_variable, figsize=(8, 5), legend_loc='best'):\n",
    "    \"\"\" Plots learning curves of MULTIPLE experiments, includes only validation accuracy \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for index, row in results_df.iterrows():\n",
    "        val_loss_hist = pd.DataFrame.from_dict(row['results']).set_index('epoch')['val_loss'] \n",
    "        plt.plot(val_loss_hist, label=\"{} ({}%)\".format(row[plot_variable], val_loss_hist.max()))\n",
    "    plt.legend(title=plot_variable, loc=legend_loc)    \n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "def plot_single_learning_curve(results, figsize=(8, 5)): \n",
    "    \"\"\" Plots learning curve of a SINGLE experiment, includes both train and validation accuracy \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    results_df = results_df.set_index('epoch')\n",
    "    results_df.plot(figsize=figsize)\n",
    "    plt.ylabel('Validation Lossy')\n",
    "    plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to count parameters \n",
    "def count_parameters(model): \n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return all_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 8.65, Val Loss: 8.92, Train BLEU: 1.60, Val BLEU: 0.55\n",
      "Epoch: 1.00, Train Loss: 8.12, Val Loss: 8.62, Train BLEU: 1.60, Val BLEU: 0.36\n",
      "Epoch: 2.00, Train Loss: 7.57, Val Loss: 8.34, Train BLEU: 1.60, Val BLEU: 0.36\n",
      "Epoch: 3.00, Train Loss: 6.99, Val Loss: 8.07, Train BLEU: 1.28, Val BLEU: 0.36\n",
      "Epoch: 4.00, Train Loss: 6.46, Val Loss: 7.85, Train BLEU: 1.28, Val BLEU: 0.36\n",
      "Epoch: 5.00, Train Loss: 6.00, Val Loss: 7.68, Train BLEU: 1.28, Val BLEU: 0.36\n",
      "Epoch: 6.00, Train Loss: 5.63, Val Loss: 7.57, Train BLEU: 1.28, Val BLEU: 0.36\n",
      "Epoch: 7.00, Train Loss: 5.33, Val Loss: 7.51, Train BLEU: 1.28, Val BLEU: 0.36\n",
      "Epoch: 8.00, Train Loss: 5.09, Val Loss: 7.48, Train BLEU: 1.59, Val BLEU: 0.36\n",
      "Epoch: 9.00, Train Loss: 4.90, Val Loss: 7.49, Train BLEU: 2.80, Val BLEU: 0.97\n",
      "Epoch: 10.00, Train Loss: 4.76, Val Loss: 7.53, Train BLEU: 3.06, Val BLEU: 1.07\n",
      "Epoch: 11.00, Train Loss: 4.65, Val Loss: 7.59, Train BLEU: 2.79, Val BLEU: 0.96\n",
      "Epoch: 12.00, Train Loss: 4.57, Val Loss: 7.67, Train BLEU: 2.36, Val BLEU: 0.79\n",
      "Epoch: 13.00, Train Loss: 4.51, Val Loss: 7.75, Train BLEU: 3.00, Val BLEU: 1.34\n",
      "Epoch: 14.00, Train Loss: 4.47, Val Loss: 7.84, Train BLEU: 3.70, Val BLEU: 1.70\n",
      "Epoch: 15.00, Train Loss: 4.44, Val Loss: 7.94, Train BLEU: 4.25, Val BLEU: 2.01\n",
      "Epoch: 16.00, Train Loss: 4.42, Val Loss: 8.05, Train BLEU: 3.72, Val BLEU: 1.80\n",
      "Epoch: 17.00, Train Loss: 4.40, Val Loss: 8.15, Train BLEU: 2.77, Val BLEU: 1.14\n",
      "Epoch: 18.00, Train Loss: 4.39, Val Loss: 8.26, Train BLEU: 2.47, Val BLEU: 1.00\n",
      "Epoch: 19.00, Train Loss: 4.38, Val Loss: 8.36, Train BLEU: 2.00, Val BLEU: 0.81\n",
      "Epoch: 20.00, Train Loss: 4.37, Val Loss: 8.45, Train BLEU: 2.03, Val BLEU: 0.66\n",
      "Epoch: 21.00, Train Loss: 4.36, Val Loss: 8.53, Train BLEU: 1.97, Val BLEU: 0.65\n",
      "Epoch: 22.00, Train Loss: 4.35, Val Loss: 8.61, Train BLEU: 1.99, Val BLEU: 0.77\n",
      "Epoch: 23.00, Train Loss: 4.34, Val Loss: 8.67, Train BLEU: 2.04, Val BLEU: 0.81\n",
      "Epoch: 24.00, Train Loss: 4.32, Val Loss: 8.72, Train BLEU: 2.23, Val BLEU: 0.84\n",
      "Epoch: 25.00, Train Loss: 4.31, Val Loss: 8.77, Train BLEU: 2.31, Val BLEU: 0.97\n",
      "Epoch: 26.00, Train Loss: 4.29, Val Loss: 8.81, Train BLEU: 2.29, Val BLEU: 0.93\n",
      "Epoch: 27.00, Train Loss: 4.28, Val Loss: 8.85, Train BLEU: 2.16, Val BLEU: 0.88\n",
      "Epoch: 28.00, Train Loss: 4.27, Val Loss: 8.90, Train BLEU: 2.07, Val BLEU: 0.84\n",
      "Epoch: 29.00, Train Loss: 4.25, Val Loss: 8.94, Train BLEU: 2.03, Val BLEU: 0.82\n",
      "Epoch: 30.00, Train Loss: 4.24, Val Loss: 8.98, Train BLEU: 2.00, Val BLEU: 0.81\n",
      "Epoch: 31.00, Train Loss: 4.23, Val Loss: 9.02, Train BLEU: 1.98, Val BLEU: 0.80\n",
      "Epoch: 32.00, Train Loss: 4.22, Val Loss: 9.05, Train BLEU: 1.98, Val BLEU: 0.80\n",
      "Epoch: 33.00, Train Loss: 4.21, Val Loss: 9.08, Train BLEU: 2.10, Val BLEU: 0.80\n",
      "Epoch: 34.00, Train Loss: 4.20, Val Loss: 9.10, Train BLEU: 2.18, Val BLEU: 0.81\n",
      "Epoch: 35.00, Train Loss: 4.19, Val Loss: 9.12, Train BLEU: 2.15, Val BLEU: 0.81\n",
      "Epoch: 36.00, Train Loss: 4.17, Val Loss: 9.14, Train BLEU: 2.18, Val BLEU: 0.82\n",
      "Epoch: 37.00, Train Loss: 4.16, Val Loss: 9.16, Train BLEU: 2.27, Val BLEU: 0.84\n",
      "Epoch: 38.00, Train Loss: 4.15, Val Loss: 9.17, Train BLEU: 2.33, Val BLEU: 0.86\n",
      "Epoch: 39.00, Train Loss: 4.13, Val Loss: 9.19, Train BLEU: 2.38, Val BLEU: 0.88\n",
      "Epoch: 40.00, Train Loss: 4.12, Val Loss: 9.20, Train BLEU: 2.38, Val BLEU: 0.91\n",
      "Epoch: 41.00, Train Loss: 4.11, Val Loss: 9.21, Train BLEU: 2.46, Val BLEU: 0.91\n",
      "Epoch: 42.00, Train Loss: 4.10, Val Loss: 9.22, Train BLEU: 2.46, Val BLEU: 0.92\n",
      "Epoch: 43.00, Train Loss: 4.08, Val Loss: 9.24, Train BLEU: 2.40, Val BLEU: 0.90\n",
      "Epoch: 44.00, Train Loss: 4.07, Val Loss: 9.25, Train BLEU: 2.38, Val BLEU: 0.89\n",
      "Epoch: 45.00, Train Loss: 4.06, Val Loss: 9.26, Train BLEU: 2.28, Val BLEU: 0.87\n",
      "Epoch: 46.00, Train Loss: 4.05, Val Loss: 9.28, Train BLEU: 2.26, Val BLEU: 0.87\n",
      "Epoch: 47.00, Train Loss: 4.03, Val Loss: 9.29, Train BLEU: 2.28, Val BLEU: 0.88\n",
      "Epoch: 48.00, Train Loss: 4.02, Val Loss: 9.32, Train BLEU: 2.29, Val BLEU: 0.88\n",
      "Epoch: 49.00, Train Loss: 4.01, Val Loss: 9.33, Train BLEU: 2.30, Val BLEU: 0.88\n",
      "Epoch: 50.00, Train Loss: 3.99, Val Loss: 9.33, Train BLEU: 2.36, Val BLEU: 0.90\n",
      "Epoch: 51.00, Train Loss: 3.98, Val Loss: 9.34, Train BLEU: 2.37, Val BLEU: 0.91\n",
      "Epoch: 52.00, Train Loss: 3.97, Val Loss: 9.36, Train BLEU: 2.26, Val BLEU: 0.89\n",
      "Epoch: 53.00, Train Loss: 3.96, Val Loss: 9.37, Train BLEU: 2.28, Val BLEU: 0.90\n",
      "Epoch: 54.00, Train Loss: 3.94, Val Loss: 9.37, Train BLEU: 2.36, Val BLEU: 0.93\n",
      "Epoch: 55.00, Train Loss: 3.93, Val Loss: 9.38, Train BLEU: 2.40, Val BLEU: 0.94\n",
      "Epoch: 56.00, Train Loss: 3.92, Val Loss: 9.41, Train BLEU: 2.32, Val BLEU: 0.92\n",
      "Epoch: 57.00, Train Loss: 3.90, Val Loss: 9.40, Train BLEU: 2.46, Val BLEU: 0.94\n",
      "Epoch: 58.00, Train Loss: 3.89, Val Loss: 9.40, Train BLEU: 2.47, Val BLEU: 0.94\n",
      "Epoch: 59.00, Train Loss: 3.88, Val Loss: 9.43, Train BLEU: 2.37, Val BLEU: 0.91\n",
      "Epoch: 60.00, Train Loss: 3.87, Val Loss: 9.43, Train BLEU: 2.39, Val BLEU: 0.91\n",
      "Epoch: 61.00, Train Loss: 3.85, Val Loss: 9.43, Train BLEU: 2.49, Val BLEU: 0.92\n",
      "Epoch: 62.00, Train Loss: 3.84, Val Loss: 9.44, Train BLEU: 2.39, Val BLEU: 0.91\n",
      "Epoch: 63.00, Train Loss: 3.83, Val Loss: 9.45, Train BLEU: 2.40, Val BLEU: 0.91\n",
      "Epoch: 64.00, Train Loss: 3.82, Val Loss: 9.47, Train BLEU: 2.37, Val BLEU: 0.97\n",
      "Epoch: 65.00, Train Loss: 3.80, Val Loss: 9.45, Train BLEU: 2.48, Val BLEU: 1.01\n",
      "Epoch: 66.00, Train Loss: 3.79, Val Loss: 9.48, Train BLEU: 2.51, Val BLEU: 0.97\n",
      "Epoch: 67.00, Train Loss: 3.78, Val Loss: 9.49, Train BLEU: 2.54, Val BLEU: 0.98\n",
      "Epoch: 68.00, Train Loss: 3.77, Val Loss: 9.47, Train BLEU: 2.84, Val BLEU: 1.02\n",
      "Epoch: 69.00, Train Loss: 3.75, Val Loss: 9.49, Train BLEU: 2.64, Val BLEU: 1.00\n",
      "Epoch: 70.00, Train Loss: 3.74, Val Loss: 9.52, Train BLEU: 2.53, Val BLEU: 0.96\n",
      "Epoch: 71.00, Train Loss: 3.72, Val Loss: 9.52, Train BLEU: 2.53, Val BLEU: 0.98\n",
      "Epoch: 72.00, Train Loss: 3.72, Val Loss: 9.49, Train BLEU: 2.75, Val BLEU: 1.03\n",
      "Epoch: 73.00, Train Loss: 3.70, Val Loss: 9.51, Train BLEU: 2.74, Val BLEU: 1.03\n",
      "Epoch: 74.00, Train Loss: 3.69, Val Loss: 9.56, Train BLEU: 2.58, Val BLEU: 1.12\n",
      "Epoch: 75.00, Train Loss: 3.68, Val Loss: 9.57, Train BLEU: 2.57, Val BLEU: 1.11\n",
      "Epoch: 76.00, Train Loss: 3.66, Val Loss: 9.55, Train BLEU: 2.72, Val BLEU: 1.08\n",
      "Epoch: 77.00, Train Loss: 3.65, Val Loss: 9.54, Train BLEU: 2.88, Val BLEU: 1.03\n",
      "Epoch: 78.00, Train Loss: 3.63, Val Loss: 9.58, Train BLEU: 2.78, Val BLEU: 1.11\n",
      "Epoch: 79.00, Train Loss: 3.63, Val Loss: 9.60, Train BLEU: 2.81, Val BLEU: 1.14\n",
      "Epoch: 80.00, Train Loss: 3.60, Val Loss: 9.59, Train BLEU: 2.83, Val BLEU: 1.06\n",
      "Epoch: 81.00, Train Loss: 3.60, Val Loss: 9.57, Train BLEU: 2.96, Val BLEU: 1.18\n",
      "Epoch: 82.00, Train Loss: 3.58, Val Loss: 9.59, Train BLEU: 2.91, Val BLEU: 1.09\n",
      "Epoch: 83.00, Train Loss: 3.58, Val Loss: 9.63, Train BLEU: 2.76, Val BLEU: 1.23\n",
      "Epoch: 84.00, Train Loss: 3.56, Val Loss: 9.63, Train BLEU: 2.83, Val BLEU: 1.17\n",
      "Epoch: 85.00, Train Loss: 3.54, Val Loss: 9.59, Train BLEU: 3.14, Val BLEU: 1.30\n",
      "Epoch: 86.00, Train Loss: 3.53, Val Loss: 9.60, Train BLEU: 3.25, Val BLEU: 1.34\n",
      "Epoch: 87.00, Train Loss: 3.51, Val Loss: 9.65, Train BLEU: 3.04, Val BLEU: 1.31\n",
      "Epoch: 88.00, Train Loss: 3.51, Val Loss: 9.67, Train BLEU: 2.98, Val BLEU: 1.30\n",
      "Epoch: 89.00, Train Loss: 3.48, Val Loss: 9.65, Train BLEU: 3.25, Val BLEU: 1.28\n",
      "Epoch: 90.00, Train Loss: 3.47, Val Loss: 9.63, Train BLEU: 3.41, Val BLEU: 1.38\n",
      "Epoch: 91.00, Train Loss: 3.45, Val Loss: 9.66, Train BLEU: 3.31, Val BLEU: 1.34\n",
      "Epoch: 92.00, Train Loss: 3.45, Val Loss: 9.69, Train BLEU: 3.28, Val BLEU: 1.34\n",
      "Epoch: 93.00, Train Loss: 3.44, Val Loss: 9.70, Train BLEU: 3.33, Val BLEU: 1.33\n",
      "Epoch: 94.00, Train Loss: 3.42, Val Loss: 9.67, Train BLEU: 3.43, Val BLEU: 1.37\n",
      "Epoch: 95.00, Train Loss: 3.40, Val Loss: 9.67, Train BLEU: 3.45, Val BLEU: 1.41\n",
      "Epoch: 96.00, Train Loss: 3.39, Val Loss: 9.72, Train BLEU: 3.43, Val BLEU: 1.35\n",
      "Epoch: 97.00, Train Loss: 3.39, Val Loss: 9.74, Train BLEU: 3.29, Val BLEU: 1.36\n",
      "Epoch: 98.00, Train Loss: 3.36, Val Loss: 9.71, Train BLEU: 3.42, Val BLEU: 1.40\n",
      "Epoch: 99.00, Train Loss: 3.35, Val Loss: 9.70, Train BLEU: 3.66, Val BLEU: 1.47\n",
      "Epoch: 100.00, Train Loss: 3.33, Val Loss: 9.71, Train BLEU: 3.65, Val BLEU: 1.47\n",
      "Epoch: 101.00, Train Loss: 3.32, Val Loss: 9.74, Train BLEU: 3.56, Val BLEU: 1.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102.00, Train Loss: 3.31, Val Loss: 9.76, Train BLEU: 3.57, Val BLEU: 1.54\n",
      "Epoch: 103.00, Train Loss: 3.29, Val Loss: 9.75, Train BLEU: 3.69, Val BLEU: 1.53\n",
      "Epoch: 104.00, Train Loss: 3.28, Val Loss: 9.73, Train BLEU: 4.00, Val BLEU: 1.50\n",
      "Epoch: 105.00, Train Loss: 3.26, Val Loss: 9.76, Train BLEU: 3.89, Val BLEU: 1.48\n",
      "Epoch: 106.00, Train Loss: 3.25, Val Loss: 9.78, Train BLEU: 3.73, Val BLEU: 1.57\n",
      "Epoch: 107.00, Train Loss: 3.23, Val Loss: 9.79, Train BLEU: 3.84, Val BLEU: 1.51\n",
      "Epoch: 108.00, Train Loss: 3.22, Val Loss: 9.78, Train BLEU: 4.10, Val BLEU: 1.56\n",
      "Epoch: 109.00, Train Loss: 3.20, Val Loss: 9.80, Train BLEU: 4.17, Val BLEU: 1.61\n",
      "Epoch: 110.00, Train Loss: 3.19, Val Loss: 9.81, Train BLEU: 4.12, Val BLEU: 1.62\n",
      "Epoch: 111.00, Train Loss: 3.17, Val Loss: 9.80, Train BLEU: 4.27, Val BLEU: 1.67\n",
      "Epoch: 112.00, Train Loss: 3.16, Val Loss: 9.82, Train BLEU: 4.14, Val BLEU: 1.61\n",
      "Epoch: 113.00, Train Loss: 3.14, Val Loss: 9.82, Train BLEU: 4.23, Val BLEU: 1.62\n",
      "Epoch: 114.00, Train Loss: 3.13, Val Loss: 9.81, Train BLEU: 4.44, Val BLEU: 1.46\n",
      "Epoch: 115.00, Train Loss: 3.11, Val Loss: 9.84, Train BLEU: 4.25, Val BLEU: 1.65\n",
      "Epoch: 116.00, Train Loss: 3.10, Val Loss: 9.86, Train BLEU: 4.27, Val BLEU: 1.63\n",
      "Epoch: 117.00, Train Loss: 3.08, Val Loss: 9.85, Train BLEU: 4.42, Val BLEU: 1.67\n",
      "Epoch: 118.00, Train Loss: 3.07, Val Loss: 9.84, Train BLEU: 4.49, Val BLEU: 1.71\n",
      "Epoch: 119.00, Train Loss: 3.06, Val Loss: 9.86, Train BLEU: 4.61, Val BLEU: 1.69\n",
      "Epoch: 120.00, Train Loss: 3.05, Val Loss: 9.89, Train BLEU: 4.42, Val BLEU: 1.63\n",
      "Epoch: 121.00, Train Loss: 3.03, Val Loss: 9.87, Train BLEU: 4.61, Val BLEU: 1.70\n",
      "Epoch: 122.00, Train Loss: 3.03, Val Loss: 9.85, Train BLEU: 4.84, Val BLEU: 1.71\n",
      "Epoch: 123.00, Train Loss: 3.00, Val Loss: 9.90, Train BLEU: 4.63, Val BLEU: 1.69\n",
      "Epoch: 124.00, Train Loss: 3.00, Val Loss: 9.92, Train BLEU: 4.47, Val BLEU: 1.67\n",
      "Epoch: 125.00, Train Loss: 2.97, Val Loss: 9.88, Train BLEU: 4.94, Val BLEU: 1.79\n",
      "Epoch: 126.00, Train Loss: 2.96, Val Loss: 9.88, Train BLEU: 5.15, Val BLEU: 1.82\n",
      "Epoch: 127.00, Train Loss: 2.94, Val Loss: 9.91, Train BLEU: 5.01, Val BLEU: 1.77\n",
      "Epoch: 128.00, Train Loss: 2.95, Val Loss: 9.95, Train BLEU: 4.53, Val BLEU: 1.70\n",
      "Epoch: 129.00, Train Loss: 2.91, Val Loss: 9.92, Train BLEU: 4.88, Val BLEU: 1.78\n",
      "Epoch: 130.00, Train Loss: 2.91, Val Loss: 9.90, Train BLEU: 4.92, Val BLEU: 1.84\n",
      "Epoch: 131.00, Train Loss: 2.89, Val Loss: 9.94, Train BLEU: 4.62, Val BLEU: 1.83\n",
      "Epoch: 132.00, Train Loss: 2.89, Val Loss: 9.97, Train BLEU: 4.62, Val BLEU: 1.76\n",
      "Epoch: 133.00, Train Loss: 2.87, Val Loss: 9.97, Train BLEU: 4.76, Val BLEU: 1.82\n",
      "Epoch: 134.00, Train Loss: 2.85, Val Loss: 9.94, Train BLEU: 4.84, Val BLEU: 1.86\n",
      "Epoch: 135.00, Train Loss: 2.83, Val Loss: 9.94, Train BLEU: 4.80, Val BLEU: 1.87\n",
      "Epoch: 136.00, Train Loss: 2.83, Val Loss: 9.99, Train BLEU: 4.77, Val BLEU: 1.77\n",
      "Epoch: 137.00, Train Loss: 2.82, Val Loss: 10.00, Train BLEU: 4.65, Val BLEU: 1.84\n",
      "Epoch: 138.00, Train Loss: 2.79, Val Loss: 9.98, Train BLEU: 4.81, Val BLEU: 1.86\n",
      "Epoch: 139.00, Train Loss: 2.79, Val Loss: 9.96, Train BLEU: 5.09, Val BLEU: 1.96\n",
      "Epoch: 140.00, Train Loss: 2.77, Val Loss: 10.01, Train BLEU: 4.93, Val BLEU: 1.95\n",
      "Epoch: 141.00, Train Loss: 2.78, Val Loss: 10.05, Train BLEU: 4.74, Val BLEU: 1.91\n",
      "Epoch: 142.00, Train Loss: 2.74, Val Loss: 10.02, Train BLEU: 5.06, Val BLEU: 2.00\n",
      "Epoch: 143.00, Train Loss: 2.73, Val Loss: 10.00, Train BLEU: 5.09, Val BLEU: 2.00\n",
      "Epoch: 144.00, Train Loss: 2.71, Val Loss: 10.02, Train BLEU: 4.96, Val BLEU: 1.96\n",
      "Epoch: 145.00, Train Loss: 2.70, Val Loss: 10.05, Train BLEU: 4.93, Val BLEU: 1.89\n",
      "Epoch: 146.00, Train Loss: 2.69, Val Loss: 10.06, Train BLEU: 4.73, Val BLEU: 1.94\n",
      "Epoch: 147.00, Train Loss: 2.67, Val Loss: 10.05, Train BLEU: 4.76, Val BLEU: 1.92\n",
      "Epoch: 148.00, Train Loss: 2.66, Val Loss: 10.05, Train BLEU: 4.96, Val BLEU: 1.96\n",
      "Epoch: 149.00, Train Loss: 2.65, Val Loss: 10.08, Train BLEU: 4.82, Val BLEU: 1.99\n",
      "Epoch: 150.00, Train Loss: 2.64, Val Loss: 10.09, Train BLEU: 4.74, Val BLEU: 1.96\n",
      "Epoch: 151.00, Train Loss: 2.62, Val Loss: 10.09, Train BLEU: 4.87, Val BLEU: 1.99\n",
      "Epoch: 152.00, Train Loss: 2.61, Val Loss: 10.06, Train BLEU: 4.97, Val BLEU: 2.08\n",
      "Epoch: 153.00, Train Loss: 2.60, Val Loss: 10.08, Train BLEU: 5.00, Val BLEU: 2.00\n",
      "Epoch: 154.00, Train Loss: 2.59, Val Loss: 10.12, Train BLEU: 4.84, Val BLEU: 1.99\n",
      "Epoch: 155.00, Train Loss: 2.57, Val Loss: 10.12, Train BLEU: 5.00, Val BLEU: 1.92\n",
      "Epoch: 156.00, Train Loss: 2.56, Val Loss: 10.11, Train BLEU: 5.12, Val BLEU: 2.02\n",
      "Epoch: 157.00, Train Loss: 2.55, Val Loss: 10.10, Train BLEU: 5.24, Val BLEU: 2.05\n",
      "Epoch: 158.00, Train Loss: 2.54, Val Loss: 10.15, Train BLEU: 5.18, Val BLEU: 1.97\n",
      "Epoch: 159.00, Train Loss: 2.53, Val Loss: 10.16, Train BLEU: 4.84, Val BLEU: 1.95\n",
      "Epoch: 160.00, Train Loss: 2.51, Val Loss: 10.12, Train BLEU: 5.20, Val BLEU: 2.06\n",
      "Epoch: 161.00, Train Loss: 2.51, Val Loss: 10.11, Train BLEU: 5.30, Val BLEU: 2.17\n",
      "Epoch: 162.00, Train Loss: 2.49, Val Loss: 10.18, Train BLEU: 5.05, Val BLEU: 1.99\n",
      "Epoch: 163.00, Train Loss: 2.48, Val Loss: 10.18, Train BLEU: 5.09, Val BLEU: 2.03\n",
      "Epoch: 164.00, Train Loss: 2.46, Val Loss: 10.15, Train BLEU: 5.24, Val BLEU: 2.16\n",
      "Epoch: 165.00, Train Loss: 2.45, Val Loss: 10.17, Train BLEU: 5.23, Val BLEU: 2.13\n",
      "Epoch: 166.00, Train Loss: 2.45, Val Loss: 10.21, Train BLEU: 4.80, Val BLEU: 2.07\n",
      "Epoch: 167.00, Train Loss: 2.43, Val Loss: 10.22, Train BLEU: 4.87, Val BLEU: 2.06\n",
      "Epoch: 168.00, Train Loss: 2.42, Val Loss: 10.18, Train BLEU: 5.12, Val BLEU: 2.13\n",
      "Epoch: 169.00, Train Loss: 2.40, Val Loss: 10.20, Train BLEU: 4.92, Val BLEU: 2.04\n",
      "Epoch: 170.00, Train Loss: 2.42, Val Loss: 10.24, Train BLEU: 4.85, Val BLEU: 1.96\n",
      "Epoch: 171.00, Train Loss: 2.38, Val Loss: 10.22, Train BLEU: 5.08, Val BLEU: 2.06\n",
      "Epoch: 172.00, Train Loss: 2.37, Val Loss: 10.22, Train BLEU: 5.16, Val BLEU: 2.11\n",
      "Epoch: 173.00, Train Loss: 2.35, Val Loss: 10.24, Train BLEU: 5.33, Val BLEU: 2.09\n",
      "Epoch: 174.00, Train Loss: 2.36, Val Loss: 10.26, Train BLEU: 4.81, Val BLEU: 2.07\n",
      "Epoch: 175.00, Train Loss: 2.33, Val Loss: 10.24, Train BLEU: 5.20, Val BLEU: 2.09\n",
      "Epoch: 176.00, Train Loss: 2.33, Val Loss: 10.23, Train BLEU: 5.23, Val BLEU: 2.20\n",
      "Epoch: 177.00, Train Loss: 2.31, Val Loss: 10.26, Train BLEU: 5.16, Val BLEU: 2.17\n",
      "Epoch: 178.00, Train Loss: 2.32, Val Loss: 10.30, Train BLEU: 4.90, Val BLEU: 2.13\n",
      "Epoch: 179.00, Train Loss: 2.29, Val Loss: 10.29, Train BLEU: 5.00, Val BLEU: 2.14\n",
      "Epoch: 180.00, Train Loss: 2.31, Val Loss: 10.25, Train BLEU: 5.33, Val BLEU: 2.23\n",
      "Epoch: 181.00, Train Loss: 2.27, Val Loss: 10.28, Train BLEU: 5.33, Val BLEU: 2.27\n",
      "Epoch: 182.00, Train Loss: 2.29, Val Loss: 10.34, Train BLEU: 4.84, Val BLEU: 2.12\n",
      "Epoch: 183.00, Train Loss: 2.27, Val Loss: 10.35, Train BLEU: 4.98, Val BLEU: 2.10\n",
      "Epoch: 184.00, Train Loss: 2.25, Val Loss: 10.30, Train BLEU: 5.28, Val BLEU: 2.25\n",
      "Epoch: 185.00, Train Loss: 2.24, Val Loss: 10.30, Train BLEU: 5.39, Val BLEU: 2.31\n",
      "Epoch: 186.00, Train Loss: 2.22, Val Loss: 10.34, Train BLEU: 4.97, Val BLEU: 2.21\n",
      "Epoch: 187.00, Train Loss: 2.23, Val Loss: 10.35, Train BLEU: 5.00, Val BLEU: 2.19\n",
      "Epoch: 188.00, Train Loss: 2.19, Val Loss: 10.34, Train BLEU: 5.09, Val BLEU: 2.13\n",
      "Epoch: 189.00, Train Loss: 2.19, Val Loss: 10.33, Train BLEU: 5.32, Val BLEU: 2.26\n",
      "Epoch: 190.00, Train Loss: 2.18, Val Loss: 10.37, Train BLEU: 5.15, Val BLEU: 2.24\n",
      "Epoch: 191.00, Train Loss: 2.19, Val Loss: 10.39, Train BLEU: 5.21, Val BLEU: 2.22\n",
      "Epoch: 192.00, Train Loss: 2.15, Val Loss: 10.37, Train BLEU: 5.39, Val BLEU: 2.32\n",
      "Epoch: 193.00, Train Loss: 2.15, Val Loss: 10.36, Train BLEU: 5.40, Val BLEU: 2.30\n",
      "Epoch: 194.00, Train Loss: 2.13, Val Loss: 10.37, Train BLEU: 5.25, Val BLEU: 2.24\n",
      "Epoch: 195.00, Train Loss: 2.16, Val Loss: 10.41, Train BLEU: 5.07, Val BLEU: 2.21\n",
      "Epoch: 196.00, Train Loss: 2.13, Val Loss: 10.41, Train BLEU: 4.96, Val BLEU: 2.22\n",
      "Epoch: 197.00, Train Loss: 2.11, Val Loss: 10.39, Train BLEU: 5.26, Val BLEU: 2.29\n",
      "Epoch: 198.00, Train Loss: 2.11, Val Loss: 10.39, Train BLEU: 5.30, Val BLEU: 2.43\n",
      "Epoch: 199.00, Train Loss: 2.09, Val Loss: 10.42, Train BLEU: 5.30, Val BLEU: 2.34\n",
      "Epoch: 200.00, Train Loss: 2.10, Val Loss: 10.44, Train BLEU: 5.02, Val BLEU: 2.26\n",
      "Epoch: 201.00, Train Loss: 2.08, Val Loss: 10.41, Train BLEU: 5.44, Val BLEU: 2.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202.00, Train Loss: 2.07, Val Loss: 10.42, Train BLEU: 5.43, Val BLEU: 2.34\n",
      "Epoch: 203.00, Train Loss: 2.05, Val Loss: 10.45, Train BLEU: 5.27, Val BLEU: 2.29\n",
      "Epoch: 204.00, Train Loss: 2.05, Val Loss: 10.47, Train BLEU: 5.21, Val BLEU: 2.29\n",
      "Epoch: 205.00, Train Loss: 2.04, Val Loss: 10.45, Train BLEU: 5.21, Val BLEU: 2.35\n",
      "Epoch: 206.00, Train Loss: 2.03, Val Loss: 10.45, Train BLEU: 5.29, Val BLEU: 2.36\n",
      "Epoch: 207.00, Train Loss: 2.06, Val Loss: 10.50, Train BLEU: 5.31, Val BLEU: 2.26\n",
      "Epoch: 208.00, Train Loss: 2.05, Val Loss: 10.51, Train BLEU: 5.38, Val BLEU: 2.28\n",
      "Epoch: 209.00, Train Loss: 2.02, Val Loss: 10.46, Train BLEU: 5.60, Val BLEU: 2.42\n",
      "Epoch: 210.00, Train Loss: 2.02, Val Loss: 10.46, Train BLEU: 5.43, Val BLEU: 2.45\n",
      "Epoch: 211.00, Train Loss: 2.00, Val Loss: 10.52, Train BLEU: 5.48, Val BLEU: 2.35\n",
      "Epoch: 212.00, Train Loss: 2.03, Val Loss: 10.54, Train BLEU: 5.54, Val BLEU: 2.36\n",
      "Epoch: 213.00, Train Loss: 1.97, Val Loss: 10.52, Train BLEU: 5.57, Val BLEU: 2.44\n",
      "Epoch: 214.00, Train Loss: 1.98, Val Loss: 10.50, Train BLEU: 5.57, Val BLEU: 2.46\n",
      "Epoch: 215.00, Train Loss: 1.95, Val Loss: 10.53, Train BLEU: 5.61, Val BLEU: 2.34\n",
      "Epoch: 216.00, Train Loss: 1.96, Val Loss: 10.56, Train BLEU: 5.38, Val BLEU: 2.30\n",
      "Epoch: 217.00, Train Loss: 1.94, Val Loss: 10.55, Train BLEU: 5.43, Val BLEU: 2.30\n",
      "Epoch: 218.00, Train Loss: 1.95, Val Loss: 10.52, Train BLEU: 5.58, Val BLEU: 2.50\n",
      "Epoch: 219.00, Train Loss: 1.92, Val Loss: 10.55, Train BLEU: 5.62, Val BLEU: 2.37\n",
      "Epoch: 220.00, Train Loss: 1.93, Val Loss: 10.59, Train BLEU: 5.81, Val BLEU: 2.43\n",
      "Epoch: 221.00, Train Loss: 1.90, Val Loss: 10.58, Train BLEU: 5.82, Val BLEU: 2.46\n",
      "Epoch: 222.00, Train Loss: 1.92, Val Loss: 10.55, Train BLEU: 5.52, Val BLEU: 2.48\n",
      "Epoch: 223.00, Train Loss: 1.88, Val Loss: 10.58, Train BLEU: 5.54, Val BLEU: 2.47\n",
      "Epoch: 224.00, Train Loss: 1.90, Val Loss: 10.61, Train BLEU: 5.61, Val BLEU: 2.43\n",
      "Epoch: 225.00, Train Loss: 1.87, Val Loss: 10.60, Train BLEU: 5.36, Val BLEU: 2.44\n",
      "Epoch: 226.00, Train Loss: 1.87, Val Loss: 10.60, Train BLEU: 5.49, Val BLEU: 2.43\n",
      "Epoch: 227.00, Train Loss: 1.86, Val Loss: 10.61, Train BLEU: 5.68, Val BLEU: 2.41\n",
      "Epoch: 228.00, Train Loss: 1.86, Val Loss: 10.63, Train BLEU: 5.60, Val BLEU: 2.44\n",
      "Epoch: 229.00, Train Loss: 1.84, Val Loss: 10.63, Train BLEU: 5.75, Val BLEU: 2.56\n",
      "Epoch: 230.00, Train Loss: 1.84, Val Loss: 10.62, Train BLEU: 5.72, Val BLEU: 2.59\n",
      "Epoch: 231.00, Train Loss: 1.83, Val Loss: 10.64, Train BLEU: 5.64, Val BLEU: 2.44\n",
      "Epoch: 232.00, Train Loss: 1.86, Val Loss: 10.69, Train BLEU: 5.52, Val BLEU: 2.34\n",
      "Epoch: 233.00, Train Loss: 1.83, Val Loss: 10.67, Train BLEU: 5.52, Val BLEU: 2.51\n",
      "Epoch: 234.00, Train Loss: 1.84, Val Loss: 10.62, Train BLEU: 5.62, Val BLEU: 2.65\n",
      "Epoch: 235.00, Train Loss: 1.80, Val Loss: 10.65, Train BLEU: 5.70, Val BLEU: 2.62\n",
      "Epoch: 236.00, Train Loss: 1.84, Val Loss: 10.69, Train BLEU: 5.75, Val BLEU: 2.45\n",
      "Epoch: 237.00, Train Loss: 1.81, Val Loss: 10.69, Train BLEU: 5.74, Val BLEU: 2.41\n",
      "Epoch: 238.00, Train Loss: 1.79, Val Loss: 10.65, Train BLEU: 6.00, Val BLEU: 2.53\n",
      "Epoch: 239.00, Train Loss: 1.80, Val Loss: 10.65, Train BLEU: 6.17, Val BLEU: 2.57\n",
      "Epoch: 240.00, Train Loss: 1.78, Val Loss: 10.71, Train BLEU: 6.30, Val BLEU: 2.50\n",
      "Epoch: 241.00, Train Loss: 1.80, Val Loss: 10.73, Train BLEU: 6.21, Val BLEU: 2.49\n",
      "Epoch: 242.00, Train Loss: 1.76, Val Loss: 10.72, Train BLEU: 6.01, Val BLEU: 2.57\n",
      "Epoch: 243.00, Train Loss: 1.76, Val Loss: 10.70, Train BLEU: 5.85, Val BLEU: 2.61\n",
      "Epoch: 244.00, Train Loss: 1.73, Val Loss: 10.72, Train BLEU: 6.02, Val BLEU: 2.49\n",
      "Epoch: 245.00, Train Loss: 1.72, Val Loss: 10.72, Train BLEU: 6.07, Val BLEU: 2.55\n",
      "Epoch: 246.00, Train Loss: 1.71, Val Loss: 10.72, Train BLEU: 6.24, Val BLEU: 2.57\n",
      "Epoch: 247.00, Train Loss: 1.71, Val Loss: 10.74, Train BLEU: 6.04, Val BLEU: 2.56\n",
      "Epoch: 248.00, Train Loss: 1.70, Val Loss: 10.74, Train BLEU: 6.08, Val BLEU: 2.58\n",
      "Epoch: 249.00, Train Loss: 1.70, Val Loss: 10.74, Train BLEU: 6.03, Val BLEU: 2.64\n",
      "Epoch: 250.00, Train Loss: 1.69, Val Loss: 10.76, Train BLEU: 6.02, Val BLEU: 2.62\n",
      "Epoch: 251.00, Train Loss: 1.68, Val Loss: 10.76, Train BLEU: 6.33, Val BLEU: 2.59\n",
      "Epoch: 252.00, Train Loss: 1.69, Val Loss: 10.75, Train BLEU: 6.50, Val BLEU: 2.65\n",
      "Epoch: 253.00, Train Loss: 1.68, Val Loss: 10.78, Train BLEU: 6.22, Val BLEU: 2.59\n",
      "Epoch: 254.00, Train Loss: 1.66, Val Loss: 10.78, Train BLEU: 6.01, Val BLEU: 2.52\n",
      "Epoch: 255.00, Train Loss: 1.68, Val Loss: 10.77, Train BLEU: 6.25, Val BLEU: 2.63\n",
      "Epoch: 256.00, Train Loss: 1.65, Val Loss: 10.79, Train BLEU: 6.58, Val BLEU: 2.62\n",
      "Epoch: 257.00, Train Loss: 1.68, Val Loss: 10.83, Train BLEU: 6.52, Val BLEU: 2.56\n",
      "Epoch: 258.00, Train Loss: 1.63, Val Loss: 10.82, Train BLEU: 6.32, Val BLEU: 2.64\n",
      "Epoch: 259.00, Train Loss: 1.68, Val Loss: 10.80, Train BLEU: 6.47, Val BLEU: 2.79\n",
      "Epoch: 260.00, Train Loss: 1.62, Val Loss: 10.84, Train BLEU: 6.44, Val BLEU: 2.59\n",
      "Epoch: 261.00, Train Loss: 1.68, Val Loss: 10.86, Train BLEU: 6.45, Val BLEU: 2.60\n",
      "Epoch: 262.00, Train Loss: 1.61, Val Loss: 10.83, Train BLEU: 6.57, Val BLEU: 2.63\n",
      "Epoch: 263.00, Train Loss: 1.63, Val Loss: 10.83, Train BLEU: 6.49, Val BLEU: 2.68\n",
      "Epoch: 264.00, Train Loss: 1.60, Val Loss: 10.87, Train BLEU: 6.30, Val BLEU: 2.51\n",
      "Epoch: 265.00, Train Loss: 1.60, Val Loss: 10.87, Train BLEU: 6.41, Val BLEU: 2.57\n",
      "Epoch: 266.00, Train Loss: 1.59, Val Loss: 10.86, Train BLEU: 6.66, Val BLEU: 2.70\n",
      "Epoch: 267.00, Train Loss: 1.59, Val Loss: 10.86, Train BLEU: 6.72, Val BLEU: 2.68\n",
      "Epoch: 268.00, Train Loss: 1.57, Val Loss: 10.89, Train BLEU: 6.47, Val BLEU: 2.57\n",
      "Epoch: 269.00, Train Loss: 1.57, Val Loss: 10.91, Train BLEU: 6.44, Val BLEU: 2.61\n",
      "Epoch: 270.00, Train Loss: 1.56, Val Loss: 10.91, Train BLEU: 6.45, Val BLEU: 2.60\n",
      "Epoch: 271.00, Train Loss: 1.56, Val Loss: 10.90, Train BLEU: 6.58, Val BLEU: 2.64\n",
      "Epoch: 272.00, Train Loss: 1.58, Val Loss: 10.92, Train BLEU: 6.53, Val BLEU: 2.64\n",
      "Epoch: 273.00, Train Loss: 1.56, Val Loss: 10.92, Train BLEU: 6.49, Val BLEU: 2.58\n",
      "Epoch: 274.00, Train Loss: 1.54, Val Loss: 10.92, Train BLEU: 6.50, Val BLEU: 2.63\n",
      "Epoch: 275.00, Train Loss: 1.53, Val Loss: 10.93, Train BLEU: 6.37, Val BLEU: 2.62\n",
      "Epoch: 276.00, Train Loss: 1.55, Val Loss: 10.95, Train BLEU: 6.58, Val BLEU: 2.54\n",
      "Epoch: 277.00, Train Loss: 1.52, Val Loss: 10.94, Train BLEU: 6.62, Val BLEU: 2.52\n",
      "Epoch: 278.00, Train Loss: 1.54, Val Loss: 10.93, Train BLEU: 6.65, Val BLEU: 2.60\n",
      "Epoch: 279.00, Train Loss: 1.51, Val Loss: 10.95, Train BLEU: 6.86, Val BLEU: 2.68\n",
      "Epoch: 280.00, Train Loss: 1.53, Val Loss: 10.98, Train BLEU: 6.59, Val BLEU: 2.58\n",
      "Epoch: 281.00, Train Loss: 1.52, Val Loss: 10.98, Train BLEU: 6.43, Val BLEU: 2.55\n",
      "Epoch: 282.00, Train Loss: 1.51, Val Loss: 10.96, Train BLEU: 6.49, Val BLEU: 2.66\n",
      "Epoch: 283.00, Train Loss: 1.49, Val Loss: 10.97, Train BLEU: 6.68, Val BLEU: 2.64\n",
      "Epoch: 284.00, Train Loss: 1.51, Val Loss: 11.00, Train BLEU: 6.66, Val BLEU: 2.61\n",
      "Epoch: 285.00, Train Loss: 1.48, Val Loss: 11.00, Train BLEU: 7.00, Val BLEU: 2.55\n",
      "Epoch: 286.00, Train Loss: 1.49, Val Loss: 11.00, Train BLEU: 6.73, Val BLEU: 2.56\n",
      "Epoch: 287.00, Train Loss: 1.47, Val Loss: 11.01, Train BLEU: 6.80, Val BLEU: 2.59\n",
      "Epoch: 288.00, Train Loss: 1.46, Val Loss: 11.01, Train BLEU: 6.91, Val BLEU: 2.65\n",
      "Epoch: 289.00, Train Loss: 1.46, Val Loss: 11.01, Train BLEU: 6.97, Val BLEU: 2.69\n",
      "Epoch: 290.00, Train Loss: 1.45, Val Loss: 11.00, Train BLEU: 6.79, Val BLEU: 2.78\n",
      "Epoch: 291.00, Train Loss: 1.46, Val Loss: 11.01, Train BLEU: 6.57, Val BLEU: 2.67\n",
      "Epoch: 292.00, Train Loss: 1.49, Val Loss: 11.05, Train BLEU: 6.36, Val BLEU: 2.52\n",
      "Epoch: 293.00, Train Loss: 1.46, Val Loss: 11.03, Train BLEU: 6.74, Val BLEU: 2.63\n",
      "Epoch: 294.00, Train Loss: 1.44, Val Loss: 11.04, Train BLEU: 6.77, Val BLEU: 2.71\n",
      "Epoch: 295.00, Train Loss: 1.45, Val Loss: 11.06, Train BLEU: 6.74, Val BLEU: 2.77\n",
      "Epoch: 296.00, Train Loss: 1.43, Val Loss: 11.05, Train BLEU: 6.81, Val BLEU: 2.72\n",
      "Epoch: 297.00, Train Loss: 1.47, Val Loss: 11.04, Train BLEU: 7.05, Val BLEU: 2.70\n",
      "Epoch: 298.00, Train Loss: 1.42, Val Loss: 11.08, Train BLEU: 6.95, Val BLEU: 2.73\n",
      "Epoch: 299.00, Train Loss: 1.43, Val Loss: 11.10, Train BLEU: 7.24, Val BLEU: 2.72\n",
      "Epoch: 300.00, Train Loss: 1.41, Val Loss: 11.10, Train BLEU: 7.51, Val BLEU: 2.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301.00, Train Loss: 1.42, Val Loss: 11.09, Train BLEU: 7.11, Val BLEU: 2.66\n",
      "Epoch: 302.00, Train Loss: 1.40, Val Loss: 11.10, Train BLEU: 7.16, Val BLEU: 2.79\n",
      "Epoch: 303.00, Train Loss: 1.40, Val Loss: 11.09, Train BLEU: 7.07, Val BLEU: 2.86\n",
      "Epoch: 304.00, Train Loss: 1.40, Val Loss: 11.09, Train BLEU: 6.96, Val BLEU: 2.88\n",
      "Epoch: 305.00, Train Loss: 1.39, Val Loss: 11.11, Train BLEU: 6.92, Val BLEU: 2.77\n",
      "Epoch: 306.00, Train Loss: 1.40, Val Loss: 11.14, Train BLEU: 6.67, Val BLEU: 2.86\n",
      "Epoch: 307.00, Train Loss: 1.37, Val Loss: 11.13, Train BLEU: 7.02, Val BLEU: 2.85\n",
      "Epoch: 308.00, Train Loss: 1.44, Val Loss: 11.12, Train BLEU: 7.11, Val BLEU: 2.77\n",
      "Epoch: 309.00, Train Loss: 1.37, Val Loss: 11.15, Train BLEU: 7.41, Val BLEU: 2.71\n",
      "Epoch: 310.00, Train Loss: 1.37, Val Loss: 11.16, Train BLEU: 7.22, Val BLEU: 2.82\n",
      "Epoch: 311.00, Train Loss: 1.37, Val Loss: 11.14, Train BLEU: 7.11, Val BLEU: 2.87\n",
      "Epoch: 312.00, Train Loss: 1.36, Val Loss: 11.16, Train BLEU: 7.15, Val BLEU: 2.69\n",
      "Epoch: 313.00, Train Loss: 1.37, Val Loss: 11.19, Train BLEU: 7.32, Val BLEU: 2.63\n",
      "Epoch: 314.00, Train Loss: 1.34, Val Loss: 11.18, Train BLEU: 7.19, Val BLEU: 2.88\n",
      "Epoch: 315.00, Train Loss: 1.38, Val Loss: 11.16, Train BLEU: 7.36, Val BLEU: 2.93\n",
      "Epoch: 316.00, Train Loss: 1.33, Val Loss: 11.18, Train BLEU: 7.10, Val BLEU: 2.73\n",
      "Epoch: 317.00, Train Loss: 1.35, Val Loss: 11.20, Train BLEU: 6.99, Val BLEU: 2.65\n",
      "Epoch: 318.00, Train Loss: 1.37, Val Loss: 11.18, Train BLEU: 7.52, Val BLEU: 2.87\n",
      "Epoch: 319.00, Train Loss: 1.32, Val Loss: 11.20, Train BLEU: 7.27, Val BLEU: 2.80\n",
      "Epoch: 320.00, Train Loss: 1.32, Val Loss: 11.23, Train BLEU: 7.21, Val BLEU: 2.75\n",
      "Epoch: 321.00, Train Loss: 1.31, Val Loss: 11.23, Train BLEU: 7.58, Val BLEU: 2.75\n",
      "Epoch: 322.00, Train Loss: 1.32, Val Loss: 11.21, Train BLEU: 7.17, Val BLEU: 2.71\n",
      "Epoch: 323.00, Train Loss: 1.30, Val Loss: 11.22, Train BLEU: 7.07, Val BLEU: 2.83\n",
      "Epoch: 324.00, Train Loss: 1.30, Val Loss: 11.23, Train BLEU: 7.32, Val BLEU: 2.79\n",
      "Epoch: 325.00, Train Loss: 1.30, Val Loss: 11.23, Train BLEU: 7.46, Val BLEU: 2.74\n",
      "Epoch: 326.00, Train Loss: 1.28, Val Loss: 11.24, Train BLEU: 7.47, Val BLEU: 2.67\n",
      "Epoch: 327.00, Train Loss: 1.28, Val Loss: 11.25, Train BLEU: 7.45, Val BLEU: 2.67\n",
      "Epoch: 328.00, Train Loss: 1.27, Val Loss: 11.25, Train BLEU: 7.50, Val BLEU: 2.84\n",
      "Epoch: 329.00, Train Loss: 1.33, Val Loss: 11.27, Train BLEU: 7.35, Val BLEU: 2.85\n",
      "Epoch: 330.00, Train Loss: 1.30, Val Loss: 11.25, Train BLEU: 7.41, Val BLEU: 2.83\n",
      "Epoch: 331.00, Train Loss: 1.30, Val Loss: 11.26, Train BLEU: 7.48, Val BLEU: 2.78\n",
      "Epoch: 332.00, Train Loss: 1.29, Val Loss: 11.29, Train BLEU: 7.02, Val BLEU: 2.63\n",
      "Epoch: 333.00, Train Loss: 1.29, Val Loss: 11.28, Train BLEU: 7.16, Val BLEU: 2.77\n",
      "Epoch: 334.00, Train Loss: 1.35, Val Loss: 11.26, Train BLEU: 7.16, Val BLEU: 2.98\n",
      "Epoch: 335.00, Train Loss: 1.26, Val Loss: 11.29, Train BLEU: 7.76, Val BLEU: 2.77\n",
      "Epoch: 336.00, Train Loss: 1.33, Val Loss: 11.33, Train BLEU: 7.56, Val BLEU: 2.68\n",
      "Epoch: 337.00, Train Loss: 1.25, Val Loss: 11.33, Train BLEU: 7.65, Val BLEU: 2.77\n",
      "Epoch: 338.00, Train Loss: 1.38, Val Loss: 11.30, Train BLEU: 7.37, Val BLEU: 3.07\n",
      "Epoch: 339.00, Train Loss: 1.24, Val Loss: 11.32, Train BLEU: 7.76, Val BLEU: 2.91\n",
      "Epoch: 340.00, Train Loss: 1.38, Val Loss: 11.35, Train BLEU: 7.30, Val BLEU: 2.72\n",
      "Epoch: 341.00, Train Loss: 1.25, Val Loss: 11.34, Train BLEU: 7.55, Val BLEU: 2.73\n",
      "Epoch: 342.00, Train Loss: 1.36, Val Loss: 11.31, Train BLEU: 7.52, Val BLEU: 2.90\n",
      "Epoch: 343.00, Train Loss: 1.24, Val Loss: 11.32, Train BLEU: 7.69, Val BLEU: 2.90\n",
      "Epoch: 344.00, Train Loss: 1.29, Val Loss: 11.35, Train BLEU: 7.38, Val BLEU: 2.78\n",
      "Epoch: 345.00, Train Loss: 1.23, Val Loss: 11.35, Train BLEU: 7.53, Val BLEU: 2.80\n",
      "Epoch: 346.00, Train Loss: 1.25, Val Loss: 11.36, Train BLEU: 7.92, Val BLEU: 2.87\n",
      "Epoch: 347.00, Train Loss: 1.23, Val Loss: 11.37, Train BLEU: 7.53, Val BLEU: 2.83\n",
      "Epoch: 348.00, Train Loss: 1.22, Val Loss: 11.39, Train BLEU: 7.71, Val BLEU: 2.78\n",
      "Epoch: 349.00, Train Loss: 1.21, Val Loss: 11.38, Train BLEU: 7.36, Val BLEU: 2.86\n",
      "Epoch: 350.00, Train Loss: 1.20, Val Loss: 11.38, Train BLEU: 7.50, Val BLEU: 2.89\n",
      "Epoch: 351.00, Train Loss: 1.20, Val Loss: 11.39, Train BLEU: 7.60, Val BLEU: 2.83\n",
      "Epoch: 352.00, Train Loss: 1.20, Val Loss: 11.40, Train BLEU: 7.47, Val BLEU: 2.74\n",
      "Epoch: 353.00, Train Loss: 1.22, Val Loss: 11.39, Train BLEU: 7.44, Val BLEU: 2.94\n",
      "Epoch: 354.00, Train Loss: 1.19, Val Loss: 11.42, Train BLEU: 7.63, Val BLEU: 2.88\n",
      "Epoch: 355.00, Train Loss: 1.18, Val Loss: 11.41, Train BLEU: 7.83, Val BLEU: 2.84\n",
      "Epoch: 356.00, Train Loss: 1.21, Val Loss: 11.40, Train BLEU: 7.49, Val BLEU: 2.79\n",
      "Epoch: 357.00, Train Loss: 1.17, Val Loss: 11.42, Train BLEU: 7.66, Val BLEU: 2.78\n",
      "Epoch: 358.00, Train Loss: 1.17, Val Loss: 11.42, Train BLEU: 7.52, Val BLEU: 2.82\n",
      "Epoch: 359.00, Train Loss: 1.17, Val Loss: 11.42, Train BLEU: 7.59, Val BLEU: 2.87\n",
      "Epoch: 360.00, Train Loss: 1.17, Val Loss: 11.43, Train BLEU: 7.69, Val BLEU: 2.77\n",
      "Epoch: 361.00, Train Loss: 1.17, Val Loss: 11.44, Train BLEU: 7.46, Val BLEU: 2.74\n",
      "Epoch: 362.00, Train Loss: 1.16, Val Loss: 11.43, Train BLEU: 7.63, Val BLEU: 2.98\n",
      "Epoch: 363.00, Train Loss: 1.16, Val Loss: 11.43, Train BLEU: 7.61, Val BLEU: 2.82\n",
      "Epoch: 364.00, Train Loss: 1.17, Val Loss: 11.42, Train BLEU: 7.64, Val BLEU: 2.98\n",
      "Epoch: 365.00, Train Loss: 1.16, Val Loss: 11.44, Train BLEU: 7.78, Val BLEU: 2.84\n",
      "Epoch: 366.00, Train Loss: 1.16, Val Loss: 11.46, Train BLEU: 7.82, Val BLEU: 2.75\n",
      "Epoch: 367.00, Train Loss: 1.15, Val Loss: 11.46, Train BLEU: 7.69, Val BLEU: 2.89\n",
      "Epoch: 368.00, Train Loss: 1.20, Val Loss: 11.45, Train BLEU: 7.36, Val BLEU: 2.95\n",
      "Epoch: 369.00, Train Loss: 1.14, Val Loss: 11.48, Train BLEU: 7.53, Val BLEU: 2.79\n",
      "Epoch: 370.00, Train Loss: 1.16, Val Loss: 11.49, Train BLEU: 7.53, Val BLEU: 2.67\n",
      "Epoch: 371.00, Train Loss: 1.14, Val Loss: 11.46, Train BLEU: 7.79, Val BLEU: 2.84\n",
      "Epoch: 372.00, Train Loss: 1.18, Val Loss: 11.47, Train BLEU: 7.74, Val BLEU: 2.85\n",
      "Epoch: 373.00, Train Loss: 1.13, Val Loss: 11.51, Train BLEU: 7.64, Val BLEU: 2.85\n",
      "Epoch: 374.00, Train Loss: 1.14, Val Loss: 11.52, Train BLEU: 7.29, Val BLEU: 2.76\n",
      "Epoch: 375.00, Train Loss: 1.18, Val Loss: 11.49, Train BLEU: 7.61, Val BLEU: 3.03\n",
      "Epoch: 376.00, Train Loss: 1.13, Val Loss: 11.51, Train BLEU: 7.86, Val BLEU: 2.98\n",
      "Epoch: 377.00, Train Loss: 1.12, Val Loss: 11.54, Train BLEU: 7.55, Val BLEU: 2.85\n",
      "Epoch: 378.00, Train Loss: 1.11, Val Loss: 11.54, Train BLEU: 7.79, Val BLEU: 2.79\n",
      "Epoch: 379.00, Train Loss: 1.15, Val Loss: 11.51, Train BLEU: 7.71, Val BLEU: 2.88\n",
      "Epoch: 380.00, Train Loss: 1.10, Val Loss: 11.52, Train BLEU: 7.75, Val BLEU: 2.80\n",
      "Epoch: 381.00, Train Loss: 1.10, Val Loss: 11.52, Train BLEU: 7.62, Val BLEU: 2.79\n",
      "Epoch: 382.00, Train Loss: 1.10, Val Loss: 11.54, Train BLEU: 7.79, Val BLEU: 2.87\n",
      "Epoch: 383.00, Train Loss: 1.10, Val Loss: 11.56, Train BLEU: 7.69, Val BLEU: 2.80\n",
      "Epoch: 384.00, Train Loss: 1.10, Val Loss: 11.57, Train BLEU: 7.48, Val BLEU: 2.82\n",
      "Epoch: 385.00, Train Loss: 1.09, Val Loss: 11.56, Train BLEU: 7.64, Val BLEU: 2.83\n",
      "Epoch: 386.00, Train Loss: 1.11, Val Loss: 11.55, Train BLEU: 7.43, Val BLEU: 2.98\n",
      "Epoch: 387.00, Train Loss: 1.09, Val Loss: 11.57, Train BLEU: 7.41, Val BLEU: 2.70\n",
      "Epoch: 388.00, Train Loss: 1.07, Val Loss: 11.57, Train BLEU: 7.47, Val BLEU: 2.74\n",
      "Epoch: 389.00, Train Loss: 1.10, Val Loss: 11.56, Train BLEU: 7.58, Val BLEU: 2.98\n",
      "Epoch: 390.00, Train Loss: 1.08, Val Loss: 11.59, Train BLEU: 7.80, Val BLEU: 2.98\n",
      "Epoch: 391.00, Train Loss: 1.07, Val Loss: 11.59, Train BLEU: 7.64, Val BLEU: 2.75\n",
      "Epoch: 392.00, Train Loss: 1.09, Val Loss: 11.59, Train BLEU: 7.77, Val BLEU: 2.69\n",
      "Epoch: 393.00, Train Loss: 1.07, Val Loss: 11.59, Train BLEU: 8.02, Val BLEU: 2.73\n",
      "Epoch: 394.00, Train Loss: 1.09, Val Loss: 11.61, Train BLEU: 7.63, Val BLEU: 2.84\n",
      "Epoch: 395.00, Train Loss: 1.07, Val Loss: 11.63, Train BLEU: 7.69, Val BLEU: 2.68\n",
      "Epoch: 396.00, Train Loss: 1.06, Val Loss: 11.63, Train BLEU: 7.66, Val BLEU: 2.73\n",
      "Epoch: 397.00, Train Loss: 1.12, Val Loss: 11.60, Train BLEU: 7.81, Val BLEU: 2.93\n",
      "Epoch: 398.00, Train Loss: 1.05, Val Loss: 11.62, Train BLEU: 7.87, Val BLEU: 2.99\n",
      "Epoch: 399.00, Train Loss: 1.08, Val Loss: 11.66, Train BLEU: 7.60, Val BLEU: 2.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400.00, Train Loss: 1.06, Val Loss: 11.66, Train BLEU: 7.62, Val BLEU: 2.66\n",
      "Epoch: 401.00, Train Loss: 1.10, Val Loss: 11.65, Train BLEU: 7.26, Val BLEU: 2.83\n",
      "Epoch: 402.00, Train Loss: 1.05, Val Loss: 11.66, Train BLEU: 7.50, Val BLEU: 2.88\n",
      "Epoch: 403.00, Train Loss: 1.05, Val Loss: 11.66, Train BLEU: 7.53, Val BLEU: 2.84\n",
      "Epoch: 404.00, Train Loss: 1.04, Val Loss: 11.65, Train BLEU: 7.62, Val BLEU: 2.87\n",
      "Epoch: 405.00, Train Loss: 1.03, Val Loss: 11.66, Train BLEU: 7.81, Val BLEU: 2.86\n",
      "Epoch: 406.00, Train Loss: 1.03, Val Loss: 11.68, Train BLEU: 8.05, Val BLEU: 2.74\n",
      "Epoch: 407.00, Train Loss: 1.03, Val Loss: 11.69, Train BLEU: 8.18, Val BLEU: 2.74\n",
      "Epoch: 408.00, Train Loss: 1.04, Val Loss: 11.70, Train BLEU: 7.88, Val BLEU: 2.96\n",
      "Epoch: 409.00, Train Loss: 1.03, Val Loss: 11.68, Train BLEU: 8.05, Val BLEU: 3.01\n",
      "Epoch: 410.00, Train Loss: 1.02, Val Loss: 11.68, Train BLEU: 7.84, Val BLEU: 2.93\n",
      "Epoch: 411.00, Train Loss: 1.02, Val Loss: 11.70, Train BLEU: 7.77, Val BLEU: 2.95\n",
      "Epoch: 412.00, Train Loss: 1.05, Val Loss: 11.69, Train BLEU: 7.60, Val BLEU: 2.91\n",
      "Epoch: 413.00, Train Loss: 1.01, Val Loss: 11.71, Train BLEU: 7.63, Val BLEU: 2.83\n",
      "Epoch: 414.00, Train Loss: 1.01, Val Loss: 11.71, Train BLEU: 7.54, Val BLEU: 2.82\n",
      "Epoch: 415.00, Train Loss: 1.01, Val Loss: 11.71, Train BLEU: 7.69, Val BLEU: 2.82\n",
      "Epoch: 416.00, Train Loss: 1.00, Val Loss: 11.71, Train BLEU: 7.99, Val BLEU: 2.79\n",
      "Epoch: 417.00, Train Loss: 0.99, Val Loss: 11.72, Train BLEU: 7.89, Val BLEU: 2.92\n",
      "Epoch: 418.00, Train Loss: 1.00, Val Loss: 11.71, Train BLEU: 7.96, Val BLEU: 2.76\n",
      "Epoch: 419.00, Train Loss: 1.00, Val Loss: 11.72, Train BLEU: 7.61, Val BLEU: 2.82\n",
      "Epoch: 420.00, Train Loss: 1.02, Val Loss: 11.72, Train BLEU: 7.87, Val BLEU: 2.79\n",
      "Epoch: 421.00, Train Loss: 1.09, Val Loss: 11.71, Train BLEU: 7.51, Val BLEU: 2.91\n",
      "Epoch: 422.00, Train Loss: 1.07, Val Loss: 11.77, Train BLEU: 7.68, Val BLEU: 2.80\n",
      "Epoch: 423.00, Train Loss: 1.00, Val Loss: 11.77, Train BLEU: 7.35, Val BLEU: 2.82\n",
      "Epoch: 424.00, Train Loss: 1.14, Val Loss: 11.75, Train BLEU: 7.45, Val BLEU: 2.87\n",
      "Epoch: 425.00, Train Loss: 0.99, Val Loss: 11.76, Train BLEU: 7.62, Val BLEU: 3.03\n",
      "Epoch: 426.00, Train Loss: 1.05, Val Loss: 11.77, Train BLEU: 7.81, Val BLEU: 3.14\n",
      "Epoch: 427.00, Train Loss: 1.06, Val Loss: 11.75, Train BLEU: 7.70, Val BLEU: 3.02\n",
      "Epoch: 428.00, Train Loss: 1.01, Val Loss: 11.79, Train BLEU: 8.19, Val BLEU: 2.89\n",
      "Epoch: 429.00, Train Loss: 1.09, Val Loss: 11.85, Train BLEU: 7.81, Val BLEU: 2.68\n",
      "Epoch: 430.00, Train Loss: 1.00, Val Loss: 11.82, Train BLEU: 7.95, Val BLEU: 2.79\n",
      "Epoch: 431.00, Train Loss: 1.05, Val Loss: 11.78, Train BLEU: 8.11, Val BLEU: 2.98\n",
      "Epoch: 432.00, Train Loss: 0.97, Val Loss: 11.78, Train BLEU: 7.91, Val BLEU: 2.90\n",
      "Epoch: 433.00, Train Loss: 1.01, Val Loss: 11.80, Train BLEU: 7.94, Val BLEU: 2.89\n",
      "Epoch: 434.00, Train Loss: 0.97, Val Loss: 11.81, Train BLEU: 7.91, Val BLEU: 2.93\n",
      "Epoch: 435.00, Train Loss: 0.99, Val Loss: 11.81, Train BLEU: 7.78, Val BLEU: 2.84\n",
      "Epoch: 436.00, Train Loss: 0.97, Val Loss: 11.83, Train BLEU: 7.88, Val BLEU: 2.86\n",
      "Epoch: 437.00, Train Loss: 0.96, Val Loss: 11.84, Train BLEU: 7.89, Val BLEU: 2.90\n",
      "Epoch: 438.00, Train Loss: 1.00, Val Loss: 11.83, Train BLEU: 7.96, Val BLEU: 2.93\n",
      "Epoch: 439.00, Train Loss: 1.05, Val Loss: 11.80, Train BLEU: 8.23, Val BLEU: 3.10\n",
      "Epoch: 440.00, Train Loss: 0.95, Val Loss: 11.82, Train BLEU: 8.33, Val BLEU: 2.93\n",
      "Epoch: 441.00, Train Loss: 1.07, Val Loss: 11.86, Train BLEU: 8.21, Val BLEU: 2.79\n",
      "Epoch: 442.00, Train Loss: 0.98, Val Loss: 11.85, Train BLEU: 8.34, Val BLEU: 2.82\n",
      "Epoch: 443.00, Train Loss: 1.03, Val Loss: 11.83, Train BLEU: 7.87, Val BLEU: 3.09\n",
      "Epoch: 444.00, Train Loss: 0.95, Val Loss: 11.86, Train BLEU: 8.15, Val BLEU: 2.99\n",
      "Epoch: 445.00, Train Loss: 1.01, Val Loss: 11.90, Train BLEU: 8.31, Val BLEU: 2.86\n",
      "Epoch: 446.00, Train Loss: 0.94, Val Loss: 11.90, Train BLEU: 8.25, Val BLEU: 2.91\n",
      "Epoch: 447.00, Train Loss: 0.94, Val Loss: 11.90, Train BLEU: 7.96, Val BLEU: 2.87\n",
      "Epoch: 448.00, Train Loss: 0.95, Val Loss: 11.89, Train BLEU: 8.32, Val BLEU: 2.97\n",
      "Epoch: 449.00, Train Loss: 0.95, Val Loss: 11.89, Train BLEU: 8.23, Val BLEU: 3.06\n",
      "Epoch: 450.00, Train Loss: 0.97, Val Loss: 11.89, Train BLEU: 8.29, Val BLEU: 2.92\n",
      "Epoch: 451.00, Train Loss: 0.92, Val Loss: 11.88, Train BLEU: 8.07, Val BLEU: 2.87\n",
      "Epoch: 452.00, Train Loss: 0.92, Val Loss: 11.89, Train BLEU: 8.06, Val BLEU: 2.90\n",
      "Epoch: 453.00, Train Loss: 0.96, Val Loss: 11.91, Train BLEU: 8.15, Val BLEU: 2.90\n",
      "Epoch: 454.00, Train Loss: 0.92, Val Loss: 11.92, Train BLEU: 8.08, Val BLEU: 3.07\n",
      "Epoch: 455.00, Train Loss: 0.91, Val Loss: 11.93, Train BLEU: 8.01, Val BLEU: 3.00\n",
      "Epoch: 456.00, Train Loss: 0.95, Val Loss: 11.94, Train BLEU: 8.19, Val BLEU: 2.96\n",
      "Epoch: 457.00, Train Loss: 0.90, Val Loss: 11.93, Train BLEU: 8.12, Val BLEU: 2.96\n",
      "Epoch: 458.00, Train Loss: 0.97, Val Loss: 11.92, Train BLEU: 8.26, Val BLEU: 2.92\n",
      "Epoch: 459.00, Train Loss: 0.90, Val Loss: 11.93, Train BLEU: 8.09, Val BLEU: 2.93\n",
      "Epoch: 460.00, Train Loss: 0.97, Val Loss: 11.94, Train BLEU: 8.07, Val BLEU: 2.88\n",
      "Epoch: 461.00, Train Loss: 0.89, Val Loss: 11.94, Train BLEU: 8.00, Val BLEU: 2.79\n",
      "Epoch: 462.00, Train Loss: 0.91, Val Loss: 11.93, Train BLEU: 8.30, Val BLEU: 2.90\n",
      "Epoch: 463.00, Train Loss: 0.89, Val Loss: 11.96, Train BLEU: 8.16, Val BLEU: 2.88\n",
      "Epoch: 464.00, Train Loss: 0.88, Val Loss: 11.97, Train BLEU: 8.22, Val BLEU: 2.88\n",
      "Epoch: 465.00, Train Loss: 0.91, Val Loss: 11.96, Train BLEU: 8.10, Val BLEU: 2.89\n",
      "Epoch: 466.00, Train Loss: 0.89, Val Loss: 11.97, Train BLEU: 7.81, Val BLEU: 2.91\n",
      "Epoch: 467.00, Train Loss: 0.92, Val Loss: 11.98, Train BLEU: 7.96, Val BLEU: 2.91\n",
      "Epoch: 468.00, Train Loss: 0.93, Val Loss: 11.97, Train BLEU: 7.83, Val BLEU: 3.07\n",
      "Epoch: 469.00, Train Loss: 0.89, Val Loss: 11.96, Train BLEU: 8.18, Val BLEU: 3.03\n",
      "Epoch: 470.00, Train Loss: 0.90, Val Loss: 11.98, Train BLEU: 8.32, Val BLEU: 2.96\n",
      "Epoch: 471.00, Train Loss: 0.87, Val Loss: 11.99, Train BLEU: 8.18, Val BLEU: 3.04\n",
      "Epoch: 472.00, Train Loss: 0.93, Val Loss: 12.00, Train BLEU: 8.27, Val BLEU: 2.90\n",
      "Epoch: 473.00, Train Loss: 0.90, Val Loss: 12.00, Train BLEU: 8.33, Val BLEU: 2.97\n",
      "Epoch: 474.00, Train Loss: 0.89, Val Loss: 11.99, Train BLEU: 8.52, Val BLEU: 3.02\n",
      "Epoch: 475.00, Train Loss: 0.93, Val Loss: 11.98, Train BLEU: 8.03, Val BLEU: 3.08\n",
      "Epoch: 476.00, Train Loss: 0.88, Val Loss: 12.00, Train BLEU: 8.42, Val BLEU: 3.09\n",
      "Epoch: 477.00, Train Loss: 0.90, Val Loss: 12.03, Train BLEU: 8.30, Val BLEU: 2.90\n",
      "Epoch: 478.00, Train Loss: 0.88, Val Loss: 12.02, Train BLEU: 8.38, Val BLEU: 2.89\n",
      "Epoch: 479.00, Train Loss: 0.86, Val Loss: 12.01, Train BLEU: 8.27, Val BLEU: 2.98\n",
      "Epoch: 480.00, Train Loss: 0.86, Val Loss: 12.01, Train BLEU: 8.22, Val BLEU: 2.91\n",
      "Epoch: 481.00, Train Loss: 0.87, Val Loss: 12.01, Train BLEU: 8.29, Val BLEU: 2.94\n",
      "Epoch: 482.00, Train Loss: 0.87, Val Loss: 12.01, Train BLEU: 8.49, Val BLEU: 2.96\n",
      "Epoch: 483.00, Train Loss: 0.87, Val Loss: 12.02, Train BLEU: 8.87, Val BLEU: 2.94\n",
      "Epoch: 484.00, Train Loss: 0.86, Val Loss: 12.03, Train BLEU: 8.47, Val BLEU: 3.11\n",
      "Epoch: 485.00, Train Loss: 0.86, Val Loss: 12.04, Train BLEU: 8.65, Val BLEU: 3.12\n",
      "Epoch: 486.00, Train Loss: 0.86, Val Loss: 12.04, Train BLEU: 8.36, Val BLEU: 3.01\n",
      "Epoch: 487.00, Train Loss: 0.86, Val Loss: 12.05, Train BLEU: 8.33, Val BLEU: 3.04\n",
      "Epoch: 488.00, Train Loss: 0.85, Val Loss: 12.06, Train BLEU: 8.23, Val BLEU: 2.92\n",
      "Epoch: 489.00, Train Loss: 0.91, Val Loss: 12.05, Train BLEU: 8.29, Val BLEU: 2.94\n",
      "Epoch: 490.00, Train Loss: 0.85, Val Loss: 12.06, Train BLEU: 7.99, Val BLEU: 2.96\n",
      "Epoch: 491.00, Train Loss: 0.85, Val Loss: 12.06, Train BLEU: 8.31, Val BLEU: 3.05\n",
      "Epoch: 492.00, Train Loss: 0.87, Val Loss: 12.05, Train BLEU: 8.42, Val BLEU: 3.04\n",
      "Epoch: 493.00, Train Loss: 0.83, Val Loss: 12.07, Train BLEU: 8.39, Val BLEU: 3.07\n",
      "Epoch: 494.00, Train Loss: 0.83, Val Loss: 12.08, Train BLEU: 8.38, Val BLEU: 3.06\n",
      "Epoch: 495.00, Train Loss: 0.83, Val Loss: 12.09, Train BLEU: 8.48, Val BLEU: 3.07\n",
      "Epoch: 496.00, Train Loss: 0.82, Val Loss: 12.09, Train BLEU: 8.26, Val BLEU: 3.07\n",
      "Epoch: 497.00, Train Loss: 0.84, Val Loss: 12.07, Train BLEU: 8.17, Val BLEU: 3.04\n",
      "Epoch: 498.00, Train Loss: 0.89, Val Loss: 12.08, Train BLEU: 8.32, Val BLEU: 3.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499.00, Train Loss: 0.84, Val Loss: 12.12, Train BLEU: 8.17, Val BLEU: 3.10\n",
      "Epoch: 500.00, Train Loss: 0.82, Val Loss: 12.12, Train BLEU: 8.35, Val BLEU: 2.88\n",
      "Epoch: 501.00, Train Loss: 0.86, Val Loss: 12.08, Train BLEU: 8.30, Val BLEU: 2.98\n",
      "Epoch: 502.00, Train Loss: 0.82, Val Loss: 12.07, Train BLEU: 8.59, Val BLEU: 2.85\n",
      "Epoch: 503.00, Train Loss: 0.82, Val Loss: 12.11, Train BLEU: 8.75, Val BLEU: 2.96\n",
      "Epoch: 504.00, Train Loss: 0.93, Val Loss: 12.14, Train BLEU: 8.46, Val BLEU: 3.00\n",
      "Epoch: 505.00, Train Loss: 0.83, Val Loss: 12.16, Train BLEU: 8.32, Val BLEU: 3.00\n",
      "Epoch: 506.00, Train Loss: 0.83, Val Loss: 12.16, Train BLEU: 8.32, Val BLEU: 3.01\n",
      "Epoch: 507.00, Train Loss: 0.83, Val Loss: 12.13, Train BLEU: 8.42, Val BLEU: 3.06\n",
      "Epoch: 508.00, Train Loss: 0.81, Val Loss: 12.13, Train BLEU: 8.40, Val BLEU: 3.01\n",
      "Epoch: 509.00, Train Loss: 0.80, Val Loss: 12.15, Train BLEU: 8.24, Val BLEU: 2.96\n",
      "Epoch: 510.00, Train Loss: 0.81, Val Loss: 12.16, Train BLEU: 8.13, Val BLEU: 2.92\n",
      "Epoch: 511.00, Train Loss: 0.80, Val Loss: 12.15, Train BLEU: 8.32, Val BLEU: 3.01\n",
      "Epoch: 512.00, Train Loss: 0.81, Val Loss: 12.16, Train BLEU: 8.77, Val BLEU: 3.07\n",
      "Epoch: 513.00, Train Loss: 0.81, Val Loss: 12.17, Train BLEU: 8.53, Val BLEU: 3.05\n",
      "Epoch: 514.00, Train Loss: 0.85, Val Loss: 12.18, Train BLEU: 8.20, Val BLEU: 2.97\n",
      "Epoch: 515.00, Train Loss: 0.85, Val Loss: 12.17, Train BLEU: 8.45, Val BLEU: 3.01\n",
      "Epoch: 516.00, Train Loss: 0.81, Val Loss: 12.18, Train BLEU: 8.09, Val BLEU: 2.97\n",
      "Epoch: 517.00, Train Loss: 0.79, Val Loss: 12.19, Train BLEU: 8.25, Val BLEU: 2.96\n",
      "Epoch: 518.00, Train Loss: 0.84, Val Loss: 12.20, Train BLEU: 8.72, Val BLEU: 2.92\n",
      "Epoch: 519.00, Train Loss: 0.82, Val Loss: 12.21, Train BLEU: 8.88, Val BLEU: 3.02\n",
      "Epoch: 520.00, Train Loss: 0.81, Val Loss: 12.21, Train BLEU: 8.50, Val BLEU: 3.01\n",
      "Epoch: 521.00, Train Loss: 0.79, Val Loss: 12.20, Train BLEU: 8.34, Val BLEU: 3.00\n",
      "Epoch: 522.00, Train Loss: 0.87, Val Loss: 12.20, Train BLEU: 8.27, Val BLEU: 3.01\n",
      "Epoch: 523.00, Train Loss: 0.82, Val Loss: 12.20, Train BLEU: 8.18, Val BLEU: 2.99\n",
      "Epoch: 524.00, Train Loss: 0.79, Val Loss: 12.17, Train BLEU: 8.51, Val BLEU: 3.03\n",
      "Epoch: 525.00, Train Loss: 0.99, Val Loss: 12.18, Train BLEU: 8.68, Val BLEU: 3.06\n",
      "Epoch: 526.00, Train Loss: 0.78, Val Loss: 12.21, Train BLEU: 8.82, Val BLEU: 3.03\n",
      "Epoch: 527.00, Train Loss: 0.83, Val Loss: 12.24, Train BLEU: 8.88, Val BLEU: 2.96\n",
      "Epoch: 528.00, Train Loss: 0.93, Val Loss: 12.24, Train BLEU: 7.93, Val BLEU: 2.98\n",
      "Epoch: 529.00, Train Loss: 0.83, Val Loss: 12.24, Train BLEU: 8.58, Val BLEU: 2.92\n",
      "Epoch: 530.00, Train Loss: 0.82, Val Loss: 12.24, Train BLEU: 8.49, Val BLEU: 3.03\n",
      "Epoch: 531.00, Train Loss: 0.78, Val Loss: 12.22, Train BLEU: 8.56, Val BLEU: 3.02\n",
      "Epoch: 532.00, Train Loss: 0.99, Val Loss: 12.23, Train BLEU: 8.35, Val BLEU: 3.13\n",
      "Epoch: 533.00, Train Loss: 0.79, Val Loss: 12.25, Train BLEU: 8.74, Val BLEU: 3.13\n",
      "Epoch: 534.00, Train Loss: 0.82, Val Loss: 12.28, Train BLEU: 8.66, Val BLEU: 3.06\n",
      "Epoch: 535.00, Train Loss: 0.76, Val Loss: 12.26, Train BLEU: 8.86, Val BLEU: 3.07\n",
      "Epoch: 536.00, Train Loss: 0.87, Val Loss: 12.24, Train BLEU: 8.38, Val BLEU: 3.14\n",
      "Epoch: 537.00, Train Loss: 0.77, Val Loss: 12.25, Train BLEU: 8.88, Val BLEU: 3.08\n",
      "Epoch: 538.00, Train Loss: 0.77, Val Loss: 12.27, Train BLEU: 8.44, Val BLEU: 3.06\n",
      "Epoch: 539.00, Train Loss: 0.86, Val Loss: 12.30, Train BLEU: 8.04, Val BLEU: 2.96\n",
      "Epoch: 540.00, Train Loss: 0.81, Val Loss: 12.29, Train BLEU: 8.09, Val BLEU: 2.89\n",
      "Epoch: 541.00, Train Loss: 0.80, Val Loss: 12.26, Train BLEU: 8.48, Val BLEU: 2.93\n",
      "Epoch: 542.00, Train Loss: 0.77, Val Loss: 12.23, Train BLEU: 8.57, Val BLEU: 2.84\n",
      "Epoch: 543.00, Train Loss: 0.91, Val Loss: 12.25, Train BLEU: 8.68, Val BLEU: 2.86\n",
      "Epoch: 544.00, Train Loss: 0.76, Val Loss: 12.31, Train BLEU: 8.49, Val BLEU: 2.77\n",
      "Epoch: 545.00, Train Loss: 0.81, Val Loss: 12.34, Train BLEU: 8.52, Val BLEU: 2.94\n",
      "Epoch: 546.00, Train Loss: 0.81, Val Loss: 12.32, Train BLEU: 8.38, Val BLEU: 3.06\n",
      "Epoch: 547.00, Train Loss: 0.89, Val Loss: 12.29, Train BLEU: 8.34, Val BLEU: 3.16\n",
      "Epoch: 548.00, Train Loss: 0.76, Val Loss: 12.32, Train BLEU: 8.58, Val BLEU: 3.00\n",
      "Epoch: 549.00, Train Loss: 0.78, Val Loss: 12.34, Train BLEU: 8.52, Val BLEU: 2.88\n",
      "Epoch: 550.00, Train Loss: 0.77, Val Loss: 12.34, Train BLEU: 8.39, Val BLEU: 2.95\n",
      "Epoch: 551.00, Train Loss: 0.77, Val Loss: 12.34, Train BLEU: 8.57, Val BLEU: 2.99\n",
      "Epoch: 552.00, Train Loss: 0.75, Val Loss: 12.34, Train BLEU: 8.57, Val BLEU: 3.01\n",
      "Epoch: 553.00, Train Loss: 0.75, Val Loss: 12.35, Train BLEU: 8.27, Val BLEU: 2.93\n",
      "Epoch: 554.00, Train Loss: 0.75, Val Loss: 12.34, Train BLEU: 8.33, Val BLEU: 2.90\n",
      "Epoch: 555.00, Train Loss: 0.79, Val Loss: 12.32, Train BLEU: 8.08, Val BLEU: 2.99\n",
      "Epoch: 556.00, Train Loss: 0.76, Val Loss: 12.31, Train BLEU: 8.11, Val BLEU: 2.94\n",
      "Epoch: 557.00, Train Loss: 0.73, Val Loss: 12.32, Train BLEU: 8.46, Val BLEU: 3.10\n",
      "Epoch: 558.00, Train Loss: 0.77, Val Loss: 12.34, Train BLEU: 8.24, Val BLEU: 3.10\n",
      "Epoch: 559.00, Train Loss: 0.75, Val Loss: 12.36, Train BLEU: 8.39, Val BLEU: 3.06\n",
      "Epoch: 560.00, Train Loss: 0.85, Val Loss: 12.38, Train BLEU: 8.63, Val BLEU: 2.99\n",
      "Epoch: 561.00, Train Loss: 0.74, Val Loss: 12.35, Train BLEU: 9.02, Val BLEU: 3.09\n",
      "Epoch: 562.00, Train Loss: 1.08, Val Loss: 12.32, Train BLEU: 8.68, Val BLEU: 3.09\n",
      "Epoch: 563.00, Train Loss: 0.78, Val Loss: 12.34, Train BLEU: 8.46, Val BLEU: 3.01\n",
      "Epoch: 564.00, Train Loss: 0.79, Val Loss: 12.38, Train BLEU: 8.58, Val BLEU: 2.97\n",
      "Epoch: 565.00, Train Loss: 0.75, Val Loss: 12.38, Train BLEU: 8.22, Val BLEU: 3.00\n",
      "Epoch: 566.00, Train Loss: 0.86, Val Loss: 12.38, Train BLEU: 8.24, Val BLEU: 3.14\n",
      "Epoch: 567.00, Train Loss: 0.73, Val Loss: 12.39, Train BLEU: 8.64, Val BLEU: 3.11\n",
      "Epoch: 568.00, Train Loss: 0.72, Val Loss: 12.38, Train BLEU: 8.84, Val BLEU: 3.14\n",
      "Epoch: 569.00, Train Loss: 0.72, Val Loss: 12.37, Train BLEU: 8.87, Val BLEU: 3.09\n",
      "Epoch: 570.00, Train Loss: 0.77, Val Loss: 12.38, Train BLEU: 8.98, Val BLEU: 3.06\n",
      "Epoch: 571.00, Train Loss: 0.71, Val Loss: 12.39, Train BLEU: 8.57, Val BLEU: 3.02\n",
      "Epoch: 572.00, Train Loss: 0.71, Val Loss: 12.39, Train BLEU: 8.70, Val BLEU: 3.04\n",
      "Epoch: 573.00, Train Loss: 0.74, Val Loss: 12.39, Train BLEU: 8.29, Val BLEU: 3.08\n",
      "Epoch: 574.00, Train Loss: 0.73, Val Loss: 12.39, Train BLEU: 8.45, Val BLEU: 2.95\n",
      "Epoch: 575.00, Train Loss: 0.70, Val Loss: 12.39, Train BLEU: 8.66, Val BLEU: 3.15\n",
      "Epoch: 576.00, Train Loss: 0.69, Val Loss: 12.39, Train BLEU: 8.77, Val BLEU: 3.14\n",
      "Epoch: 577.00, Train Loss: 0.73, Val Loss: 12.40, Train BLEU: 8.35, Val BLEU: 3.04\n",
      "Epoch: 578.00, Train Loss: 0.72, Val Loss: 12.43, Train BLEU: 8.33, Val BLEU: 2.92\n",
      "Epoch: 579.00, Train Loss: 0.69, Val Loss: 12.44, Train BLEU: 8.52, Val BLEU: 3.01\n",
      "Epoch: 580.00, Train Loss: 0.69, Val Loss: 12.43, Train BLEU: 8.51, Val BLEU: 3.13\n",
      "Epoch: 581.00, Train Loss: 0.69, Val Loss: 12.42, Train BLEU: 8.59, Val BLEU: 3.11\n",
      "Epoch: 582.00, Train Loss: 0.69, Val Loss: 12.43, Train BLEU: 8.65, Val BLEU: 3.02\n",
      "Epoch: 583.00, Train Loss: 0.69, Val Loss: 12.46, Train BLEU: 8.68, Val BLEU: 3.11\n",
      "Epoch: 584.00, Train Loss: 0.71, Val Loss: 12.48, Train BLEU: 8.81, Val BLEU: 3.08\n",
      "Epoch: 585.00, Train Loss: 0.68, Val Loss: 12.48, Train BLEU: 9.18, Val BLEU: 2.95\n",
      "Epoch: 586.00, Train Loss: 0.68, Val Loss: 12.47, Train BLEU: 8.97, Val BLEU: 3.04\n",
      "Epoch: 587.00, Train Loss: 0.71, Val Loss: 12.46, Train BLEU: 8.64, Val BLEU: 3.16\n",
      "Epoch: 588.00, Train Loss: 0.68, Val Loss: 12.47, Train BLEU: 8.71, Val BLEU: 2.93\n",
      "Epoch: 589.00, Train Loss: 0.69, Val Loss: 12.47, Train BLEU: 8.82, Val BLEU: 2.96\n",
      "Epoch: 590.00, Train Loss: 0.71, Val Loss: 12.48, Train BLEU: 8.38, Val BLEU: 3.08\n",
      "Epoch: 591.00, Train Loss: 0.74, Val Loss: 12.47, Train BLEU: 8.41, Val BLEU: 2.97\n",
      "Epoch: 592.00, Train Loss: 0.70, Val Loss: 12.48, Train BLEU: 8.69, Val BLEU: 3.03\n",
      "Epoch: 593.00, Train Loss: 0.69, Val Loss: 12.50, Train BLEU: 8.74, Val BLEU: 3.10\n",
      "Epoch: 594.00, Train Loss: 0.75, Val Loss: 12.50, Train BLEU: 8.30, Val BLEU: 3.06\n",
      "Epoch: 595.00, Train Loss: 0.69, Val Loss: 12.51, Train BLEU: 8.66, Val BLEU: 3.03\n",
      "Epoch: 596.00, Train Loss: 0.66, Val Loss: 12.51, Train BLEU: 8.83, Val BLEU: 2.98\n",
      "Epoch: 597.00, Train Loss: 0.66, Val Loss: 12.50, Train BLEU: 9.02, Val BLEU: 3.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 598.00, Train Loss: 0.68, Val Loss: 12.49, Train BLEU: 9.00, Val BLEU: 2.93\n",
      "Epoch: 599.00, Train Loss: 0.68, Val Loss: 12.49, Train BLEU: 9.01, Val BLEU: 3.07\n",
      "Epoch: 600.00, Train Loss: 0.66, Val Loss: 12.51, Train BLEU: 8.84, Val BLEU: 3.05\n",
      "Epoch: 601.00, Train Loss: 0.67, Val Loss: 12.53, Train BLEU: 8.72, Val BLEU: 3.03\n",
      "Epoch: 602.00, Train Loss: 0.67, Val Loss: 12.53, Train BLEU: 8.63, Val BLEU: 3.07\n",
      "Epoch: 603.00, Train Loss: 0.65, Val Loss: 12.54, Train BLEU: 8.83, Val BLEU: 2.99\n",
      "Epoch: 604.00, Train Loss: 0.67, Val Loss: 12.53, Train BLEU: 8.91, Val BLEU: 3.09\n",
      "Epoch: 605.00, Train Loss: 0.74, Val Loss: 12.54, Train BLEU: 8.89, Val BLEU: 3.07\n",
      "Epoch: 606.00, Train Loss: 0.68, Val Loss: 12.55, Train BLEU: 8.78, Val BLEU: 3.06\n",
      "Epoch: 607.00, Train Loss: 0.70, Val Loss: 12.56, Train BLEU: 8.92, Val BLEU: 3.00\n",
      "Epoch: 608.00, Train Loss: 0.74, Val Loss: 12.55, Train BLEU: 8.74, Val BLEU: 3.11\n",
      "Epoch: 609.00, Train Loss: 0.75, Val Loss: 12.57, Train BLEU: 8.46, Val BLEU: 3.08\n",
      "Epoch: 610.00, Train Loss: 0.69, Val Loss: 12.58, Train BLEU: 8.49, Val BLEU: 3.07\n",
      "Epoch: 611.00, Train Loss: 0.65, Val Loss: 12.56, Train BLEU: 8.70, Val BLEU: 3.02\n",
      "Epoch: 612.00, Train Loss: 0.84, Val Loss: 12.55, Train BLEU: 8.86, Val BLEU: 3.14\n",
      "Epoch: 613.00, Train Loss: 0.65, Val Loss: 12.53, Train BLEU: 9.16, Val BLEU: 2.91\n",
      "Epoch: 614.00, Train Loss: 0.73, Val Loss: 12.57, Train BLEU: 8.82, Val BLEU: 2.95\n",
      "Epoch: 615.00, Train Loss: 0.69, Val Loss: 12.58, Train BLEU: 8.36, Val BLEU: 3.15\n",
      "Epoch: 616.00, Train Loss: 0.69, Val Loss: 12.58, Train BLEU: 8.31, Val BLEU: 3.03\n",
      "Epoch: 617.00, Train Loss: 0.69, Val Loss: 12.60, Train BLEU: 8.56, Val BLEU: 3.00\n",
      "Epoch: 618.00, Train Loss: 0.67, Val Loss: 12.60, Train BLEU: 8.91, Val BLEU: 3.06\n",
      "Epoch: 619.00, Train Loss: 0.75, Val Loss: 12.61, Train BLEU: 8.54, Val BLEU: 3.16\n",
      "Epoch: 620.00, Train Loss: 0.65, Val Loss: 12.61, Train BLEU: 8.50, Val BLEU: 2.95\n",
      "Epoch: 621.00, Train Loss: 0.65, Val Loss: 12.61, Train BLEU: 9.31, Val BLEU: 2.94\n",
      "Epoch: 622.00, Train Loss: 0.65, Val Loss: 12.60, Train BLEU: 8.60, Val BLEU: 3.11\n",
      "Epoch: 623.00, Train Loss: 0.68, Val Loss: 12.60, Train BLEU: 8.76, Val BLEU: 3.10\n",
      "Epoch: 624.00, Train Loss: 0.67, Val Loss: 12.59, Train BLEU: 8.81, Val BLEU: 3.02\n",
      "Epoch: 625.00, Train Loss: 0.66, Val Loss: 12.59, Train BLEU: 8.85, Val BLEU: 3.10\n",
      "Epoch: 626.00, Train Loss: 0.65, Val Loss: 12.62, Train BLEU: 9.01, Val BLEU: 2.96\n",
      "Epoch: 627.00, Train Loss: 0.63, Val Loss: 12.62, Train BLEU: 8.78, Val BLEU: 3.06\n",
      "Epoch: 628.00, Train Loss: 0.64, Val Loss: 12.61, Train BLEU: 8.68, Val BLEU: 3.16\n",
      "Epoch: 629.00, Train Loss: 0.68, Val Loss: 12.60, Train BLEU: 8.86, Val BLEU: 3.09\n",
      "Epoch: 630.00, Train Loss: 0.63, Val Loss: 12.61, Train BLEU: 9.06, Val BLEU: 3.09\n",
      "Epoch: 631.00, Train Loss: 0.67, Val Loss: 12.61, Train BLEU: 8.94, Val BLEU: 3.11\n",
      "Epoch: 632.00, Train Loss: 0.68, Val Loss: 12.62, Train BLEU: 8.73, Val BLEU: 3.13\n",
      "Epoch: 633.00, Train Loss: 0.76, Val Loss: 12.65, Train BLEU: 8.92, Val BLEU: 3.06\n",
      "Epoch: 634.00, Train Loss: 0.70, Val Loss: 12.65, Train BLEU: 8.88, Val BLEU: 3.11\n",
      "Epoch: 635.00, Train Loss: 0.66, Val Loss: 12.64, Train BLEU: 8.82, Val BLEU: 2.97\n",
      "Epoch: 636.00, Train Loss: 0.86, Val Loss: 12.65, Train BLEU: 8.19, Val BLEU: 3.26\n",
      "Epoch: 637.00, Train Loss: 0.69, Val Loss: 12.66, Train BLEU: 8.49, Val BLEU: 3.07\n",
      "Epoch: 638.00, Train Loss: 0.92, Val Loss: 12.70, Train BLEU: 8.69, Val BLEU: 2.88\n",
      "Epoch: 639.00, Train Loss: 0.84, Val Loss: 12.70, Train BLEU: 8.86, Val BLEU: 2.87\n",
      "Epoch: 640.00, Train Loss: 0.86, Val Loss: 12.66, Train BLEU: 8.40, Val BLEU: 2.90\n",
      "Epoch: 641.00, Train Loss: 0.80, Val Loss: 12.65, Train BLEU: 8.65, Val BLEU: 3.10\n",
      "Epoch: 642.00, Train Loss: 0.70, Val Loss: 12.67, Train BLEU: 8.58, Val BLEU: 3.11\n",
      "Epoch: 643.00, Train Loss: 0.80, Val Loss: 12.69, Train BLEU: 8.66, Val BLEU: 3.12\n",
      "Epoch: 644.00, Train Loss: 0.68, Val Loss: 12.68, Train BLEU: 8.30, Val BLEU: 3.15\n",
      "Epoch: 645.00, Train Loss: 0.85, Val Loss: 12.66, Train BLEU: 8.62, Val BLEU: 3.00\n",
      "Epoch: 646.00, Train Loss: 0.64, Val Loss: 12.66, Train BLEU: 9.30, Val BLEU: 2.94\n",
      "Epoch: 647.00, Train Loss: 0.72, Val Loss: 12.66, Train BLEU: 9.00, Val BLEU: 2.75\n",
      "Epoch: 648.00, Train Loss: 0.65, Val Loss: 12.67, Train BLEU: 8.81, Val BLEU: 2.89\n",
      "Epoch: 649.00, Train Loss: 0.69, Val Loss: 12.68, Train BLEU: 8.94, Val BLEU: 2.96\n",
      "Epoch: 650.00, Train Loss: 0.64, Val Loss: 12.70, Train BLEU: 8.94, Val BLEU: 2.97\n",
      "Epoch: 651.00, Train Loss: 0.65, Val Loss: 12.72, Train BLEU: 9.00, Val BLEU: 2.93\n",
      "Epoch: 652.00, Train Loss: 0.63, Val Loss: 12.73, Train BLEU: 8.97, Val BLEU: 2.94\n",
      "Epoch: 653.00, Train Loss: 0.65, Val Loss: 12.71, Train BLEU: 8.95, Val BLEU: 3.10\n",
      "Epoch: 654.00, Train Loss: 0.63, Val Loss: 12.70, Train BLEU: 9.15, Val BLEU: 2.94\n",
      "Epoch: 655.00, Train Loss: 0.65, Val Loss: 12.70, Train BLEU: 8.81, Val BLEU: 2.91\n",
      "Epoch: 656.00, Train Loss: 0.63, Val Loss: 12.72, Train BLEU: 9.08, Val BLEU: 2.78\n",
      "Epoch: 657.00, Train Loss: 0.62, Val Loss: 12.76, Train BLEU: 8.94, Val BLEU: 2.85\n",
      "Epoch: 658.00, Train Loss: 0.63, Val Loss: 12.76, Train BLEU: 9.23, Val BLEU: 2.85\n",
      "Epoch: 659.00, Train Loss: 0.67, Val Loss: 12.74, Train BLEU: 9.15, Val BLEU: 2.86\n",
      "Epoch: 660.00, Train Loss: 0.66, Val Loss: 12.73, Train BLEU: 8.65, Val BLEU: 2.84\n",
      "Epoch: 661.00, Train Loss: 0.64, Val Loss: 12.72, Train BLEU: 8.33, Val BLEU: 2.79\n",
      "Epoch: 662.00, Train Loss: 0.65, Val Loss: 12.73, Train BLEU: 8.28, Val BLEU: 2.86\n",
      "Epoch: 663.00, Train Loss: 0.66, Val Loss: 12.73, Train BLEU: 8.40, Val BLEU: 2.72\n",
      "Epoch: 664.00, Train Loss: 0.61, Val Loss: 12.70, Train BLEU: 9.28, Val BLEU: 2.73\n",
      "Epoch: 665.00, Train Loss: 0.62, Val Loss: 12.69, Train BLEU: 9.11, Val BLEU: 2.84\n",
      "Epoch: 666.00, Train Loss: 0.63, Val Loss: 12.69, Train BLEU: 9.27, Val BLEU: 2.86\n",
      "Epoch: 667.00, Train Loss: 0.62, Val Loss: 12.72, Train BLEU: 9.22, Val BLEU: 2.92\n",
      "Epoch: 668.00, Train Loss: 0.61, Val Loss: 12.75, Train BLEU: 8.73, Val BLEU: 2.87\n",
      "Epoch: 669.00, Train Loss: 0.61, Val Loss: 12.77, Train BLEU: 8.71, Val BLEU: 3.00\n",
      "Epoch: 670.00, Train Loss: 0.63, Val Loss: 12.77, Train BLEU: 8.43, Val BLEU: 2.88\n",
      "Epoch: 671.00, Train Loss: 0.62, Val Loss: 12.75, Train BLEU: 8.29, Val BLEU: 2.89\n",
      "Epoch: 672.00, Train Loss: 0.62, Val Loss: 12.74, Train BLEU: 8.71, Val BLEU: 2.85\n",
      "Epoch: 673.00, Train Loss: 0.63, Val Loss: 12.75, Train BLEU: 8.38, Val BLEU: 2.92\n",
      "Epoch: 674.00, Train Loss: 0.64, Val Loss: 12.76, Train BLEU: 8.76, Val BLEU: 3.09\n",
      "Epoch: 675.00, Train Loss: 0.60, Val Loss: 12.76, Train BLEU: 9.33, Val BLEU: 3.14\n",
      "Epoch: 676.00, Train Loss: 0.61, Val Loss: 12.76, Train BLEU: 9.02, Val BLEU: 3.14\n",
      "Epoch: 677.00, Train Loss: 0.61, Val Loss: 12.75, Train BLEU: 9.12, Val BLEU: 3.00\n",
      "Epoch: 678.00, Train Loss: 0.65, Val Loss: 12.75, Train BLEU: 9.17, Val BLEU: 2.95\n",
      "Epoch: 679.00, Train Loss: 0.58, Val Loss: 12.76, Train BLEU: 8.78, Val BLEU: 3.00\n",
      "Epoch: 680.00, Train Loss: 0.58, Val Loss: 12.77, Train BLEU: 8.71, Val BLEU: 2.82\n",
      "Epoch: 681.00, Train Loss: 0.60, Val Loss: 12.79, Train BLEU: 8.86, Val BLEU: 2.90\n",
      "Epoch: 682.00, Train Loss: 0.65, Val Loss: 12.80, Train BLEU: 8.46, Val BLEU: 2.87\n",
      "Epoch: 683.00, Train Loss: 0.64, Val Loss: 12.83, Train BLEU: 8.66, Val BLEU: 2.90\n",
      "Epoch: 684.00, Train Loss: 0.60, Val Loss: 12.86, Train BLEU: 8.62, Val BLEU: 2.98\n",
      "Epoch: 685.00, Train Loss: 0.74, Val Loss: 12.82, Train BLEU: 8.39, Val BLEU: 2.80\n",
      "Epoch: 686.00, Train Loss: 0.60, Val Loss: 12.79, Train BLEU: 8.53, Val BLEU: 2.86\n",
      "Epoch: 687.00, Train Loss: 0.64, Val Loss: 12.78, Train BLEU: 8.77, Val BLEU: 2.85\n",
      "Epoch: 688.00, Train Loss: 0.58, Val Loss: 12.77, Train BLEU: 8.81, Val BLEU: 2.82\n",
      "Epoch: 689.00, Train Loss: 0.59, Val Loss: 12.77, Train BLEU: 9.08, Val BLEU: 2.99\n",
      "Epoch: 690.00, Train Loss: 0.57, Val Loss: 12.79, Train BLEU: 9.14, Val BLEU: 2.89\n",
      "Epoch: 691.00, Train Loss: 0.58, Val Loss: 12.83, Train BLEU: 9.08, Val BLEU: 2.96\n",
      "Epoch: 692.00, Train Loss: 0.56, Val Loss: 12.85, Train BLEU: 8.69, Val BLEU: 2.84\n",
      "Epoch: 693.00, Train Loss: 0.56, Val Loss: 12.87, Train BLEU: 9.18, Val BLEU: 2.96\n",
      "Epoch: 694.00, Train Loss: 0.56, Val Loss: 12.86, Train BLEU: 9.03, Val BLEU: 2.99\n",
      "Epoch: 695.00, Train Loss: 0.55, Val Loss: 12.87, Train BLEU: 9.29, Val BLEU: 2.83\n",
      "Epoch: 696.00, Train Loss: 0.55, Val Loss: 12.86, Train BLEU: 9.29, Val BLEU: 3.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 697.00, Train Loss: 0.56, Val Loss: 12.86, Train BLEU: 9.27, Val BLEU: 2.99\n",
      "Epoch: 698.00, Train Loss: 0.55, Val Loss: 12.86, Train BLEU: 8.95, Val BLEU: 2.99\n",
      "Epoch: 699.00, Train Loss: 0.56, Val Loss: 12.86, Train BLEU: 8.92, Val BLEU: 2.99\n",
      "Epoch: 700.00, Train Loss: 0.55, Val Loss: 12.85, Train BLEU: 8.98, Val BLEU: 3.06\n",
      "Epoch: 701.00, Train Loss: 0.55, Val Loss: 12.86, Train BLEU: 9.10, Val BLEU: 3.05\n",
      "Epoch: 702.00, Train Loss: 0.59, Val Loss: 12.86, Train BLEU: 9.36, Val BLEU: 3.13\n",
      "Epoch: 703.00, Train Loss: 0.54, Val Loss: 12.85, Train BLEU: 9.29, Val BLEU: 3.09\n",
      "Epoch: 704.00, Train Loss: 0.55, Val Loss: 12.86, Train BLEU: 9.12, Val BLEU: 3.07\n",
      "Epoch: 705.00, Train Loss: 0.56, Val Loss: 12.86, Train BLEU: 9.04, Val BLEU: 2.92\n",
      "Epoch: 706.00, Train Loss: 0.55, Val Loss: 12.87, Train BLEU: 8.94, Val BLEU: 2.92\n",
      "Epoch: 707.00, Train Loss: 0.57, Val Loss: 12.88, Train BLEU: 8.66, Val BLEU: 3.03\n",
      "Epoch: 708.00, Train Loss: 0.55, Val Loss: 12.89, Train BLEU: 8.82, Val BLEU: 2.98\n",
      "Epoch: 709.00, Train Loss: 0.74, Val Loss: 12.90, Train BLEU: 8.91, Val BLEU: 2.99\n",
      "Epoch: 710.00, Train Loss: 0.57, Val Loss: 12.88, Train BLEU: 9.05, Val BLEU: 2.96\n",
      "Epoch: 711.00, Train Loss: 0.73, Val Loss: 12.88, Train BLEU: 9.09, Val BLEU: 2.90\n",
      "Epoch: 712.00, Train Loss: 0.56, Val Loss: 12.88, Train BLEU: 9.41, Val BLEU: 2.94\n",
      "Epoch: 713.00, Train Loss: 0.65, Val Loss: 12.90, Train BLEU: 9.07, Val BLEU: 2.83\n",
      "Epoch: 714.00, Train Loss: 0.62, Val Loss: 12.90, Train BLEU: 8.87, Val BLEU: 2.94\n",
      "Epoch: 715.00, Train Loss: 0.60, Val Loss: 12.92, Train BLEU: 8.85, Val BLEU: 2.94\n",
      "Epoch: 716.00, Train Loss: 0.56, Val Loss: 12.95, Train BLEU: 9.26, Val BLEU: 3.00\n",
      "Epoch: 717.00, Train Loss: 0.54, Val Loss: 12.96, Train BLEU: 9.07, Val BLEU: 2.88\n",
      "Epoch: 718.00, Train Loss: 0.61, Val Loss: 12.96, Train BLEU: 9.17, Val BLEU: 2.92\n",
      "Epoch: 719.00, Train Loss: 0.55, Val Loss: 12.94, Train BLEU: 8.72, Val BLEU: 2.94\n",
      "Epoch: 720.00, Train Loss: 0.56, Val Loss: 12.93, Train BLEU: 8.92, Val BLEU: 2.96\n",
      "Epoch: 721.00, Train Loss: 0.64, Val Loss: 12.93, Train BLEU: 8.56, Val BLEU: 2.89\n",
      "Epoch: 722.00, Train Loss: 0.56, Val Loss: 12.94, Train BLEU: 8.99, Val BLEU: 2.97\n",
      "Epoch: 723.00, Train Loss: 0.59, Val Loss: 12.95, Train BLEU: 8.90, Val BLEU: 3.03\n",
      "Epoch: 724.00, Train Loss: 0.59, Val Loss: 12.93, Train BLEU: 8.81, Val BLEU: 2.98\n",
      "Epoch: 725.00, Train Loss: 0.60, Val Loss: 12.92, Train BLEU: 8.94, Val BLEU: 3.04\n",
      "Epoch: 726.00, Train Loss: 0.55, Val Loss: 12.93, Train BLEU: 9.04, Val BLEU: 3.12\n",
      "Epoch: 727.00, Train Loss: 0.59, Val Loss: 12.95, Train BLEU: 8.88, Val BLEU: 3.14\n",
      "Epoch: 728.00, Train Loss: 0.56, Val Loss: 12.95, Train BLEU: 9.31, Val BLEU: 3.00\n",
      "Epoch: 729.00, Train Loss: 0.55, Val Loss: 12.94, Train BLEU: 8.86, Val BLEU: 3.03\n",
      "Epoch: 730.00, Train Loss: 0.55, Val Loss: 12.94, Train BLEU: 9.27, Val BLEU: 3.01\n",
      "Epoch: 731.00, Train Loss: 0.54, Val Loss: 12.94, Train BLEU: 9.27, Val BLEU: 2.90\n",
      "Epoch: 732.00, Train Loss: 0.64, Val Loss: 12.96, Train BLEU: 8.84, Val BLEU: 2.90\n",
      "Epoch: 733.00, Train Loss: 0.65, Val Loss: 12.95, Train BLEU: 8.77, Val BLEU: 2.78\n",
      "Epoch: 734.00, Train Loss: 0.59, Val Loss: 12.94, Train BLEU: 8.71, Val BLEU: 2.89\n",
      "Epoch: 735.00, Train Loss: 0.58, Val Loss: 12.93, Train BLEU: 9.02, Val BLEU: 2.91\n",
      "Epoch: 736.00, Train Loss: 0.62, Val Loss: 12.95, Train BLEU: 9.12, Val BLEU: 2.92\n",
      "Epoch: 737.00, Train Loss: 0.55, Val Loss: 12.96, Train BLEU: 8.97, Val BLEU: 2.72\n",
      "Epoch: 738.00, Train Loss: 0.60, Val Loss: 12.96, Train BLEU: 8.97, Val BLEU: 3.10\n",
      "Epoch: 739.00, Train Loss: 0.58, Val Loss: 12.93, Train BLEU: 8.72, Val BLEU: 3.06\n",
      "Epoch: 740.00, Train Loss: 0.75, Val Loss: 12.93, Train BLEU: 8.63, Val BLEU: 3.01\n",
      "Epoch: 741.00, Train Loss: 0.57, Val Loss: 12.93, Train BLEU: 8.95, Val BLEU: 3.10\n",
      "Epoch: 742.00, Train Loss: 0.67, Val Loss: 12.99, Train BLEU: 8.72, Val BLEU: 2.93\n",
      "Epoch: 743.00, Train Loss: 0.56, Val Loss: 12.99, Train BLEU: 8.84, Val BLEU: 3.04\n",
      "Epoch: 744.00, Train Loss: 0.75, Val Loss: 12.98, Train BLEU: 9.18, Val BLEU: 2.98\n",
      "Epoch: 745.00, Train Loss: 0.56, Val Loss: 12.96, Train BLEU: 9.52, Val BLEU: 3.26\n",
      "Epoch: 746.00, Train Loss: 0.67, Val Loss: 12.97, Train BLEU: 9.15, Val BLEU: 3.06\n",
      "Epoch: 747.00, Train Loss: 0.57, Val Loss: 12.97, Train BLEU: 9.06, Val BLEU: 3.10\n",
      "Epoch: 748.00, Train Loss: 0.80, Val Loss: 13.00, Train BLEU: 8.46, Val BLEU: 3.06\n",
      "Epoch: 749.00, Train Loss: 0.57, Val Loss: 13.04, Train BLEU: 8.47, Val BLEU: 2.95\n",
      "Epoch: 750.00, Train Loss: 0.61, Val Loss: 13.08, Train BLEU: 8.89, Val BLEU: 2.97\n",
      "Epoch: 751.00, Train Loss: 0.55, Val Loss: 13.07, Train BLEU: 9.07, Val BLEU: 3.02\n",
      "Epoch: 752.00, Train Loss: 0.72, Val Loss: 13.05, Train BLEU: 8.90, Val BLEU: 3.10\n",
      "Epoch: 753.00, Train Loss: 0.55, Val Loss: 13.00, Train BLEU: 9.30, Val BLEU: 2.83\n",
      "Epoch: 754.00, Train Loss: 0.64, Val Loss: 13.01, Train BLEU: 9.19, Val BLEU: 2.65\n",
      "Epoch: 755.00, Train Loss: 0.55, Val Loss: 13.03, Train BLEU: 9.05, Val BLEU: 2.82\n",
      "Epoch: 756.00, Train Loss: 0.71, Val Loss: 13.07, Train BLEU: 8.92, Val BLEU: 2.83\n",
      "Epoch: 757.00, Train Loss: 0.55, Val Loss: 13.12, Train BLEU: 8.88, Val BLEU: 2.95\n",
      "Epoch: 758.00, Train Loss: 0.58, Val Loss: 13.13, Train BLEU: 8.99, Val BLEU: 3.05\n",
      "Epoch: 759.00, Train Loss: 0.56, Val Loss: 13.07, Train BLEU: 8.94, Val BLEU: 3.21\n",
      "Epoch: 760.00, Train Loss: 0.66, Val Loss: 13.05, Train BLEU: 8.83, Val BLEU: 3.20\n",
      "Epoch: 761.00, Train Loss: 0.52, Val Loss: 13.06, Train BLEU: 9.11, Val BLEU: 3.06\n",
      "Epoch: 762.00, Train Loss: 0.57, Val Loss: 13.09, Train BLEU: 8.77, Val BLEU: 2.89\n",
      "Epoch: 763.00, Train Loss: 0.55, Val Loss: 13.09, Train BLEU: 8.58, Val BLEU: 2.90\n",
      "Epoch: 764.00, Train Loss: 0.61, Val Loss: 13.07, Train BLEU: 9.01, Val BLEU: 2.88\n",
      "Epoch: 765.00, Train Loss: 0.53, Val Loss: 13.03, Train BLEU: 9.50, Val BLEU: 3.11\n",
      "Epoch: 766.00, Train Loss: 0.56, Val Loss: 13.05, Train BLEU: 9.24, Val BLEU: 3.09\n",
      "Epoch: 767.00, Train Loss: 0.59, Val Loss: 13.08, Train BLEU: 9.07, Val BLEU: 2.96\n",
      "Epoch: 768.00, Train Loss: 0.55, Val Loss: 13.10, Train BLEU: 9.04, Val BLEU: 3.04\n",
      "Epoch: 769.00, Train Loss: 0.55, Val Loss: 13.09, Train BLEU: 8.81, Val BLEU: 3.08\n",
      "Epoch: 770.00, Train Loss: 0.53, Val Loss: 13.05, Train BLEU: 9.14, Val BLEU: 2.92\n",
      "Epoch: 771.00, Train Loss: 0.59, Val Loss: 13.05, Train BLEU: 9.16, Val BLEU: 3.10\n",
      "Epoch: 772.00, Train Loss: 0.52, Val Loss: 13.07, Train BLEU: 9.31, Val BLEU: 3.14\n",
      "Epoch: 773.00, Train Loss: 0.52, Val Loss: 13.13, Train BLEU: 9.02, Val BLEU: 3.08\n",
      "Epoch: 774.00, Train Loss: 0.54, Val Loss: 13.13, Train BLEU: 8.85, Val BLEU: 2.99\n",
      "Epoch: 775.00, Train Loss: 0.53, Val Loss: 13.09, Train BLEU: 9.07, Val BLEU: 3.03\n",
      "Epoch: 776.00, Train Loss: 0.51, Val Loss: 13.03, Train BLEU: 8.93, Val BLEU: 3.21\n",
      "Epoch: 777.00, Train Loss: 0.51, Val Loss: 13.02, Train BLEU: 9.20, Val BLEU: 3.27\n",
      "Epoch: 778.00, Train Loss: 0.50, Val Loss: 13.05, Train BLEU: 9.50, Val BLEU: 3.17\n",
      "Epoch: 779.00, Train Loss: 0.51, Val Loss: 13.09, Train BLEU: 9.00, Val BLEU: 3.04\n",
      "Epoch: 780.00, Train Loss: 0.52, Val Loss: 13.09, Train BLEU: 8.59, Val BLEU: 3.08\n",
      "Epoch: 781.00, Train Loss: 0.55, Val Loss: 13.07, Train BLEU: 8.55, Val BLEU: 3.05\n",
      "Epoch: 782.00, Train Loss: 0.51, Val Loss: 13.04, Train BLEU: 9.07, Val BLEU: 3.14\n",
      "Epoch: 783.00, Train Loss: 0.50, Val Loss: 13.05, Train BLEU: 9.20, Val BLEU: 2.88\n",
      "Epoch: 784.00, Train Loss: 0.55, Val Loss: 13.11, Train BLEU: 9.03, Val BLEU: 3.18\n",
      "Epoch: 785.00, Train Loss: 0.60, Val Loss: 13.15, Train BLEU: 9.11, Val BLEU: 3.06\n",
      "Epoch: 786.00, Train Loss: 0.50, Val Loss: 13.13, Train BLEU: 9.38, Val BLEU: 2.90\n",
      "Epoch: 787.00, Train Loss: 0.49, Val Loss: 13.09, Train BLEU: 9.49, Val BLEU: 3.14\n",
      "Epoch: 788.00, Train Loss: 0.64, Val Loss: 13.05, Train BLEU: 9.02, Val BLEU: 3.05\n",
      "Epoch: 789.00, Train Loss: 0.53, Val Loss: 13.08, Train BLEU: 8.97, Val BLEU: 3.00\n",
      "Epoch: 790.00, Train Loss: 0.64, Val Loss: 13.12, Train BLEU: 8.70, Val BLEU: 2.81\n",
      "Epoch: 791.00, Train Loss: 0.53, Val Loss: 13.12, Train BLEU: 9.06, Val BLEU: 2.94\n",
      "Epoch: 792.00, Train Loss: 0.86, Val Loss: 13.11, Train BLEU: 8.97, Val BLEU: 3.12\n",
      "Epoch: 793.00, Train Loss: 0.73, Val Loss: 13.10, Train BLEU: 9.23, Val BLEU: 3.18\n",
      "Epoch: 794.00, Train Loss: 0.58, Val Loss: 13.11, Train BLEU: 9.31, Val BLEU: 3.03\n",
      "Epoch: 795.00, Train Loss: 0.55, Val Loss: 13.12, Train BLEU: 9.32, Val BLEU: 3.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 796.00, Train Loss: 0.73, Val Loss: 13.13, Train BLEU: 9.09, Val BLEU: 2.85\n",
      "Epoch: 797.00, Train Loss: 0.64, Val Loss: 13.15, Train BLEU: 9.02, Val BLEU: 3.07\n",
      "Epoch: 798.00, Train Loss: 0.57, Val Loss: 13.14, Train BLEU: 8.88, Val BLEU: 2.82\n",
      "Epoch: 799.00, Train Loss: 0.54, Val Loss: 13.13, Train BLEU: 9.27, Val BLEU: 2.98\n",
      "Epoch: 800.00, Train Loss: 0.57, Val Loss: 13.12, Train BLEU: 9.01, Val BLEU: 3.08\n",
      "Epoch: 801.00, Train Loss: 0.57, Val Loss: 13.10, Train BLEU: 9.03, Val BLEU: 3.06\n",
      "Epoch: 802.00, Train Loss: 0.55, Val Loss: 13.11, Train BLEU: 9.02, Val BLEU: 2.69\n",
      "Epoch: 803.00, Train Loss: 0.54, Val Loss: 13.12, Train BLEU: 8.79, Val BLEU: 2.84\n",
      "Epoch: 804.00, Train Loss: 0.52, Val Loss: 13.11, Train BLEU: 9.14, Val BLEU: 3.06\n",
      "Epoch: 805.00, Train Loss: 0.54, Val Loss: 13.12, Train BLEU: 8.94, Val BLEU: 3.00\n",
      "Epoch: 806.00, Train Loss: 0.51, Val Loss: 13.13, Train BLEU: 9.06, Val BLEU: 3.25\n",
      "Epoch: 807.00, Train Loss: 0.51, Val Loss: 13.15, Train BLEU: 8.91, Val BLEU: 3.28\n",
      "Epoch: 808.00, Train Loss: 0.52, Val Loss: 13.18, Train BLEU: 8.74, Val BLEU: 3.10\n",
      "Epoch: 809.00, Train Loss: 0.56, Val Loss: 13.20, Train BLEU: 8.54, Val BLEU: 2.87\n",
      "Epoch: 810.00, Train Loss: 0.62, Val Loss: 13.19, Train BLEU: 8.72, Val BLEU: 2.81\n",
      "Epoch: 811.00, Train Loss: 0.49, Val Loss: 13.15, Train BLEU: 8.87, Val BLEU: 2.99\n",
      "Epoch: 812.00, Train Loss: 0.52, Val Loss: 13.15, Train BLEU: 9.14, Val BLEU: 3.03\n",
      "Epoch: 813.00, Train Loss: 0.52, Val Loss: 13.19, Train BLEU: 8.64, Val BLEU: 2.95\n",
      "Epoch: 814.00, Train Loss: 0.67, Val Loss: 13.22, Train BLEU: 8.71, Val BLEU: 2.84\n",
      "Epoch: 815.00, Train Loss: 0.51, Val Loss: 13.20, Train BLEU: 8.85, Val BLEU: 2.89\n",
      "Epoch: 816.00, Train Loss: 0.54, Val Loss: 13.20, Train BLEU: 9.17, Val BLEU: 2.98\n",
      "Epoch: 817.00, Train Loss: 0.53, Val Loss: 13.19, Train BLEU: 8.99, Val BLEU: 2.98\n",
      "Epoch: 818.00, Train Loss: 0.62, Val Loss: 13.16, Train BLEU: 9.15, Val BLEU: 3.07\n",
      "Epoch: 819.00, Train Loss: 0.48, Val Loss: 13.15, Train BLEU: 9.33, Val BLEU: 3.19\n",
      "Epoch: 820.00, Train Loss: 0.51, Val Loss: 13.16, Train BLEU: 9.33, Val BLEU: 3.09\n",
      "Epoch: 821.00, Train Loss: 0.49, Val Loss: 13.16, Train BLEU: 9.15, Val BLEU: 3.18\n",
      "Epoch: 822.00, Train Loss: 0.50, Val Loss: 13.18, Train BLEU: 8.70, Val BLEU: 2.85\n",
      "Epoch: 823.00, Train Loss: 0.49, Val Loss: 13.22, Train BLEU: 9.03, Val BLEU: 2.89\n",
      "Epoch: 824.00, Train Loss: 0.51, Val Loss: 13.23, Train BLEU: 8.91, Val BLEU: 3.03\n",
      "Epoch: 825.00, Train Loss: 0.50, Val Loss: 13.24, Train BLEU: 8.80, Val BLEU: 3.12\n",
      "Epoch: 826.00, Train Loss: 0.52, Val Loss: 13.24, Train BLEU: 8.85, Val BLEU: 3.11\n",
      "Epoch: 827.00, Train Loss: 0.50, Val Loss: 13.22, Train BLEU: 9.21, Val BLEU: 3.07\n",
      "Epoch: 828.00, Train Loss: 0.49, Val Loss: 13.24, Train BLEU: 8.97, Val BLEU: 2.93\n",
      "Epoch: 829.00, Train Loss: 0.50, Val Loss: 13.25, Train BLEU: 8.90, Val BLEU: 2.88\n",
      "Epoch: 830.00, Train Loss: 0.52, Val Loss: 13.22, Train BLEU: 8.88, Val BLEU: 2.88\n",
      "Epoch: 831.00, Train Loss: 0.48, Val Loss: 13.21, Train BLEU: 9.12, Val BLEU: 3.03\n",
      "Epoch: 832.00, Train Loss: 0.48, Val Loss: 13.22, Train BLEU: 9.30, Val BLEU: 3.34\n",
      "Epoch: 833.00, Train Loss: 0.46, Val Loss: 13.24, Train BLEU: 9.73, Val BLEU: 3.08\n",
      "Epoch: 834.00, Train Loss: 0.52, Val Loss: 13.26, Train BLEU: 9.24, Val BLEU: 2.89\n",
      "Epoch: 835.00, Train Loss: 0.48, Val Loss: 13.22, Train BLEU: 9.27, Val BLEU: 3.03\n",
      "Epoch: 836.00, Train Loss: 0.46, Val Loss: 13.20, Train BLEU: 9.53, Val BLEU: 2.96\n",
      "Epoch: 837.00, Train Loss: 0.48, Val Loss: 13.20, Train BLEU: 9.13, Val BLEU: 2.79\n",
      "Epoch: 838.00, Train Loss: 0.50, Val Loss: 13.23, Train BLEU: 8.83, Val BLEU: 2.92\n",
      "Epoch: 839.00, Train Loss: 0.45, Val Loss: 13.26, Train BLEU: 8.81, Val BLEU: 2.88\n",
      "Epoch: 840.00, Train Loss: 0.45, Val Loss: 13.27, Train BLEU: 8.86, Val BLEU: 2.98\n",
      "Epoch: 841.00, Train Loss: 0.47, Val Loss: 13.26, Train BLEU: 9.06, Val BLEU: 2.97\n",
      "Epoch: 842.00, Train Loss: 0.45, Val Loss: 13.27, Train BLEU: 9.53, Val BLEU: 2.98\n",
      "Epoch: 843.00, Train Loss: 0.45, Val Loss: 13.27, Train BLEU: 9.49, Val BLEU: 3.07\n",
      "Epoch: 844.00, Train Loss: 0.45, Val Loss: 13.27, Train BLEU: 9.60, Val BLEU: 3.10\n",
      "Epoch: 845.00, Train Loss: 0.45, Val Loss: 13.26, Train BLEU: 9.53, Val BLEU: 3.09\n",
      "Epoch: 846.00, Train Loss: 0.47, Val Loss: 13.25, Train BLEU: 9.03, Val BLEU: 3.07\n",
      "Epoch: 847.00, Train Loss: 0.48, Val Loss: 13.26, Train BLEU: 9.05, Val BLEU: 2.95\n",
      "Epoch: 848.00, Train Loss: 0.46, Val Loss: 13.28, Train BLEU: 9.38, Val BLEU: 2.74\n",
      "Epoch: 849.00, Train Loss: 0.46, Val Loss: 13.29, Train BLEU: 9.52, Val BLEU: 2.87\n",
      "Epoch: 850.00, Train Loss: 0.55, Val Loss: 13.30, Train BLEU: 9.28, Val BLEU: 2.88\n",
      "Epoch: 851.00, Train Loss: 0.45, Val Loss: 13.31, Train BLEU: 9.25, Val BLEU: 2.88\n",
      "Epoch: 852.00, Train Loss: 0.46, Val Loss: 13.30, Train BLEU: 9.48, Val BLEU: 2.69\n",
      "Epoch: 853.00, Train Loss: 0.49, Val Loss: 13.26, Train BLEU: 9.07, Val BLEU: 3.23\n",
      "Epoch: 854.00, Train Loss: 0.45, Val Loss: 13.26, Train BLEU: 9.22, Val BLEU: 3.13\n",
      "Epoch: 855.00, Train Loss: 0.46, Val Loss: 13.27, Train BLEU: 9.19, Val BLEU: 2.93\n",
      "Epoch: 856.00, Train Loss: 0.48, Val Loss: 13.27, Train BLEU: 8.66, Val BLEU: 2.83\n",
      "Epoch: 857.00, Train Loss: 0.53, Val Loss: 13.27, Train BLEU: 8.75, Val BLEU: 2.82\n",
      "Epoch: 858.00, Train Loss: 0.48, Val Loss: 13.28, Train BLEU: 9.26, Val BLEU: 2.79\n",
      "Epoch: 859.00, Train Loss: 0.47, Val Loss: 13.28, Train BLEU: 9.50, Val BLEU: 2.96\n",
      "Epoch: 860.00, Train Loss: 0.57, Val Loss: 13.28, Train BLEU: 9.20, Val BLEU: 2.72\n",
      "Epoch: 861.00, Train Loss: 0.45, Val Loss: 13.31, Train BLEU: 9.57, Val BLEU: 3.08\n",
      "Epoch: 862.00, Train Loss: 0.45, Val Loss: 13.32, Train BLEU: 8.94, Val BLEU: 2.89\n",
      "Epoch: 863.00, Train Loss: 0.58, Val Loss: 13.31, Train BLEU: 8.86, Val BLEU: 3.13\n",
      "Epoch: 864.00, Train Loss: 0.46, Val Loss: 13.27, Train BLEU: 8.97, Val BLEU: 3.08\n",
      "Epoch: 865.00, Train Loss: 0.46, Val Loss: 13.29, Train BLEU: 9.08, Val BLEU: 2.90\n",
      "Epoch: 866.00, Train Loss: 0.50, Val Loss: 13.31, Train BLEU: 9.03, Val BLEU: 2.57\n",
      "Epoch: 867.00, Train Loss: 0.54, Val Loss: 13.35, Train BLEU: 9.00, Val BLEU: 2.85\n",
      "Epoch: 868.00, Train Loss: 0.46, Val Loss: 13.36, Train BLEU: 9.25, Val BLEU: 2.96\n",
      "Epoch: 869.00, Train Loss: 0.44, Val Loss: 13.33, Train BLEU: 9.31, Val BLEU: 3.03\n",
      "Epoch: 870.00, Train Loss: 0.53, Val Loss: 13.32, Train BLEU: 9.11, Val BLEU: 3.14\n",
      "Epoch: 871.00, Train Loss: 0.47, Val Loss: 13.31, Train BLEU: 9.43, Val BLEU: 3.16\n",
      "Epoch: 872.00, Train Loss: 0.44, Val Loss: 13.32, Train BLEU: 9.22, Val BLEU: 2.95\n",
      "Epoch: 873.00, Train Loss: 0.45, Val Loss: 13.33, Train BLEU: 9.07, Val BLEU: 3.09\n",
      "Epoch: 874.00, Train Loss: 0.46, Val Loss: 13.35, Train BLEU: 9.22, Val BLEU: 3.04\n",
      "Epoch: 875.00, Train Loss: 0.46, Val Loss: 13.35, Train BLEU: 9.16, Val BLEU: 3.00\n",
      "Epoch: 876.00, Train Loss: 0.43, Val Loss: 13.35, Train BLEU: 9.07, Val BLEU: 2.99\n",
      "Epoch: 877.00, Train Loss: 0.43, Val Loss: 13.34, Train BLEU: 9.45, Val BLEU: 3.02\n",
      "Epoch: 878.00, Train Loss: 0.49, Val Loss: 13.34, Train BLEU: 9.16, Val BLEU: 2.90\n",
      "Epoch: 879.00, Train Loss: 0.46, Val Loss: 13.32, Train BLEU: 9.20, Val BLEU: 2.90\n",
      "Epoch: 880.00, Train Loss: 0.43, Val Loss: 13.32, Train BLEU: 9.19, Val BLEU: 2.93\n",
      "Epoch: 881.00, Train Loss: 0.46, Val Loss: 13.32, Train BLEU: 9.20, Val BLEU: 2.96\n",
      "Epoch: 882.00, Train Loss: 0.47, Val Loss: 13.33, Train BLEU: 9.14, Val BLEU: 2.87\n",
      "Epoch: 883.00, Train Loss: 0.42, Val Loss: 13.33, Train BLEU: 9.44, Val BLEU: 2.99\n",
      "Epoch: 884.00, Train Loss: 0.45, Val Loss: 13.33, Train BLEU: 9.62, Val BLEU: 2.91\n",
      "Epoch: 885.00, Train Loss: 0.46, Val Loss: 13.33, Train BLEU: 9.21, Val BLEU: 2.85\n",
      "Epoch: 886.00, Train Loss: 0.44, Val Loss: 13.35, Train BLEU: 9.05, Val BLEU: 2.83\n",
      "Epoch: 887.00, Train Loss: 0.45, Val Loss: 13.37, Train BLEU: 9.21, Val BLEU: 2.92\n",
      "Epoch: 888.00, Train Loss: 0.47, Val Loss: 13.37, Train BLEU: 9.16, Val BLEU: 2.93\n",
      "Epoch: 889.00, Train Loss: 0.41, Val Loss: 13.38, Train BLEU: 9.57, Val BLEU: 3.15\n",
      "Epoch: 890.00, Train Loss: 0.41, Val Loss: 13.38, Train BLEU: 9.50, Val BLEU: 2.97\n",
      "Epoch: 891.00, Train Loss: 0.44, Val Loss: 13.36, Train BLEU: 9.10, Val BLEU: 3.08\n",
      "Epoch: 892.00, Train Loss: 0.49, Val Loss: 13.35, Train BLEU: 9.27, Val BLEU: 3.01\n",
      "Epoch: 893.00, Train Loss: 0.48, Val Loss: 13.40, Train BLEU: 9.11, Val BLEU: 2.91\n",
      "Epoch: 894.00, Train Loss: 0.45, Val Loss: 13.37, Train BLEU: 9.11, Val BLEU: 2.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 895.00, Train Loss: 0.67, Val Loss: 13.38, Train BLEU: 8.96, Val BLEU: 2.98\n",
      "Epoch: 896.00, Train Loss: 0.43, Val Loss: 13.35, Train BLEU: 9.30, Val BLEU: 2.97\n",
      "Epoch: 897.00, Train Loss: 0.44, Val Loss: 13.36, Train BLEU: 9.28, Val BLEU: 3.03\n",
      "Epoch: 898.00, Train Loss: 0.54, Val Loss: 13.38, Train BLEU: 9.05, Val BLEU: 2.78\n",
      "Epoch: 899.00, Train Loss: 0.54, Val Loss: 13.39, Train BLEU: 8.72, Val BLEU: 2.83\n",
      "Epoch: 900.00, Train Loss: 0.43, Val Loss: 13.40, Train BLEU: 9.42, Val BLEU: 2.88\n",
      "Epoch: 901.00, Train Loss: 0.43, Val Loss: 13.37, Train BLEU: 9.26, Val BLEU: 2.99\n",
      "Epoch: 902.00, Train Loss: 0.44, Val Loss: 13.39, Train BLEU: 9.34, Val BLEU: 3.04\n",
      "Epoch: 903.00, Train Loss: 0.44, Val Loss: 13.42, Train BLEU: 9.14, Val BLEU: 3.00\n",
      "Epoch: 904.00, Train Loss: 0.42, Val Loss: 13.43, Train BLEU: 9.18, Val BLEU: 2.93\n",
      "Epoch: 905.00, Train Loss: 0.42, Val Loss: 13.41, Train BLEU: 9.18, Val BLEU: 2.94\n",
      "Epoch: 906.00, Train Loss: 0.59, Val Loss: 13.36, Train BLEU: 9.22, Val BLEU: 2.92\n",
      "Epoch: 907.00, Train Loss: 0.44, Val Loss: 13.35, Train BLEU: 9.19, Val BLEU: 2.65\n",
      "Epoch: 908.00, Train Loss: 0.47, Val Loss: 13.39, Train BLEU: 9.39, Val BLEU: 2.91\n",
      "Epoch: 909.00, Train Loss: 0.50, Val Loss: 13.40, Train BLEU: 9.16, Val BLEU: 2.82\n",
      "Epoch: 910.00, Train Loss: 0.46, Val Loss: 13.45, Train BLEU: 9.26, Val BLEU: 2.91\n",
      "Epoch: 911.00, Train Loss: 0.46, Val Loss: 13.43, Train BLEU: 9.27, Val BLEU: 2.98\n",
      "Epoch: 912.00, Train Loss: 0.45, Val Loss: 13.38, Train BLEU: 9.26, Val BLEU: 3.07\n",
      "Epoch: 913.00, Train Loss: 0.59, Val Loss: 13.34, Train BLEU: 9.08, Val BLEU: 3.23\n",
      "Epoch: 914.00, Train Loss: 0.45, Val Loss: 13.33, Train BLEU: 9.36, Val BLEU: 3.06\n",
      "Epoch: 915.00, Train Loss: 0.42, Val Loss: 13.38, Train BLEU: 9.25, Val BLEU: 2.90\n",
      "Epoch: 916.00, Train Loss: 0.57, Val Loss: 13.43, Train BLEU: 8.79, Val BLEU: 2.80\n",
      "Epoch: 917.00, Train Loss: 0.44, Val Loss: 13.45, Train BLEU: 8.59, Val BLEU: 2.81\n",
      "Epoch: 918.00, Train Loss: 0.43, Val Loss: 13.44, Train BLEU: 9.01, Val BLEU: 2.94\n",
      "Epoch: 919.00, Train Loss: 0.45, Val Loss: 13.42, Train BLEU: 9.14, Val BLEU: 3.08\n",
      "Epoch: 920.00, Train Loss: 0.41, Val Loss: 13.42, Train BLEU: 9.66, Val BLEU: 3.02\n",
      "Epoch: 921.00, Train Loss: 0.44, Val Loss: 13.44, Train BLEU: 9.06, Val BLEU: 2.95\n",
      "Epoch: 922.00, Train Loss: 0.47, Val Loss: 13.45, Train BLEU: 8.97, Val BLEU: 2.80\n",
      "Epoch: 923.00, Train Loss: 0.44, Val Loss: 13.44, Train BLEU: 9.30, Val BLEU: 3.23\n",
      "Epoch: 924.00, Train Loss: 0.43, Val Loss: 13.44, Train BLEU: 9.18, Val BLEU: 3.17\n",
      "Epoch: 925.00, Train Loss: 0.44, Val Loss: 13.45, Train BLEU: 9.17, Val BLEU: 3.14\n",
      "Epoch: 926.00, Train Loss: 0.48, Val Loss: 13.46, Train BLEU: 9.01, Val BLEU: 3.06\n",
      "Epoch: 927.00, Train Loss: 0.41, Val Loss: 13.46, Train BLEU: 9.07, Val BLEU: 2.91\n",
      "Epoch: 928.00, Train Loss: 0.43, Val Loss: 13.46, Train BLEU: 8.98, Val BLEU: 2.86\n",
      "Epoch: 929.00, Train Loss: 0.42, Val Loss: 13.45, Train BLEU: 9.06, Val BLEU: 2.93\n",
      "Epoch: 930.00, Train Loss: 0.45, Val Loss: 13.44, Train BLEU: 9.12, Val BLEU: 3.02\n",
      "Epoch: 931.00, Train Loss: 0.42, Val Loss: 13.46, Train BLEU: 9.48, Val BLEU: 3.01\n",
      "Epoch: 932.00, Train Loss: 0.46, Val Loss: 13.48, Train BLEU: 9.11, Val BLEU: 3.00\n",
      "Epoch: 933.00, Train Loss: 0.49, Val Loss: 13.48, Train BLEU: 9.14, Val BLEU: 3.06\n",
      "Epoch: 934.00, Train Loss: 0.41, Val Loss: 13.48, Train BLEU: 9.04, Val BLEU: 3.01\n",
      "Epoch: 935.00, Train Loss: 0.42, Val Loss: 13.46, Train BLEU: 9.07, Val BLEU: 3.02\n",
      "Epoch: 936.00, Train Loss: 0.49, Val Loss: 13.46, Train BLEU: 8.87, Val BLEU: 3.13\n",
      "Epoch: 937.00, Train Loss: 0.41, Val Loss: 13.47, Train BLEU: 9.12, Val BLEU: 3.10\n",
      "Epoch: 938.00, Train Loss: 0.46, Val Loss: 13.47, Train BLEU: 9.00, Val BLEU: 3.10\n",
      "Epoch: 939.00, Train Loss: 0.49, Val Loss: 13.47, Train BLEU: 9.02, Val BLEU: 3.03\n",
      "Epoch: 940.00, Train Loss: 0.40, Val Loss: 13.48, Train BLEU: 9.02, Val BLEU: 2.91\n",
      "Epoch: 941.00, Train Loss: 0.41, Val Loss: 13.50, Train BLEU: 9.22, Val BLEU: 2.95\n",
      "Epoch: 942.00, Train Loss: 0.41, Val Loss: 13.51, Train BLEU: 9.53, Val BLEU: 2.94\n",
      "Epoch: 943.00, Train Loss: 0.40, Val Loss: 13.50, Train BLEU: 9.37, Val BLEU: 3.16\n",
      "Epoch: 944.00, Train Loss: 0.42, Val Loss: 13.49, Train BLEU: 9.16, Val BLEU: 3.08\n",
      "Epoch: 945.00, Train Loss: 0.41, Val Loss: 13.47, Train BLEU: 9.32, Val BLEU: 3.06\n",
      "Epoch: 946.00, Train Loss: 0.38, Val Loss: 13.49, Train BLEU: 9.38, Val BLEU: 3.17\n",
      "Epoch: 947.00, Train Loss: 0.40, Val Loss: 13.49, Train BLEU: 9.06, Val BLEU: 3.14\n",
      "Epoch: 948.00, Train Loss: 0.40, Val Loss: 13.51, Train BLEU: 9.30, Val BLEU: 3.10\n",
      "Epoch: 949.00, Train Loss: 0.43, Val Loss: 13.53, Train BLEU: 9.19, Val BLEU: 2.96\n",
      "Epoch: 950.00, Train Loss: 0.42, Val Loss: 13.52, Train BLEU: 9.25, Val BLEU: 2.93\n",
      "Epoch: 951.00, Train Loss: 0.38, Val Loss: 13.53, Train BLEU: 8.95, Val BLEU: 3.01\n",
      "Epoch: 952.00, Train Loss: 0.39, Val Loss: 13.53, Train BLEU: 9.19, Val BLEU: 2.94\n",
      "Epoch: 953.00, Train Loss: 0.43, Val Loss: 13.55, Train BLEU: 9.47, Val BLEU: 3.15\n",
      "Epoch: 954.00, Train Loss: 0.37, Val Loss: 13.56, Train BLEU: 9.56, Val BLEU: 3.19\n",
      "Epoch: 955.00, Train Loss: 0.41, Val Loss: 13.55, Train BLEU: 9.71, Val BLEU: 3.14\n",
      "Epoch: 956.00, Train Loss: 0.41, Val Loss: 13.54, Train BLEU: 9.37, Val BLEU: 3.04\n",
      "Epoch: 957.00, Train Loss: 0.41, Val Loss: 13.55, Train BLEU: 9.34, Val BLEU: 3.05\n",
      "Epoch: 958.00, Train Loss: 0.38, Val Loss: 13.55, Train BLEU: 9.46, Val BLEU: 3.04\n",
      "Epoch: 959.00, Train Loss: 0.54, Val Loss: 13.54, Train BLEU: 8.90, Val BLEU: 2.92\n",
      "Epoch: 960.00, Train Loss: 0.38, Val Loss: 13.50, Train BLEU: 9.46, Val BLEU: 2.92\n",
      "Epoch: 961.00, Train Loss: 0.40, Val Loss: 13.51, Train BLEU: 9.39, Val BLEU: 3.04\n",
      "Epoch: 962.00, Train Loss: 0.53, Val Loss: 13.52, Train BLEU: 8.77, Val BLEU: 3.10\n",
      "Epoch: 963.00, Train Loss: 0.39, Val Loss: 13.56, Train BLEU: 8.85, Val BLEU: 3.13\n",
      "Epoch: 964.00, Train Loss: 0.41, Val Loss: 13.60, Train BLEU: 9.26, Val BLEU: 2.96\n",
      "Epoch: 965.00, Train Loss: 0.49, Val Loss: 13.59, Train BLEU: 8.98, Val BLEU: 3.07\n",
      "Epoch: 966.00, Train Loss: 0.40, Val Loss: 13.57, Train BLEU: 9.32, Val BLEU: 3.18\n",
      "Epoch: 967.00, Train Loss: 0.40, Val Loss: 13.54, Train BLEU: 9.56, Val BLEU: 3.16\n",
      "Epoch: 968.00, Train Loss: 0.39, Val Loss: 13.54, Train BLEU: 9.47, Val BLEU: 3.19\n",
      "Epoch: 969.00, Train Loss: 0.49, Val Loss: 13.55, Train BLEU: 9.05, Val BLEU: 3.10\n",
      "Epoch: 970.00, Train Loss: 0.37, Val Loss: 13.60, Train BLEU: 9.23, Val BLEU: 3.07\n",
      "Epoch: 971.00, Train Loss: 0.38, Val Loss: 13.62, Train BLEU: 9.01, Val BLEU: 2.98\n",
      "Epoch: 972.00, Train Loss: 0.49, Val Loss: 13.60, Train BLEU: 9.04, Val BLEU: 3.02\n",
      "Epoch: 973.00, Train Loss: 0.38, Val Loss: 13.57, Train BLEU: 9.24, Val BLEU: 3.12\n",
      "Epoch: 974.00, Train Loss: 0.38, Val Loss: 13.57, Train BLEU: 9.31, Val BLEU: 3.07\n",
      "Epoch: 975.00, Train Loss: 0.43, Val Loss: 13.57, Train BLEU: 9.23, Val BLEU: 3.17\n",
      "Epoch: 976.00, Train Loss: 0.35, Val Loss: 13.62, Train BLEU: 9.65, Val BLEU: 3.04\n",
      "Epoch: 977.00, Train Loss: 0.39, Val Loss: 13.61, Train BLEU: 9.33, Val BLEU: 3.07\n",
      "Epoch: 978.00, Train Loss: 0.37, Val Loss: 13.59, Train BLEU: 9.15, Val BLEU: 2.89\n",
      "Epoch: 979.00, Train Loss: 0.37, Val Loss: 13.56, Train BLEU: 9.19, Val BLEU: 2.84\n",
      "Epoch: 980.00, Train Loss: 0.39, Val Loss: 13.54, Train BLEU: 8.83, Val BLEU: 2.93\n",
      "Epoch: 981.00, Train Loss: 0.36, Val Loss: 13.54, Train BLEU: 9.20, Val BLEU: 2.94\n",
      "Epoch: 982.00, Train Loss: 0.37, Val Loss: 13.56, Train BLEU: 9.15, Val BLEU: 3.12\n",
      "Epoch: 983.00, Train Loss: 0.36, Val Loss: 13.59, Train BLEU: 9.00, Val BLEU: 3.03\n",
      "Epoch: 984.00, Train Loss: 0.35, Val Loss: 13.59, Train BLEU: 9.28, Val BLEU: 2.78\n",
      "Epoch: 985.00, Train Loss: 0.37, Val Loss: 13.57, Train BLEU: 9.51, Val BLEU: 3.05\n",
      "Epoch: 986.00, Train Loss: 0.39, Val Loss: 13.58, Train BLEU: 9.35, Val BLEU: 2.75\n",
      "Epoch: 987.00, Train Loss: 0.36, Val Loss: 13.57, Train BLEU: 9.82, Val BLEU: 2.88\n",
      "Epoch: 988.00, Train Loss: 0.37, Val Loss: 13.58, Train BLEU: 9.23, Val BLEU: 2.81\n",
      "Epoch: 989.00, Train Loss: 0.37, Val Loss: 13.60, Train BLEU: 9.39, Val BLEU: 2.96\n",
      "Epoch: 990.00, Train Loss: 0.35, Val Loss: 13.61, Train BLEU: 9.51, Val BLEU: 2.81\n",
      "Epoch: 991.00, Train Loss: 0.38, Val Loss: 13.60, Train BLEU: 9.12, Val BLEU: 2.74\n",
      "Epoch: 992.00, Train Loss: 0.35, Val Loss: 13.59, Train BLEU: 9.62, Val BLEU: 2.89\n",
      "Epoch: 993.00, Train Loss: 0.37, Val Loss: 13.60, Train BLEU: 9.59, Val BLEU: 3.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 994.00, Train Loss: 0.36, Val Loss: 13.62, Train BLEU: 9.67, Val BLEU: 2.93\n",
      "Epoch: 995.00, Train Loss: 0.47, Val Loss: 13.64, Train BLEU: 9.25, Val BLEU: 3.12\n",
      "Epoch: 996.00, Train Loss: 0.36, Val Loss: 13.64, Train BLEU: 9.62, Val BLEU: 3.02\n",
      "Epoch: 997.00, Train Loss: 0.38, Val Loss: 13.65, Train BLEU: 9.52, Val BLEU: 2.86\n",
      "Epoch: 998.00, Train Loss: 0.45, Val Loss: 13.66, Train BLEU: 8.85, Val BLEU: 2.86\n",
      "Epoch: 999.00, Train Loss: 0.37, Val Loss: 13.69, Train BLEU: 9.01, Val BLEU: 2.87\n",
      "Experiment completed in 155 minutes with 7.48 validation loss and 3.34 validation BLEU.\n"
     ]
    }
   ],
   "source": [
    "# results, hyperparams, runtime, model, train_loader, dev_loader = \\\n",
    "#     run_experiment(train_loader, dev_loader, model_type='without_attention', num_epochs=10, learning_rate=0.0005,\n",
    "#                    num_layers=2, enc_hidden_dim=300, dec_hidden_dim=2*300, experiment_name='test_run', \n",
    "#                    model_name='test_run', inspect=True, lazy_eval=True, save_to_log=True, save_checkpoint=True, \n",
    "#                    print_summary=True, print_intermediate=True)\n",
    "\n",
    "results, hyperparams, runtime, model = \\\n",
    "    run_experiment(model_type='attention_bahdanau', num_epochs=1000, learning_rate=0.0005,\n",
    "                   num_layers=2, enc_hidden_dim=300, dec_hidden_dim=2*300, experiment_name='test_run', \n",
    "                   model_name='test_run', inspect=False, lazy_eval=False, lazy_train=True, \n",
    "                   save_to_log=True, save_checkpoint=True, print_summary=True, print_intermediate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: life in the deep oceans with vibrant video clips captured by <UNK> , david <UNK> takes us to some of earth <UNK> darkest , most violent , toxic and beautiful habitats , the valleys and volcanic <UNK> of the oceans <UNK> depths , where this is bill <UNK> . i <UNK> dave <UNK> . and we <UNK> going to tell you some stories from the sea here in video . we <UNK> got some of the most incredible video of titanic that <UNK> ever been seen , and we <UNK> not going to show you any of it .\n",
      "MODEL TRANSLATION: <UNK> . . . . . . the the the the the . . . <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> these <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> life life habitats <UNK> <UNK> the the the the the the the the the the the the standing standing standing into at at at at at up up up up up up up up up up captured to oceans i show show that of of of of of of in in in in in in in in in in of of of of new new new new new new it it it it it it oceans and and and and and and the and and and and the and and and and and the the the the the the the the the the the <UNK> <UNK> <UNK> <UNK> it it it that that that toxic toxic toxic stories stories of . . . . . . of of of the the the of of the the the the new new new new new new new new new new new new new\n"
     ]
    }
   ],
   "source": [
    "inspect_model(model, 'train', train_loader_limited, dev_loader_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = summarize_results(load_experiment_log(experiment_name='test_run', filename=RESULTS_LOG))\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split BLEU and Loss \n",
    "plot_single_learning_curve(all_results.iloc[1]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, num_layers=2, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "#                                                             data['train']['source']['token2id']))\n",
    "\n",
    "# decoder = DecoderRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=300, dec_hidden_dim=2*300, \n",
    "#                      enc_hidden_dim=300, num_layers=2, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "#                                                             data['train']['target']['token2id']))\n",
    "\n",
    "# decoder_attn = DecoderAttnRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=300, dec_hidden_dim=2*300, \n",
    "#                               enc_hidden_dim=300, num_layers=2, \n",
    "#                               pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "#                                                                      data['train']['target']['token2id']))\n",
    "\n",
    "# #model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\n",
    "# model = EncoderDecoder(encoder, decoder_attn, data['train']['target']['token2id'])\n",
    "# train(model, train_loader, dev_loader, data['train']['target']['id2token'], num_epochs=20, learning_rate=0.0005) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "#                                                             data['train']['source']['token2id']))\n",
    "# attention = Attention(enc_hidden_dim=300, dec_hidden_dim=600, num_annotations=SRC_MAX_SENTENCE_LEN)\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader): \n",
    "#     enc_outputs, enc_final_hidden = encoder(src_idxs, src_lens)\n",
    "#     attn_weights = attention(encoder_outputs=enc_outputs, last_dec_hidden=enc_final_hidden)\n",
    "#     print(\"attn weights are: {}\".format(attn_weights.size()))\n",
    "#     print(\"example: {}\".format(attn_weights[0].sum()))\n",
    "# #     print(\"enc_outputs size is {}\".format(enc_outputs.size()))\n",
    "# #     print(\"enc_final_hidden size is {}\".format(enc_final_hidden.size()))\n",
    "    \n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
