{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "import sacrebleu\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "BATCH_SIZE = 16\n",
    "SRC_MAX_SENTENCE_LEN = 20 \n",
    "TARG_MAX_SENTENCE_LEN = 20\n",
    "SRC_VOCAB_SIZE = 10000\n",
    "TARG_VOCAB_SIZE = 10000\n",
    "HIDDEN_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text2tokens(raw_text_fp): \n",
    "#     \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "#     with open(raw_text_fp) as f:\n",
    "#         tokens_data = [line.lower().split() for line in f.readlines()]       \n",
    "#         tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "#     return tokens_data \n",
    "\n",
    "def text2tokens(raw_text_fp, lang_type): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]\n",
    "        if lang_type == 'source': \n",
    "            tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "        elif lang_type == 'target': \n",
    "            tokens_data = [['<SOS'] + datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(num_vocab))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + list(vocab)\n",
    "    token2id = dict(zip(id2token, range(max_vocab_size)))\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "\n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # attach vocab sizes and word2vec models \n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size \n",
    "    data['train']['source']['word2vec'] = load_word2vec(src_lang) \n",
    "    data['train']['target']['word2vec'] = load_word2vec(targ_lang) \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'], lang_type)\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    token_lists = data['train'][lang_type]['tokens'], \n",
    "                    max_vocab_size = data['train'][lang_type]['max_vocab_size'], \n",
    "                    word2vec = data['train'][lang_type]['word2vec']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_emb(word2vec, token2id): \n",
    "    \"\"\" Given word2vec model and the vocab's token2id, extract pretrained word embeddings \"\"\"\n",
    "    pretrained_emb = np.zeros((len(token2id), 300)) \n",
    "    for token in token2id: \n",
    "        try: \n",
    "            pretrained_emb[token2id[token]] = word2vec[token]\n",
    "        except: \n",
    "            pretrained_emb[token2id[token]] = np.random.normal(size=(300,))\n",
    "    return torch.from_numpy(pretrained_emb.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len, batch_size=BATCH_SIZE): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "#train_loader_, dev_loader_, test_loader_ = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '现在', '可以', '去', '个', '真正', '的', '学校', '念书', '了', '他', '说', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['source']['tokens'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS', '&quot;', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '&quot;', 'he', 'said', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['target']['tokens'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6391e-01,  1.0639e+00, -7.2269e-01,  3.8855e-01,  8.0669e-01,\n",
       "       -3.9772e-01, -6.4854e-01, -2.0340e-02,  3.2016e-01, -1.8594e-02,\n",
       "        7.5796e-01,  9.5875e-02,  1.3857e+00, -8.8656e-01,  8.9787e-02,\n",
       "       -7.8805e-01,  8.2156e-01, -4.5274e-01, -1.3599e+00, -5.8308e-01,\n",
       "       -1.2423e+00,  1.2513e+00,  1.3082e+00,  7.1150e-03,  5.6409e-01,\n",
       "       -5.0618e-01,  9.6601e-01,  2.2565e-01,  7.9516e-01,  2.9112e-01,\n",
       "        9.4196e-01, -1.0480e+00, -8.9431e-01, -6.9267e-01, -9.3206e-01,\n",
       "        1.1966e+00,  1.1582e+00, -5.6526e-01,  1.7063e-01,  1.4300e+00,\n",
       "        7.6709e-01,  1.2418e+00,  1.1196e+00, -1.2855e+00, -3.9796e-01,\n",
       "       -4.0813e-01, -7.4501e-01, -4.0786e-01,  5.6266e-01, -9.1049e-01,\n",
       "        1.2451e+00, -2.1408e-01, -5.1217e-01, -5.3418e-01,  1.1298e-01,\n",
       "       -1.1111e+00, -1.1714e+00, -1.3583e+00, -3.2776e+00,  4.4865e-01,\n",
       "        1.1381e+00, -1.4361e-01,  9.8352e-01,  1.3363e+00,  5.9738e-01,\n",
       "        1.0656e+00, -1.0906e+00, -5.8233e-01, -5.5804e-01, -3.8899e-01,\n",
       "        1.4958e+00, -9.1816e-01,  1.2217e+00, -5.6197e-01, -9.1967e-01,\n",
       "        2.8408e-01, -6.6672e-02, -8.2325e-01, -5.7572e-01, -1.1177e+00,\n",
       "       -2.5858e-01,  7.1498e-03,  8.9017e-01, -8.0305e-01, -8.1328e-01,\n",
       "       -8.9870e-01,  2.4938e-01,  5.9711e-01,  6.5360e-02, -1.0975e+00,\n",
       "       -7.6651e-01,  8.9634e-01, -1.3244e-01, -2.2426e-02, -9.0831e-01,\n",
       "        1.2333e+00,  8.6232e-01,  7.2508e-01, -1.7962e-02,  1.9577e-01,\n",
       "        6.4059e-01,  6.5498e-01, -5.7352e-01,  9.5553e-01, -6.8620e-01,\n",
       "        6.7117e-01,  2.7553e-02, -8.1152e-01, -7.4209e-01,  7.8452e-02,\n",
       "        2.1996e-01, -7.5938e-01, -1.3916e-01,  6.2511e-01,  2.4316e-01,\n",
       "        3.6103e-01, -7.3676e-01,  1.8879e-01, -6.0778e-01, -9.7829e-01,\n",
       "       -7.7176e-01,  9.3991e-01,  7.5902e-01,  2.3560e-02, -6.9437e-01,\n",
       "       -8.9607e-01,  7.3124e-01,  7.4123e-01, -1.1011e+00,  3.5925e-01,\n",
       "        8.1056e-01,  2.2771e-01, -9.0569e-01,  1.0399e+00, -1.1795e+00,\n",
       "        3.6409e-01, -4.0508e-01, -3.9343e-01, -8.9886e-01, -5.1658e-01,\n",
       "        7.8766e-02,  8.8267e-01,  1.2621e+00, -1.1790e+00, -8.3421e-01,\n",
       "        4.1145e-01, -1.5983e+00, -1.3313e-01, -8.3042e-01, -2.9095e-01,\n",
       "        5.6606e-01,  8.5247e-01, -1.0356e-01,  2.9817e-01,  1.0145e+00,\n",
       "       -8.0707e-01,  3.3252e-01, -4.1859e-01,  2.0578e-03,  7.4378e-01,\n",
       "        3.5127e-01,  3.4332e-01,  3.9476e-01,  7.9520e-03,  5.6837e-02,\n",
       "        1.9579e-01, -2.5149e-01,  8.7891e-01,  4.3723e-01,  3.7107e-01,\n",
       "        5.7137e-01,  6.8381e-01,  1.0813e+00, -4.7799e-02,  9.1588e-02,\n",
       "        1.1153e-01,  8.4817e-01,  8.6699e-01, -6.7174e-02, -6.0056e-01,\n",
       "       -7.6134e-01, -5.5208e-01,  3.0215e-01,  9.9575e-03, -4.6225e-01,\n",
       "        8.3512e-01, -1.0748e-02, -7.9808e-01, -5.0147e-01,  8.3243e-01,\n",
       "       -3.1315e-01, -8.3514e-02, -7.0079e-01, -5.6033e-01, -6.4789e-02,\n",
       "       -6.9048e-01, -8.8293e-01, -7.4071e-01,  2.6856e-02,  5.8349e-01,\n",
       "       -4.7795e-02, -4.4563e-01, -1.7875e-01, -6.0016e-01,  7.2793e-02,\n",
       "        1.5451e-01,  2.6200e-01, -4.7335e-01, -1.4017e-01,  4.0792e-01,\n",
       "       -1.0640e+00,  3.9888e-01, -5.1001e-01,  3.9962e-01, -8.5021e-01,\n",
       "        1.7305e-01,  2.8913e-01, -1.3508e-01,  9.3687e-01,  6.0144e-01,\n",
       "       -6.0228e-01,  7.9056e-01,  3.3801e-01, -5.7203e-02, -1.0779e+00,\n",
       "        8.2638e-01, -2.9985e-01, -2.3117e-01, -1.0227e+00,  5.4373e-02,\n",
       "       -8.8121e-01, -3.8100e-01,  3.6903e-01, -7.1654e-01, -8.2499e-02,\n",
       "        1.1802e+00,  3.9736e-01, -1.0854e+00, -2.0758e-01, -2.2772e-01,\n",
       "        6.9006e-01, -1.0682e+00,  7.2935e-01,  1.2665e-03, -2.4493e-01,\n",
       "       -2.4806e-02, -4.5564e-01, -2.3917e-01, -9.4934e-01,  1.1925e-01,\n",
       "       -4.0391e-01,  3.7711e-02, -8.3502e-01,  1.5569e-01, -4.1158e-01,\n",
       "       -6.2988e-02,  5.4909e-01,  7.5530e-01, -1.5463e+00, -3.9758e-02,\n",
       "        5.7422e-01, -4.0611e-01, -2.9411e-01, -1.6940e-01, -2.7391e-01,\n",
       "       -2.0142e-01, -8.7102e-01, -9.2617e-01, -1.8897e+00, -1.2419e+00,\n",
       "       -1.3943e-01,  4.7293e-01,  1.1383e+00, -6.6022e-01, -1.3899e-02,\n",
       "       -2.7761e-01,  1.7229e-02,  1.0169e+00, -1.1558e-01, -6.5791e-01,\n",
       "        6.1787e-01,  1.2840e+00,  1.8424e-01,  8.7510e-01,  3.0483e-02,\n",
       "       -2.2935e-01,  3.6822e-01,  4.3399e-01, -8.0390e-01, -2.0275e-02,\n",
       "       -7.2834e-01, -1.1894e+00, -7.6395e-01, -6.9246e-01, -1.1771e-03,\n",
       "       -8.1935e-01,  5.1514e-01,  6.5656e-02,  1.0131e+00,  3.2171e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['word2vec']['的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([16, 20])\n",
      "tensor([[   5,    3,  171,    3,  409,    3,  184,  928, 1316,  929,  361,   18,\n",
      "         2102,    4,  327,    1,    2,    2,    2,    2],\n",
      "        [   5,    4,  199,    7,   70,   13,    4, 2103,  200, 1317, 1318,  612,\n",
      "         2104,  521,    1,    2,    2,    2,    2,    2],\n",
      "        [  13,    3,    3,    3, 2105,   15, 1319,    3,   28,  522,  523,  233,\n",
      "            4,  521,   25,  261,   19,  201,   13,  732],\n",
      "        [   3,    3,  234,    8,  199, 2106,  216,   48,    1,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   5,  137,   53,   83,  287,    3,   29,   45,    5,   23,  930,  199,\n",
      "           71,   71,    3,  733,    1,    2,    2,    2],\n",
      "        [  11,   49,   40,   57,   72,  288,    4,  159, 2107,    8,   13,   43,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   5,  461,  160,  931,  148,  928,    1,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [  12,  288,    4,  159,    1,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   5,    3,  171,    3,    3,    3, 2108,  138,   63, 2109,  217,  734,\n",
      "            9, 1320,    4,    1,    2,    2,    2,    2],\n",
      "        [  26,    7,   83,  121,    3,  100,    5, 2110, 2111, 2112,   48,    5,\n",
      "         1321,   57,   12,  362,  159, 1321,  218,  120],\n",
      "        [  15,    9,    6, 2114,   24,  524,    4,    3,  185,  289,    1,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   6,  735,   44,  234,  122,    4, 2115,   61,  186,   38,  149,   19,\n",
      "          932,    6,   44,   57, 1324,    1,    2,    2],\n",
      "        [   6,   33, 2116,  525, 2117, 2118,    3,   61,  410,   16,   19,  262,\n",
      "            6,  172,   44,   57, 2119,    1,    2,    2],\n",
      "        [ 159,    7,   12, 1325,    3,    3,    6,    3,  191,   24, 2120,    7,\n",
      "           12,  200, 2121,    3,    1,    2,    2,    2],\n",
      "        [2122,   34,  933,   32, 2123,  202,    3,    1,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   6,   25,   53,   35,  328,  934,   48,  411,    4,  329, 1326,  363,\n",
      "          150,  935,   25,   34,  330,    1,    2,    2]])\n",
      "tensor([16, 15, 20,  9, 17, 13,  7,  5, 16, 20, 11, 18, 18, 17,  8, 18])\n",
      "torch.Size([16, 20])\n",
      "tensor([[   3,   50,    9,   20,    3,    4,    9,  209, 1653,   68,   34,  387,\n",
      "            8,    6,  519,   11,  780,   12,   19,  256],\n",
      "        [   3,   19,  217,   20, 1070,    8, 1654,  781,   27,   67,  192,    4,\n",
      "         1071, 1072,    5,    1,    2,    2,    2,    2],\n",
      "        [   3,   54,   20,   10,  181, 1655,   27,   67,  241,   86,   20, 1656,\n",
      "           87,    4,   48,    6,  781,  782, 1657,  111],\n",
      "        [   3,    3,    6,  436,   28,  631,  437,    3,   19,  217,    3,    5,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   3,    9,  133,    3,   84,   32,   16,  632,    4,   24,    9,   82,\n",
      "           55,   13,   19,  217,   20,   52,    4,   52],\n",
      "        [   3,    3,   15,   26,   97,    8,   10,  300,  122,   60,    4,    3,\n",
      "           30,  112,    5,    1,    2,    2,    2,    2],\n",
      "        [   3,   10,  387,   13,    9,   63,  115,  633,    5,    1,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   3,   10,  300,  122,    5,    1,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   3,   15,   55,    4,    9,   20,  242,   50,    6,  436,  193,   98,\n",
      "          194,    7,  128,   16,  438,   23,  218,    8],\n",
      "        [   3,   22,   23,    6,  169,  276,   75,    4,    9, 1073,   35,   10,\n",
      "          331,    8, 1658,   19,  634,  783,    4,   51],\n",
      "        [   3,   16,   20,    6,   99,  126,   14,  439,   82,   39,  784,    5,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   3,  333,  103,    4,   14,  193,   10,  170, 1660,   22,   13,   85,\n",
      "           34,   76, 1661,   73,   14,   45,   78,    5],\n",
      "        [   3,   14,   76, 1662,   44, 1075,   12, 1663, 1076,   22,   16,   76,\n",
      "          785,   14,   45,   57,   69, 1664,    5,    1],\n",
      "        [   3,    6,  122,   20,   12,   10,  256,    4,   71,  108,    3,   11,\n",
      "           77, 1665,   12,   34,  192,  440,  441,    5],\n",
      "        [   3,   16,   20, 1666,   12, 1667,   24, 1077,  786,   12, 1668,    5,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   3,   14,   47,  334,   14,   45,    3,   44,  195,    3,    6,  787,\n",
      "            4,    6,  390,    7,   44,  788,    5,    1]])\n",
      "tensor([20, 16, 20, 13, 20, 16, 10,  6, 20, 20, 13, 20, 20, 20, 13, 20])\n"
     ]
    }
   ],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    print(targ_idxs.size())\n",
    "    print(targ_idxs)\n",
    "    print(targ_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, pretrained_word2vec):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "#        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "    \n",
    "    def forward(self, input, src_lens):\n",
    "        batch_size = input.size()[0]\n",
    "        _, idx_sort = torch.sort(src_lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        x, x_lengths = input.index_select(0, idx_sort), src_lens.index_select(0, idx_sort)\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, x_lengths, batch_first=True)\n",
    "        hidden = self.initHidden(batch_size).to(device)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        final_hidden = torch.cat([output[:, -1, :self.hidden_size], output[:, 0, self.hidden_size:]], dim=1).unsqueeze(0) \n",
    "#        print(\"final hidden size is: {}\".format(final_hidden.size()))\n",
    "        return output, final_hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, pretrained_word2vec):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_size = output_size\n",
    "#        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size*2)\n",
    "        self.out = nn.Linear(hidden_size*2, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         print(\"Before embedding, targ[i-1] size is: {}\".format(input.size()))\n",
    "#         print(\"Before embedding, targ[i-1] is: {}\".format(input))     \n",
    "        batch_size = input.size()[0]\n",
    "        output = self.embedding(input).view(1, batch_size, -1)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "        output = F.relu(output)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "#         print(\"hidden size is: {}\".format(hidden.size()))        \n",
    "#         print(\"hidden is: {}\".format(hidden))        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#        print(output.size())\n",
    "#         print(\"Original output is: {}\".format(output.size()))\n",
    "#         print(\"output[0] is: {}\".format(output[0].size()))\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "#        print(\"LogSoftMax output is: {}\".format(output.size()))\n",
    "#         output = output.squeeze(0) # B x N\n",
    "#         output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder, decoder_token2id): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_size = self.decoder.output_size\n",
    "        \n",
    "    def forward(self, src, targ, src_lens, targ_lens): \n",
    "        batch_size = src.size()[0]\n",
    "#        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_output, encoder_hidden = self.encoder(src, src_lens)\n",
    "        decoder_hidden = encoder_hidden \n",
    "#        print(\"Encoder Hidden size: {}\".format(encoder_hidden.size()))\n",
    "        final_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size, self.output_size))\n",
    "        hypotheses = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size))\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "#            print(\"targ[di-1] Size is {}\".format(targ[:, di-1].size()))\n",
    "#             print(\"targ[:, di-1] size is {}\".format(targ[:, di-1].size()))\n",
    "#             print(\"decoder_hidden size is {}\".format(decoder_hidden.size()))\n",
    "#             decoder_outputs = targ[:, 0] \n",
    "            decoder_outputs, decoder_hidden = self.decoder(targ[:, di-1], decoder_hidden)\n",
    "#            print(\"Final Outputs {} is {}\".format(di, decoder_outputs))\n",
    "#             top1 = decoder_outputs.data.max(1)[1]\n",
    "#             print(\"At position {}, Top 1 is {}\".format(di, top1))\n",
    "            hypotheses[di] = decoder_outputs.data.max(1)[1]\n",
    "            final_outputs[di] = decoder_outputs\n",
    "#         print(\"Shape of hypotheses: {}\".format(hypotheses.size()))\n",
    "#         print(\"Hypotheses: {}\".format(hypotheses))\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_outputs(predictions): \n",
    "#     \"\"\" Given a prediction output tensor representing the log probabilities of each word in a sentence, \n",
    "#         reduce to a tensor representing a sentence compromising the most likely words in each position \n",
    "#     \"\"\"\n",
    "#     reduced = predictions.data.max(dim=2)[1]\n",
    "#     return reduced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tensors2sentences(tensors, id2token): \n",
    "# #    print(type(tensors))\n",
    "# #    print(tensors.size())\n",
    "# #    print(type(tensors.numpy()))\n",
    "#     tokens = [id2token[idx] for idx in tensors.numpy()]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_bleu(outputs, targets, id2token): \n",
    "#     print(\"original outputs shape: {}\".format(outputs.size()))\n",
    "#     print(\"original targets  shape: {}\".format(targets.size()))\n",
    "#     outputs = reduce_outputs(outputs)\n",
    "# #     print(\"reduced outputs shape: {}\".format(outputs.size()))\n",
    "# #     print(\"target outputs shape: {}\".format(targets.size()))\n",
    "#     references = tensors2sentences(targets, id2token)\n",
    "#     hypotheses = tensors2sentences(outputs, id2token)\n",
    "# #     print(\"hypotheses are: {}\".format(hypotheses))\n",
    "# #     print(\"references are: {}\".format(references))\n",
    "#     bleu = sentence_bleu(references, hypotheses)\n",
    "#     return bleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with packing  \n",
    "\n",
    "# loss = F.nll_loss(output[1:].view(-1, vocab_size),\n",
    "#                                trg[1:].contiguous().view(-1),\n",
    "#                                ignore_index=pad)\n",
    "\n",
    "def evaluate(model, loader, id2token): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "#     reference_corpus = []\n",
    "#     hypothesis_corpus = [] \n",
    "    \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader): \n",
    "        batch_size = src_idxs.size()[0]\n",
    "        outputs = model(src_idxs, targ_idxs, src_lens, targ_lens)\n",
    "#         if i == 0: \n",
    "#             print(outputs[0:TARG_MAX_SENTENCE_LEN, :].size())\n",
    "#             print(targets[0:TARG_MAX_SENTENCE_LEN].size())\n",
    "        \n",
    "#         outputs = model(src_idxs, targ_idxs).view(batch_size, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN)\n",
    "#         targets = targ_idxs.view(batch_size, TARG_MAX_SENTENCE_LEN)\n",
    "#         print(\"outputs size: {}\".format(outputs.size()))\n",
    "#         print(\"outputs: {}\".format(outputs))\n",
    "#        print(\"1st data point outputs size: {}\".format(outputs[1].size()))\n",
    "#        print(\"1st data point outputs: {}\".format(outputs[1]))\n",
    "#         print(\"targets size: {}\".format(targets.size()))\n",
    "#         print(\"targets: {}\".format(targets))\n",
    "#        print(\"1st data point targets size: {}\".format(targets[1].size()))\n",
    "#        print(\"1st data point targets: {}\".format(targets[1]))\n",
    "#        loss = criterion(outputs.view(-1, TARG_VOCAB_SIZE), targ_idxs.view(-1))    \n",
    "        loss = F.nll_loss(outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1), ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "#         final_outputs = model(src_idxs, targ_idxs) \n",
    "#         loss = criterion(final_outputs.view(input_len, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN), \n",
    "#                          targ_idxs.view(input_len, TARG_MAX_SENTENCE_LEN))\n",
    "        total_loss += loss.item()  \n",
    "#         reference_corpus = targ_idxs\n",
    "#         output_corpus = outputs.data.max(dim=2)[1].transpose(0,1)\n",
    "#         print(type(reference_corpus))\n",
    "#         bleu_score = sacrebleu.corpus_bleu(reference_corpus, reference_corpus)\n",
    "#         print(\"BLEU score: {}\".format(bleu_score))\n",
    "#         print(\"Size of reference corpus: {}\".format(reference_corpus.size()))\n",
    "#         print(\"Size of output corpus: {}\".format(output_corpus.size()))\n",
    "#        bleu = calculate_bleu(outputs.transpose(0,1), targ_idxs, id2token)\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional with packing sequence\n",
    "\n",
    "def train(model, train_loader, dev_loader, id2token, num_epochs=3, learning_rate=0.1, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs = model(src_idxs, targ_idxs, src_lens, targ_lens) \n",
    "#             print(\"final_outputs[1:] shape is {}\".format(final_outputs[1:].size()))\n",
    "#             print(\"final_outputs[1:].view(-1, TARG_VOCAB_SIZE) shape is {}\".format(final_outputs[1:].view(-1, TARG_VOCAB_SIZE).size()))\n",
    "#             print(\"targ_idxs[:,1:] shape is {}\".format(targ_idxs[:,1:].size()))\n",
    "#             print(\"targ_idxs[:,1:].contiguous().view(-1) shape is {}\".format(targ_idxs[:,1:].contiguous().view(-1).size()))\n",
    "            loss = criterion(final_outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1))\n",
    "#             loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN), \n",
    "#                              targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 10 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "                result['train_loss'] = evaluate(model, train_loader, id2token) \n",
    "                result['val_loss'] = evaluate(model, dev_loader, id2token)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Loss: {:.2f}, Validation Loss: {:.2f}'.format(\n",
    "                        result['epoch'], result['train_loss'], result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'时候'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['id2token'][86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 8.04, Validation Loss: 7.85\n",
      "Epoch: 0.13, Train Loss: 183.80, Validation Loss: 166.15\n",
      "Epoch: 0.25, Train Loss: 221.90, Validation Loss: 201.76\n",
      "Epoch: 0.38, Train Loss: 235.49, Validation Loss: 201.57\n",
      "Epoch: 0.51, Train Loss: 203.28, Validation Loss: 164.08\n",
      "Epoch: 0.63, Train Loss: 210.37, Validation Loss: 178.60\n",
      "Epoch: 0.76, Train Loss: 207.15, Validation Loss: 174.57\n",
      "Epoch: 0.89, Train Loss: 199.22, Validation Loss: 172.24\n",
      "Epoch: 1.00, Train Loss: 217.85, Validation Loss: 187.73\n",
      "Epoch: 1.13, Train Loss: 210.45, Validation Loss: 184.68\n",
      "Epoch: 1.25, Train Loss: 211.25, Validation Loss: 184.92\n",
      "Epoch: 1.38, Train Loss: 237.25, Validation Loss: 217.40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-31523500b24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                             data['train']['target']['token2id']))\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token2id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id2token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-171-f198f444ec5e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader, id2token, num_epochs, learning_rate, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#             loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#                              targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "                                                            data['train']['source']['token2id']))\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                            data['train']['target']['token2id']))\n",
    "model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\n",
    "train(model, train_loader, dev_loader, data['train']['target']['id2token']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'pretrained_word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-e5b930f201c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSRC_VOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARG_VOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print(\"Targ shape is {}\".format(targ_idxs.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'pretrained_word2vec'"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=10)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=10)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(\"Targ shape is {}\".format(targ_idxs.size()))\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)    \n",
    "    final_outputs = model(src_idxs, targ_idxs)\n",
    "    print(final_outputs.size())\n",
    "    print(final_outputs)\n",
    "    print(targ_idxs.size())\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(input_size=10000, hidden_size=10)\n",
    "# decoder = DecoderRNN(hidden_size=10, output_size=10000)\n",
    "# encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     output, hidden = encoder(src_idxs, encoder_hidden)\n",
    "#     print(\"Output:::\")\n",
    "#     print(output.size())\n",
    "#     print(output)\n",
    "#     print(\"Hidden:::\")\n",
    "#     print(hidden.size())\n",
    "#     print(hidden)\n",
    "#     dec_output, dec_hidden = decoder(targ_idxs, hidden) \n",
    "#     print(\"Decoder Output:::\")\n",
    "#     print(dec_output.size())\n",
    "#     print(dec_output)    \n",
    "#     print(\"Decoder Hidden:::\")\n",
    "#     print(dec_hidden.size())\n",
    "#     print(dec_hidden)\n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
