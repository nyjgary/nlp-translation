{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "import sacrebleu\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "BATCH_SIZE = 16\n",
    "SRC_MAX_SENTENCE_LEN = 20 \n",
    "TARG_MAX_SENTENCE_LEN = 20\n",
    "SRC_VOCAB_SIZE = 10000\n",
    "TARG_VOCAB_SIZE = 10000\n",
    "HIDDEN_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]       \n",
    "        tokens_data = [['<SOS>'] + datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(num_vocab))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + list(vocab)\n",
    "    token2id = dict(zip(id2token, range(max_vocab_size)))\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "\n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # attach vocab sizes and word2vec models \n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size \n",
    "    data['train']['source']['word2vec'] = load_word2vec(src_lang) \n",
    "    data['train']['target']['word2vec'] = load_word2vec(targ_lang) \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'])\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    token_lists = data['train'][lang_type]['tokens'], \n",
    "                    max_vocab_size = data['train'][lang_type]['max_vocab_size'], \n",
    "                    word2vec = data['train'][lang_type]['word2vec']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_emb(word2vec, token2id): \n",
    "    \"\"\" Given word2vec model and the vocab's token2id, extract pretrained word embeddings \"\"\"\n",
    "    pretrained_emb = np.zeros((len(token2id), 300)) \n",
    "    for token in token2id: \n",
    "        try: \n",
    "            pretrained_emb[token2id[token]] = word2vec[token]\n",
    "        except: \n",
    "            pretrained_emb[token2id[token]] = np.random.normal(size=(300,))\n",
    "    return torch.from_numpy(pretrained_emb.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len, batch_size=BATCH_SIZE): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "#train_loader_, dev_loader_, test_loader_ = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', '你', '现在', '可以', '去', '个', '真正', '的', '学校', '念书', '了', '他', '说', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['source']['tokens'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', '&quot;', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '&quot;', 'he', 'said', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['target']['tokens'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6391e-01,  1.0639e+00, -7.2269e-01,  3.8855e-01,  8.0669e-01,\n",
       "       -3.9772e-01, -6.4854e-01, -2.0340e-02,  3.2016e-01, -1.8594e-02,\n",
       "        7.5796e-01,  9.5875e-02,  1.3857e+00, -8.8656e-01,  8.9787e-02,\n",
       "       -7.8805e-01,  8.2156e-01, -4.5274e-01, -1.3599e+00, -5.8308e-01,\n",
       "       -1.2423e+00,  1.2513e+00,  1.3082e+00,  7.1150e-03,  5.6409e-01,\n",
       "       -5.0618e-01,  9.6601e-01,  2.2565e-01,  7.9516e-01,  2.9112e-01,\n",
       "        9.4196e-01, -1.0480e+00, -8.9431e-01, -6.9267e-01, -9.3206e-01,\n",
       "        1.1966e+00,  1.1582e+00, -5.6526e-01,  1.7063e-01,  1.4300e+00,\n",
       "        7.6709e-01,  1.2418e+00,  1.1196e+00, -1.2855e+00, -3.9796e-01,\n",
       "       -4.0813e-01, -7.4501e-01, -4.0786e-01,  5.6266e-01, -9.1049e-01,\n",
       "        1.2451e+00, -2.1408e-01, -5.1217e-01, -5.3418e-01,  1.1298e-01,\n",
       "       -1.1111e+00, -1.1714e+00, -1.3583e+00, -3.2776e+00,  4.4865e-01,\n",
       "        1.1381e+00, -1.4361e-01,  9.8352e-01,  1.3363e+00,  5.9738e-01,\n",
       "        1.0656e+00, -1.0906e+00, -5.8233e-01, -5.5804e-01, -3.8899e-01,\n",
       "        1.4958e+00, -9.1816e-01,  1.2217e+00, -5.6197e-01, -9.1967e-01,\n",
       "        2.8408e-01, -6.6672e-02, -8.2325e-01, -5.7572e-01, -1.1177e+00,\n",
       "       -2.5858e-01,  7.1498e-03,  8.9017e-01, -8.0305e-01, -8.1328e-01,\n",
       "       -8.9870e-01,  2.4938e-01,  5.9711e-01,  6.5360e-02, -1.0975e+00,\n",
       "       -7.6651e-01,  8.9634e-01, -1.3244e-01, -2.2426e-02, -9.0831e-01,\n",
       "        1.2333e+00,  8.6232e-01,  7.2508e-01, -1.7962e-02,  1.9577e-01,\n",
       "        6.4059e-01,  6.5498e-01, -5.7352e-01,  9.5553e-01, -6.8620e-01,\n",
       "        6.7117e-01,  2.7553e-02, -8.1152e-01, -7.4209e-01,  7.8452e-02,\n",
       "        2.1996e-01, -7.5938e-01, -1.3916e-01,  6.2511e-01,  2.4316e-01,\n",
       "        3.6103e-01, -7.3676e-01,  1.8879e-01, -6.0778e-01, -9.7829e-01,\n",
       "       -7.7176e-01,  9.3991e-01,  7.5902e-01,  2.3560e-02, -6.9437e-01,\n",
       "       -8.9607e-01,  7.3124e-01,  7.4123e-01, -1.1011e+00,  3.5925e-01,\n",
       "        8.1056e-01,  2.2771e-01, -9.0569e-01,  1.0399e+00, -1.1795e+00,\n",
       "        3.6409e-01, -4.0508e-01, -3.9343e-01, -8.9886e-01, -5.1658e-01,\n",
       "        7.8766e-02,  8.8267e-01,  1.2621e+00, -1.1790e+00, -8.3421e-01,\n",
       "        4.1145e-01, -1.5983e+00, -1.3313e-01, -8.3042e-01, -2.9095e-01,\n",
       "        5.6606e-01,  8.5247e-01, -1.0356e-01,  2.9817e-01,  1.0145e+00,\n",
       "       -8.0707e-01,  3.3252e-01, -4.1859e-01,  2.0578e-03,  7.4378e-01,\n",
       "        3.5127e-01,  3.4332e-01,  3.9476e-01,  7.9520e-03,  5.6837e-02,\n",
       "        1.9579e-01, -2.5149e-01,  8.7891e-01,  4.3723e-01,  3.7107e-01,\n",
       "        5.7137e-01,  6.8381e-01,  1.0813e+00, -4.7799e-02,  9.1588e-02,\n",
       "        1.1153e-01,  8.4817e-01,  8.6699e-01, -6.7174e-02, -6.0056e-01,\n",
       "       -7.6134e-01, -5.5208e-01,  3.0215e-01,  9.9575e-03, -4.6225e-01,\n",
       "        8.3512e-01, -1.0748e-02, -7.9808e-01, -5.0147e-01,  8.3243e-01,\n",
       "       -3.1315e-01, -8.3514e-02, -7.0079e-01, -5.6033e-01, -6.4789e-02,\n",
       "       -6.9048e-01, -8.8293e-01, -7.4071e-01,  2.6856e-02,  5.8349e-01,\n",
       "       -4.7795e-02, -4.4563e-01, -1.7875e-01, -6.0016e-01,  7.2793e-02,\n",
       "        1.5451e-01,  2.6200e-01, -4.7335e-01, -1.4017e-01,  4.0792e-01,\n",
       "       -1.0640e+00,  3.9888e-01, -5.1001e-01,  3.9962e-01, -8.5021e-01,\n",
       "        1.7305e-01,  2.8913e-01, -1.3508e-01,  9.3687e-01,  6.0144e-01,\n",
       "       -6.0228e-01,  7.9056e-01,  3.3801e-01, -5.7203e-02, -1.0779e+00,\n",
       "        8.2638e-01, -2.9985e-01, -2.3117e-01, -1.0227e+00,  5.4373e-02,\n",
       "       -8.8121e-01, -3.8100e-01,  3.6903e-01, -7.1654e-01, -8.2499e-02,\n",
       "        1.1802e+00,  3.9736e-01, -1.0854e+00, -2.0758e-01, -2.2772e-01,\n",
       "        6.9006e-01, -1.0682e+00,  7.2935e-01,  1.2665e-03, -2.4493e-01,\n",
       "       -2.4806e-02, -4.5564e-01, -2.3917e-01, -9.4934e-01,  1.1925e-01,\n",
       "       -4.0391e-01,  3.7711e-02, -8.3502e-01,  1.5569e-01, -4.1158e-01,\n",
       "       -6.2988e-02,  5.4909e-01,  7.5530e-01, -1.5463e+00, -3.9758e-02,\n",
       "        5.7422e-01, -4.0611e-01, -2.9411e-01, -1.6940e-01, -2.7391e-01,\n",
       "       -2.0142e-01, -8.7102e-01, -9.2617e-01, -1.8897e+00, -1.2419e+00,\n",
       "       -1.3943e-01,  4.7293e-01,  1.1383e+00, -6.6022e-01, -1.3899e-02,\n",
       "       -2.7761e-01,  1.7229e-02,  1.0169e+00, -1.1558e-01, -6.5791e-01,\n",
       "        6.1787e-01,  1.2840e+00,  1.8424e-01,  8.7510e-01,  3.0483e-02,\n",
       "       -2.2935e-01,  3.6822e-01,  4.3399e-01, -8.0390e-01, -2.0275e-02,\n",
       "       -7.2834e-01, -1.1894e+00, -7.6395e-01, -6.9246e-01, -1.1771e-03,\n",
       "       -8.1935e-01,  5.1514e-01,  6.5656e-02,  1.0131e+00,  3.2171e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['word2vec']['的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([16, 20])\n",
      "tensor([[   0,    5,    3,  171,    3,  409,    3,  184,  928, 1316,  929,  361,\n",
      "           18, 2102,    4,  327,    1,    2,    2,    2],\n",
      "        [   0,    5,    4,  199,    7,   70,   13,    4, 2103,  200, 1317, 1318,\n",
      "          612, 2104,  521,    1,    2,    2,    2,    2],\n",
      "        [   0,   13,    3,    3,    3, 2105,   15, 1319,    3,   28,  522,  523,\n",
      "          233,    4,  521,   25,  261,   19,  201,   13],\n",
      "        [   0,    3,    3,  234,    8,  199, 2106,  216,   48,    1,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,    5,  137,   53,   83,  287,    3,   29,   45,    5,   23,  930,\n",
      "          199,   71,   71,    3,  733,    1,    2,    2],\n",
      "        [   0,   11,   49,   40,   57,   72,  288,    4,  159, 2107,    8,   13,\n",
      "           43,    1,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,    5,  461,  160,  931,  148,  928,    1,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,   12,  288,    4,  159,    1,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,    5,    3,  171,    3,    3,    3, 2108,  138,   63, 2109,  217,\n",
      "          734,    9, 1320,    4,    1,    2,    2,    2],\n",
      "        [   0,   26,    7,   83,  121,    3,  100,    5, 2110, 2111, 2112,   48,\n",
      "            5, 1321,   57,   12,  362,  159, 1321,  218],\n",
      "        [   0,   15,    9,    6, 2114,   24,  524,    4,    3,  185,  289,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,    6,  735,   44,  234,  122,    4, 2115,   61,  186,   38,  149,\n",
      "           19,  932,    6,   44,   57, 1324,    1,    2],\n",
      "        [   0,    6,   33, 2116,  525, 2117, 2118,    3,   61,  410,   16,   19,\n",
      "          262,    6,  172,   44,   57, 2119,    1,    2],\n",
      "        [   0,  159,    7,   12, 1325,    3,    3,    6,    3,  191,   24, 2120,\n",
      "            7,   12,  200, 2121,    3,    1,    2,    2],\n",
      "        [   0, 2122,   34,  933,   32, 2123,  202,    3,    1,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,    6,   25,   53,   35,  328,  934,   48,  411,    4,  329, 1326,\n",
      "          363,  150,  935,   25,   34,  330,    1,    2]])\n",
      "tensor([17, 16, 20, 10, 18, 14,  8,  6, 17, 20, 12, 19, 19, 18,  9, 19])\n",
      "torch.Size([16, 20])\n",
      "tensor([[   0,   50,    9,   20,    3,    4,    9,  209, 1653,   68,   34,  387,\n",
      "            8,    6,  519,   11,  780,   12,   19,  256],\n",
      "        [   0,   19,  217,   20, 1070,    8, 1654,  781,   27,   67,  192,    4,\n",
      "         1071, 1072,    5,    1,    2,    2,    2,    2],\n",
      "        [   0,   54,   20,   10,  181, 1655,   27,   67,  241,   86,   20, 1656,\n",
      "           87,    4,   48,    6,  781,  782, 1657,  111],\n",
      "        [   0,    3,    6,  436,   28,  631,  437,    3,   19,  217,    3,    5,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,    9,  133,    3,   84,   32,   16,  632,    4,   24,    9,   82,\n",
      "           55,   13,   19,  217,   20,   52,    4,   52],\n",
      "        [   0,    3,   15,   26,   97,    8,   10,  300,  122,   60,    4,    3,\n",
      "           30,  112,    5,    1,    2,    2,    2,    2],\n",
      "        [   0,   10,  387,   13,    9,   63,  115,  633,    5,    1,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,   10,  300,  122,    5,    1,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,   15,   55,    4,    9,   20,  242,   50,    6,  436,  193,   98,\n",
      "          194,    7,  128,   16,  438,   23,  218,    8],\n",
      "        [   0,   22,   23,    6,  169,  276,   75,    4,    9, 1073,   35,   10,\n",
      "          331,    8, 1658,   19,  634,  783,    4,   51],\n",
      "        [   0,   16,   20,    6,   99,  126,   14,  439,   82,   39,  784,    5,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,  333,  103,    4,   14,  193,   10,  170, 1660,   22,   13,   85,\n",
      "           34,   76, 1661,   73,   14,   45,   78,    5],\n",
      "        [   0,   14,   76, 1662,   44, 1075,   12, 1663, 1076,   22,   16,   76,\n",
      "          785,   14,   45,   57,   69, 1664,    5,    1],\n",
      "        [   0,    6,  122,   20,   12,   10,  256,    4,   71,  108,    3,   11,\n",
      "           77, 1665,   12,   34,  192,  440,  441,    5],\n",
      "        [   0,   16,   20, 1666,   12, 1667,   24, 1077,  786,   12, 1668,    5,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [   0,   14,   47,  334,   14,   45,    3,   44,  195,    3,    6,  787,\n",
      "            4,    6,  390,    7,   44,  788,    5,    1]])\n",
      "tensor([20, 16, 20, 13, 20, 16, 10,  6, 20, 20, 13, 20, 20, 20, 13, 20])\n"
     ]
    }
   ],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    print(targ_idxs.size())\n",
    "    print(targ_idxs)\n",
    "    print(targ_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, pretrained_word2vec):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "#        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "    \n",
    "    def forward(self, input, src_lens):\n",
    "        batch_size = input.size()[0]\n",
    "        _, idx_sort = torch.sort(src_lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        x, x_lengths = input.index_select(0, idx_sort), src_lens.index_select(0, idx_sort)\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, x_lengths, batch_first=True)\n",
    "        hidden = self.initHidden(batch_size).to(device)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        final_hidden = torch.cat([output[:, -1, :self.hidden_size], output[:, 0, self.hidden_size:]], dim=1).unsqueeze(0) \n",
    "#        print(\"final hidden size is: {}\".format(final_hidden.size()))\n",
    "        return output, final_hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, pretrained_word2vec):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_size = output_size\n",
    "#        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size*2)\n",
    "        self.out = nn.Linear(hidden_size*2, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         print(\"Before embedding, targ[i-1] size is: {}\".format(input.size()))\n",
    "#         print(\"Before embedding, targ[i-1] is: {}\".format(input))     \n",
    "        batch_size = input.size()[0]\n",
    "        output = self.embedding(input).view(1, batch_size, -1)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "        output = F.relu(output)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "#         print(\"hidden size is: {}\".format(hidden.size()))        \n",
    "#         print(\"hidden is: {}\".format(hidden))        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "#        print(output.size())\n",
    "#         print(\"Original output is: {}\".format(output.size()))\n",
    "#         print(\"output[0] is: {}\".format(output[0].size()))\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "#        print(\"LogSoftMax output is: {}\".format(output.size()))\n",
    "#         output = output.squeeze(0) # B x N\n",
    "#         output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_size = self.decoder.output_size\n",
    "        \n",
    "    def forward(self, src, targ, src_lens, targ_lens): \n",
    "        batch_size = src.size()[0]\n",
    "#        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_output, encoder_hidden = self.encoder(src, src_lens)\n",
    "        decoder_hidden = encoder_hidden \n",
    "#        print(\"Encoder Hidden size: {}\".format(encoder_hidden.size()))\n",
    "        final_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size, self.output_size))\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "#            print(\"targ[di-1] Size is {}\".format(targ[:, di-1].size()))\n",
    "#             print(\"targ[:, di-1] size is {}\".format(targ[:, di-1].size()))\n",
    "#             print(\"decoder_hidden size is {}\".format(decoder_hidden.size()))\n",
    "            decoder_outputs, decoder_hidden = self.decoder(targ[:, di-1], decoder_hidden)\n",
    "#            print(\"Final Outputs {} is {}\".format(di, decoder_outputs))\n",
    "            top1 = decoder_outputs.data.max(1)[1]\n",
    "#            print(\"At position {}, Top 1 is {}\".format(di, top1))\n",
    "            final_outputs[di] = decoder_outputs\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_outputs(predictions): \n",
    "#     \"\"\" Given a prediction output tensor representing the log probabilities of each word in a sentence, \n",
    "#         reduce to a tensor representing a sentence compromising the most likely words in each position \n",
    "#     \"\"\"\n",
    "#     reduced = predictions.data.max(dim=2)[1]\n",
    "#     return reduced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tensors2sentences(tensors, id2token): \n",
    "# #    print(type(tensors))\n",
    "# #    print(tensors.size())\n",
    "# #    print(type(tensors.numpy()))\n",
    "#     tokens = [id2token[idx] for idx in tensors.numpy()]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_bleu(outputs, targets, id2token): \n",
    "#     print(\"original outputs shape: {}\".format(outputs.size()))\n",
    "#     print(\"original targets  shape: {}\".format(targets.size()))\n",
    "#     outputs = reduce_outputs(outputs)\n",
    "# #     print(\"reduced outputs shape: {}\".format(outputs.size()))\n",
    "# #     print(\"target outputs shape: {}\".format(targets.size()))\n",
    "#     references = tensors2sentences(targets, id2token)\n",
    "#     hypotheses = tensors2sentences(outputs, id2token)\n",
    "# #     print(\"hypotheses are: {}\".format(hypotheses))\n",
    "# #     print(\"references are: {}\".format(references))\n",
    "#     bleu = sentence_bleu(references, hypotheses)\n",
    "#     return bleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with packing  \n",
    "\n",
    "def evaluate(model, loader, id2token): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "#     reference_corpus = []\n",
    "#     hypothesis_corpus = [] \n",
    "    \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader): \n",
    "        batch_size = src_idxs.size()[0]\n",
    "        outputs = model(src_idxs, targ_idxs, src_lens, targ_lens)\n",
    "#         if i == 0: \n",
    "#             print(outputs[0:TARG_MAX_SENTENCE_LEN, :].size())\n",
    "#             print(targets[0:TARG_MAX_SENTENCE_LEN].size())\n",
    "        \n",
    "#         outputs = model(src_idxs, targ_idxs).view(batch_size, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN)\n",
    "#         targets = targ_idxs.view(batch_size, TARG_MAX_SENTENCE_LEN)\n",
    "#         print(\"outputs size: {}\".format(outputs.size()))\n",
    "#         print(\"outputs: {}\".format(outputs))\n",
    "#        print(\"1st data point outputs size: {}\".format(outputs[1].size()))\n",
    "#        print(\"1st data point outputs: {}\".format(outputs[1]))\n",
    "#         print(\"targets size: {}\".format(targets.size()))\n",
    "#         print(\"targets: {}\".format(targets))\n",
    "#        print(\"1st data point targets size: {}\".format(targets[1].size()))\n",
    "#        print(\"1st data point targets: {}\".format(targets[1]))\n",
    "        loss = criterion(outputs.view(-1, TARG_VOCAB_SIZE), targ_idxs.view(-1))        \n",
    "#         final_outputs = model(src_idxs, targ_idxs) \n",
    "#         loss = criterion(final_outputs.view(input_len, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN), \n",
    "#                          targ_idxs.view(input_len, TARG_MAX_SENTENCE_LEN))\n",
    "        total_loss += loss.item()  \n",
    "#         reference_corpus = targ_idxs\n",
    "#         output_corpus = outputs.data.max(dim=2)[1].transpose(0,1)\n",
    "#         print(type(reference_corpus))\n",
    "#         bleu_score = sacrebleu.corpus_bleu(reference_corpus, reference_corpus)\n",
    "#         print(\"BLEU score: {}\".format(bleu_score))\n",
    "#         print(\"Size of reference corpus: {}\".format(reference_corpus.size()))\n",
    "#         print(\"Size of output corpus: {}\".format(output_corpus.size()))\n",
    "#        bleu = calculate_bleu(outputs.transpose(0,1), targ_idxs, id2token)\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional with packing sequence\n",
    "\n",
    "def train(model, train_loader, dev_loader, id2token, num_epochs=3, learning_rate=0.1, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs = model(src_idxs, targ_idxs, src_lens, targ_lens) \n",
    "            loss = criterion(final_outputs.view(-1, TARG_VOCAB_SIZE), targ_idxs.view(-1))\n",
    "#             loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN), \n",
    "#                              targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 10 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "                result['train_loss'] = evaluate(model, train_loader, id2token) \n",
    "                result['val_loss'] = evaluate(model, dev_loader, id2token)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Loss: {:.2f}, Validation Loss: {:.2f}'.format(\n",
    "                        result['epoch'], result['train_loss'], result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'时候'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['id2token'][86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 9.52, Validation Loss: 8.32\n",
      "Epoch: 0.13, Train Loss: 79.50, Validation Loss: 72.30\n",
      "Epoch: 0.25, Train Loss: 148.79, Validation Loss: 123.37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-af5bb8d17fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                             data['train']['target']['token2id']))\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id2token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-3886b7c44a5c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader, id2token, num_epochs, learning_rate, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "                                                            data['train']['source']['token2id']))\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                            data['train']['target']['token2id']))\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train(model, train_loader, dev_loader, data['train']['target']['id2token']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=10)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=10)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(\"Targ shape is {}\".format(targ_idxs.size()))\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)    \n",
    "    final_outputs = model(src_idxs, targ_idxs)\n",
    "    print(final_outputs.size())\n",
    "    print(final_outputs)\n",
    "    print(targ_idxs.size())\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(input_size=10000, hidden_size=10)\n",
    "# decoder = DecoderRNN(hidden_size=10, output_size=10000)\n",
    "# encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     output, hidden = encoder(src_idxs, encoder_hidden)\n",
    "#     print(\"Output:::\")\n",
    "#     print(output.size())\n",
    "#     print(output)\n",
    "#     print(\"Hidden:::\")\n",
    "#     print(hidden.size())\n",
    "#     print(hidden)\n",
    "#     dec_output, dec_hidden = decoder(targ_idxs, hidden) \n",
    "#     print(\"Decoder Output:::\")\n",
    "#     print(dec_output.size())\n",
    "#     print(dec_output)    \n",
    "#     print(\"Decoder Hidden:::\")\n",
    "#     print(dec_hidden.size())\n",
    "#     print(dec_hidden)\n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
