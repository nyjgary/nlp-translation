{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "import sacrebleu\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle as pkl\n",
    "import string\n",
    "import os\n",
    "from os import listdir \n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "\n",
    "pd.set_option('max_colwidth',100)\n",
    "mpl.style.use('bmh')\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp, lang_type): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]\n",
    "        if lang_type == 'source': \n",
    "            tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "        elif lang_type == 'target': \n",
    "            tokens_data = [['<SOS'] + datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(num_vocab))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + list(vocab)\n",
    "    token2id = dict(zip(id2token, range(max_vocab_size)))\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "\n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # attach vocab sizes and word2vec models \n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size \n",
    "    data['train']['source']['word2vec'] = load_word2vec(src_lang) \n",
    "    data['train']['target']['word2vec'] = load_word2vec(targ_lang) \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'], lang_type)\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    token_lists = data['train'][lang_type]['tokens'], \n",
    "                    max_vocab_size = data['train'][lang_type]['max_vocab_size'], \n",
    "                    word2vec = data['train'][lang_type]['word2vec']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = 1000\n",
    "TARG_VOCAB_SIZE = 1000\n",
    "ENC_EMBED_DIM = 300 \n",
    "DEC_EMBED_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data(SRC_LANG, TARG_LANG, src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Source: ['你', '现在', '可以', '去', '个', '真正', '的', '学校', '念书', '了', '他', '说', '<EOS>']\n",
      "Example Target: ['<SOS', '&quot;', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '&quot;', 'he', 'said', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# print example for sanity check  \n",
    "print(\"Example Source: {}\".format(data['train']['source']['tokens'][5]))\n",
    "print(\"Example Target: {}\".format(data['train']['target']['tokens'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEcRJREFUeJzt3W+MXFd5x/HvU4dA8BY7IWQxdtQ1wkpBsfjjVZQ2LdpNoDUJwn6RIFBEDTXaF6UpbVOBaaVWlVrJqcqfVEWoVgIxFWWThqS2QkQVGW+jSpBikwgHTJoQrGDH2FBsw0aoweXpi7lOp2Zn587uzM7sme9HWu3cu2fmnsdn/duzZ++9E5mJJGn5+6V+d0CS1B0GuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQF9RpFBGrgTuAK4EEfhd4ArgbGAOOAO/MzFPzvc6ll16aY2Nj8x7rueeeY+XKlXW6VQxrHg7DVvOw1Qu9q/ngwYM/zMxXtG2YmW0/gN3A+6vHFwKrgb8BdlT7dgC3tXudTZs2ZTv79+9v26Y01jwchq3mYas3s3c1AweyRla3XXKJiJcBbwburH4APJ+Zp4EtVdCfC/ytHf3IkSR1VZ019FcDPwA+ExGPRsQdEbESGM3M4wDV58t62E9JUhuRbe62GBHjwFeBazLzkYi4HfgxcEtmrm5qdyozL57j+VPAFMDo6Oim6enpeY83OzvLyMhIx4UsZ9Y8HIat5mGrF3pX8+Tk5MHMHG/bsN2aDPBK4EjT9m8CX6TxR9E11b41wBPtXss19LlZ83AYtpqHrd7MZbCGnpnfB74XEVdUu64DvgXsBbZV+7YBe+r/vJEkdVut0xaBW4DPRcSFwNPA+2isv98TEduBZ4CbetNFSVIdtQI9Mx8D5lq/ua673ZEkLZRXikpSIQx0SSpE3TV0VcZ2fPGFx0d23tDHnkjS/+cMXZIKYaBLUiFccpmDyyqSliNn6JJUCANdkgphoEtSIVxDb6N5PV2SBpkzdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIT1vsMW8jIGmpOEOXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVIhal/5HxBHgJ8D/AGczczwiLgHuBsaAI8A7M/NUb7rZPa0uxfediSQtd53M0Ccz8w2ZOV5t7wD2ZeYGYF+1LUnqk8UsuWwBdlePdwNbF98dSdJCRWa2bxTxXeAUkMA/ZOauiDidmaub2pzKzIvneO4UMAUwOjq6aXp6et5jzc7OMjIy0lkVHTh07MwLjzeuXTXn/rqan9/p8Zr1uuZBZM3lG7Z6oXc1T05OHmxaHWmpbqC/KjOfjYjLgIeAW4C9dQK92fj4eB44cGDeY83MzDAxMdG2TwvVzTX0OrfDrXP73F7XPIisuXzDVi/0ruaIqBXotZZcMvPZ6vNJ4H7gKuBERKypDrYGOLnw7kqSFqttoEfEyoj45XOPgd8CHgf2AtuqZtuAPb3qpCSpvTqnLY4C90fEufb/lJlfioivAfdExHbgGeCm3nVzMPluRJIGSdtAz8yngdfPsf+/gOt60SlJUue8UlSSCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgpR6x2LlrtWd1L0XYoklcQZuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiGG4m6LS6H5zo1Hdt7Qx55IGlbO0CWpEAa6JBWidqBHxIqIeDQiHqi210fEIxHxZETcHREX9q6bkqR2OpmhfxA43LR9G/DxzNwAnAK2d7NjkqTO1Ar0iFgH3ADcUW0HcC1wb9VkN7C1Fx2UJNVTd4b+CeBDwM+r7ZcDpzPzbLV9FFjb5b5JkjoQmTl/g4i3A9dn5u9FxATwJ8D7gK9k5muqNpcDD2bmxjmePwVMAYyOjm6anp6e93izs7OMjIwsoJTWDh0709XXa2fj2lVzHrvV/vWrVnS95kHXi3EedMNW87DVC72reXJy8mBmjrdrV+c89GuAd0TE9cBLgJfRmLGvjogLqln6OuDZuZ6cmbuAXQDj4+M5MTEx78FmZmZo16ZT7206R3wpHLl5Ys5jt9p/1+aVXa950PVinAfdsNU8bPVC/2tuu+SSmR/JzHWZOQa8C/hyZt4M7AdurJptA/b0rJeSpLYWcx76h4E/joinaKyp39mdLkmSFqKjS/8zcwaYqR4/DVzV/S5JkhbCK0UlqRAGuiQVwrst9sBYi7NqWu2XpG5whi5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIK4WmLy4RvQi2pHWfoklQIA12SCmGgS1Ihil1D9zJ7ScPGGbokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1Ih2t5tMSJeAjwMvLhqf29m/kVErAemgUuArwPvyczne9nZUh06dob3znF3SN+ZSFIn6szQ/xu4NjNfD7wB2BwRVwO3AR/PzA3AKWB777opSWqnbaBnw2y1+aLqI4FrgXur/buBrT3poSSpllpr6BGxIiIeA04CDwHfAU5n5tmqyVFgbW+6KEmqIzKzfuOI1cD9wJ8Dn8nM11T7LwcezMyNczxnCpgCGB0d3TQ9PT3vMWZnZxkZGandp1YOHTuz6NdYKqMXwYmf/uL+jWtXvfC4uZ7m/a102n6pdWucl5Nhq3nY6oXe1Tw5OXkwM8fbtevoLegy83REzABXA6sj4oJqlr4OeLbFc3YBuwDGx8dzYmJi3mPMzMzQrk0dc/2RcVDduvEsHz30i0Nx5OaJFx4319O8v5VO2y+1bo3zcjJsNQ9bvdD/mtsuuUTEK6qZORFxEfAW4DCwH7ixarYN2NOrTkqS2qszQ18D7I6IFTR+ANyTmQ9ExLeA6Yj4K+BR4M4e9nMo+UbXkjrRNtAz8xvAG+fY/zRwVS86JUnqnFeKSlIhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFaKjN7jQYGi+re6RnTf0sSeSBokzdEkqhIEuSYVwyaUgrd7hyCUaaTg4Q5ekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmF8LTFIeMpjFK5nKFLUiEMdEkqhIEuSYVou4YeEZcDnwVeCfwc2JWZt0fEJcDdwBhwBHhnZp7qXVc1l1aX+3f6XNfTpeWvzgz9LHBrZr4WuBr4QES8DtgB7MvMDcC+aluS1CdtAz0zj2fm16vHPwEOA2uBLcDuqtluYGuvOilJai8ys37jiDHgYeBK4JnMXN30tVOZefEcz5kCpgBGR0c3TU9Pz3uM2dlZRkZGavep2aFjZxb0vH4bvQhO/LS/fdi4dtWSHm8x47xcDVvNw1Yv9K7mycnJg5k53q5d7UCPiBHg34C/zsz7IuJ0nUBvNj4+ngcOHJj3ODMzM0xMTNTq0/kWs57cT7duPMtHD/X3koClXkNfzDgvV8NW87DVC72rOSJqBXqts1wi4kXAF4DPZeZ91e4TEbGm+voa4ORCOytJWry2gR4RAdwJHM7MjzV9aS+wrXq8DdjT/e5Jkuqq83v+NcB7gEMR8Vi170+BncA9EbEdeAa4qTddlCTV0TbQM/PfgWjx5eu62x1J0kJ5pagkFcJAl6RCePtcAZ3fBuD8U0S9dYDUf87QJakQBrokFcIlF3WFd26U+s8ZuiQVwkCXpEIY6JJUiGW/hr5c77A4yFqth/tvLQ02Z+iSVAgDXZIKsSyXXPzVX5J+kTN0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYVYlpf+a+l08zYLza911+aVXXtdSQ3O0CWpEAa6JBWibaBHxKcj4mREPN6075KIeCginqw+X9zbbkqS2qmzhn4X8PfAZ5v27QD2ZebOiNhRbX+4+93TctTqHY8G7TWl0rSdoWfmw8CPztu9BdhdPd4NbO1yvyRJHVroGvpoZh4HqD5f1r0uSZIWIjKzfaOIMeCBzLyy2j6dmaubvn4qM+dcR4+IKWAKYHR0dNP09PS8x5qdnWVkZGTeNoeOnWnb5+Vk9CI48dN+92JprV+14oVxbh7PjWtXzdm+TptBV+d7uyTDVi/0rubJycmDmTnert1Cz0M/ERFrMvN4RKwBTrZqmJm7gF0A4+PjOTExMe8Lz8zM0K7Newt7C7pbN57lo4eG65KAuzavfGGcm8fzyM0Tc7av02bQ1fneLsmw1Qv9r3mhSy57gW3V423Anu50R5K0UHVOW/w88BXgiog4GhHbgZ3AWyPiSeCt1bYkqY/a/p6fme9u8aXrutwXSdIieKWoJBXCQJekQgzXqRUaeF4RKi2cM3RJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCE9bVF8cOnZmwTdZ69epjZ5SqUHnDF2SCmGgS1IhDHRJKsSyWUMfK+xNLdTeYsa81Xq36+D/x3+L8jhDl6RCGOiSVAgDXZIKsWzW0KW5dGsdeCnWk5vPvXfNWr3gDF2SCmGgS1IhXHJR8To9/bHO8kud0yLne/5ijr2Y9iqbM3RJKoSBLkmFMNAlqRCuoasYi1kr79Zrzvf8Wzcu6qUWfFwND2foklQIA12SCrGoJZeI2AzcDqwA7sjMnV3plVS4QVwe6obmPty1eWXXXqvZcjk9sx+nlC54hh4RK4BPAm8DXge8OyJe162OSZI6s5gll6uApzLz6cx8HpgGtnSnW5KkTi0m0NcC32vaPlrtkyT1QWTmwp4YcRPw25n5/mr7PcBVmXnLee2mgKlq8wrgiTYvfSnwwwV1avmy5uEwbDUPW73Qu5p/JTNf0a7RYv4oehS4vGl7HfDs+Y0ycxewq+6LRsSBzBxfRL+WHWseDsNW87DVC/2veTFLLl8DNkTE+oi4EHgXsLc73ZIkdWrBM/TMPBsRvw/8K43TFj+dmd/sWs8kSR1Z1Hnomfkg8GCX+nJO7eWZgljzcBi2moetXuhzzQv+o6gkabB46b8kFWJgAj0iNkfEExHxVETs6Hd/eiEiLo+I/RFxOCK+GREfrPZfEhEPRcST1eeL+93XbouIFRHxaEQ8UG2vj4hHqprvrv6wXoyIWB0R90bEt6vx/rXSxzki/qj6vn48Ij4fES8pbZwj4tMRcTIiHm/aN+e4RsPfVZn2jYh4U6/7NxCBPkS3ETgL3JqZrwWuBj5Q1bkD2JeZG4B91XZpPggcbtq+Dfh4VfMpYHtfetU7twNfysxfBV5Po/Zixzki1gJ/AIxn5pU0TpR4F+WN813A5vP2tRrXtwEbqo8p4FO97txABDpDchuBzDyemV+vHv+Exn/ytTRq3V012w1s7U8PeyMi1gE3AHdU2wFcC9xbNSmq5oh4GfBm4E6AzHw+M09T+DjTOMniooi4AHgpcJzCxjkzHwZ+dN7uVuO6BfhsNnwVWB0Ra3rZv0EJ9KG7jUBEjAFvBB4BRjPzODRCH7isfz3riU8AHwJ+Xm2/HDidmWer7dLG+9XAD4DPVMtMd0TESgoe58w8Bvwt8AyNID8DHKTscT6n1bguea4NSqDHHPuKPf0mIkaALwB/mJk/7nd/eiki3g6czMyDzbvnaFrSeF8AvAn4VGa+EXiOgpZX5lKtG28B1gOvAlbSWHI4X0nj3M6Sf58PSqDXuo1ACSLiRTTC/HOZeV+1+8S5X8Wqzyf71b8euAZ4R0QcobGUdi2NGfvq6ldzKG+8jwJHM/ORavteGgFf8ji/BfhuZv4gM38G3Af8OmWP8zmtxnXJc21QAn0obiNQrR3fCRzOzI81fWkvsK16vA3Ys9R965XM/EhmrsvMMRrj+uXMvBnYD9xYNSut5u8D34uIK6pd1wHfouBxprHUcnVEvLT6Pj9Xc7Hj3KTVuO4Ffqc62+Vq4My5pZmeycyB+ACuB/4T+A7wZ/3uT49q/A0av3J9A3is+riexpryPuDJ6vMl/e5rj+qfAB6oHr8a+A/gKeCfgRf3u39drvUNwIFqrP8FuLj0cQb+Evg28Djwj8CLSxtn4PM0/kbwMxoz8O2txpXGkssnq0w7ROMMoJ72zytFJakQg7LkIklaJANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RC/C+JWDx9TtrT5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of source sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEetJREFUeJzt3V+MXGd5x/HvQwyBZEvsEFi5dtQNwgrQuECyigKp0G6CRP6gJBdEDbLAoUa+oZBSV8QpF1GlVhiVAkGiVFZCYyrEAiZtrCRAI5Mt4iJuvYDigEnjBjexY2IQsWFD1WD16cUcL1N7196dM+OZOe/3I612zv/30bv67bvvnDkbmYkkqfle0u8GSJLODANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVIhl/W4AwAUXXJBjY2Nzyy+88ALnnntu/xrUQ02tzbqGT1Nra2pdcHJtMzMzP8/MVy/2+IEI/LGxMXbv3j23PD09zcTERP8a1ENNrc26hk9Ta2tqXXBybRHxX0s53ikdSSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqxEB80nbQjW1+cO71/i3X97ElktQ5R/iSVIjTBn5EfCEiDkfE423rzo+IhyPiyer7imp9RMRnI2JfRDwWEZf2svGSpMVbzAj/XuCaE9ZtBnZm5hpgZ7UMcC2wpvraCHy+O82UJNV12sDPzO8Avzhh9Y3Atur1NuCmtvVfzJZHgeURsbJbjZUkda7TOfzRzDwEUH1/TbV+FfBM234HqnWSpD6LzDz9ThFjwAOZeUm1fCQzl7dtfz4zV0TEg8DHM/O71fqdwEczc2aec26kNe3D6OjoZVNTU3PbZmdnGRkZqVNXV+05eHTu9dpV59U616DV1i3WNXyaWltT64KTa5ucnJzJzPHFHt/pbZnPRcTKzDxUTdkcrtYfAC5s22818Ox8J8jMrcBWgPHx8Wx/qP+g/QODW9tvy1w3Uetcg1Zbt1jX8GlqbU2tC+rX1umUzg5gffV6PXB/2/r3VXfrXAEcPT71I0nqr9OO8CPiy8AEcEFEHADuBLYAX42IDcDTwM3V7g8B1wH7gF8D7+9BmyVJHTht4GfmexbYdPU8+ybwwbqNkiR1n5+0laRCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEMvqHBwRHwE+ACSwB3g/sBKYAs4Hvge8NzNfrNnORhvb/ODc6/1bru9jSyQ1Wccj/IhYBXwYGM/MS4CzgFuATwCfzsw1wPPAhm40VJJUT90pnWXAKyJiGXAOcAi4Cthebd8G3FTzGpKkLug48DPzIPBJ4GlaQX8UmAGOZOaxarcDwKq6jZQk1ReZ2dmBESuArwN/BBwBvlYt35mZr6v2uRB4KDPXznP8RmAjwOjo6GVTU1Nz22ZnZxkZGemoXb2w5+DRuddrV51X61zz1dbN8/fLoPVZtzS1LmhubU2tC06ubXJyciYzxxd7fJ03bd8B/CQzfwYQEfcBbwOWR8SyapS/Gnh2voMzcyuwFWB8fDwnJibmtk1PT9O+3G+3tr+pum6i1rnmq62b5++XQeuzbmlqXdDc2ppaF9Svrc4c/tPAFRFxTkQEcDXwI+AR4N3VPuuB+2tcQ5LUJXXm8HfRenP2e7RuyXwJrRH77cCfRcQ+4FXAPV1opySpplr34WfmncCdJ6x+Cri8znklSd3nJ20lqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIs63cDBsnY5gfnXu/fcn0fWyJJ3ecIX5IKYeBLUiEMfEkqhIEvSYWoFfgRsTwitkfEjyNib0S8NSLOj4iHI+LJ6vuKbjVWktS5uiP8u4BvZubrgTcBe4HNwM7MXAPsrJYlSX3WceBHxCuBtwP3AGTmi5l5BLgR2Fbttg24qW4jJUn1RWZ2dmDEm4GtwI9oje5ngNuAg5m5vG2/5zPzpGmdiNgIbAQYHR29bGpqam7b7OwsIyMjHbWrjj0Hj869XrvqvNOu78R8tXXz/P3Srz7rtabWBc2tral1wcm1TU5OzmTm+GKPrxP448CjwJWZuSsi7gJ+CXxoMYHfbnx8PHfv3j23PD09zcTEREftqmOhD1518wNZ89XWhA989avPeq2pdUFza2tqXXBybRGxpMCvM4d/ADiQmbuq5e3ApcBzEbGyasxK4HCNa0iSuqTjwM/MnwLPRMTF1aqraU3v7ADWV+vWA/fXaqEkqSvqPkvnQ8CXIuJlwFPA+2n9EvlqRGwAngZurnkNSVIX1Ar8zPwBMN/80dV1zitJ6j4/aStJhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRB1/6etKmObH5x7vX/L9X1siSTNzxG+JBWiuBF++0hckkriCF+SCmHgS1IhDHxJKoSBL0mFMPAlqRDF3aWzWN7NI6lpHOFLUiEc4dfgXwGShokjfEkqhCP8HvMZO5IGRe0RfkScFRHfj4gHquWLImJXRDwZEV+JiJfVb6Ykqa5uTOncBuxtW/4E8OnMXAM8D2zowjUkSTXVCvyIWA1cD9xdLQdwFbC92mUbcFOda0iSuiMys/ODI7YDHwd+B/hz4Fbg0cx8XbX9QuAbmXnJPMduBDYCjI6OXjY1NTW3bXZ2lpGRkY7bdSp7Dh6tdfzaVeed9lwL7bN21Xnz1nbiPsOol33WT02tC5pbW1PrgpNrm5ycnMnM8cUe3/GbthHxLuBwZs5ExMTx1fPsOu9vlMzcCmwFGB8fz4mJiblt09PTtC930601b6Xcv27itOdaaJ/96ybmre3EfYZRL/usn5paFzS3tqbWBfVrq3OXzpXADRFxHfBy4JXAZ4DlEbEsM48Bq4Fna1yjsRa6h9+7eiT1Ssdz+Jl5R2auzswx4Bbg25m5DngEeHe123rg/tqtlCTV1ov78G8HpiLir4DvA/f04BoD7VSj901rj9WeVpKkTnQl8DNzGpiuXj8FXN6N80qSusdP2hbG9wikcvksHUkqhIEvSYUw8CWpEAa+JBXCwJekQniXzhL5X64kDStH+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFaKx9+H7VMjf8rMDksARviQVo7Ej/HYljnBLrFnSqTnCl6RCGPiSVIgipnSazDenJS2WI3xJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgrhffgDbKF77H1sgqROOMKXpEIY+JJUCANfkgrRceBHxIUR8UhE7I2IH0bEbdX68yPi4Yh4svq+onvNlSR1qs4I/xiwKTPfAFwBfDAi3ghsBnZm5hpgZ7UsSeqzju/SycxDwKHq9a8iYi+wCrgRmKh22wZMA7fXauUp+LRISVqcrszhR8QY8BZgFzBa/TI4/kvhNd24hiSpnsjMeieIGAH+FfjrzLwvIo5k5vK27c9n5knz+BGxEdgIMDo6etnU1NTcttnZWUZGRhZ1/T0Hj869XrvqvHnXD5LRV8Bz/7304xZT21LrP9X+7dsWYyl9NkyaWhc0t7am1gUn1zY5OTmTmeOLPb5W4EfES4EHgG9l5qeqdU8AE5l5KCJWAtOZefGpzjM+Pp67d++eW56enmZiYmJRbRi2DydtWnuMv92z9Jm0xdS21PpPtf9Sp8eW0mfDpKl1QXNra2pdcHJtEbGkwO94Dj8iArgH2Hs87Cs7gPXAlur7/Z1eQ791pn+B+d6I1Dx1Hq1wJfBeYE9E/KBa9xe0gv6rEbEBeBq4uV4TJUndUOcune8CscDmqzs9rzo3qNNYkgaDn7SVpEIY+JJUCANfkgph4EtSIQx8SSqE//FKXec9/NJgcoQvSYVwhK/TWsz9/ZvWHuNWPwcgDTRH+JJUCEf4Gjq+RyB1xhG+JBXCEb56ytG4NDgc4UtSIRzhF8xn7EtlcYQvSYUw8CWpEAa+JBWiUXP4/scnSVqYI3xJKkSjRvhqLv96k+pzhC9JhXCEr77z/nzpzHCEL0mFcISvvljMnHydeftu/dWw5+DR//ecf/8C0TBzhC9JhXCEr6HWi7t32s+5aW3XTy/1jSN8SSqEI3w13qDfBTTo7VNzOMKXpEI4wpfo7nsBjtg1qBzhS1IhejLCj4hrgLuAs4C7M3NLL64jLVWvRvJL3X+hkX8v/jpYqJ33XnNuV85/4jX8q2ZwdX2EHxFnAZ8DrgXeCLwnIt7Y7etIkpamFyP8y4F9mfkUQERMATcCP+rBtdQwTXsq5kL1LPWTxksdNdf566Nd+3V7NYrv1nmPn2fT2mNMnMHrdqJf1+7FHP4q4Jm25QPVOklSH0VmdveEETcD78zMD1TL7wUuz8wPnbDfRmBjtXgx8ETb5guAn3e1YYOjqbVZ1/Bpam1NrQtOru33MvPViz24F1M6B4AL25ZXA8+euFNmbgW2zneCiNidmeM9aFvfNbU26xo+Ta2tqXVB/dp6MaXz78CaiLgoIl4G3ALs6MF1JElL0PURfmYei4g/Ab5F67bML2TmD7t9HUnS0vTkPvzMfAh4qMYp5p3qaYim1mZdw6eptTW1LqhZW9fftJUkDSYfrSBJhRi4wI+IayLiiYjYFxGb+92eTkXEhRHxSETsjYgfRsRt1frzI+LhiHiy+r6i323tREScFRHfj4gHquWLImJXVddXqjfsh05ELI+I7RHx46rv3tqEPouIj1Q/h49HxJcj4uXD2mcR8YWIOBwRj7etm7ePouWzVZ48FhGX9q/lp7ZAXX9T/Sw+FhH/FBHL27bdUdX1RES8czHXGKjAb9hjGY4BmzLzDcAVwAerWjYDOzNzDbCzWh5GtwF725Y/AXy6qut5YENfWlXfXcA3M/P1wJto1TjUfRYRq4APA+OZeQmtmyluYXj77F7gmhPWLdRH1wJrqq+NwOfPUBs7cS8n1/UwcElm/gHwH8AdAFWW3AL8fnXM31X5eUoDFfi0PZYhM18Ejj+WYehk5qHM/F71+le0gmMVrXq2VbttA27qTws7FxGrgeuBu6vlAK4Ctle7DGtdrwTeDtwDkJkvZuYRGtBntG7QeEVELAPOAQ4xpH2Wmd8BfnHC6oX66Ebgi9nyKLA8IlaemZYuzXx1Zea/ZOaxavFRWp9rglZdU5n5P5n5E2Afrfw8pUEL/EY+liEixoC3ALuA0cw8BK1fCsBr+teyjn0G+Cjwv9Xyq4AjbT+Yw9pvrwV+BvxDNV11d0Scy5D3WWYeBD4JPE0r6I8CMzSjz45bqI+alCl/DHyjet1RXYMW+DHPuqG+jSgiRoCvA3+amb/sd3vqioh3AYczc6Z99Ty7DmO/LQMuBT6fmW8BXmDIpm/mU81n3whcBPwucC6tqY4TDWOfnU4jfjYj4mO0pom/dHzVPLudtq5BC/xFPZZhWETES2mF/Zcy875q9XPH/6Ssvh/uV/s6dCVwQ0TspzXldhWtEf/yaroAhrffDgAHMnNXtbyd1i+AYe+zdwA/ycyfZeZvgPuAt9GMPjtuoT4a+kyJiPXAu4B1+dv76Duqa9ACvzGPZajmte8B9mbmp9o27QDWV6/XA/ef6bbVkZl3ZObqzByj1T/fzsx1wCPAu6vdhq4ugMz8KfBMRFxcrbqa1mO9h7rPaE3lXBER51Q/l8frGvo+a7NQH+0A3lfdrXMFcPT41M8wiNY/k7oduCEzf922aQdwS0ScHREX0XpT+t9Oe8LMHKgv4Dpa70b/J/CxfrenRh1/SOtPrMeAH1Rf19Ga794JPFl9P7/fba1R4wTwQPX6tdUP3D7ga8DZ/W5fhzW9Gdhd9ds/Ayua0GfAXwI/Bh4H/hE4e1j7DPgyrfcifkNrpLthoT6iNfXxuSpP9tC6U6nvNSyhrn205uqPZ8jft+3/saquJ4BrF3MNP2krSYUYtCkdSVKPGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXi/wBWb2p7Qqq0tAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of target sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len, batch_size): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SRC_MAX_SENTENCE_LEN = 40 \n",
    "TARG_MAX_SENTENCE_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "train_loader_, dev_loader_, test_loader_ = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 40])\n",
      "tensor([[  5,   3, 171,  ...,   2,   2,   2],\n",
      "        [  5,   4, 199,  ...,   2,   2,   2],\n",
      "        [ 13,   3,   3,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [ 51,  13, 204,  ...,   2,   2,   2],\n",
      "        [  7,   3,   3,  ...,   2,   2,   2],\n",
      "        [  5,  37,  92,  ...,   2,   2,   2]])\n",
      "tensor([16, 15, 21,  9, 17, 13,  7,  5, 16, 28, 11, 18, 18, 17,  8, 18, 13,  9,\n",
      "         6,  7, 10, 16, 13, 36, 13,  4, 23, 19, 23, 15, 25, 28])\n",
      "torch.Size([32, 40])\n",
      "tensor([[  3,  50,   9,  ...,   2,   2,   2],\n",
      "        [  3,  19, 217,  ...,   2,   2,   2],\n",
      "        [  3,  54,  20,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  3,   8, 111,  ...,   2,   2,   2],\n",
      "        [  3, 339, 436,  ...,   2,   2,   2],\n",
      "        [  3,   9,  76,  ...,   2,   2,   2]])\n",
      "tensor([22, 16, 22, 13, 23, 16, 10,  6, 25, 35, 13, 21, 20, 21, 13, 20, 21, 11,\n",
      "         7,  9, 17, 21, 12, 35, 10,  6, 24, 23, 26, 15, 29, 32])\n"
     ]
    }
   ],
   "source": [
    "# check that loader works \n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    print(targ_idxs.size())\n",
    "    print(targ_idxs)\n",
    "    print(targ_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_emb(word2vec, token2id): \n",
    "    \"\"\" Given word2vec model and the vocab's token2id, extract pretrained word embeddings \"\"\"\n",
    "    pretrained_emb = np.zeros((len(token2id), 300)) \n",
    "    for token in token2id: \n",
    "        try: \n",
    "            pretrained_emb[token2id[token]] = word2vec[token]\n",
    "        except: \n",
    "            pretrained_emb[token2id[token]] = np.random.normal(size=(300,))\n",
    "    return torch.from_numpy(pretrained_emb.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, enc_input_dim, enc_embed_dim, enc_hidden_dim, num_layers, pretrained_word2vec): \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.enc_input_dim = enc_input_dim \n",
    "        self.enc_embed_dim = enc_embed_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim \n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(input_size=enc_embed_dim, hidden_size=enc_hidden_dim, num_layers=num_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, enc_input, enc_input_lens):\n",
    "        batch_size = enc_input.size()[0]\n",
    "        _, idx_sort = torch.sort(enc_input_lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        enc_input, enc_input_lens = enc_input.index_select(0, idx_sort), enc_input_lens.index_select(0, idx_sort)\n",
    "        embedded = self.embedding(enc_input)\n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, enc_input_lens, batch_first=True)\n",
    "        hidden = self.initHidden(batch_size).to(device)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n",
    "                                                           total_length=SRC_MAX_SENTENCE_LEN,\n",
    "                                                           padding_value=RESERVED_TOKENS['<PAD>'])\n",
    "        output = output.index_select(0, idx_unsort)\n",
    "        hidden = hidden.index_select(1, idx_unsort).transpose(0, 1).contiguous().view(self.num_layers, batch_size, -1)\n",
    "#        print(\"now hidden size is {}\".format(hidden.size()))\n",
    "#         final_hidden = torch.cat([output[:, -1, :self.enc_hidden_dim], \n",
    "#                                   output[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0) \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2*self.num_layers, batch_size, self.enc_hidden_dim, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, dec_input_dim, dec_embed_dim, dec_hidden_dim, enc_hidden_dim, num_layers, pretrained_word2vec):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.dec_input_dim = dec_input_dim\n",
    "        self.dec_embed_dim = dec_embed_dim\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(dec_embed_dim + 2 * enc_hidden_dim, dec_hidden_dim, num_layers=num_layers)\n",
    "        self.out = nn.Linear(dec_hidden_dim, dec_input_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "        context = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n",
    "                             enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden\n",
    "        \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder, decoder_token2id): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_dim = self.decoder.dec_input_dim\n",
    "        self.output_seq_len = TARG_MAX_SENTENCE_LEN\n",
    "\n",
    "    def forward(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio): \n",
    "        batch_size = src_idx.size()[0]\n",
    "        enc_outputs, enc_hidden = self.encoder(src_idx, src_lens)\n",
    "        dec_hidden = enc_hidden \n",
    "        dec_outputs = Variable(torch.zeros(self.output_seq_len, batch_size, self.output_dim))\n",
    "        hypotheses = Variable(torch.zeros(self.output_seq_len, batch_size))\n",
    "        dec_output = targ_idx[:, 0] # initialize with <SOS>\n",
    "        for di in range(1, self.output_seq_len): \n",
    "            dec_output, dec_hidden = self.decoder(dec_output, dec_hidden, enc_outputs)\n",
    "            dec_outputs[di] = dec_output \n",
    "            teacher_labels = targ_idx[:, di-1] \n",
    "            greedy_labels = dec_output.data.max(1)[1]\n",
    "            dec_output = teacher_labels if random.random() < teacher_forcing_ratio else greedy_labels \n",
    "            hypotheses[di] = greedy_labels\n",
    "\n",
    "        return dec_outputs, hypotheses.transpose(0,1)\n",
    "    \n",
    "class Attention(nn.Module): \n",
    "    \n",
    "    \"\"\" Implements the attention mechanism by Bahdanau et al. (2015) \"\"\"\n",
    "    \n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim, num_annotations, num_layers): \n",
    "        super(Attention, self).__init__() \n",
    "        self.num_annotations = num_annotations\n",
    "        self.input_dim = enc_hidden_dim * 2 + dec_hidden_dim\n",
    "        self.attn = nn.Linear(self.input_dim, self.num_annotations)\n",
    "        self.v = nn.Parameter(torch.rand(self.num_annotations))\n",
    "        self.num_layers = num_layers \n",
    "        nn.init.normal_(self.v)\n",
    "        \n",
    "    def forward(self, encoder_outputs, last_dec_hidden): \n",
    "        batch_size = encoder_outputs.size()[0]\n",
    "        last_dec_hidden = last_dec_hidden.transpose(0, 1)[:, -1, :].unsqueeze(1) \n",
    "#         print(\"encoder_outputs size is {}\".format(encoder_outputs.size()))\n",
    "#        print(\"last_dec_hidden size is {}\".format(last_dec_hidden.size()))\n",
    "        hidden_broadcast = last_dec_hidden.repeat(1, self.num_annotations, 1)\n",
    "#         print(\"hidden_broadcast size is {}\".format(hidden_broadcast.size()))\n",
    "        v_broadcast = self.v.repeat(batch_size, 1, 1)\n",
    "#         print(\"v_broadcast size is {}\".format(v_broadcast.size()))\n",
    "#         print(\"encoder_outputs size is {}, hidden_broadcast size is {}\".format(encoder_outputs.size(), \n",
    "#                                                                                hidden_broadcast.size()))\n",
    "        concat = torch.cat([encoder_outputs, hidden_broadcast], dim=2)\n",
    "#         print(\"concat size is {}\".format(concat.size()))\n",
    "        energies = v_broadcast.bmm(torch.tanh(self.attn(concat)))\n",
    "#         print(\"after attention energies size is {}\".format(energies.size()))\n",
    "        attn_weights = F.softmax(energies, dim=2).squeeze(1)\n",
    "        return attn_weights\n",
    "\n",
    "class DecoderAttnRNN(nn.Module):\n",
    "    def __init__(self, dec_input_dim, dec_embed_dim, dec_hidden_dim, enc_hidden_dim, num_layers, pretrained_word2vec):\n",
    "        super(DecoderAttnRNN, self).__init__()\n",
    "        self.dec_input_dim = dec_input_dim\n",
    "        self.dec_embed_dim = dec_embed_dim\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.num_layers = num_layers \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.attn = Attention(enc_hidden_dim, dec_hidden_dim, num_annotations=SRC_MAX_SENTENCE_LEN, num_layers=num_layers)\n",
    "        self.gru = nn.GRU(dec_embed_dim + 2 * enc_hidden_dim, dec_hidden_dim, num_layers=num_layers)\n",
    "        self.out = nn.Linear(dec_hidden_dim, dec_input_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        \n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "#        print(\"embedded size is {}\".format(embedded.size()))\n",
    "        attn_weights = self.attn(encoder_outputs=enc_outputs, last_dec_hidden=dec_hidden).unsqueeze(1)\n",
    "#         print(\"enc_outputs size is {}\".format(enc_outputs.size()))\n",
    "#         print(\"last_dec_hidden size is {}\".format(dec_hidden.size()))\n",
    "#         print(\"attn_weights size is {}\".format(attn_weights.size()))\n",
    "        context = attn_weights.bmm(enc_outputs).transpose(0, 1)\n",
    "#        print(\"context size is {}\".format(context.size()))\n",
    "#         context = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n",
    "#                              enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "#         print(\"concat size is {}\".format(concat.size()))\n",
    "#         print(\"dec_hidden is {}\".format(dec_hidden.size()))\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2corpus(tensor, id2token): \n",
    "    \"\"\" Takes a tensor (num_sentences x max_sentence_length) representing the corpus, \n",
    "        returns its string equivalent \n",
    "    \"\"\"\n",
    "    tensor = tensor.view(-1)\n",
    "    ignored_idx = [RESERVED_TOKENS[token] for token in ['<SOS>', '<EOS>', '<PAD>']] \n",
    "    filtered_list = [id2token[idx] for idx in tensor.numpy().astype(int).tolist() if idx not in ignored_idx] \n",
    "    corpus = ' '.join(filtered_list)\n",
    "    return corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, id2token): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "    reference_corpus = []\n",
    "    hypothesis_corpus = [] \n",
    "    \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader): \n",
    "        batch_size = src_idxs.size()[0]\n",
    "        outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.0)\n",
    "        outputs = outputs[1:].view(-1, TARG_VOCAB_SIZE)\n",
    "        targets = targ_idxs[:,1:]\n",
    "        hypothesis_corpus.append(hypotheses)\n",
    "        reference_corpus.append(targets)\n",
    " \n",
    "        loss = F.nll_loss(outputs.view(-1, TARG_VOCAB_SIZE), targets.contiguous().view(-1), \n",
    "                          ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "        total_loss += loss.item()  \n",
    "\n",
    "    # reconstruct corpus and compute bleu score \n",
    "    hypothesis_corpus = torch.cat(hypothesis_corpus, dim=0) \n",
    "    reference_corpus = torch.cat(reference_corpus, dim=0)\n",
    "    hypothesis_corpus = tensor2corpus(hypothesis_corpus, id2token)\n",
    "    reference_corpus = tensor2corpus(reference_corpus, id2token)\n",
    "    bleu_score = sacrebleu.corpus_bleu(hypothesis_corpus, reference_corpus).score\n",
    "    \n",
    "    return total_loss / len(loader), bleu_score, hypothesis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, dev_loader, id2token, learning_rate, num_epochs, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.5) \n",
    "            loss = criterion(final_outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 100 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "#                result['train_loss'], result['train_bleu'], train_hypotheses = 0, 0, None\n",
    "                result['train_loss'], result['train_bleu'], train_hypotheses = evaluate(model, train_loader, id2token) \n",
    "                result['val_loss'], result['val_bleu'], val_hypotheses = evaluate(model, dev_loader, id2token)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Loss: {:.2f}, Val Loss: {:.2f}, Train BLEU: {:.2f}, Val BLEU: {:.2f}'\\\n",
    "                          .format(result['epoch'], result['train_loss'], result['val_loss'], \n",
    "                                  result['train_bleu'], result['val_bleu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to save results to and load results from a pkl logfile \n",
    "\n",
    "RESULTS_LOG = 'experiment_results/experiment_results_log.pkl'\n",
    "\n",
    "def append_to_log(hyperparams, results, runtime, experiment_name, dt_created, filename=RESULTS_LOG): \n",
    "    \"\"\" Appends results and details of a single experiment to a log file \"\"\"\n",
    "    \n",
    "    # create directory if doesn't already exist \n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "        \n",
    "    # store experiment details in a dictionary \n",
    "    new_result = {'experiment_name': experiment_name, 'hyperparams': hyperparams, 'results': results, \n",
    "                  'runtime': runtime, 'dt_created': dt_created}\n",
    "    \n",
    "    # if log already exists, append to log \n",
    "    try: \n",
    "        results_log = pkl.load(open(filename, \"rb\"))\n",
    "        results_log.append(new_result)\n",
    "\n",
    "    # if log doesn't exists, initialize first result as the log \n",
    "    except (OSError, IOError) as e:\n",
    "        results_log = [new_result]\n",
    "    \n",
    "    # save to pickle \n",
    "    pkl.dump(results_log, open(filename, \"wb\"))\n",
    "\n",
    "# def append_to_log(hyperparams, results, runtime, experiment_name, filename=RESULTS_LOG): \n",
    "#     \"\"\" Appends results and details of a single experiment to a log file \"\"\"\n",
    "    \n",
    "#     # create directory if doesn't already exist \n",
    "#     if not os.path.exists(os.path.dirname(filename)):\n",
    "#         os.makedirs(os.path.dirname(filename))\n",
    "        \n",
    "#     # store experiment details in a dictionary \n",
    "#     new_result = {'experiment_name': experiment_name, 'hyperparams': hyperparams, 'results': results, \n",
    "#                   'runtime': runtime, 'dt_created': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    \n",
    "#     # if log already exists, append to log \n",
    "#     try: \n",
    "#         results_log = pkl.load(open(filename, \"rb\"))\n",
    "#         results_log.append(new_result)\n",
    "\n",
    "#     # if log doesn't exists, initialize first result as the log \n",
    "#     except (OSError, IOError) as e:\n",
    "#         results_log = [new_result]\n",
    "    \n",
    "#     # save to pickle \n",
    "#     pkl.dump(results_log, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_log(experiment_name=None, filename=RESULTS_LOG): \n",
    "    \"\"\" Loads experiment log, with option to filter for a specific experiment_name \"\"\"\n",
    "    \n",
    "    results_log = pkl.load(open(filename, \"rb\"))\n",
    "    \n",
    "    if experiment_name is not None: \n",
    "        results_log = [r for r in results_log if r['experiment_name'] == experiment_name]\n",
    "        \n",
    "    return results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, dev_loader, id2token, learning_rate, num_epochs, \n",
    "                   print_intermediate=True, save_checkpoint=False, model_name='NA'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.5) \n",
    "            loss = criterion(final_outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 100 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "#                result['train_loss'], result['train_bleu'], train_hypotheses = 0, 0, None\n",
    "                result['train_loss'], result['train_bleu'], train_hypotheses = evaluate(model, train_loader, id2token) \n",
    "                result['val_loss'], result['val_bleu'], val_hypotheses = evaluate(model, dev_loader, id2token)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Loss: {:.2f}, Val Loss: {:.2f}, Train BLEU: {:.2f}, Val BLEU: {:.2f}'\\\n",
    "                          .format(result['epoch'], result['train_loss'], result['val_loss'], \n",
    "                                  result['train_bleu'], result['val_bleu']))\n",
    "                    \n",
    "                if save_checkpoint: #TODO: handle creation of directory if not exist \n",
    "                    if result['val_loss'] == pd.DataFrame.from_dict(results)['val_loss'].min(): \n",
    "                        checkpoint_fp = 'model_checkpoints/{}.pth.tar'.format(model_name)\n",
    "                        torch.save(model.state_dict(), checkpoint_fp)\n",
    "                \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_loader, dev_loader, model_type, num_epochs=10, learning_rate=0.0005, num_layers=2,\n",
    "                   enc_hidden_dim=300, dec_hidden_dim=2*300, experiment_name='NA', model_name='NA',\n",
    "                   save_to_log=True, save_checkpoint=False, print_summary=True, print_intermediate=True):  \n",
    "    \n",
    "    \"\"\" Wraps all processing, training and evaluation steps in a function to facilitate hyperparam tuning. \n",
    "        Note that the function takes as input tokenized data rather than raw data since there's significant \n",
    "        lag time in generating tokens.  \n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time() \n",
    "    \n",
    "    # TODO: try dropout and optimization algorithms. for now use as default: \n",
    "    optimizer = 'Adam' \n",
    "    enc_dropout = 0 \n",
    "    dec_dropout = 0 \n",
    "    \n",
    "    # instantiate model and optimizer \n",
    "    if model_type == 'without_attention': \n",
    "        encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=ENC_EMBED_DIM, enc_hidden_dim=enc_hidden_dim, \n",
    "                             num_layers=num_layers, pretrained_word2vec=get_pretrained_emb(\n",
    "                                 data['train']['source']['word2vec'], data['train']['source']['token2id']))\n",
    "        decoder = DecoderRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=DEC_EMBED_DIM, dec_hidden_dim=dec_hidden_dim, \n",
    "                             enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                             pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                                    data['train']['target']['token2id']))\n",
    "        model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\n",
    "        \n",
    "    elif model_type == 'attention_bahdanau': \n",
    "        encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=ENC_EMBED_DIM, enc_hidden_dim=enc_hidden_dim, \n",
    "                             num_layers=num_layers, pretrained_word2vec=get_pretrained_emb(\n",
    "                                 data['train']['source']['word2vec'], data['train']['source']['token2id']))\n",
    "        decoder = DecoderAttnRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=DEC_EMBED_DIM, \n",
    "                                 dec_hidden_dim=dec_hidden_dim, enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                                 pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                                        data['train']['target']['token2id']))\n",
    "        model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\n",
    "        \n",
    "    else: \n",
    "        raise ValueError(\"Invalid model_type. Must be either 'without_attention' or 'attention_bahdanau'\")\n",
    "        \n",
    "    # train and evaluate \n",
    "    results = train_and_eval(model, train_loader, dev_loader, id2token=data['train']['target']['id2token'], \n",
    "                             num_epochs=num_epochs, learning_rate=learning_rate, \n",
    "                             print_intermediate=print_intermediate, save_checkpoint=save_checkpoint)\n",
    "    \n",
    "    # store, print, and save results \n",
    "    runtime = (time.time() - start_time) / 60 \n",
    "    dt_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    hyperparams = {'model_type': model_type, 'num_epochs': num_epochs, 'learning_rate': learning_rate, \n",
    "                   'enc_hidden_dim': enc_hidden_dim, 'dec_hidden_dim': dec_hidden_dim, 'num_layers': num_layers, \n",
    "                   'enc_embed_dim': ENC_EMBED_DIM,  'dec_embed_dim': DEC_EMBED_DIM, \n",
    "                   'optimizer': optimizer, 'enc_dropout': enc_dropout, 'dec_dropout': dec_dropout, \n",
    "                   'batch_size': BATCH_SIZE, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "                   'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "                   'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN}  \n",
    "        \n",
    "    if save_to_log: \n",
    "        append_to_log(hyperparams, results, runtime, experiment_name, dt_created)\n",
    "    if print_summary: \n",
    "        print(\"Experiment completed in {} minutes with {:.2f} validation loss and {:.2f} validation BLEU.\".format(\n",
    "            int(runtime), pd.DataFrame.from_dict(results)['val_loss'].min(), \n",
    "            pd.DataFrame.from_dict(results)['val_bleu'].max()))\n",
    "        \n",
    "    return results, hyperparams, runtime, model, train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to summarize, evaluate, and plot results \n",
    "\n",
    "def summarize_results(results_log): \n",
    "    \"\"\" Summarizes results_log (list) into a dataframe, splitting hyperparameters string into columns, and reducing \n",
    "        the val_acc dict into the best validation accuracy obtained amongst all the epochs logged \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results_log)\n",
    "    results_df = pd.concat([results_df, results_df['hyperparams'].apply(pd.Series)], axis=1)\n",
    "    results_df['val_loss'] = results_df['results'].apply(lambda d: pd.DataFrame.from_dict(d)['val_loss'].min())\n",
    "    return results_df.sort_values(by='val_loss', ascending=True) \n",
    "\n",
    "def plot_multiple_learning_curves(results_df, plot_variable, figsize=(8, 5), legend_loc='best'):\n",
    "    \"\"\" Plots learning curves of MULTIPLE experiments, includes only validation accuracy \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for index, row in results_df.iterrows():\n",
    "        val_loss_hist = pd.DataFrame.from_dict(row['results']).set_index('epoch')['val_loss'] \n",
    "        plt.plot(val_loss_hist, label=\"{} ({}%)\".format(row[plot_variable], val_loss_hist.max()))\n",
    "    plt.legend(title=plot_variable, loc=legend_loc)    \n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "def plot_single_learning_curve(results, figsize=(8, 5)): \n",
    "    \"\"\" Plots learning curve of a SINGLE experiment, includes both train and validation accuracy \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    results_df = results_df.set_index('epoch')\n",
    "    results_df.plot(figsize=figsize)\n",
    "    plt.ylabel('Validation Lossy')\n",
    "    plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to count parameters \n",
    "def count_parameters(model): \n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return all_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 6.50, Val Loss: 6.37, Train BLEU: 4.89, Val BLEU: 5.44\n",
      "Epoch: 1.00, Train Loss: 4.90, Val Loss: 4.58, Train BLEU: 3.89, Val BLEU: 4.94\n",
      "Epoch: 1.98, Train Loss: 4.88, Val Loss: 4.55, Train BLEU: 3.89, Val BLEU: 4.94\n",
      "Experiment completed in 6 minutes with 4.553896340456876 validation loss and 5.441153669090521 validation BLEU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'epoch': 0.0,\n",
       "   'train_loss': 6.49982488155365,\n",
       "   'train_bleu': 4.887123794512932,\n",
       "   'val_loss': 6.374488733031533,\n",
       "   'val_bleu': 5.441153669090521},\n",
       "  {'epoch': 1.0,\n",
       "   'train_loss': 4.895068407058716,\n",
       "   'train_bleu': 3.890917982252671,\n",
       "   'val_loss': 4.583915255286477,\n",
       "   'val_bleu': 4.935157993221384},\n",
       "  {'epoch': 1.975,\n",
       "   'train_loss': 4.878758823871612,\n",
       "   'train_bleu': 3.890917982252671,\n",
       "   'val_loss': 4.553896340456876,\n",
       "   'val_bleu': 4.935157993221384}],\n",
       " {'model_type': 'attention_bahdanau',\n",
       "  'num_epochs': 2,\n",
       "  'learning_rate': 0.0005,\n",
       "  'enc_hidden_dim': 300,\n",
       "  'dec_hidden_dim': 600,\n",
       "  'num_layers': 2,\n",
       "  'enc_embed_dim': 300,\n",
       "  'dec_embed_dim': 300,\n",
       "  'optimizer': 'Adam',\n",
       "  'enc_dropout': 0,\n",
       "  'dec_dropout': 0,\n",
       "  'batch_size': 32,\n",
       "  'src_lang': 'zh',\n",
       "  'targ_lang': 'en',\n",
       "  'src_vocab_size': 1000,\n",
       "  'targ_vocab_size': 1000,\n",
       "  'src_max_sentence_len': 40,\n",
       "  'targ_max_sentence_len': 40},\n",
       " 6.441831266880035,\n",
       " EncoderDecoder(\n",
       "   (encoder): EncoderRNN(\n",
       "     (embedding): Embedding(1000, 300)\n",
       "     (gru): GRU(300, 300, num_layers=2, batch_first=True, bidirectional=True)\n",
       "   )\n",
       "   (decoder): DecoderAttnRNN(\n",
       "     (embedding): Embedding(1000, 300)\n",
       "     (attn): Attention(\n",
       "       (attn): Linear(in_features=1200, out_features=40, bias=True)\n",
       "     )\n",
       "     (gru): GRU(900, 600, num_layers=2)\n",
       "     (out): Linear(in_features=600, out_features=1000, bias=True)\n",
       "     (softmax): LogSoftmax()\n",
       "   )\n",
       " ),\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1a912c8be0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1a912f2f60>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, hyperparams, runtime, model, train_loader, dev_loader = \\\n",
    "    run_experiment(train_loader, dev_loader, model_type='attention_bahdanau', num_epochs=2, learning_rate=0.0005, \n",
    "               num_layers=2, enc_hidden_dim=300, dec_hidden_dim=2*300, experiment_name='test_run', model_name='test_run',\n",
    "               save_to_log=True, save_checkpoint=True, print_summary=True, print_intermediate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_created</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>results</th>\n",
       "      <th>runtime</th>\n",
       "      <th>model_type</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>enc_hidden_dim</th>\n",
       "      <th>dec_hidden_dim</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_dropout</th>\n",
       "      <th>dec_dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>src_lang</th>\n",
       "      <th>targ_lang</th>\n",
       "      <th>src_vocab_size</th>\n",
       "      <th>targ_vocab_size</th>\n",
       "      <th>src_max_sentence_len</th>\n",
       "      <th>targ_max_sentence_len</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-25 02:38:58</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epochs': 2, 'learning_rate': 0.0005, 'enc_hidden_dim':...</td>\n",
       "      <td>[{'epoch': 0.0, 'train_loss': 6.49982488155365, 'train_bleu': 4.887123794512932, 'val_loss': 6.3...</td>\n",
       "      <td>6.441831</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>zh</td>\n",
       "      <td>en</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>4.553896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-25 02:27:18</td>\n",
       "      <td>test_run</td>\n",
       "      <td>{'model_type': 'attention_bahdanau', 'num_epochs': 2, 'learning_rate': 0.0005, 'enc_hidden_dim':...</td>\n",
       "      <td>[{'epoch': 0.0, 'train_loss': 6.59386328458786, 'train_bleu': 4.278570546021468, 'val_loss': 6.5...</td>\n",
       "      <td>6.343195</td>\n",
       "      <td>attention_bahdanau</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>zh</td>\n",
       "      <td>en</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>4.557455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt_created experiment_name  \\\n",
       "1  2018-11-25 02:38:58        test_run   \n",
       "0  2018-11-25 02:27:18        test_run   \n",
       "\n",
       "                                                                                           hyperparams  \\\n",
       "1  {'model_type': 'attention_bahdanau', 'num_epochs': 2, 'learning_rate': 0.0005, 'enc_hidden_dim':...   \n",
       "0  {'model_type': 'attention_bahdanau', 'num_epochs': 2, 'learning_rate': 0.0005, 'enc_hidden_dim':...   \n",
       "\n",
       "                                                                                               results  \\\n",
       "1  [{'epoch': 0.0, 'train_loss': 6.49982488155365, 'train_bleu': 4.887123794512932, 'val_loss': 6.3...   \n",
       "0  [{'epoch': 0.0, 'train_loss': 6.59386328458786, 'train_bleu': 4.278570546021468, 'val_loss': 6.5...   \n",
       "\n",
       "    runtime          model_type  num_epochs  learning_rate  enc_hidden_dim  \\\n",
       "1  6.441831  attention_bahdanau           2         0.0005             300   \n",
       "0  6.343195  attention_bahdanau           2         0.0005             300   \n",
       "\n",
       "   dec_hidden_dim    ...     enc_dropout  dec_dropout  batch_size src_lang  \\\n",
       "1             600    ...               0            0          32       zh   \n",
       "0             600    ...               0            0          32       zh   \n",
       "\n",
       "   targ_lang  src_vocab_size  targ_vocab_size src_max_sentence_len  \\\n",
       "1         en            1000             1000                   40   \n",
       "0         en            1000             1000                   40   \n",
       "\n",
       "  targ_max_sentence_len  val_loss  \n",
       "1                    40  4.553896  \n",
       "0                    40  4.557455  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = summarize_results(load_experiment_log(experiment_name='test_run', filename=RESULTS_LOG))\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Split BLEU and Loss \n",
    "plot_single_learning_curve(all_results.iloc[1]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 6.65, Val Loss: 6.53, Train BLEU: 4.48, Val BLEU: 5.46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-28065489e1d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token2id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id2token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-154-2b4fa4c08273>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader, id2token, learning_rate, num_epochs, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARG_VOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, num_layers=2, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "                                                            data['train']['source']['token2id']))\n",
    "\n",
    "decoder = DecoderRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=300, dec_hidden_dim=2*300, \n",
    "                     enc_hidden_dim=300, num_layers=2, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                            data['train']['target']['token2id']))\n",
    "\n",
    "decoder_attn = DecoderAttnRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=300, dec_hidden_dim=2*300, \n",
    "                              enc_hidden_dim=300, num_layers=2, \n",
    "                              pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                                     data['train']['target']['token2id']))\n",
    "\n",
    "#model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\n",
    "model = EncoderDecoder(encoder, decoder_attn, data['train']['target']['token2id'])\n",
    "train(model, train_loader, dev_loader, data['train']['target']['id2token'], num_epochs=20, learning_rate=0.0005) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden shape is torch.Size([2, 32, 300])\n",
      "output shape is torch.Size([32, 40, 600])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index out of range at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/TH/generic/THTensorMath.cpp:352",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-528dc9d76ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_final_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_dec_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_final_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attn weights are: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-450d2c609a4e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_input, enc_input_lens)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden shape is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output shape is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_unsort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_unsort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         final_hidden = torch.cat([output[:, -1, :self.enc_hidden_dim], \n",
      "\u001b[0;31mRuntimeError\u001b[0m: index out of range at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/TH/generic/THTensorMath.cpp:352"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "                                                            data['train']['source']['token2id']))\n",
    "attention = Attention(enc_hidden_dim=300, dec_hidden_dim=600, num_annotations=SRC_MAX_SENTENCE_LEN)\n",
    "\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader): \n",
    "    enc_outputs, enc_final_hidden = encoder(src_idxs, src_lens)\n",
    "    attn_weights = attention(encoder_outputs=enc_outputs, last_dec_hidden=enc_final_hidden)\n",
    "    print(\"attn weights are: {}\".format(attn_weights.size()))\n",
    "    print(\"example: {}\".format(attn_weights[0].sum()))\n",
    "#     print(\"enc_outputs size is {}\".format(enc_outputs.size()))\n",
    "#     print(\"enc_final_hidden size is {}\".format(enc_final_hidden.size()))\n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
