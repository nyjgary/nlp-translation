{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "import sacrebleu\n",
    "import random\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp, lang_type): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]\n",
    "        if lang_type == 'source': \n",
    "            tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "        elif lang_type == 'target': \n",
    "            tokens_data = [['<SOS'] + datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(num_vocab))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + list(vocab)\n",
    "    token2id = dict(zip(id2token, range(max_vocab_size)))\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "\n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # attach vocab sizes and word2vec models \n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size \n",
    "    data['train']['source']['word2vec'] = load_word2vec(src_lang) \n",
    "    data['train']['target']['word2vec'] = load_word2vec(targ_lang) \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'], lang_type)\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    token_lists = data['train'][lang_type]['tokens'], \n",
    "                    max_vocab_size = data['train'][lang_type]['max_vocab_size'], \n",
    "                    word2vec = data['train'][lang_type]['word2vec']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = 1000\n",
    "TARG_VOCAB_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Source: ['你', '现在', '可以', '去', '个', '真正', '的', '学校', '念书', '了', '他', '说', '<EOS>']\n",
      "Example Target: ['<SOS', '&quot;', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '&quot;', 'he', 'said', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# print example for sanity check  \n",
    "print(\"Example Source: {}\".format(data['train']['source']['tokens'][5]))\n",
    "print(\"Example Target: {}\".format(data['train']['target']['tokens'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of source sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEetJREFUeJzt3V+MXGd5x/HvQwyBZEvsEFi5dtQNwgrQuECyigKp0G6CRP6gJBdEDbLAoUa+oZBSV8QpF1GlVhiVAkGiVFZCYyrEAiZtrCRAI5Mt4iJuvYDigEnjBjexY2IQsWFD1WD16cUcL1N7196dM+OZOe/3I612zv/30bv67bvvnDkbmYkkqfle0u8GSJLODANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVIhl/W4AwAUXXJBjY2Nzyy+88ALnnntu/xrUQ02tzbqGT1Nra2pdcHJtMzMzP8/MVy/2+IEI/LGxMXbv3j23PD09zcTERP8a1ENNrc26hk9Ta2tqXXBybRHxX0s53ikdSSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqxEB80nbQjW1+cO71/i3X97ElktQ5R/iSVIjTBn5EfCEiDkfE423rzo+IhyPiyer7imp9RMRnI2JfRDwWEZf2svGSpMVbzAj/XuCaE9ZtBnZm5hpgZ7UMcC2wpvraCHy+O82UJNV12sDPzO8Avzhh9Y3Atur1NuCmtvVfzJZHgeURsbJbjZUkda7TOfzRzDwEUH1/TbV+FfBM234HqnWSpD6LzDz9ThFjwAOZeUm1fCQzl7dtfz4zV0TEg8DHM/O71fqdwEczc2aec26kNe3D6OjoZVNTU3PbZmdnGRkZqVNXV+05eHTu9dpV59U616DV1i3WNXyaWltT64KTa5ucnJzJzPHFHt/pbZnPRcTKzDxUTdkcrtYfAC5s22818Ox8J8jMrcBWgPHx8Wx/qP+g/QODW9tvy1w3Uetcg1Zbt1jX8GlqbU2tC+rX1umUzg5gffV6PXB/2/r3VXfrXAEcPT71I0nqr9OO8CPiy8AEcEFEHADuBLYAX42IDcDTwM3V7g8B1wH7gF8D7+9BmyVJHTht4GfmexbYdPU8+ybwwbqNkiR1n5+0laRCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEMvqHBwRHwE+ACSwB3g/sBKYAs4Hvge8NzNfrNnORhvb/ODc6/1bru9jSyQ1Wccj/IhYBXwYGM/MS4CzgFuATwCfzsw1wPPAhm40VJJUT90pnWXAKyJiGXAOcAi4Cthebd8G3FTzGpKkLug48DPzIPBJ4GlaQX8UmAGOZOaxarcDwKq6jZQk1ReZ2dmBESuArwN/BBwBvlYt35mZr6v2uRB4KDPXznP8RmAjwOjo6GVTU1Nz22ZnZxkZGemoXb2w5+DRuddrV51X61zz1dbN8/fLoPVZtzS1LmhubU2tC06ubXJyciYzxxd7fJ03bd8B/CQzfwYQEfcBbwOWR8SyapS/Gnh2voMzcyuwFWB8fDwnJibmtk1PT9O+3G+3tr+pum6i1rnmq62b5++XQeuzbmlqXdDc2ppaF9Svrc4c/tPAFRFxTkQEcDXwI+AR4N3VPuuB+2tcQ5LUJXXm8HfRenP2e7RuyXwJrRH77cCfRcQ+4FXAPV1opySpplr34WfmncCdJ6x+Cri8znklSd3nJ20lqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIs63cDBsnY5gfnXu/fcn0fWyJJ3ecIX5IKYeBLUiEMfEkqhIEvSYWoFfgRsTwitkfEjyNib0S8NSLOj4iHI+LJ6vuKbjVWktS5uiP8u4BvZubrgTcBe4HNwM7MXAPsrJYlSX3WceBHxCuBtwP3AGTmi5l5BLgR2Fbttg24qW4jJUn1RWZ2dmDEm4GtwI9oje5ngNuAg5m5vG2/5zPzpGmdiNgIbAQYHR29bGpqam7b7OwsIyMjHbWrjj0Hj869XrvqvNOu78R8tXXz/P3Srz7rtabWBc2tral1wcm1TU5OzmTm+GKPrxP448CjwJWZuSsi7gJ+CXxoMYHfbnx8PHfv3j23PD09zcTEREftqmOhD1518wNZ89XWhA989avPeq2pdUFza2tqXXBybRGxpMCvM4d/ADiQmbuq5e3ApcBzEbGyasxK4HCNa0iSuqTjwM/MnwLPRMTF1aqraU3v7ADWV+vWA/fXaqEkqSvqPkvnQ8CXIuJlwFPA+2n9EvlqRGwAngZurnkNSVIX1Ar8zPwBMN/80dV1zitJ6j4/aStJhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRB1/6etKmObH5x7vX/L9X1siSTNzxG+JBWiuBF++0hckkriCF+SCmHgS1IhDHxJKoSBL0mFMPAlqRDF3aWzWN7NI6lpHOFLUiEc4dfgXwGShokjfEkqhCP8HvMZO5IGRe0RfkScFRHfj4gHquWLImJXRDwZEV+JiJfVb6Ykqa5uTOncBuxtW/4E8OnMXAM8D2zowjUkSTXVCvyIWA1cD9xdLQdwFbC92mUbcFOda0iSuiMys/ODI7YDHwd+B/hz4Fbg0cx8XbX9QuAbmXnJPMduBDYCjI6OXjY1NTW3bXZ2lpGRkY7bdSp7Dh6tdfzaVeed9lwL7bN21Xnz1nbiPsOol33WT02tC5pbW1PrgpNrm5ycnMnM8cUe3/GbthHxLuBwZs5ExMTx1fPsOu9vlMzcCmwFGB8fz4mJiblt09PTtC930601b6Xcv27itOdaaJ/96ybmre3EfYZRL/usn5paFzS3tqbWBfVrq3OXzpXADRFxHfBy4JXAZ4DlEbEsM48Bq4Fna1yjsRa6h9+7eiT1Ssdz+Jl5R2auzswx4Bbg25m5DngEeHe123rg/tqtlCTV1ov78G8HpiLir4DvA/f04BoD7VSj901rj9WeVpKkTnQl8DNzGpiuXj8FXN6N80qSusdP2hbG9wikcvksHUkqhIEvSYUw8CWpEAa+JBXCwJekQniXzhL5X64kDStH+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFaKx9+H7VMjf8rMDksARviQVo7Ej/HYljnBLrFnSqTnCl6RCGPiSVIgipnSazDenJS2WI3xJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgrhffgDbKF77H1sgqROOMKXpEIY+JJUCANfkgrRceBHxIUR8UhE7I2IH0bEbdX68yPi4Yh4svq+onvNlSR1qs4I/xiwKTPfAFwBfDAi3ghsBnZm5hpgZ7UsSeqzju/SycxDwKHq9a8iYi+wCrgRmKh22wZMA7fXauUp+LRISVqcrszhR8QY8BZgFzBa/TI4/kvhNd24hiSpnsjMeieIGAH+FfjrzLwvIo5k5vK27c9n5knz+BGxEdgIMDo6etnU1NTcttnZWUZGRhZ1/T0Hj869XrvqvHnXD5LRV8Bz/7304xZT21LrP9X+7dsWYyl9NkyaWhc0t7am1gUn1zY5OTmTmeOLPb5W4EfES4EHgG9l5qeqdU8AE5l5KCJWAtOZefGpzjM+Pp67d++eW56enmZiYmJRbRi2DydtWnuMv92z9Jm0xdS21PpPtf9Sp8eW0mfDpKl1QXNra2pdcHJtEbGkwO94Dj8iArgH2Hs87Cs7gPXAlur7/Z1eQ791pn+B+d6I1Dx1Hq1wJfBeYE9E/KBa9xe0gv6rEbEBeBq4uV4TJUndUOcune8CscDmqzs9rzo3qNNYkgaDn7SVpEIY+JJUCANfkgph4EtSIQx8SSqE//FKXec9/NJgcoQvSYVwhK/TWsz9/ZvWHuNWPwcgDTRH+JJUCEf4Gjq+RyB1xhG+JBXCEb56ytG4NDgc4UtSIRzhF8xn7EtlcYQvSYUw8CWpEAa+JBWiUXP4/scnSVqYI3xJKkSjRvhqLv96k+pzhC9JhXCEr77z/nzpzHCEL0mFcISvvljMnHydeftu/dWw5+DR//ecf/8C0TBzhC9JhXCEr6HWi7t32s+5aW3XTy/1jSN8SSqEI3w13qDfBTTo7VNzOMKXpEI4wpfo7nsBjtg1qBzhS1IhejLCj4hrgLuAs4C7M3NLL64jLVWvRvJL3X+hkX8v/jpYqJ33XnNuV85/4jX8q2ZwdX2EHxFnAZ8DrgXeCLwnIt7Y7etIkpamFyP8y4F9mfkUQERMATcCP+rBtdQwTXsq5kL1LPWTxksdNdf566Nd+3V7NYrv1nmPn2fT2mNMnMHrdqJf1+7FHP4q4Jm25QPVOklSH0VmdveEETcD78zMD1TL7wUuz8wPnbDfRmBjtXgx8ETb5guAn3e1YYOjqbVZ1/Bpam1NrQtOru33MvPViz24F1M6B4AL25ZXA8+euFNmbgW2zneCiNidmeM9aFvfNbU26xo+Ta2tqXVB/dp6MaXz78CaiLgoIl4G3ALs6MF1JElL0PURfmYei4g/Ab5F67bML2TmD7t9HUnS0vTkPvzMfAh4qMYp5p3qaYim1mZdw6eptTW1LqhZW9fftJUkDSYfrSBJhRi4wI+IayLiiYjYFxGb+92eTkXEhRHxSETsjYgfRsRt1frzI+LhiHiy+r6i323tREScFRHfj4gHquWLImJXVddXqjfsh05ELI+I7RHx46rv3tqEPouIj1Q/h49HxJcj4uXD2mcR8YWIOBwRj7etm7ePouWzVZ48FhGX9q/lp7ZAXX9T/Sw+FhH/FBHL27bdUdX1RES8czHXGKjAb9hjGY4BmzLzDcAVwAerWjYDOzNzDbCzWh5GtwF725Y/AXy6qut5YENfWlXfXcA3M/P1wJto1TjUfRYRq4APA+OZeQmtmyluYXj77F7gmhPWLdRH1wJrqq+NwOfPUBs7cS8n1/UwcElm/gHwH8AdAFWW3AL8fnXM31X5eUoDFfi0PZYhM18Ejj+WYehk5qHM/F71+le0gmMVrXq2VbttA27qTws7FxGrgeuBu6vlAK4Ctle7DGtdrwTeDtwDkJkvZuYRGtBntG7QeEVELAPOAQ4xpH2Wmd8BfnHC6oX66Ebgi9nyKLA8IlaemZYuzXx1Zea/ZOaxavFRWp9rglZdU5n5P5n5E2Afrfw8pUEL/EY+liEixoC3ALuA0cw8BK1fCsBr+teyjn0G+Cjwv9Xyq4AjbT+Yw9pvrwV+BvxDNV11d0Scy5D3WWYeBD4JPE0r6I8CMzSjz45bqI+alCl/DHyjet1RXYMW+DHPuqG+jSgiRoCvA3+amb/sd3vqioh3AYczc6Z99Ty7DmO/LQMuBT6fmW8BXmDIpm/mU81n3whcBPwucC6tqY4TDWOfnU4jfjYj4mO0pom/dHzVPLudtq5BC/xFPZZhWETES2mF/Zcy875q9XPH/6Ssvh/uV/s6dCVwQ0TspzXldhWtEf/yaroAhrffDgAHMnNXtbyd1i+AYe+zdwA/ycyfZeZvgPuAt9GMPjtuoT4a+kyJiPXAu4B1+dv76Duqa9ACvzGPZajmte8B9mbmp9o27QDWV6/XA/ef6bbVkZl3ZObqzByj1T/fzsx1wCPAu6vdhq4ugMz8KfBMRFxcrbqa1mO9h7rPaE3lXBER51Q/l8frGvo+a7NQH+0A3lfdrXMFcPT41M8wiNY/k7oduCEzf922aQdwS0ScHREX0XpT+t9Oe8LMHKgv4Dpa70b/J/CxfrenRh1/SOtPrMeAH1Rf19Ga794JPFl9P7/fba1R4wTwQPX6tdUP3D7ga8DZ/W5fhzW9Gdhd9ds/Ayua0GfAXwI/Bh4H/hE4e1j7DPgyrfcifkNrpLthoT6iNfXxuSpP9tC6U6nvNSyhrn205uqPZ8jft+3/saquJ4BrF3MNP2krSYUYtCkdSVKPGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXi/wBWb2p7Qqq0tAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of target sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len, batch_size): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SRC_MAX_SENTENCE_LEN = 40 \n",
    "TARG_MAX_SENTENCE_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "train_loader_, dev_loader_, test_loader_ = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([16, 40])\n",
      "tensor([[  5,   3, 171,   3, 409,   3, 184, 928,   3, 929, 361,  18,   3,   4,\n",
      "         327,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  5,   4, 199,   7,  70,  13,   4,   3, 200,   3,   3, 612,   3, 521,\n",
      "           1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 13,   3,   3,   3,   3,  15,   3,   3,  28, 522, 523, 233,   4, 521,\n",
      "          25, 261,  19, 201,  13, 732,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,   3, 234,   8, 199,   3, 216,  48,   1,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  5, 137,  53,  83, 287,   3,  29,  45,   5,  23, 930, 199,  71,  71,\n",
      "           3, 733,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 11,  49,  40,  57,  72, 288,   4, 159,   3,   8,  13,  43,   1,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  5, 461, 160, 931, 148, 928,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 12, 288,   4, 159,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  5,   3, 171,   3,   3,   3,   3, 138,  63,   3, 217, 734,   9,   3,\n",
      "           4,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 26,   7,  83, 121,   3, 100,   5,   3,   3,   3,  48,   5,   3,  57,\n",
      "          12, 362, 159,   3, 218, 120,  50,  21,   3,   3,   3,   3,   8,   1,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [ 15,   9,   6,   3,  24, 524,   4,   3, 185, 289,   1,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  6, 735,  44, 234, 122,   4,   3,  61, 186,  38, 149,  19, 932,   6,\n",
      "          44,  57,   3,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  6,  33,   3, 525,   3,   3,   3,  61, 410,  16,  19, 262,   6, 172,\n",
      "          44,  57,   3,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [159,   7,  12,   3,   3,   3,   6,   3, 191,  24,   3,   7,  12, 200,\n",
      "           3,   3,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  34, 933,  32,   3, 202,   3,   1,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  6,  25,  53,  35, 328, 934,  48, 411,   4, 329,   3, 363, 150, 935,\n",
      "          25,  34, 330,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2]])\n",
      "tensor([16, 15, 21,  9, 17, 13,  7,  5, 16, 28, 11, 18, 18, 17,  8, 18])\n",
      "torch.Size([16, 40])\n",
      "tensor([[  3,  50,   9,  20,   3,   4,   9, 209,   3,  68,  34, 387,   8,   6,\n",
      "         519,  11, 780,  12,  19, 256,   5,   1,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  19, 217,  20,   3,   8,   3, 781,  27,  67, 192,   4,   3,   3,\n",
      "           5,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  54,  20,  10, 181,   3,  27,  67, 241,  86,  20,   3,  87,   4,\n",
      "          48,   6, 781, 782,   3, 111,   5,   1,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,   3,   6, 436,  28, 631, 437,   3,  19, 217,   3,   5,   1,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,   9, 133,   3,  84,  32,  16, 632,   4,  24,   9,  82,  55,  13,\n",
      "          19, 217,  20,  52,   4,  52, 388,   5,   1,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,   3,  15,  26,  97,   8,  10, 300, 122,  60,   4,   3,  30, 112,\n",
      "           5,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  10, 387,  13,   9,  63, 115, 633,   5,   1,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  10, 300, 122,   5,   1,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  15,  55,   4,   9,  20, 242,  50,   6, 436, 193,  98, 194,   7,\n",
      "         128,  16, 438,  23, 218,   8,  97,   8, 122,   5,   1,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  22,  23,   6, 169, 276,  75,   4,   9,   3,  35,  10, 331,   8,\n",
      "           3,  19, 634, 783,   4,  51,  20,  85,   3,   3,   8,  39, 301, 332,\n",
      "           4,   8,  10, 389, 122,   5,   1,   2,   2,   2,   2,   2],\n",
      "        [  3,  16,  20,   6,  99, 126,  14, 439,  82,  39, 784,   5,   1,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3, 333, 103,   4,  14, 193,  10, 170,   3,  22,  13,  85,  34,  76,\n",
      "           3,  73,  14,  45,  78,   5,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  14,  76,   3,  44,   3,  12,   3,   3,  22,  16,  76, 785,  14,\n",
      "          45,  57,  69,   3,   5,   1,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,   6, 122,  20,  12,  10, 256,   4,  71, 108,   3,  11,  77,   3,\n",
      "          12,  34, 192, 440, 441,   5,   1,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  16,  20,   3,  12,   3,  24,   3, 786,  12,   3,   5,   1,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
      "        [  3,  14,  47, 334,  14,  45,   3,  44, 195,   3,   6, 787,   4,   6,\n",
      "         390,   7,  44, 788,   5,   1,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2]])\n",
      "tensor([22, 16, 22, 13, 23, 16, 10,  6, 25, 35, 13, 21, 20, 21, 13, 20])\n"
     ]
    }
   ],
   "source": [
    "# check that loader works \n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    print(targ_idxs.size())\n",
    "    print(targ_idxs)\n",
    "    print(targ_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_emb(word2vec, token2id): \n",
    "    \"\"\" Given word2vec model and the vocab's token2id, extract pretrained word embeddings \"\"\"\n",
    "    pretrained_emb = np.zeros((len(token2id), 300)) \n",
    "    for token in token2id: \n",
    "        try: \n",
    "            pretrained_emb[token2id[token]] = word2vec[token]\n",
    "        except: \n",
    "            pretrained_emb[token2id[token]] = np.random.normal(size=(300,))\n",
    "    return torch.from_numpy(pretrained_emb.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, enc_input_dim, enc_embed_dim, enc_hidden_dim, pretrained_word2vec): \n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.enc_input_dim = enc_input_dim \n",
    "#         self.enc_embed_dim = enc_embed_dim \n",
    "#         self.enc_hidden_dim = enc_hidden_dim \n",
    "#         self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "#         self.gru = nn.GRU(input_size=enc_embed_dim, hidden_size=enc_hidden_dim, num_layers=1, bidirectional=True)\n",
    "    \n",
    "#     def forward(self, enc_input, enc_input_lens):\n",
    "#         batch_size = enc_input.size()[0]\n",
    "#         _, idx_sort = torch.sort(enc_input_lens, dim=0, descending=True)\n",
    "#         _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "#         enc_input, enc_input_lens = enc_input.index_select(0, idx_sort), enc_input_lens.index_select(0, idx_sort)\n",
    "#         embedded = self.embedding(enc_input)\n",
    "#         embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, enc_input_lens, batch_first=True)\n",
    "#         hidden = self.initHidden(batch_size).to(device)\n",
    "#         output, hidden = self.gru(embedded, hidden)\n",
    "#         output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n",
    "#                                                            total_length=SRC_MAX_SENTENCE_LEN,\n",
    "#                                                            padding_value=RESERVED_TOKENS['<PAD>'])\n",
    "#         output = output.index_select(0, idx_unsort)\n",
    "#         final_hidden = torch.cat([output[:, -1, :self.enc_hidden_dim], \n",
    "#                                   output[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0) \n",
    "#         return output, final_hidden\n",
    "\n",
    "#     def initHidden(self, batch_size):\n",
    "#         return torch.zeros(2, batch_size, self.enc_hidden_dim, device=device)\n",
    "    \n",
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, dec_input_dim, dec_embed_dim, dec_hidden_dim, pretrained_word2vec):\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.dec_input_dim = dec_input_dim\n",
    "#         self.dec_embed_dim = dec_embed_dim\n",
    "#         self.dec_hidden_dim = dec_hidden_dim \n",
    "#         self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "#         self.gru = nn.GRU(dec_embed_dim, dec_hidden_dim)\n",
    "#         self.out = nn.Linear(dec_hidden_dim, dec_input_dim)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, dec_input, dec_hidden): \n",
    "#         batch_size = dec_input.size()[0]\n",
    "#         embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "#         output, hidden = self.gru(embedded, dec_hidden)\n",
    "#         output = self.softmax(self.out(output[0]))    \n",
    "#         return output, hidden\n",
    "    \n",
    "# class EncoderDecoder(nn.Module): \n",
    "#     def __init__(self, encoder, decoder, decoder_token2id): \n",
    "#         super(EncoderDecoder, self).__init__() \n",
    "#         self.encoder = encoder \n",
    "#         self.decoder = decoder \n",
    "#         self.output_dim = self.decoder.dec_input_dim\n",
    "#         self.output_seq_len = TARG_MAX_SENTENCE_LEN\n",
    "        \n",
    "#     def forward(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio): \n",
    "#         batch_size = src_idx.size()[0]\n",
    "#         enc_output, enc_hidden = self.encoder(src_idx, src_lens)\n",
    "#         dec_hidden = enc_hidden \n",
    "#         dec_outputs = Variable(torch.zeros(self.output_seq_len, batch_size, self.output_dim))\n",
    "#         hypotheses = Variable(torch.zeros(self.output_seq_len, batch_size))\n",
    "#         dec_output = targ_idx[:, 0] # initialize with <SOS>\n",
    "#         for di in range(1, self.output_seq_len): \n",
    "#             dec_output, dec_hidden = self.decoder(dec_output, dec_hidden)\n",
    "#             dec_outputs[di] = dec_output \n",
    "#             teacher_labels = targ_idx[:, di-1] \n",
    "#             greedy_labels = dec_output.data.max(1)[1]\n",
    "#             dec_output = teacher_labels if random.random() < teacher_forcing_ratio else greedy_labels \n",
    "#             hypotheses[di] = greedy_labels\n",
    "\n",
    "#         return dec_outputs, hypotheses.transpose(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with context vector \n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, enc_input_dim, enc_embed_dim, enc_hidden_dim, pretrained_word2vec): \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.enc_input_dim = enc_input_dim \n",
    "        self.enc_embed_dim = enc_embed_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(input_size=enc_embed_dim, hidden_size=enc_hidden_dim, num_layers=1, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, enc_input, enc_input_lens):\n",
    "        batch_size = enc_input.size()[0]\n",
    "        _, idx_sort = torch.sort(enc_input_lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        enc_input, enc_input_lens = enc_input.index_select(0, idx_sort), enc_input_lens.index_select(0, idx_sort)\n",
    "        embedded = self.embedding(enc_input)\n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, enc_input_lens, batch_first=True)\n",
    "        hidden = self.initHidden(batch_size).to(device)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n",
    "                                                           total_length=SRC_MAX_SENTENCE_LEN,\n",
    "                                                           padding_value=RESERVED_TOKENS['<PAD>'])\n",
    "        output = output.index_select(0, idx_unsort)\n",
    "        final_hidden = torch.cat([output[:, -1, :self.enc_hidden_dim], \n",
    "                                  output[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0) \n",
    "        return output, final_hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.enc_hidden_dim, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, dec_input_dim, dec_embed_dim, dec_hidden_dim, enc_hidden_dim, pretrained_word2vec):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.dec_input_dim = dec_input_dim\n",
    "        self.dec_embed_dim = dec_embed_dim\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(dec_embed_dim + enc_hidden_dim, dec_hidden_dim)\n",
    "        self.out = nn.Linear(dec_hidden_dim, dec_input_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "        context = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n",
    "                             enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden\n",
    "        \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder, decoder_token2id): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_dim = self.decoder.dec_input_dim\n",
    "        self.output_seq_len = TARG_MAX_SENTENCE_LEN\n",
    "\n",
    "    def forward(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio): \n",
    "        batch_size = src_idx.size()[0]\n",
    "        enc_outputs, enc_hidden = self.encoder(src_idx, src_lens)\n",
    "        dec_hidden = enc_hidden \n",
    "        dec_outputs = Variable(torch.zeros(self.output_seq_len, batch_size, self.output_dim))\n",
    "        hypotheses = Variable(torch.zeros(self.output_seq_len, batch_size))\n",
    "        dec_output = targ_idx[:, 0] # initialize with <SOS>\n",
    "        for di in range(1, self.output_seq_len): \n",
    "            dec_output, dec_hidden = self.decoder(dec_output, dec_hidden, enc_outputs)\n",
    "            dec_outputs[di] = dec_output \n",
    "            teacher_labels = targ_idx[:, di-1] \n",
    "            greedy_labels = dec_output.data.max(1)[1]\n",
    "            dec_output = teacher_labels if random.random() < teacher_forcing_ratio else greedy_labels \n",
    "            hypotheses[di] = greedy_labels\n",
    "\n",
    "        return dec_outputs, hypotheses.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention \n",
    "\n",
    "class Attention(nn.Module): \n",
    "    \n",
    "    \"\"\" Implements the attention mechanism by Bahdanau et al. (2015) \"\"\"\n",
    "    \n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim): \n",
    "        super(Attention, self).__init__() \n",
    "        self.num_annotations = enc_hidden_dim\n",
    "        self.input_dim = enc_hidden_dim * 2 + dec_hidden_dim\n",
    "        self.attn = nn.Linear(self.input_dim, self.num_annotations)\n",
    "        self.v = nn.Parameter(torch.rand(self.num_annotations))\n",
    "        \n",
    "    def forward(self, encoder_outputs, last_dec_hidden): \n",
    "        batch_size = encoder_outputs.size()[0]\n",
    "        hidden_broadcast = last_dec_hidden.repeat(1, self.num_annotations, 1)\n",
    "        v_broadcast = v.repeat(batch_size, self.num_annotations, self.input_dim)\n",
    "        concat = torch.cat([encoder_outputs, hidden_broadcast], dim=2)\n",
    "        energies = torch.bmm(v_broadcast, torch.nn.Tanh(self.attn(concat))) \n",
    "        attn_weights = F.softmax(energies, dim=2).squeeze(1)\n",
    "        return attn_weights\n",
    "\n",
    "class DecoderAttnRNN(nn.Module):\n",
    "    def __init__(self, dec_input_dim, dec_embed_dim, dec_hidden_dim, enc_hidden_dim, pretrained_word2vec):\n",
    "        super(DecoderAttnRNN, self).__init__()\n",
    "        self.dec_input_dim = dec_input_dim\n",
    "        self.dec_embed_dim = dec_embed_dim\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.attn = nn.Linear(self.enc_hidden_dim * 2, self.enc_hidden_dim)\n",
    "        self.gru = nn.GRU(dec_embed_dim + enc_hidden_dim, dec_hidden_dim)\n",
    "        self.out = nn.Linear(dec_hidden_dim, dec_input_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "        context = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n",
    "                             enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2corpus(tensor, id2token): \n",
    "    \"\"\" Takes a tensor (num_sentences x max_sentence_length) representing the corpus, \n",
    "        returns its string equivalent \n",
    "    \"\"\"\n",
    "    tensor = tensor.view(-1)\n",
    "    ignored_idx = [RESERVED_TOKENS[token] for token in ['<SOS>', '<EOS>', '<PAD>']] \n",
    "    filtered_list = [id2token[idx] for idx in tensor.numpy().astype(int).tolist() if idx not in ignored_idx] \n",
    "    corpus = ' '.join(filtered_list)\n",
    "    return corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, id2token): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "    reference_corpus = []\n",
    "    hypothesis_corpus = [] \n",
    "    \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader): \n",
    "        batch_size = src_idxs.size()[0]\n",
    "        outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.0)\n",
    "        outputs = outputs[1:].view(-1, TARG_VOCAB_SIZE)\n",
    "        targets = targ_idxs[:,1:]\n",
    "        hypothesis_corpus.append(hypotheses)\n",
    "        reference_corpus.append(targets)\n",
    " \n",
    "        loss = F.nll_loss(outputs.view(-1, TARG_VOCAB_SIZE), targets.contiguous().view(-1), \n",
    "                          ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "        total_loss += loss.item()  \n",
    "\n",
    "    # reconstruct corpus and compute bleu score \n",
    "    hypothesis_corpus = torch.cat(hypothesis_corpus, dim=0) \n",
    "    reference_corpus = torch.cat(reference_corpus, dim=0)\n",
    "    hypothesis_corpus = tensor2corpus(hypothesis_corpus, id2token)\n",
    "    reference_corpus = tensor2corpus(reference_corpus, id2token)\n",
    "    bleu_score = sacrebleu.corpus_bleu(hypothesis_corpus, reference_corpus).score\n",
    "    \n",
    "    return total_loss / len(loader), bleu_score, hypothesis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, dev_loader, id2token, learning_rate, num_epochs=3, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.5) \n",
    "            loss = criterion(final_outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 10 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "                result['train_loss'], result['train_bleu'], train_hypotheses = evaluate(model, train_loader, id2token) \n",
    "                result['val_loss'], result['val_bleu'], val_hypotheses = evaluate(model, dev_loader, id2token)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Loss: {:.2f}, Val Loss: {:.2f}, Train BLEU: {:.2f}, Val BLEU: {:.2f}'\\\n",
    "                          .format(result['epoch'], result['train_loss'], result['val_loss'], \n",
    "                                  result['train_bleu'], result['val_bleu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_outputs size is torch.Size([16, 40, 600])\n",
      "enc_final_hidden size is torch.Size([1, 16, 600])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "                                                            data['train']['source']['token2id']))\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader): \n",
    "    if i == 10: \n",
    "        enc_outputs, enc_final_hidden = encoder(src_idxs, src_lens)\n",
    "        print(\"enc_outputs size is {}\".format(enc_outputs.size()))\n",
    "        print(\"enc_final_hidden size is {}\".format(enc_final_hidden.size()))\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 6.11, Val Loss: 5.86, Train BLEU: 4.24, Val BLEU: 5.09\n",
      "Epoch: 0.13, Train Loss: 5.06, Val Loss: 4.73, Train BLEU: 3.89, Val BLEU: 4.94\n",
      "Epoch: 0.25, Train Loss: 5.05, Val Loss: 4.63, Train BLEU: 3.89, Val BLEU: 4.94\n",
      "Epoch: 0.38, Train Loss: 5.02, Val Loss: 4.63, Train BLEU: 3.89, Val BLEU: 4.94\n",
      "Epoch: 0.51, Train Loss: 4.97, Val Loss: 4.58, Train BLEU: 3.89, Val BLEU: 4.94\n",
      "Epoch: 0.63, Train Loss: 4.95, Val Loss: 4.61, Train BLEU: 3.89, Val BLEU: 4.94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-b74c4f6bc832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                             data['train']['target']['token2id']))\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token2id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id2token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-4d159b6bef58>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader, id2token, learning_rate, num_epochs, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_bleu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_bleu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-9b5ddf3ea8a5>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, id2token)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARG_VOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-52d155e3a035>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdec_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-52d155e3a035>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_input, enc_input_lens)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_input_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n\u001b[1;32m     23\u001b[0m                                                            \u001b[0mtotal_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSRC_MAX_SENTENCE_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflat_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mi_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mh_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "                                                            data['train']['source']['token2id']))\n",
    "decoder = DecoderRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=300, dec_hidden_dim=2*300, enc_hidden_dim=2*300, # fix \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                            data['train']['target']['token2id']))\n",
    "model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\n",
    "train(model, train_loader, dev_loader, data['train']['target']['id2token'], learning_rate=0.0005) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
