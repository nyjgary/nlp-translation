{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "import sacrebleu\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle as pkl\n",
    "import string\n",
    "import os\n",
    "from os import listdir \n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "\n",
    "pd.set_option('max_colwidth',100)\n",
    "mpl.style.use('bmh')\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp, lang_type): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]\n",
    "        if lang_type == 'source': \n",
    "            tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "        elif lang_type == 'target': \n",
    "            tokens_data = [['<SOS>'] + datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(num_vocab))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + list(vocab)\n",
    "    token2id = dict(zip(id2token, range(max_vocab_size)))\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    \n",
    "    # store language names \n",
    "    fps['languages'] = {} \n",
    "    fps['languages']['source'] = src_lang\n",
    "    fps['languages']['target'] = targ_lang \n",
    "    \n",
    "    # store filepaths \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "            \n",
    "    return fps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = 10000\n",
    "TARG_VOCAB_SIZE = 10000\n",
    "# ENC_EMBED_DIM = 300 \n",
    "# DEC_EMBED_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate language dictionaries from train sets \n",
    "\n",
    "def generate_vocab(src_lang, targ_lang, src_vocab_size, targ_vocab_size):\n",
    "    \"\"\" Outputs a nested dictionary containing token2id, id2token, and word embeddings \n",
    "    for source and target lang's vocab \"\"\"\n",
    "    \n",
    "    vocab = {} \n",
    "    for lang, vocab_size in zip([src_lang, targ_lang], [src_vocab_size, targ_vocab_size]): \n",
    "        \n",
    "        # load train data \n",
    "        train_data_fp = get_filepath(split='train', src_lang=SRC_LANG, targ_lang=TARG_LANG, \n",
    "                                     lang_type='target' if lang == 'en' else 'source')\n",
    "        with open(train_data_fp) as f:\n",
    "            train_tokens = [line.lower().split() for line in f.readlines()]        \n",
    "        \n",
    "        # load word embeddings, generate token2id and id2token \n",
    "        word2vec_full = load_word2vec(lang)\n",
    "        token2id, id2token = build_vocab(train_tokens, vocab_size, word2vec_full) \n",
    "        word2vec_reduced = {word: word2vec_full[word] for word in token2id if word not in RESERVED_TOKENS} \n",
    "        \n",
    "        # store token2id, id2token, and word embeddings as a dict in nested dict lang \n",
    "        vocab[lang] = {'token2id': token2id, 'id2token': id2token, 'word2vec': word2vec_reduced}\n",
    "        \n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new process_data that uses separate generate_vocab function \n",
    "\n",
    "def process_data(src_lang, targ_lang): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'], lang_type)\n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], vocab[data['languages'][lang_type]]['token2id'])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data(SRC_LANG, TARG_LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Source: ['你', '现在', '可以', '去', '个', '真正', '的', '学校', '念书', '了', '他', '说', '<EOS>']\n",
      "Example Target: ['<SOS>', '&quot;', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '&quot;', 'he', 'said', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# print example for sanity check  \n",
    "print(\"Example Source: {}\".format(data['train']['source']['tokens'][5]))\n",
    "print(\"Example Target: {}\".format(data['train']['target']['tokens'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of source sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check distribution of target sentence lengths \n",
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values=RESERVED_TOKENS['<PAD>'])\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len, batch_size): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SRC_MAX_SENTENCE_LEN = 40 \n",
    "TARG_MAX_SENTENCE_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "#train_loader_, dev_loader_, test_loader_ = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 40])\n",
      "tensor([[  5,   3, 331,  ...,   2,   2,   2],\n",
      "        [  5,   4, 684,  ...,   2,   2,   2],\n",
      "        [ 19,   3,   3,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [ 33,  19, 171,  ...,   2,   2,   2],\n",
      "        [  8,   3,   3,  ...,   2,   2,   2],\n",
      "        [  5,  49,  96,  ...,   2,   2,   2]])\n",
      "tensor([16, 15, 21,  9, 17, 13,  7,  5, 16, 28, 11, 18, 18, 17,  8, 18, 13,  9,\n",
      "         6,  7, 10, 16, 13, 36, 13,  4, 23, 19, 23, 15, 25, 28])\n",
      "torch.Size([32, 40])\n",
      "tensor([[   0,   52,   12,  ...,    2,    2,    2],\n",
      "        [   0,   40,  577,  ...,    2,    2,    2],\n",
      "        [   0,   32,   21,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,    8,  193,  ...,    2,    2,    2],\n",
      "        [   0,  529, 4353,  ...,    2,    2,    2],\n",
      "        [   0,   12,   74,  ...,    2,    2,    2]])\n",
      "tensor([22, 16, 22, 13, 23, 16, 10,  6, 25, 35, 13, 21, 20, 21, 13, 20, 21, 11,\n",
      "         7,  9, 17, 21, 12, 35, 10,  6, 24, 23, 26, 15, 29, 32])\n"
     ]
    }
   ],
   "source": [
    "# check that loader works \n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    print(targ_idxs.size())\n",
    "    print(targ_idxs)\n",
    "    print(targ_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_emb(word2vec, token2id): \n",
    "    \"\"\" Given word2vec model and the vocab's token2id, extract pretrained word embeddings \"\"\"\n",
    "    pretrained_emb = np.zeros((len(token2id), 300)) \n",
    "    for token in token2id: \n",
    "        try: \n",
    "            pretrained_emb[token2id[token]] = word2vec[token]\n",
    "        except: \n",
    "            pretrained_emb[token2id[token]] = np.random.normal(size=(300,))\n",
    "    return torch.from_numpy(pretrained_emb.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_dim, num_layers, pretrained_word2vec): \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.enc_embed_dim = 300\n",
    "        self.enc_hidden_dim = enc_hidden_dim \n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(input_size=self.enc_embed_dim, hidden_size=self.enc_hidden_dim, num_layers=self.num_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, enc_input, enc_input_lens):\n",
    "        batch_size = enc_input.size()[0]\n",
    "        _, idx_sort = torch.sort(enc_input_lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        enc_input, enc_input_lens = enc_input.index_select(0, idx_sort), enc_input_lens.index_select(0, idx_sort)\n",
    "        embedded = self.embedding(enc_input)\n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, enc_input_lens, batch_first=True)\n",
    "        hidden = self.initHidden(batch_size).to(device)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n",
    "                                                           total_length=SRC_MAX_SENTENCE_LEN,\n",
    "                                                           padding_value=RESERVED_TOKENS['<PAD>'])\n",
    "        output = output.index_select(0, idx_unsort)\n",
    "        hidden = hidden.index_select(1, idx_unsort).transpose(0, 1).contiguous().view(self.num_layers, batch_size, -1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2*self.num_layers, batch_size, self.enc_hidden_dim, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, dec_hidden_dim, enc_hidden_dim, num_layers, pretrained_word2vec):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.dec_embed_dim = 300\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(self.dec_embed_dim + 2 * self.enc_hidden_dim, self.dec_hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(dec_hidden_dim, TARG_VOCAB_SIZE)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "        context = torch.cat([enc_outputs[:, -1, :self.enc_hidden_dim], \n",
    "                             enc_outputs[:, 0, self.enc_hidden_dim:]], dim=1).unsqueeze(0)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden\n",
    "        \n",
    "class EncoderDecoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, encoder, decoder, decoder_token2id): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "\n",
    "    def forward(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio): \n",
    "        batch_size = src_idx.size()[0]\n",
    "        enc_outputs, enc_hidden = self.encoder(src_idx, src_lens)\n",
    "        dec_hidden = enc_hidden \n",
    "        dec_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size, TARG_VOCAB_SIZE))\n",
    "        hypotheses = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size))\n",
    "        dec_output = targ_idx[:, 0] # initialize with <SOS>\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "            dec_output, dec_hidden = self.decoder(dec_output, dec_hidden, enc_outputs)\n",
    "            dec_outputs[di] = dec_output \n",
    "            teacher_labels = targ_idx[:, di-1] \n",
    "            greedy_labels = dec_output.data.max(1)[1]\n",
    "            dec_output = teacher_labels if random.random() < teacher_forcing_ratio else greedy_labels \n",
    "            hypotheses[di] = greedy_labels\n",
    "\n",
    "        return dec_outputs, hypotheses.transpose(0,1)\n",
    "    \n",
    "class Attention(nn.Module): \n",
    "    \n",
    "    \"\"\" Implements the attention mechanism by Bahdanau et al. (2015) \"\"\"\n",
    "    \n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim, num_annotations, num_layers): \n",
    "        super(Attention, self).__init__() \n",
    "        self.num_annotations = num_annotations\n",
    "        self.input_dim = enc_hidden_dim * 2 + dec_hidden_dim\n",
    "        self.attn = nn.Linear(self.input_dim, self.num_annotations)\n",
    "        self.v = nn.Parameter(torch.rand(self.num_annotations))\n",
    "        self.num_layers = num_layers \n",
    "        nn.init.normal_(self.v)\n",
    "        \n",
    "    def forward(self, encoder_outputs, last_dec_hidden): \n",
    "        batch_size = encoder_outputs.size()[0]\n",
    "        last_dec_hidden = last_dec_hidden.transpose(0, 1)[:, -1, :].unsqueeze(1) \n",
    "        hidden_broadcast = last_dec_hidden.repeat(1, self.num_annotations, 1)\n",
    "        v_broadcast = self.v.repeat(batch_size, 1, 1)\n",
    "        concat = torch.cat([encoder_outputs, hidden_broadcast], dim=2)\n",
    "        energies = v_broadcast.bmm(torch.tanh(self.attn(concat)))\n",
    "        attn_weights = F.softmax(energies, dim=2).squeeze(1)\n",
    "        return attn_weights\n",
    "\n",
    "class DecoderAttnRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dec_hidden_dim, enc_hidden_dim, num_layers, pretrained_word2vec):\n",
    "        super(DecoderAttnRNN, self).__init__()\n",
    "        self.dec_embed_dim = 300\n",
    "        self.dec_hidden_dim = dec_hidden_dim \n",
    "        self.enc_hidden_dim = enc_hidden_dim\n",
    "        self.num_layers = num_layers \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.attn = Attention(self.enc_hidden_dim, self.dec_hidden_dim, \n",
    "                              num_annotations=SRC_MAX_SENTENCE_LEN, num_layers=self.num_layers)\n",
    "        self.gru = nn.GRU(self.dec_embed_dim + 2 * self.enc_hidden_dim, self.dec_hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.dec_hidden_dim, TARG_VOCAB_SIZE)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, dec_input, dec_hidden, enc_outputs): \n",
    "        batch_size = dec_input.size()[0]\n",
    "        embedded = self.embedding(dec_input).view(1, batch_size, -1)\n",
    "        attn_weights = self.attn(encoder_outputs=enc_outputs, last_dec_hidden=dec_hidden).unsqueeze(1)\n",
    "        context = attn_weights.bmm(enc_outputs).transpose(0, 1)\n",
    "        concat = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(concat, dec_hidden)\n",
    "        output = self.softmax(self.out(output[0]))    \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2corpus(tensor, id2token): \n",
    "    \"\"\" Takes a tensor (num_sentences x max_sentence_length) representing the corpus, \n",
    "        returns its string equivalent \n",
    "    \"\"\"\n",
    "    tensor = tensor.view(-1)\n",
    "    ignored_idx = [RESERVED_TOKENS[token] for token in ['<SOS>', '<EOS>', '<PAD>']] \n",
    "    filtered_list = [id2token[idx] for idx in tensor.numpy().astype(int).tolist() if idx not in ignored_idx] \n",
    "    corpus = ' '.join(filtered_list)\n",
    "    return corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, id2token): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "    reference_corpus = []\n",
    "    hypothesis_corpus = [] \n",
    "    \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader): \n",
    "        batch_size = src_idxs.size()[0]\n",
    "        outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.0)\n",
    "        outputs = outputs[1:].view(-1, TARG_VOCAB_SIZE)\n",
    "        targets = targ_idxs[:,1:]\n",
    "        hypothesis_corpus.append(hypotheses)\n",
    "        reference_corpus.append(targets)\n",
    " \n",
    "        loss = F.nll_loss(outputs.view(-1, TARG_VOCAB_SIZE), targets.contiguous().view(-1), \n",
    "                          ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "        total_loss += loss.item()  \n",
    "\n",
    "    # reconstruct corpus and compute bleu score \n",
    "    hypothesis_corpus = torch.cat(hypothesis_corpus, dim=0) \n",
    "    reference_corpus = torch.cat(reference_corpus, dim=0)\n",
    "    hypothesis_corpus = tensor2corpus(hypothesis_corpus, id2token)\n",
    "    reference_corpus = tensor2corpus(reference_corpus, id2token)\n",
    "    bleu_score = sacrebleu.corpus_bleu(hypothesis_corpus, reference_corpus).score\n",
    "    \n",
    "    return total_loss / len(loader), bleu_score, hypothesis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to save results to and load results from a pkl logfile \n",
    "\n",
    "RESULTS_LOG = 'experiment_results/experiment_results_log.pkl'\n",
    "\n",
    "def check_dir_exists(filename): \n",
    "    \"\"\" Helper function to check that the directory of filename exists, otherwise creates it \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    else: \n",
    "        pass \n",
    "        \n",
    "def append_to_log(hyperparams, results, runtime, experiment_name, dt_created, filename=RESULTS_LOG): \n",
    "    \"\"\" Appends results and details of a single experiment to a log file \"\"\"\n",
    "    \n",
    "    # check directory exists, else creates it \n",
    "    check_dir_exists(filename)\n",
    "        \n",
    "    # store experiment details in a dictionary \n",
    "    new_result = {'experiment_name': experiment_name, 'hyperparams': hyperparams, 'results': results, \n",
    "                  'runtime': runtime, 'dt_created': dt_created}\n",
    "    \n",
    "    # if log already exists, append to log \n",
    "    try: \n",
    "        results_log = pkl.load(open(filename, \"rb\"))\n",
    "        results_log.append(new_result)\n",
    "\n",
    "    # if log doesn't exists, initialize first result as the log \n",
    "    except (OSError, IOError) as e:\n",
    "        results_log = [new_result]\n",
    "    \n",
    "    # save to pickle \n",
    "    pkl.dump(results_log, open(filename, \"wb\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_log(experiment_name=None, filename=RESULTS_LOG): \n",
    "    \"\"\" Loads experiment log, with option to filter for a specific experiment_name \"\"\"\n",
    "    \n",
    "    results_log = pkl.load(open(filename, \"rb\"))\n",
    "    \n",
    "    if experiment_name is not None: \n",
    "        results_log = [r for r in results_log if r['experiment_name'] == experiment_name]\n",
    "        \n",
    "    return results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, dev_loader, id2token, learning_rate, num_epochs, \n",
    "                   print_intermediate, save_checkpoint, model_name, lazy_eval, inspect): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(ignore_index=RESERVED_TOKENS['<PAD>'])\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0.5) \n",
    "            loss = criterion(final_outputs[1:].view(-1, TARG_VOCAB_SIZE), targ_idxs[:,1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 100 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader) \n",
    "                result['val_loss'], result['val_bleu'], val_hypotheses = evaluate(model, dev_loader, id2token)\n",
    "                if lazy_eval: \n",
    "                    # eval on full train set is very expensive \n",
    "                    result['train_loss'], result['train_bleu'], train_hypotheses = 0, 0, None\n",
    "                else: \n",
    "                    result['train_loss'], result['train_bleu'], train_hypotheses = evaluate(model, train_loader, id2token)\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Loss: {:.2f}, Val Loss: {:.2f}, Train BLEU: {:.2f}, Val BLEU: {:.2f}'\\\n",
    "                          .format(result['epoch'], result['train_loss'], result['val_loss'], \n",
    "                                  result['train_bleu'], result['val_bleu']))\n",
    "                    \n",
    "                if inspect: \n",
    "                    inspect_model(model, data_split='train')\n",
    "                    inspect_model(model, data_split='val')\n",
    "                    \n",
    "                if save_checkpoint: \n",
    "                    if result['val_loss'] == pd.DataFrame.from_dict(results)['val_loss'].min(): \n",
    "                        checkpoint_fp = 'model_checkpoints/{}.pth.tar'.format(model_name)\n",
    "                        check_dir_exists(filename=checkpoint_fp)\n",
    "                        torch.save(model.state_dict(), checkpoint_fp)\n",
    "                \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_loader, dev_loader, model_type, num_epochs=10, learning_rate=0.0005, num_layers=2,\n",
    "                   enc_hidden_dim=300, dec_hidden_dim=2*300, experiment_name='NA', model_name='NA', inspect=True, \n",
    "                   lazy_eval=True, save_to_log=True, save_checkpoint=False, print_summary=True, print_intermediate=True):  \n",
    "    \n",
    "    \"\"\" Wraps all processing, training and evaluation steps in a function to facilitate hyperparam tuning. \n",
    "        Note that the function takes as input tokenized data rather than raw data since there's significant \n",
    "        lag time in generating tokens.  \n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time() \n",
    "    \n",
    "    # TODO: try dropout and optimization algorithms. for now use as default: \n",
    "    optimizer = 'Adam' \n",
    "    enc_dropout = 0 \n",
    "    dec_dropout = 0 \n",
    "    \n",
    "    # instantiate model and optimizer \n",
    "    if model_type == 'without_attention': \n",
    "        encoder = EncoderRNN(enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "        decoder = DecoderRNN(dec_hidden_dim=dec_hidden_dim, enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "        model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id']) \n",
    "        \n",
    "    elif model_type == 'attention_bahdanau': \n",
    "        encoder = EncoderRNN(enc_hidden_dim=enc_hidden_dim, num_layers=num_layers, \n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "        decoder = DecoderAttnRNN(dec_hidden_dim=dec_hidden_dim, enc_hidden_dim=enc_hidden_dim, num_layers=num_layers,\n",
    "                                 pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "        model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id'])\n",
    "        \n",
    "    else: \n",
    "        raise ValueError(\"Invalid model_type. Must be either 'without_attention' or 'attention_bahdanau'\")\n",
    "        \n",
    "    # train and evaluate \n",
    "    results = train_and_eval(model, train_loader, dev_loader, id2token=vocab[TARG_LANG]['id2token'], \n",
    "                             learning_rate=learning_rate, num_epochs=num_epochs, \n",
    "                             print_intermediate=print_intermediate, save_checkpoint=save_checkpoint, \n",
    "                             model_name=model_name, lazy_eval=lazy_eval, inspect=inspect)\n",
    "    \n",
    "    # store, print, and save results \n",
    "    runtime = (time.time() - start_time) / 60 \n",
    "    dt_created = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    hyperparams = {'model_type': model_type, 'num_epochs': num_epochs, 'learning_rate': learning_rate, \n",
    "                   'enc_hidden_dim': enc_hidden_dim, 'dec_hidden_dim': dec_hidden_dim, 'num_layers': num_layers, \n",
    "                   'optimizer': optimizer, 'enc_dropout': enc_dropout, 'dec_dropout': dec_dropout, \n",
    "                   'batch_size': BATCH_SIZE, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "                   'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "                   'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN}  \n",
    "        \n",
    "    if save_to_log: \n",
    "        append_to_log(hyperparams, results, runtime, experiment_name, dt_created)\n",
    "    if print_summary: \n",
    "        print(\"Experiment completed in {} minutes with {:.2f} validation loss and {:.2f} validation BLEU.\".format(\n",
    "            int(runtime), pd.DataFrame.from_dict(results)['val_loss'].min(), \n",
    "            pd.DataFrame.from_dict(results)['val_bleu'].max()))\n",
    "        \n",
    "    return results, hyperparams, runtime, model, train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to summarize, evaluate, and plot results \n",
    "\n",
    "def summarize_results(results_log): \n",
    "    \"\"\" Summarizes results_log (list) into a dataframe, splitting hyperparameters string into columns, and reducing \n",
    "        the val_acc dict into the best validation accuracy obtained amongst all the epochs logged \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results_log)\n",
    "    results_df = pd.concat([results_df, results_df['hyperparams'].apply(pd.Series)], axis=1)\n",
    "    results_df['val_loss'] = results_df['results'].apply(lambda d: pd.DataFrame.from_dict(d)['val_loss'].min())\n",
    "    return results_df.sort_values(by='val_loss', ascending=True) \n",
    "\n",
    "def plot_multiple_learning_curves(results_df, plot_variable, figsize=(8, 5), legend_loc='best'):\n",
    "    \"\"\" Plots learning curves of MULTIPLE experiments, includes only validation accuracy \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for index, row in results_df.iterrows():\n",
    "        val_loss_hist = pd.DataFrame.from_dict(row['results']).set_index('epoch')['val_loss'] \n",
    "        plt.plot(val_loss_hist, label=\"{} ({}%)\".format(row[plot_variable], val_loss_hist.max()))\n",
    "    plt.legend(title=plot_variable, loc=legend_loc)    \n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "def plot_single_learning_curve(results, figsize=(8, 5)): \n",
    "    \"\"\" Plots learning curve of a SINGLE experiment, includes both train and validation accuracy \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    results_df = results_df.set_index('epoch')\n",
    "    results_df.plot(figsize=figsize)\n",
    "    plt.ylabel('Validation Lossy')\n",
    "    plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to count parameters \n",
    "def count_parameters(model): \n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return all_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model(model, data_split, batch=6, num_samples=5): \n",
    "    \"\"\" Use the model and output translates for first num_samples in chosen batch in chosen loader \"\"\"\n",
    "    \n",
    "    # set loader based on data_split choice \n",
    "    if data_split == 'train': \n",
    "        loader = train_loader \n",
    "    elif data_split == 'val': \n",
    "        loader = dev_loader \n",
    "        \n",
    "    for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(loader):\n",
    "        if i == batch: \n",
    "            src_idxs = src_idxs[:num_samples, :]\n",
    "            targ_idxs = targ_idxs[:num_samples, :]\n",
    "            src_lens = src_lens[:num_samples]\n",
    "            targ_lens = targ_lens[:num_samples]              \n",
    "            output, hypotheses = model(src_idxs, targ_idxs, src_lens, targ_lens, teacher_forcing_ratio=0)\n",
    "            \n",
    "            if data_split == 'train': \n",
    "                print(\"Inspecting model on training data...\")\n",
    "            elif data_split == 'val': \n",
    "                print(\"Inspecting model on validation data...\")\n",
    "                \n",
    "            print(\"REFERENCE TRANSLATION: {}\".format(tensor2corpus(targ_idxs, vocab[TARG_LANG]['id2token'])))\n",
    "            print(\"MODEL TRANSLATION: {}\".format(tensor2corpus(torch.cat([hypotheses], dim=0), vocab[TARG_LANG]['id2token'])))\n",
    "            break \n",
    "        else: \n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 8.93, Train BLEU: 0.00, Val BLEU: 0.01\n",
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: and i believe the key to opening that door is great communication . we desperately need great communication from our scientists and engineers in order to change the world . our scientists and engineers are the ones that are <UNK> our <UNK> challenges , from energy to environment to health care , among others , and if we don <UNK> know about it and understand it , then the but these great conversations can <UNK> occur if our scientists and engineers don <UNK> invite us in to see their <UNK> . so scientists and engineers , please , talk nerdy to us .\n",
      "MODEL TRANSLATION: , , , , , , , are are are cap uranium ashamed ashamed jeff proved proved presence presence cap password donations we we we colleagues we colleagues we colleagues sharing sharing colleagues colleagues colleagues declared declared cap uranium uranium ashamed ashamed iranian , , , , , , , , , , , , , , , ,\n",
      "Inspecting model on validation data...\n",
      "REFERENCE TRANSLATION: and i believe the key to opening that door is great communication . we desperately need great communication from our scientists and engineers in order to change the world . our scientists and engineers are the ones that are <UNK> our <UNK> challenges , from energy to environment to health care , among others , and if we don <UNK> know about it and understand it , then the but these great conversations can <UNK> occur if our scientists and engineers don <UNK> invite us in to see their <UNK> . so scientists and engineers , please , talk nerdy to us .\n",
      "MODEL TRANSLATION: , , , , , , , are are are cap uranium ashamed ashamed jeff proved proved presence presence cap password donations we we we colleagues we colleagues we colleagues sharing sharing colleagues colleagues colleagues declared declared cap uranium uranium ashamed ashamed iranian , , , , , , , , , , , , , , , ,\n",
      "Epoch: 1.00, Train Loss: 0.00, Val Loss: 5.68, Train BLEU: 0.00, Val BLEU: 2.30\n",
      "Inspecting model on training data...\n",
      "REFERENCE TRANSLATION: and i believe the key to opening that door is great communication . we desperately need great communication from our scientists and engineers in order to change the world . our scientists and engineers are the ones that are <UNK> our <UNK> challenges , from energy to environment to health care , among others , and if we don <UNK> know about it and understand it , then the but these great conversations can <UNK> occur if our scientists and engineers don <UNK> invite us in to see their <UNK> . so scientists and engineers , please , talk nerdy to us .\n",
      "MODEL TRANSLATION: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> , , , , , , , , , , , , , , , , , , , , , , , , , , , , <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Inspecting model on validation data...\n",
      "REFERENCE TRANSLATION: and i believe the key to opening that door is great communication . we desperately need great communication from our scientists and engineers in order to change the world . our scientists and engineers are the ones that are <UNK> our <UNK> challenges , from energy to environment to health care , among others , and if we don <UNK> know about it and understand it , then the but these great conversations can <UNK> occur if our scientists and engineers don <UNK> invite us in to see their <UNK> . so scientists and engineers , please , talk nerdy to us .\n",
      "MODEL TRANSLATION: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> , , , , , , , , , , , , , , , , , , , , , , , , , , , , <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-541d1bd4c830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                    \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_run'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                    print_summary=True, print_intermediate=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-83c42e5990ba>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(train_loader, dev_loader, model_type, num_epochs, learning_rate, num_layers, enc_hidden_dim, dec_hidden_dim, experiment_name, model_name, inspect, lazy_eval, save_to_log, save_checkpoint, print_summary, print_intermediate)\u001b[0m\n\u001b[1;32m     37\u001b[0m                              \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                              \u001b[0mprint_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_intermediate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                              model_name=model_name, lazy_eval=lazy_eval, inspect=inspect)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# store, print, and save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-8c0cc508a8bc>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_loader, dev_loader, id2token, learning_rate, num_epochs, print_intermediate, save_checkpoint, model_name, lazy_eval, inspect)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARG_VOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-f3c9b4cca23b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_idx, targ_idx, src_lens, targ_lens, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# initialize with <SOS>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARG_MAX_SENTENCE_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mdec_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mteacher_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-f3c9b4cca23b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dec_input, dec_hidden, enc_outputs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results, hyperparams, runtime, model, train_loader, dev_loader = \\\n",
    "    run_experiment(train_loader, dev_loader, model_type='attention_bahdanau', num_epochs=10, learning_rate=0.0005,\n",
    "                   num_layers=2, enc_hidden_dim=300, dec_hidden_dim=2*300, experiment_name='test_run', \n",
    "                   model_name='test_run', inspect=True, lazy_eval=True, save_to_log=True, save_checkpoint=True, \n",
    "                   print_summary=True, print_intermediate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19632480, 13632480)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = summarize_results(load_experiment_log(experiment_name='test_run', filename=RESULTS_LOG))\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split BLEU and Loss \n",
    "plot_single_learning_curve(all_results.iloc[1]['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, num_layers=2, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "#                                                             data['train']['source']['token2id']))\n",
    "\n",
    "# decoder = DecoderRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=300, dec_hidden_dim=2*300, \n",
    "#                      enc_hidden_dim=300, num_layers=2, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "#                                                             data['train']['target']['token2id']))\n",
    "\n",
    "# decoder_attn = DecoderAttnRNN(dec_input_dim=TARG_VOCAB_SIZE, dec_embed_dim=300, dec_hidden_dim=2*300, \n",
    "#                               enc_hidden_dim=300, num_layers=2, \n",
    "#                               pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "#                                                                      data['train']['target']['token2id']))\n",
    "\n",
    "# #model = EncoderDecoder(encoder, decoder, data['train']['target']['token2id'])\n",
    "# model = EncoderDecoder(encoder, decoder_attn, data['train']['target']['token2id'])\n",
    "# train(model, train_loader, dev_loader, data['train']['target']['id2token'], num_epochs=20, learning_rate=0.0005) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(enc_input_dim=SRC_VOCAB_SIZE, enc_embed_dim=300, enc_hidden_dim=300, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "#                                                             data['train']['source']['token2id']))\n",
    "# attention = Attention(enc_hidden_dim=300, dec_hidden_dim=600, num_annotations=SRC_MAX_SENTENCE_LEN)\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader): \n",
    "#     enc_outputs, enc_final_hidden = encoder(src_idxs, src_lens)\n",
    "#     attn_weights = attention(encoder_outputs=enc_outputs, last_dec_hidden=enc_final_hidden)\n",
    "#     print(\"attn weights are: {}\".format(attn_weights.size()))\n",
    "#     print(\"example: {}\".format(attn_weights[0].sum()))\n",
    "# #     print(\"enc_outputs size is {}\".format(enc_outputs.size()))\n",
    "# #     print(\"enc_final_hidden size is {}\".format(enc_final_hidden.size()))\n",
    "    \n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
