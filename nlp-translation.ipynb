{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "from gensim.models import KeyedVectors\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "BATCH_SIZE = 32\n",
    "SRC_MAX_SENTENCE_LEN = 20 \n",
    "TARG_MAX_SENTENCE_LEN = 20\n",
    "SRC_VOCAB_SIZE = 50000\n",
    "TARG_VOCAB_SIZE = 5000\n",
    "HIDDEN_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]       \n",
    "        tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def load_word2vec(lang): \n",
    "    \"\"\" Loads pretrained vectors for a given language \"\"\"\n",
    "    filepath = \"data/pretrained_word2vec/wiki.zh.vec\".format(lang)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filepath)\n",
    "    return word2vec\n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "        Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "    \"\"\"\n",
    "    num_vocab = max_vocab_size - len(RESERVED_TOKENS)\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "    vocab, count = zip(*token_counter_filtered.most_common(num_vocab))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + list(vocab)\n",
    "    token2id = dict(zip(id2token, range(max_vocab_size)))\n",
    "    return token2id, id2token \n",
    "\n",
    "# def build_vocab(token_lists, max_vocab_size, word2vec): \n",
    "#     \"\"\" Takes lists of tokens (representing sentences of words), max_vocab_size, word2vec model and returns: \n",
    "#         - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "#         - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "#         Note that the vocab will comprise N=max_vocab_size-len(RESERVED_TOKENS) tokens that are in word2vec model \n",
    "#     \"\"\"\n",
    "#     all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "#     token_counter = Counter(all_tokens)\n",
    "#     token_counter_filtered = Counter({token: token_counter[token] for token in token_counter if token in word2vec})\n",
    "#     vocab, count = zip(*token_counter_filtered.most_common(max_vocab_size))\n",
    "#     id2token = list(vocab)\n",
    "#     token2id = dict(zip(vocab, range(len(RESERVED_TOKENS), len(RESERVED_TOKENS)+len(vocab))))\n",
    "#     id2token = list(RESERVED_TOKENS.keys()) + id2token \n",
    "#     for t in RESERVED_TOKENS: \n",
    "#         token2id[t] = RESERVED_TOKENS[t]\n",
    "#     return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "\n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    \n",
    "    # attach vocab sizes and word2vec models \n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size \n",
    "    data['train']['source']['word2vec'] = load_word2vec(src_lang) \n",
    "    data['train']['target']['word2vec'] = load_word2vec(targ_lang) \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'])\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    token_lists = data['train'][lang_type]['tokens'], \n",
    "                    max_vocab_size = data['train'][lang_type]['max_vocab_size'], \n",
    "                    word2vec = data['train'][lang_type]['word2vec']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_emb(word2vec, token2id): \n",
    "    \"\"\" Given word2vec model and the vocab's token2id, extract pretrained word embeddings \"\"\"\n",
    "    pretrained_emb = np.zeros((len(token2id), 300)) \n",
    "    for token in token2id: \n",
    "        try: \n",
    "            pretrained_emb[token2id[token]] = word2vec[token]\n",
    "        except: \n",
    "            pretrained_emb[token2id[token]] = np.random.normal(size=(300,))\n",
    "    return torch.from_numpy(pretrained_emb.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values= 0)\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values= 0)\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海', '海中', '的', '生命', '大卫', '盖罗', '<EOS>']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_test = get_pretrained_emb(data['train']['source']['word2vec'] , data['train']['source']['token2id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6391e-01,  1.0639e+00, -7.2269e-01,  3.8855e-01,  8.0669e-01,\n",
       "       -3.9772e-01, -6.4854e-01, -2.0340e-02,  3.2016e-01, -1.8594e-02,\n",
       "        7.5796e-01,  9.5875e-02,  1.3857e+00, -8.8656e-01,  8.9787e-02,\n",
       "       -7.8805e-01,  8.2156e-01, -4.5274e-01, -1.3599e+00, -5.8308e-01,\n",
       "       -1.2423e+00,  1.2513e+00,  1.3082e+00,  7.1150e-03,  5.6409e-01,\n",
       "       -5.0618e-01,  9.6601e-01,  2.2565e-01,  7.9516e-01,  2.9112e-01,\n",
       "        9.4196e-01, -1.0480e+00, -8.9431e-01, -6.9267e-01, -9.3206e-01,\n",
       "        1.1966e+00,  1.1582e+00, -5.6526e-01,  1.7063e-01,  1.4300e+00,\n",
       "        7.6709e-01,  1.2418e+00,  1.1196e+00, -1.2855e+00, -3.9796e-01,\n",
       "       -4.0813e-01, -7.4501e-01, -4.0786e-01,  5.6266e-01, -9.1049e-01,\n",
       "        1.2451e+00, -2.1408e-01, -5.1217e-01, -5.3418e-01,  1.1298e-01,\n",
       "       -1.1111e+00, -1.1714e+00, -1.3583e+00, -3.2776e+00,  4.4865e-01,\n",
       "        1.1381e+00, -1.4361e-01,  9.8352e-01,  1.3363e+00,  5.9738e-01,\n",
       "        1.0656e+00, -1.0906e+00, -5.8233e-01, -5.5804e-01, -3.8899e-01,\n",
       "        1.4958e+00, -9.1816e-01,  1.2217e+00, -5.6197e-01, -9.1967e-01,\n",
       "        2.8408e-01, -6.6672e-02, -8.2325e-01, -5.7572e-01, -1.1177e+00,\n",
       "       -2.5858e-01,  7.1498e-03,  8.9017e-01, -8.0305e-01, -8.1328e-01,\n",
       "       -8.9870e-01,  2.4938e-01,  5.9711e-01,  6.5360e-02, -1.0975e+00,\n",
       "       -7.6651e-01,  8.9634e-01, -1.3244e-01, -2.2426e-02, -9.0831e-01,\n",
       "        1.2333e+00,  8.6232e-01,  7.2508e-01, -1.7962e-02,  1.9577e-01,\n",
       "        6.4059e-01,  6.5498e-01, -5.7352e-01,  9.5553e-01, -6.8620e-01,\n",
       "        6.7117e-01,  2.7553e-02, -8.1152e-01, -7.4209e-01,  7.8452e-02,\n",
       "        2.1996e-01, -7.5938e-01, -1.3916e-01,  6.2511e-01,  2.4316e-01,\n",
       "        3.6103e-01, -7.3676e-01,  1.8879e-01, -6.0778e-01, -9.7829e-01,\n",
       "       -7.7176e-01,  9.3991e-01,  7.5902e-01,  2.3560e-02, -6.9437e-01,\n",
       "       -8.9607e-01,  7.3124e-01,  7.4123e-01, -1.1011e+00,  3.5925e-01,\n",
       "        8.1056e-01,  2.2771e-01, -9.0569e-01,  1.0399e+00, -1.1795e+00,\n",
       "        3.6409e-01, -4.0508e-01, -3.9343e-01, -8.9886e-01, -5.1658e-01,\n",
       "        7.8766e-02,  8.8267e-01,  1.2621e+00, -1.1790e+00, -8.3421e-01,\n",
       "        4.1145e-01, -1.5983e+00, -1.3313e-01, -8.3042e-01, -2.9095e-01,\n",
       "        5.6606e-01,  8.5247e-01, -1.0356e-01,  2.9817e-01,  1.0145e+00,\n",
       "       -8.0707e-01,  3.3252e-01, -4.1859e-01,  2.0578e-03,  7.4378e-01,\n",
       "        3.5127e-01,  3.4332e-01,  3.9476e-01,  7.9520e-03,  5.6837e-02,\n",
       "        1.9579e-01, -2.5149e-01,  8.7891e-01,  4.3723e-01,  3.7107e-01,\n",
       "        5.7137e-01,  6.8381e-01,  1.0813e+00, -4.7799e-02,  9.1588e-02,\n",
       "        1.1153e-01,  8.4817e-01,  8.6699e-01, -6.7174e-02, -6.0056e-01,\n",
       "       -7.6134e-01, -5.5208e-01,  3.0215e-01,  9.9575e-03, -4.6225e-01,\n",
       "        8.3512e-01, -1.0748e-02, -7.9808e-01, -5.0147e-01,  8.3243e-01,\n",
       "       -3.1315e-01, -8.3514e-02, -7.0079e-01, -5.6033e-01, -6.4789e-02,\n",
       "       -6.9048e-01, -8.8293e-01, -7.4071e-01,  2.6856e-02,  5.8349e-01,\n",
       "       -4.7795e-02, -4.4563e-01, -1.7875e-01, -6.0016e-01,  7.2793e-02,\n",
       "        1.5451e-01,  2.6200e-01, -4.7335e-01, -1.4017e-01,  4.0792e-01,\n",
       "       -1.0640e+00,  3.9888e-01, -5.1001e-01,  3.9962e-01, -8.5021e-01,\n",
       "        1.7305e-01,  2.8913e-01, -1.3508e-01,  9.3687e-01,  6.0144e-01,\n",
       "       -6.0228e-01,  7.9056e-01,  3.3801e-01, -5.7203e-02, -1.0779e+00,\n",
       "        8.2638e-01, -2.9985e-01, -2.3117e-01, -1.0227e+00,  5.4373e-02,\n",
       "       -8.8121e-01, -3.8100e-01,  3.6903e-01, -7.1654e-01, -8.2499e-02,\n",
       "        1.1802e+00,  3.9736e-01, -1.0854e+00, -2.0758e-01, -2.2772e-01,\n",
       "        6.9006e-01, -1.0682e+00,  7.2935e-01,  1.2665e-03, -2.4493e-01,\n",
       "       -2.4806e-02, -4.5564e-01, -2.3917e-01, -9.4934e-01,  1.1925e-01,\n",
       "       -4.0391e-01,  3.7711e-02, -8.3502e-01,  1.5569e-01, -4.1158e-01,\n",
       "       -6.2988e-02,  5.4909e-01,  7.5530e-01, -1.5463e+00, -3.9758e-02,\n",
       "        5.7422e-01, -4.0611e-01, -2.9411e-01, -1.6940e-01, -2.7391e-01,\n",
       "       -2.0142e-01, -8.7102e-01, -9.2617e-01, -1.8897e+00, -1.2419e+00,\n",
       "       -1.3943e-01,  4.7293e-01,  1.1383e+00, -6.6022e-01, -1.3899e-02,\n",
       "       -2.7761e-01,  1.7229e-02,  1.0169e+00, -1.1558e-01, -6.5791e-01,\n",
       "        6.1787e-01,  1.2840e+00,  1.8424e-01,  8.7510e-01,  3.0483e-02,\n",
       "       -2.2935e-01,  3.6822e-01,  4.3399e-01, -8.0390e-01, -2.0275e-02,\n",
       "       -7.2834e-01, -1.1894e+00, -7.6395e-01, -6.9246e-01, -1.1771e-03,\n",
       "       -8.1935e-01,  5.1514e-01,  6.5656e-02,  1.0131e+00,  3.2171e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['word2vec']['的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1639,  1.0639, -0.7227,  0.3886,  0.8067, -0.3977, -0.6485, -0.0203,\n",
       "         0.3202, -0.0186,  0.7580,  0.0959,  1.3857, -0.8866,  0.0898, -0.7880,\n",
       "         0.8216, -0.4527, -1.3599, -0.5831, -1.2423,  1.2513,  1.3082,  0.0071,\n",
       "         0.5641, -0.5062,  0.9660,  0.2256,  0.7952,  0.2911,  0.9420, -1.0480,\n",
       "        -0.8943, -0.6927, -0.9321,  1.1966,  1.1582, -0.5653,  0.1706,  1.4300,\n",
       "         0.7671,  1.2418,  1.1196, -1.2855, -0.3980, -0.4081, -0.7450, -0.4079,\n",
       "         0.5627, -0.9105,  1.2451, -0.2141, -0.5122, -0.5342,  0.1130, -1.1111,\n",
       "        -1.1714, -1.3583, -3.2776,  0.4487,  1.1381, -0.1436,  0.9835,  1.3363,\n",
       "         0.5974,  1.0656, -1.0906, -0.5823, -0.5580, -0.3890,  1.4958, -0.9182,\n",
       "         1.2217, -0.5620, -0.9197,  0.2841, -0.0667, -0.8232, -0.5757, -1.1177,\n",
       "        -0.2586,  0.0071,  0.8902, -0.8030, -0.8133, -0.8987,  0.2494,  0.5971,\n",
       "         0.0654, -1.0975, -0.7665,  0.8963, -0.1324, -0.0224, -0.9083,  1.2333,\n",
       "         0.8623,  0.7251, -0.0180,  0.1958,  0.6406,  0.6550, -0.5735,  0.9555,\n",
       "        -0.6862,  0.6712,  0.0276, -0.8115, -0.7421,  0.0785,  0.2200, -0.7594,\n",
       "        -0.1392,  0.6251,  0.2432,  0.3610, -0.7368,  0.1888, -0.6078, -0.9783,\n",
       "        -0.7718,  0.9399,  0.7590,  0.0236, -0.6944, -0.8961,  0.7312,  0.7412,\n",
       "        -1.1011,  0.3593,  0.8106,  0.2277, -0.9057,  1.0399, -1.1795,  0.3641,\n",
       "        -0.4051, -0.3934, -0.8989, -0.5166,  0.0788,  0.8827,  1.2621, -1.1790,\n",
       "        -0.8342,  0.4114, -1.5983, -0.1331, -0.8304, -0.2910,  0.5661,  0.8525,\n",
       "        -0.1036,  0.2982,  1.0145, -0.8071,  0.3325, -0.4186,  0.0021,  0.7438,\n",
       "         0.3513,  0.3433,  0.3948,  0.0080,  0.0568,  0.1958, -0.2515,  0.8789,\n",
       "         0.4372,  0.3711,  0.5714,  0.6838,  1.0813, -0.0478,  0.0916,  0.1115,\n",
       "         0.8482,  0.8670, -0.0672, -0.6006, -0.7613, -0.5521,  0.3022,  0.0100,\n",
       "        -0.4622,  0.8351, -0.0107, -0.7981, -0.5015,  0.8324, -0.3131, -0.0835,\n",
       "        -0.7008, -0.5603, -0.0648, -0.6905, -0.8829, -0.7407,  0.0269,  0.5835,\n",
       "        -0.0478, -0.4456, -0.1787, -0.6002,  0.0728,  0.1545,  0.2620, -0.4733,\n",
       "        -0.1402,  0.4079, -1.0640,  0.3989, -0.5100,  0.3996, -0.8502,  0.1731,\n",
       "         0.2891, -0.1351,  0.9369,  0.6014, -0.6023,  0.7906,  0.3380, -0.0572,\n",
       "        -1.0779,  0.8264, -0.2998, -0.2312, -1.0227,  0.0544, -0.8812, -0.3810,\n",
       "         0.3690, -0.7165, -0.0825,  1.1802,  0.3974, -1.0854, -0.2076, -0.2277,\n",
       "         0.6901, -1.0682,  0.7293,  0.0013, -0.2449, -0.0248, -0.4556, -0.2392,\n",
       "        -0.9493,  0.1192, -0.4039,  0.0377, -0.8350,  0.1557, -0.4116, -0.0630,\n",
       "         0.5491,  0.7553, -1.5463, -0.0398,  0.5742, -0.4061, -0.2941, -0.1694,\n",
       "        -0.2739, -0.2014, -0.8710, -0.9262, -1.8897, -1.2419, -0.1394,  0.4729,\n",
       "         1.1383, -0.6602, -0.0139, -0.2776,  0.0172,  1.0169, -0.1156, -0.6579,\n",
       "         0.6179,  1.2840,  0.1842,  0.8751,  0.0305, -0.2294,  0.3682,  0.4340,\n",
       "        -0.8039, -0.0203, -0.7283, -1.1894, -0.7639, -0.6925, -0.0012, -0.8194,\n",
       "         0.5151,  0.0657,  1.0131,  0.3217])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_test[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 20])\n",
      "tensor([[ 3318,  3071,     4,   204,  1606,     3,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1606,     3,   124,  2128,     3,  4192,     4,  1901,    39,     7,\n",
      "          1551,     9,   290,   118,  1840,   118, 13834,   198,    23,  3754],\n",
      "        [ 1606,     3,  1106,     6,     3,     3,     5,     6,  1606,     3,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    7,    52,    84,    67,  1901,  1818,   791,    67,  3318,  3020,\n",
      "             4,   146,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    7,    11,    15,  2319,  2042,     4,  8379,     3,     3,  3089,\n",
      "             4,  1901,  4470,   928,   195,  1765,   457,     1,     0,     0],\n",
      "        [ 8379,     3,     3,     3,  3089,  1244,     6,   597,     9,  2319,\n",
      "         16171,  4237,    36,   142,     3,    16,    71,    55,   144,     3],\n",
      "        [  282,   470,     7,   227,   384,    39,   585, 15269,     3,     3,\n",
      "          2661,     1,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  116,   852,   585,  1585,     9,   290, 17221,  1607,     4,     3,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  290,     4,   583,   596,   134,    22,     6,  3053,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  585,     4,   876,  3072,     6,     3,   821,     1,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   56,    10,   651,     8,  3891,    37,   528,    56,    10,    72,\n",
      "           585,    78,     4,   897,    56,    10,     3,   202,  1379,  5230],\n",
      "        [  242,  3594,   290,    37,  6746,     4,  3563,    22,     8,   585,\n",
      "            78,     1,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  583,   596,   134,     4,   248,    23,    22,   114,     8,   585,\n",
      "            78,     1,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  414,     3,   381,  2715,    14,  3685,  7859,    23,    22,   106,\n",
      "             3,   585,    78,     8,   585,     4,   118,  2566,     1,     0],\n",
      "        [  585,    78,   192,     4,  1211,     3,    14,  2230,    51,   185,\n",
      "          2676,     3,    82,   339,     1,     0,     0,     0,     0,     0],\n",
      "        [  702,   325,   745,    22,    38,    45,   710,  9992,    53,    81,\n",
      "            24,   951,     4,  8008,    16,   924,     3,     7,    71,    45],\n",
      "        [   36,     5,    49,   131,    10,     4,     6,    56,    10,   651,\n",
      "             8,  8571,    91,    10,   667,     4,     6,    12,   206,  2050],\n",
      "        [    7,     3,    64,  1079,     4,  2156,   406,     3,   929,   172,\n",
      "          2050,     4,    60,     1,     0,     0,     0,     0,     0,     0],\n",
      "        [    7,    84,     4,     6,  3318,  2128,     3, 20056,  1244,    14,\n",
      "          1522,     3,  1522,     3,     6,     3,     3,    14,     3,   693],\n",
      "        [    3,     3, 20057,     3,     3,    26,    96,   209,     4,   672,\n",
      "         16173,    55,   188,   156,     4,   100,   308,   188,   237,   156],\n",
      "        [   14,     7,   386,     4,    76,   916,     7,   237,     9,   156,\n",
      "             4,  1922,     7,   299,    72,     9,  1269,    88,   208,     4],\n",
      "        [   11,     6,   570,  3755,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    6,     5,   118,   196,     4,    31,    16,   845,    22,     3,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1715,    16,     6,   585,     3,  6746,     4,   192,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   16,    21,  5965,     3,     3,   942,   451,     1,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   72,    24,     8,  3168,     4,    87,     9,   121,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    5,  1279,   196,    24,    87,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 3564,    24,    22,     6, 22201,    65,     3, 17222,  2436,     3,\n",
      "             4,  9229,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  119,    24,  6865,    79,  1973,     3,     4,  7701,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   11,     6,    86,  3856, 16174,   248,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  242,    65,    22,     6,   243,  2879,     4,   248,   965,     3,\n",
      "             3,   153,    17,   574,     9,    46,   178,     4,    12,   192],\n",
      "        [  119,  1540,     4,    18,     6,  2420,  2143,    16,     3,     3,\n",
      "             3,     3,  1368,    16,   119,    67,   891,     1,     0,     0]])\n",
      "tensor([ 7, 20, 11, 13, 18, 20, 12, 11,  9,  8, 20, 12, 12, 19, 15, 20, 20, 14,\n",
      "        20, 20, 20,  5, 11,  9,  8,  9,  6, 13,  9,  7, 20, 18])\n"
     ]
    }
   ],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, pretrained_word2vec):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "#        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#        embedded = self.embedding(input).view(1, 1, -1)\n",
    "#        print(input)\n",
    "#         print(input.size())\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input).view(SRC_MAX_SENTENCE_LEN, batch_size, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, pretrained_word2vec):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "#        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_word2vec, freeze=True)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         print(\"Before embedding, targ[i-1] size is: {}\".format(input.size()))\n",
    "#         print(\"Before embedding, targ[i-1] is: {}\".format(input))     \n",
    "        batch_size = input.size()[0]\n",
    "        output = self.embedding(input).view(1, batch_size, -1)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "        output = F.relu(output)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "#         print(\"hidden size is: {}\".format(hidden.size()))        \n",
    "#         print(\"hidden is: {}\".format(hidden))        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))       \n",
    "#         output = output.squeeze(0) # B x N\n",
    "#         output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)\n",
    "    \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_size = self.decoder.output_size\n",
    "        \n",
    "    def forward(self, src, targ): \n",
    "        batch_size = src.size()[0]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_output, encoder_hidden = self.encoder(src, encoder_hidden)\n",
    "        decoder_hidden = encoder_hidden \n",
    "#        print(\"Encoder Hidden size: {}\".format(encoder_hidden.size()))\n",
    "        final_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size, self.output_size))\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "#            print(\"targ[di-1] Size is {}\".format(targ[:, di-1].size()))\n",
    "            decoder_outputs, decoder_hidden = self.decoder(targ[:, di-1], decoder_hidden)\n",
    "#            print(\"Final Outputs {} is {}\".format(di, decoder_outputs))\n",
    "            top1 = decoder_outputs.data.max(1)[1]\n",
    "#            print(\"Top 1 is {}\".format(top1))\n",
    "            final_outputs[di] = decoder_outputs\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for src_idxs, targ_idxs, src_lens, targ_lens in loader: \n",
    "        input_len = src_idxs.size()[0]\n",
    "        final_outputs = model(src_idxs, targ_idxs) \n",
    "        loss = criterion(final_outputs.view(input_len, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN), \n",
    "                         targ_idxs.view(input_len, TARG_MAX_SENTENCE_LEN))\n",
    "        total_loss += loss.item()  \n",
    "    \n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, dev_loader, num_epochs=3, learning_rate=0.1, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs = model(src_idxs, targ_idxs) \n",
    "            loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN), \n",
    "                             targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 10 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "#                result['train_loss'] = evaluate(model, train_loader) \n",
    "                result['val_loss'] = evaluate(model, dev_loader)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Validation Loss: {:.2f}'.format(\n",
    "                        result['epoch'], result['val_loss']))\n",
    "            \n",
    "        print(\"Epoch 1 {}: Total loss is {}\".format(i, total_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Validation Loss: 321.06\n",
      "Epoch: 0.00, Validation Loss: 3449.22\n",
      "Epoch: 0.00, Validation Loss: 4754.30\n",
      "Epoch: 0.00, Validation Loss: 5152.16\n",
      "Epoch: 0.01, Validation Loss: 5994.51\n",
      "Epoch: 0.01, Validation Loss: 6560.50\n",
      "Epoch: 0.01, Validation Loss: 6848.36\n",
      "Epoch: 0.01, Validation Loss: 7395.76\n",
      "Epoch: 0.01, Validation Loss: 7443.46\n",
      "Epoch: 0.01, Validation Loss: 7738.49\n",
      "Epoch: 0.01, Validation Loss: 8047.98\n",
      "Epoch: 0.02, Validation Loss: 8804.66\n",
      "Epoch: 0.02, Validation Loss: 9480.60\n",
      "Epoch: 0.02, Validation Loss: 9895.98\n",
      "Epoch: 0.02, Validation Loss: 10692.24\n",
      "Epoch: 0.02, Validation Loss: 10804.84\n",
      "Epoch: 0.02, Validation Loss: 10953.47\n",
      "Epoch: 0.03, Validation Loss: 11247.80\n",
      "Epoch: 0.03, Validation Loss: 11718.22\n",
      "Epoch: 0.03, Validation Loss: 11982.94\n",
      "Epoch: 0.03, Validation Loss: 11972.60\n",
      "Epoch: 0.03, Validation Loss: 12642.39\n",
      "Epoch: 0.03, Validation Loss: 12731.02\n",
      "Epoch: 0.03, Validation Loss: 12457.44\n",
      "Epoch: 0.04, Validation Loss: 12675.03\n",
      "Epoch: 0.04, Validation Loss: 12837.38\n",
      "Epoch: 0.04, Validation Loss: 12597.05\n",
      "Epoch: 0.04, Validation Loss: 12765.40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-02176462693d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                             data['train']['target']['token2id']))\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-219-c4f20574e083>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader, num_epochs, learning_rate, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m             loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE, TARG_MAX_SENTENCE_LEN), \n\u001b[1;32m     17\u001b[0m                              targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['source']['word2vec'],\n",
    "                                                            data['train']['source']['token2id']))\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                     pretrained_word2vec=get_pretrained_emb(data['train']['target']['word2vec'], \n",
    "                                                            data['train']['target']['token2id']))\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train(model, train_loader, dev_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=10)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=10)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(\"Targ shape is {}\".format(targ_idxs.size()))\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)    \n",
    "    final_outputs = model(src_idxs, targ_idxs)\n",
    "    print(final_outputs.size())\n",
    "    print(final_outputs)\n",
    "    print(targ_idxs.size())\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(input_size=10000, hidden_size=10)\n",
    "# decoder = DecoderRNN(hidden_size=10, output_size=10000)\n",
    "# encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     output, hidden = encoder(src_idxs, encoder_hidden)\n",
    "#     print(\"Output:::\")\n",
    "#     print(output.size())\n",
    "#     print(output)\n",
    "#     print(\"Hidden:::\")\n",
    "#     print(hidden.size())\n",
    "#     print(hidden)\n",
    "#     dec_output, dec_hidden = decoder(targ_idxs, hidden) \n",
    "#     print(\"Decoder Output:::\")\n",
    "#     print(dec_output.size())\n",
    "#     print(dec_output)    \n",
    "#     print(\"Decoder Hidden:::\")\n",
    "#     print(dec_hidden.size())\n",
    "#     print(dec_hidden)\n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
