{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from io import open\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESERVED_TOKENS = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "BATCH_SIZE = 32\n",
    "SRC_MAX_SENTENCE_LEN = 20 \n",
    "TARG_MAX_SENTENCE_LEN = 20\n",
    "SRC_VOCAB_SIZE = 1000\n",
    "TARG_VOCAB_SIZE = 1000\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tokens(raw_text_fp): \n",
    "    \"\"\" Takes filepath to raw text and outputs a list of lists, each representing a sentence of words (tokens) \"\"\"\n",
    "    with open(raw_text_fp) as f:\n",
    "        tokens_data = [line.lower().split() for line in f.readlines()]       \n",
    "        tokens_data = [datum + ['<EOS>'] for datum in tokens_data]\n",
    "    return tokens_data \n",
    "\n",
    "def build_vocab(token_lists, max_vocab_size): \n",
    "    \"\"\" Takes lists of tokens (representing sentences of words) and max_vocab_size and returns: \n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "    \"\"\"\n",
    "    all_tokens = [token for sublist in token_lists for token in sublist]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(len(RESERVED_TOKENS), len(RESERVED_TOKENS)+len(vocab))))\n",
    "    id2token = list(RESERVED_TOKENS.keys()) + id2token \n",
    "    for t in RESERVED_TOKENS: \n",
    "        token2id[t] = RESERVED_TOKENS[t]\n",
    "    return token2id, id2token \n",
    "\n",
    "def tokens2indices(tokens_data, token2id): \n",
    "    \"\"\" Takes tokenized data and token2id dictionary and returns indexed data \"\"\"\n",
    "    indices_data = [] \n",
    "    for datum in tokens_data: \n",
    "        indices_datum = [token2id[token] if token in token2id else RESERVED_TOKENS['<UNK>'] for token in datum ]\n",
    "        indices_data.append(indices_datum)    \n",
    "    return indices_data\n",
    "\n",
    "def get_filepath(split, src_lang, targ_lang, lang_type): \n",
    "    \"\"\" Locates data filepath given data split type (train/dev/test), translation pairs (src_lang -> targ_lang), \n",
    "        and the language type (source or target)\n",
    "    \"\"\"\n",
    "    folder_name = \"data/iwslt-{}-{}/\".format(src_lang, targ_lang)\n",
    "    if lang_type == 'source': \n",
    "        file_name = \"{}.tok.{}\".format(split, src_lang)\n",
    "    elif lang_type == 'target': \n",
    "        file_name = \"{}.tok.{}\".format(split, targ_lang)\n",
    "    return folder_name + file_name \n",
    "\n",
    "def get_filepaths(src_lang, targ_lang): \n",
    "    \"\"\" Takes language names to be translated from and to (in_lang and out_lang respectively) as inputs, \n",
    "        returns a nested dictionary containing the filepaths for input/output data for train/dev/test sets  \n",
    "    \"\"\"\n",
    "    fps = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        fps[split] = {} \n",
    "        for lang_type in ['source', 'target']: \n",
    "            fps[split][lang_type] = {} \n",
    "            fps[split][lang_type]['filepath'] = get_filepath(split, src_lang, targ_lang, lang_type)\n",
    "    return fps \n",
    "    \n",
    "def process_data(src_lang, targ_lang, src_max_vocab_size, targ_max_vocab_size): \n",
    "    \"\"\" Takes source language and target language names and respective max vocab sizes as inputs \n",
    "        and returns as a nested dictionary containing: \n",
    "        - train_indices, val_indices, test_indices (as lists of source-target tuples)\n",
    "        - train_tokens, val_tokens, test_tokens (as lists of source-target tuples)\n",
    "        - source language's token2id and id2token \n",
    "        - target language's token2id and id2token\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepaths \n",
    "    data = get_filepaths(src_lang, targ_lang)\n",
    "    data['train']['source']['max_vocab_size'] = src_max_vocab_size\n",
    "    data['train']['target']['max_vocab_size'] = targ_max_vocab_size    \n",
    "    \n",
    "    # loop through each file, read in text, convert to tokens, then to indices \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        for lang_type in ['source', 'target']: \n",
    "            \n",
    "            # read in tokens \n",
    "            data[split][lang_type]['tokens'] = text2tokens(data[split][lang_type]['filepath'])\n",
    "            \n",
    "            # build vocab from training data\n",
    "            if split == 'train': \n",
    "                data['train'][lang_type]['token2id'], data['train'][lang_type]['id2token'] = build_vocab(\n",
    "                    data['train'][lang_type]['tokens'], data['train'][lang_type]['max_vocab_size']) \n",
    "                \n",
    "            # convert tokens to indices \n",
    "            data[split][lang_type]['indices'] = tokens2indices(\n",
    "                data[split][lang_type]['tokens'], data['train'][lang_type]['token2id'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海', '海中', '的', '生命', '大卫', '盖罗', '<EOS>']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['source']['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['source']['indices']])).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array([len(l) for l in data['train']['target']['indices']])).hist(bins=100); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test/dataset that's readable for Pytorch. \n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_indices, targ_indices, src_max_sentence_len, targ_max_sentence_len):\n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of input indices and a list of output indices \n",
    "        \"\"\"\n",
    "        self.src_indices = src_indices\n",
    "        self.targ_indices = targ_indices\n",
    "        self.src_max_sentence_len = src_max_sentence_len\n",
    "        self.targ_max_sentence_len = targ_max_sentence_len\n",
    "        assert (len(self.src_indices) == len(self.targ_indices))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.src_indices)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\" \n",
    "        Triggered when dataset[i] is called, outputs lists of input and output indices, as well as their \n",
    "        respective lengths\n",
    "        \"\"\"\n",
    "        src_idx = self.src_indices[key][:self.src_max_sentence_len]\n",
    "        src_len = len(src_idx)\n",
    "        targ_idx = self.targ_indices[key][:self.targ_max_sentence_len]\n",
    "        targ_len = len(targ_idx)\n",
    "        return [src_idx, targ_idx, src_len, targ_len]\n",
    "    \n",
    "def collate_func(src_max_sentence_len, targ_max_sentence_len, batch): \n",
    "    \"\"\" Customized function for DataLoader that dynamically pads the batch so that all data have the same length\"\"\"\n",
    "    \n",
    "    src_idxs = [] \n",
    "    targ_idxs = [] \n",
    "    src_lens = [] \n",
    "    targ_lens = [] \n",
    "    \n",
    "    for datum in batch: \n",
    "        # append original lengths of sequences \n",
    "        src_lens.append(datum[2]) \n",
    "        targ_lens.append(datum[3])\n",
    "        \n",
    "        # pad sequences before appending \n",
    "        src_idx_padded = np.pad(array=np.array(datum[0]), pad_width = ((0, src_max_sentence_len - datum[2])), \n",
    "                                mode='constant', constant_values= 0)\n",
    "        targ_idx_padded = np.pad(array=np.array(datum[1]), pad_width = ((0, targ_max_sentence_len - datum[3])),\n",
    "                                 mode='constant', constant_values= 0)\n",
    "        src_idxs.append(src_idx_padded)\n",
    "        targ_idxs.append(targ_idx_padded)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(src_idxs)), torch.from_numpy(np.array(targ_idxs)), \n",
    "            torch.LongTensor(src_lens), torch.LongTensor(targ_lens)]\n",
    "\n",
    "def create_dataloaders(processed_data, src_max_sentence_len, targ_max_sentence_len): \n",
    "    \"\"\" Takes processed_data as dictionary output from process_data func, maximum sentence lengths, \n",
    "        and outputs train_loader, dev_loader, and test_loaders \n",
    "    \"\"\"\n",
    "    loaders = {} \n",
    "    for split in ['train', 'dev', 'test']: \n",
    "        dataset = TranslationDataset(data[split]['source']['indices'], data[split]['target']['indices'], \n",
    "                                     src_max_sentence_len, targ_max_sentence_len)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                    collate_fn=partial(collate_func, src_max_sentence_len, targ_max_sentence_len))\n",
    "    return loaders['train'], loaders['dev'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data('zh', 'en', src_max_vocab_size=SRC_VOCAB_SIZE, targ_max_vocab_size=TARG_VOCAB_SIZE)\n",
    "train_loader, dev_loader, test_loader = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 20])\n",
      "tensor([[   3,    3,    4,  206,    3,    3,    1,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   3,    3,  125,    3,    3,    3,    4,    3,   40,    8,    3,   10,\n",
      "          296,  119,    3,  119,    3,  199,   24,    3],\n",
      "        [   3,    3,    3,    7,    3,    3,    6,    7,    3,    3,    1,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   8,   53,   85,   68,    3,    3,  836,   68,    3,    3,    4,  147,\n",
      "            1,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   8,   12,   16,    3,    3,    4,    3,    3,    3,    3,    4,    3,\n",
      "            3,  984,  196,    3,  474,    1,    0,    0],\n",
      "        [   3,    3,    3,    3,    3,    3,    7,  625,   10,    3,    3,    3,\n",
      "           37,  143,  211,   17,   72,   56,  145,    3],\n",
      "        [ 287,  487,    8,  231,  394,   40,  612,    3,    3,    3,    3,    1,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 117,  900,  612,    3,   10,  296,    3,    3,    4,    3,    1,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 296,    4,  610,  624,  135,   23,    7,    3,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 612,    4,  926,    3,    7,    3,  868,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  57,   11,  684,    9,    3,   38,  546,   57,   11,   73,  612,   79,\n",
      "            4,  951,   57,   11,  649,  204,    3,    3],\n",
      "        [ 246,    3,  296,   38,    3,    4,    3,   23,    9,  612,   79,    1,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 610,  624,  135,    4,  253,   24,   23,  115,    9,  612,   79,    1,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 427,  490,  390,    3,   15,    3,    3,   24,   23,  107,  671,  612,\n",
      "           79,    9,  612,    4,  119,    3,    1,    0],\n",
      "        [ 612,   79,  193,    4,    3,    3,   15,    3,   52,  186,    3,    3,\n",
      "           83,  347,    1,    0,    0,    0,    0,    0],\n",
      "        [ 740,  331,  787,   23,   39,   46,  749,    3,   54,   82,   25,    3,\n",
      "            4,    3,   17,  980,    3,    8,   72,   46],\n",
      "        [  37,    6,   50,  132,   11,    4,    7,   57,   11,  684,    9,    3,\n",
      "           92,   11,  700,    4,    7,   13,  208,    3],\n",
      "        [   8,    3,   65,    3,    4,    3,  416,    3,  985,  173,    3,    4,\n",
      "           61,    1,    0,    0,    0,    0,    0,    0],\n",
      "        [   8,   85,    4,    7,    3,    3,    3,    3,    3,   15,    3,    3,\n",
      "            3,    3,    7,    3,    3,   15,    3,  728],\n",
      "        [   3,    3,    3,    3,    3,   27,   97,  212,    4,  705,    3,   56,\n",
      "          189,  157,    4,  101,  314,  189,  241,  157],\n",
      "        [  15,    8,  396,    4,   77,  970,    8,  241,   10,  157,    4,    3,\n",
      "            8,  305,   73,   10,    3,   89,  210,    4],\n",
      "        [  12,    7,  592,    3,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   7,    6,  119,  197,    4,   32,   17,  893,   23,    3,    1,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   3,   17,    7,  612,    3,    3,    4,  193,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  17,   22,    3,    3,    3, 1001,  466,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  73,   25,    9,    3,    4,   88,   10,  122,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   6,    3,  197,   25,   88,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   3,   25,   23,    7,    3,   66,    3,    3,    3,    3,    4,    3,\n",
      "            1,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 120,   25,    3,   80,    3,    3,    4,    3,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  12,    7,   87,    3,    3,  253,    1,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 246,   66,   23,    7,  248,    3,    4,  253,    3,    3,    3,  154,\n",
      "           18,  597,   10,   47,  179,    4,   13,  193],\n",
      "        [ 120,    3,    4,   19,    7,    3,    3,   17,    3,    3,    3,    3,\n",
      "            3,   17,  120,   68,  943,    1,    0,    0]])\n",
      "tensor([ 7, 20, 11, 13, 18, 20, 12, 11,  9,  8, 20, 12, 12, 19, 15, 20, 20, 14,\n",
      "        20, 20, 20,  5, 11,  9,  8,  9,  6, 13,  9,  7, 20, 18])\n"
     ]
    }
   ],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(src_idxs.size())\n",
    "    print(src_idxs)\n",
    "    print(src_lens)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#        embedded = self.embedding(input).view(1, 1, -1)\n",
    "#        print(input)\n",
    "#         print(input.size())\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input).view(SRC_MAX_SENTENCE_LEN, batch_size, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "#         print(\"Before embedding, targ[i-1] size is: {}\".format(input.size()))\n",
    "#         print(\"Before embedding, targ[i-1] is: {}\".format(input))     \n",
    "        batch_size = input.size()[0]\n",
    "        output = self.embedding(input).view(1, batch_size, -1)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "        output = F.relu(output)\n",
    "#         print(\"After embedding, targ[i-1] size is: {}\".format(output.size()))\n",
    "#         print(\"After embedding, targ[i-1] is: {}\".format(output))\n",
    "#         print(\"hidden size is: {}\".format(hidden.size()))        \n",
    "#         print(\"hidden is: {}\".format(hidden))        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))       \n",
    "#         output = output.squeeze(0) # B x N\n",
    "#         output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)\n",
    "    \n",
    "class EncoderDecoder(nn.Module): \n",
    "    def __init__(self, encoder, decoder): \n",
    "        super(EncoderDecoder, self).__init__() \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.output_size = self.decoder.output_size\n",
    "        \n",
    "    def forward(self, src, targ): \n",
    "        batch_size = src.size()[0]\n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        encoder_output, encoder_hidden = self.encoder(src, encoder_hidden)\n",
    "        decoder_hidden = encoder_hidden \n",
    "#        print(\"Encoder Hidden size: {}\".format(encoder_hidden.size()))\n",
    "        final_outputs = Variable(torch.zeros(TARG_MAX_SENTENCE_LEN, batch_size, self.output_size))\n",
    "        for di in range(1, TARG_MAX_SENTENCE_LEN): \n",
    "#            print(\"targ[di-1] Size is {}\".format(targ[:, di-1].size()))\n",
    "            decoder_outputs, decoder_hidden = self.decoder(targ[:, di-1], decoder_hidden)\n",
    "#            print(\"Final Outputs {} is {}\".format(di, decoder_outputs))\n",
    "            top1 = decoder_outputs.data.max(1)[1]\n",
    "#            print(\"Top 1 is {}\".format(top1))\n",
    "            final_outputs[di] = decoder_outputs\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for src_idxs, targ_idxs, src_lens, targ_lens in loader: \n",
    "        input_len = src_idxs.size()[0]\n",
    "        final_outputs = model(src_idxs, targ_idxs) \n",
    "        loss = criterion(final_outputs.view(input_len, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n",
    "                         targ_idxs.view(input_len, TARG_MAX_SENTENCE_LEN))\n",
    "        total_loss += loss.item()  \n",
    "    \n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, dev_loader, num_epochs=3, learning_rate=0.1, \n",
    "          print_intermediate=True, save_checkpoint=False, model_name='default'): \n",
    "    \n",
    "    # initialize optimizer and criterion \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    results = [] \n",
    "    \n",
    "    # loop through train data in batches and train \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0 \n",
    "        for batch, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            final_outputs = model(src_idxs, targ_idxs) \n",
    "            loss = criterion(final_outputs.view(BATCH_SIZE, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n",
    "                             targ_idxs.view(BATCH_SIZE, TARG_MAX_SENTENCE_LEN))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 10 == 0 or ((epoch==num_epochs-1) & (batch==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + batch / len(train_loader)\n",
    "#                result['train_loss'] = evaluate(model, train_loader) \n",
    "                result['val_loss'] = evaluate(model, dev_loader)\n",
    "                results.append(result)\n",
    "                \n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Validation Loss: {:.2f}'.format(\n",
    "                        result['epoch'], result['val_loss']))\n",
    "            \n",
    "        print(\"Epoch 1 {}: Total loss is {}\".format(i, total_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Validation Loss: 250.20\n",
      "Epoch: 0.00, Validation Loss: 465.78\n",
      "Epoch: 0.00, Validation Loss: 440.68\n",
      "Epoch: 0.00, Validation Loss: 459.26\n",
      "Epoch: 0.01, Validation Loss: 444.02\n",
      "Epoch: 0.01, Validation Loss: 467.10\n",
      "Epoch: 0.01, Validation Loss: 458.29\n",
      "Epoch: 0.01, Validation Loss: 449.41\n",
      "Epoch: 0.01, Validation Loss: 459.28\n",
      "Epoch: 0.01, Validation Loss: 431.67\n",
      "Epoch: 0.01, Validation Loss: 430.63\n",
      "Epoch: 0.02, Validation Loss: 425.03\n",
      "Epoch: 0.02, Validation Loss: 417.90\n",
      "Epoch: 0.02, Validation Loss: 407.76\n",
      "Epoch: 0.02, Validation Loss: 400.48\n",
      "Epoch: 0.02, Validation Loss: 386.37\n",
      "Epoch: 0.02, Validation Loss: 383.81\n",
      "Epoch: 0.03, Validation Loss: 390.30\n",
      "Epoch: 0.03, Validation Loss: 392.86\n",
      "Epoch: 0.03, Validation Loss: 383.31\n",
      "Epoch: 0.03, Validation Loss: 371.86\n",
      "Epoch: 0.03, Validation Loss: 360.88\n",
      "Epoch: 0.03, Validation Loss: 357.20\n",
      "Epoch: 0.03, Validation Loss: 355.02\n",
      "Epoch: 0.04, Validation Loss: 374.98\n",
      "Epoch: 0.04, Validation Loss: 369.01\n",
      "Epoch: 0.04, Validation Loss: 354.69\n",
      "Epoch: 0.04, Validation Loss: 341.44\n",
      "Epoch: 0.04, Validation Loss: 347.32\n",
      "Epoch: 0.04, Validation Loss: 341.24\n",
      "Epoch: 0.04, Validation Loss: 343.13\n",
      "Epoch: 0.05, Validation Loss: 326.69\n",
      "Epoch: 0.05, Validation Loss: 313.78\n",
      "Epoch: 0.05, Validation Loss: 313.11\n",
      "Epoch: 0.05, Validation Loss: 315.51\n",
      "Epoch: 0.05, Validation Loss: 320.55\n",
      "Epoch: 0.05, Validation Loss: 321.15\n",
      "Epoch: 0.06, Validation Loss: 322.49\n",
      "Epoch: 0.06, Validation Loss: 326.53\n",
      "Epoch: 0.06, Validation Loss: 319.19\n",
      "Epoch: 0.06, Validation Loss: 319.57\n",
      "Epoch: 0.06, Validation Loss: 318.95\n",
      "Epoch: 0.06, Validation Loss: 330.01\n",
      "Epoch: 0.06, Validation Loss: 318.61\n",
      "Epoch: 0.07, Validation Loss: 319.78\n",
      "Epoch: 0.07, Validation Loss: 313.79\n",
      "Epoch: 0.07, Validation Loss: 311.43\n",
      "Epoch: 0.07, Validation Loss: 323.49\n",
      "Epoch: 0.07, Validation Loss: 319.54\n",
      "Epoch: 0.07, Validation Loss: 318.20\n",
      "Epoch: 0.07, Validation Loss: 332.76\n",
      "Epoch: 0.08, Validation Loss: 334.27\n",
      "Epoch: 0.08, Validation Loss: 322.24\n",
      "Epoch: 0.08, Validation Loss: 328.56\n",
      "Epoch: 0.08, Validation Loss: 329.03\n",
      "Epoch: 0.08, Validation Loss: 319.48\n",
      "Epoch: 0.08, Validation Loss: 324.06\n",
      "Epoch: 0.09, Validation Loss: 324.77\n",
      "Epoch: 0.09, Validation Loss: 325.74\n",
      "Epoch: 0.09, Validation Loss: 328.88\n",
      "Epoch: 0.09, Validation Loss: 322.01\n",
      "Epoch: 0.09, Validation Loss: 324.90\n",
      "Epoch: 0.09, Validation Loss: 318.09\n",
      "Epoch: 0.09, Validation Loss: 317.21\n",
      "Epoch: 0.10, Validation Loss: 321.55\n",
      "Epoch: 0.10, Validation Loss: 329.76\n",
      "Epoch: 0.10, Validation Loss: 322.85\n",
      "Epoch: 0.10, Validation Loss: 316.63\n",
      "Epoch: 0.10, Validation Loss: 327.64\n",
      "Epoch: 0.10, Validation Loss: 326.46\n",
      "Epoch: 0.10, Validation Loss: 322.60\n",
      "Epoch: 0.11, Validation Loss: 324.09\n",
      "Epoch: 0.11, Validation Loss: 330.83\n",
      "Epoch: 0.11, Validation Loss: 330.66\n",
      "Epoch: 0.11, Validation Loss: 318.51\n",
      "Epoch: 0.11, Validation Loss: 331.11\n",
      "Epoch: 0.11, Validation Loss: 333.39\n",
      "Epoch: 0.12, Validation Loss: 334.84\n",
      "Epoch: 0.12, Validation Loss: 335.82\n",
      "Epoch: 0.12, Validation Loss: 338.94\n",
      "Epoch: 0.12, Validation Loss: 331.53\n",
      "Epoch: 0.12, Validation Loss: 331.49\n",
      "Epoch: 0.12, Validation Loss: 324.77\n",
      "Epoch: 0.12, Validation Loss: 322.83\n",
      "Epoch: 0.13, Validation Loss: 330.99\n",
      "Epoch: 0.13, Validation Loss: 322.83\n",
      "Epoch: 0.13, Validation Loss: 327.53\n",
      "Epoch: 0.13, Validation Loss: 324.45\n",
      "Epoch: 0.13, Validation Loss: 311.72\n",
      "Epoch: 0.13, Validation Loss: 317.16\n",
      "Epoch: 0.13, Validation Loss: 324.80\n",
      "Epoch: 0.14, Validation Loss: 332.87\n",
      "Epoch: 0.14, Validation Loss: 330.21\n",
      "Epoch: 0.14, Validation Loss: 331.65\n",
      "Epoch: 0.14, Validation Loss: 329.47\n",
      "Epoch: 0.14, Validation Loss: 324.39\n",
      "Epoch: 0.14, Validation Loss: 324.28\n",
      "Epoch: 0.15, Validation Loss: 331.31\n",
      "Epoch: 0.15, Validation Loss: 360.15\n",
      "Epoch: 0.15, Validation Loss: 348.66\n",
      "Epoch: 0.15, Validation Loss: 366.59\n",
      "Epoch: 0.15, Validation Loss: 414.02\n",
      "Epoch: 0.15, Validation Loss: 416.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-7811c2c30ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARG_VOCAB_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-6aedf4678dc5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, dev_loader, num_epochs, learning_rate, print_intermediate, save_checkpoint, model_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#                result['train_loss'] = evaluate(model, train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-ab4286042283>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfinal_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         loss = criterion(final_outputs.view(input_len, TARG_VOCAB_SIZE+4, TARG_MAX_SENTENCE_LEN), \n\u001b[1;32m     15\u001b[0m                          targ_idxs.view(input_len, TARG_MAX_SENTENCE_LEN))\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-9664fe8d3bab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, targ)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m#        print(\"Encoder Hidden size: {}\".format(encoder_hidden.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-9664fe8d3bab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_MAX_SENTENCE_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mresetgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0minputgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mnewgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresetgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewgate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnewgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE+4, hidden_size=HIDDEN_SIZE)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE+4, hidden_size=HIDDEN_SIZE)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "train(model, train_loader, dev_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=SRC_VOCAB_SIZE, hidden_size=10)\n",
    "decoder = DecoderRNN(output_size=TARG_VOCAB_SIZE, hidden_size=10)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(\"Targ shape is {}\".format(targ_idxs.size()))\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)    \n",
    "    final_outputs = model(src_idxs, targ_idxs)\n",
    "    print(final_outputs.size())\n",
    "    print(final_outputs)\n",
    "    print(targ_idxs.size())\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(input_size=10000, hidden_size=10)\n",
    "# decoder = DecoderRNN(hidden_size=10, output_size=10000)\n",
    "# encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(train_loader):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     output, hidden = encoder(src_idxs, encoder_hidden)\n",
    "#     print(\"Output:::\")\n",
    "#     print(output.size())\n",
    "#     print(output)\n",
    "#     print(\"Hidden:::\")\n",
    "#     print(hidden.size())\n",
    "#     print(hidden)\n",
    "#     dec_output, dec_hidden = decoder(targ_idxs, hidden) \n",
    "#     print(\"Decoder Output:::\")\n",
    "#     print(dec_output.size())\n",
    "#     print(dec_output)    \n",
    "#     print(\"Decoder Hidden:::\")\n",
    "#     print(dec_hidden.size())\n",
    "#     print(dec_hidden)\n",
    "#     break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
