{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from data_processing import generate_vocab, process_data, create_dataloaders \n",
    "from model import get_pretrained_emb, EncoderDecoder, EncoderRNN, DecoderRNN, EncoderDecoderAttn, DecoderAttnRNN\n",
    "from train_eval import count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "from train_eval import train_and_eval\n",
    "import importlib\n",
    "import pickle as pkl \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model identification\n",
    "MODEL_NAME = 'zh-seq2seq-rnn-attn'\n",
    "SRC_LANG = 'zh'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "# data processing params  \n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 #30000\n",
    "TARG_VOCAB_SIZE = 30000 #30000\n",
    "\n",
    "# model architecture params \n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 #2 \n",
    "ENC_HIDDEN_DIM = 256 #512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM #2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 # to actually implement\n",
    "DEC_DROPOUT = 0.2 # to actually implement\n",
    "USE_ATTN = False\n",
    "\n",
    "# training params  \n",
    "BATCH_SIZE = 32 #32\n",
    "NUM_EPOCHS = 200\n",
    "LR = 0.0003 # 0.0005\n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, 'rnn_cell_type': RNN_CELL_TYPE, \n",
    "          'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, 'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, \n",
    "          'src_vocab_size': SRC_VOCAB_SIZE, 'targ_vocab_size': TARG_VOCAB_SIZE, \n",
    "          'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, 'dec_hidden_dim': DEC_HIDDEN_DIM,\n",
    "          'teacher_forcing_ratio': TEACHER_FORCING_RATIO, 'clip_grad_max_norm': CLIP_GRAD_MAX_NORM,\n",
    "          'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, 'use_attn': USE_ATTN, \n",
    "          'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, 'learning_rate': LR, 'optimizer': OPTIMIZER, \n",
    "          'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, vocab)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, vocab, sample_limit=BATCH_SIZE) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, vocab, sample_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab-fake.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_filename = \"{}-{}-vocab-fake.p\".format(SRC_LANG, TARG_LANG)\n",
    "# vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "# data = process_data(SRC_LANG, TARG_LANG, vocab)\n",
    "# limited_data = process_data(SRC_LANG, TARG_LANG, vocab, sample_limit=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                     src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=ENC_DROPOUT, \n",
    "                     pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "\n",
    "# without attention \n",
    "# decoder = DecoderRNN(dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
    "#                      targ_vocab_size=TARG_VOCAB_SIZE, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "#                      pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "# model = EncoderDecoder(encoder, decoder, vocab[TARG_LANG]['token2id'])\n",
    "\n",
    "# with attention \n",
    "decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                         num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, src_max_sentence_len=SRC_MAX_SENTENCE_LEN, \n",
    "                         targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, dec_dropout=DEC_DROPOUT, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], vocab[TARG_LANG]['token2id']))\n",
    "model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.23, Train BLEU: 0.00, Val BLEU: 0.20, Minutes Elapsed: 0.09\n",
      "Sampling from val predictions...\n",
      "Source: 掌声 <UNK> 学院 是 全美 美国 的 顶级 文理 <UNK>\n",
      "Reference: when i returned to afghanistan , my grandfather ,\n",
      "Model: <SOS> saturn the the the the milkweeds milkweeds milkweeds milkweeds\n",
      "Attention Weights: tensor([[0.0990, 0.0956, 0.0956, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081],\n",
      "        [0.0990, 0.0956, 0.0956, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081],\n",
      "        [0.0990, 0.0956, 0.0957, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1068,\n",
      "         0.1080],\n",
      "        [0.0990, 0.0956, 0.0957, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081],\n",
      "        [0.0989, 0.0956, 0.0956, 0.0951, 0.0956, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081],\n",
      "        [0.0990, 0.0956, 0.0956, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081],\n",
      "        [0.0990, 0.0957, 0.0956, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081],\n",
      "        [0.0990, 0.0957, 0.0956, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081],\n",
      "        [0.0990, 0.0957, 0.0956, 0.0951, 0.0955, 0.0976, 0.1008, 0.1058, 0.1069,\n",
      "         0.1081]], grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 1.00, Train Loss: 0.00, Val Loss: 4.89, Train BLEU: 0.00, Val BLEU: 4.93, Minutes Elapsed: 56.87\n",
      "Sampling from val predictions...\n",
      "Source: 你 可以 又 一次 看到 波纹 在 桌子 上 展开\n",
      "Reference: you can watch the ripples again washing over the\n",
      "Model: <SOS> you can see see the , the the the\n",
      "Attention Weights: tensor([[9.9726e-01, 2.6840e-03, 2.7948e-05, 2.2784e-05, 3.5884e-06, 3.0612e-07,\n",
      "         2.6516e-07, 7.4052e-08, 2.6532e-08, 1.8169e-08],\n",
      "        [9.5524e-01, 4.1481e-02, 2.2741e-03, 8.3849e-04, 1.5160e-04, 6.4477e-06,\n",
      "         7.3417e-06, 2.2053e-06, 1.1415e-06, 8.2199e-07],\n",
      "        [4.6473e-01, 3.7059e-01, 1.0786e-01, 5.0707e-02, 5.6455e-03, 2.2885e-04,\n",
      "         1.6834e-04, 4.1726e-05, 1.7125e-05, 9.8697e-06],\n",
      "        [4.8928e-03, 2.7030e-02, 1.7467e-01, 5.6721e-01, 2.1220e-01, 6.6325e-03,\n",
      "         5.5487e-03, 1.0956e-03, 4.2258e-04, 2.9296e-04],\n",
      "        [3.3884e-03, 7.1689e-03, 7.3234e-02, 5.2496e-01, 3.3256e-01, 2.8344e-02,\n",
      "         2.3174e-02, 4.8192e-03, 1.2412e-03, 1.1055e-03],\n",
      "        [4.5086e-03, 1.4699e-03, 9.7074e-03, 2.0117e-01, 4.2881e-01, 1.9158e-01,\n",
      "         1.1611e-01, 3.9803e-02, 3.9946e-03, 2.8462e-03],\n",
      "        [8.3760e-03, 1.7336e-03, 7.7464e-03, 9.8381e-02, 3.1148e-01, 2.2867e-01,\n",
      "         2.0527e-01, 1.0433e-01, 1.8797e-02, 1.5220e-02],\n",
      "        [1.8622e-02, 2.9039e-03, 5.7770e-03, 6.1892e-02, 4.0191e-01, 1.1306e-01,\n",
      "         2.3567e-01, 9.9845e-02, 3.1491e-02, 2.8821e-02],\n",
      "        [1.0483e-02, 2.1155e-03, 5.0984e-03, 4.5431e-02, 3.0169e-01, 1.2648e-01,\n",
      "         2.5287e-01, 1.4099e-01, 5.6459e-02, 5.8384e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 2.00, Train Loss: 0.00, Val Loss: 4.55, Train BLEU: 0.00, Val BLEU: 5.87, Minutes Elapsed: 113.67\n",
      "Sampling from val predictions...\n",
      "Source: 它们 帮 我们 记录 保存 回忆 和 我们 的 过去\n",
      "Reference: they &apos;re our <UNK> and our histories , the\n",
      "Model: <SOS> they &apos;re us to to , and and and\n",
      "Attention Weights: tensor([[9.9589e-01, 4.0949e-03, 1.3227e-05, 7.8889e-07, 3.4484e-07, 1.3160e-07,\n",
      "         2.7231e-08, 5.1253e-09, 5.5556e-09, 8.9793e-09],\n",
      "        [6.6414e-01, 3.1157e-01, 1.8138e-02, 5.0936e-03, 7.8365e-04, 1.8617e-04,\n",
      "         3.8433e-05, 1.2228e-05, 1.4115e-05, 1.7576e-05],\n",
      "        [8.8805e-02, 8.3325e-01, 4.4799e-02, 2.9562e-02, 3.0424e-03, 4.2917e-04,\n",
      "         5.9203e-05, 1.8799e-05, 1.9079e-05, 1.5429e-05],\n",
      "        [1.9302e-02, 3.6824e-01, 2.0392e-01, 3.4265e-01, 5.0923e-02, 1.1922e-02,\n",
      "         1.7377e-03, 4.0198e-04, 4.4262e-04, 4.6716e-04],\n",
      "        [3.8786e-02, 1.9531e-01, 2.7955e-01, 3.5562e-01, 9.2720e-02, 2.9865e-02,\n",
      "         5.1553e-03, 1.0028e-03, 9.9812e-04, 9.9082e-04],\n",
      "        [9.4133e-03, 4.3554e-02, 9.2554e-02, 3.7285e-01, 2.7985e-01, 1.6944e-01,\n",
      "         2.5450e-02, 2.1508e-03, 2.3550e-03, 2.3907e-03],\n",
      "        [6.8521e-03, 6.0227e-03, 9.3332e-02, 7.8723e-02, 1.8117e-01, 4.3331e-01,\n",
      "         1.1972e-01, 5.1027e-02, 9.0303e-03, 2.0809e-02],\n",
      "        [9.3974e-04, 1.0663e-03, 1.7787e-02, 5.1272e-02, 1.6900e-01, 4.0006e-01,\n",
      "         1.7090e-01, 8.4440e-02, 3.0113e-02, 7.4423e-02],\n",
      "        [4.6811e-04, 1.4816e-03, 4.1985e-03, 3.0257e-02, 9.9576e-02, 4.2506e-01,\n",
      "         2.1925e-01, 5.6173e-02, 4.2427e-02, 1.2111e-01]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 3.00, Train Loss: 0.00, Val Loss: 4.41, Train BLEU: 0.00, Val BLEU: 6.51, Minutes Elapsed: 170.49\n",
      "Sampling from val predictions...\n",
      "Source: 我 不是 那个 国家 的 公民 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: i wasn &apos;t a citizen of that country .\n",
      "Model: <SOS> i &apos;m not a a . . . .\n",
      "Attention Weights: tensor([[9.9169e-01, 8.2481e-03, 2.4379e-05, 2.3855e-05, 6.1109e-06, 6.0110e-06,\n",
      "         1.8176e-07, 4.4382e-07, 4.4382e-07, 4.4382e-07],\n",
      "        [9.6544e-01, 3.3292e-02, 8.0564e-04, 2.7917e-04, 7.5713e-05, 6.9732e-05,\n",
      "         1.4614e-05, 7.1208e-06, 7.1208e-06, 7.1208e-06],\n",
      "        [8.2314e-01, 1.5273e-01, 1.6930e-02, 3.5573e-03, 2.2942e-03, 5.9042e-04,\n",
      "         1.1120e-04, 2.1264e-04, 2.1264e-04, 2.1264e-04],\n",
      "        [5.2402e-03, 2.0944e-01, 5.5435e-01, 1.7751e-01, 2.8616e-02, 2.1350e-02,\n",
      "         1.5746e-03, 6.4140e-04, 6.4140e-04, 6.4140e-04],\n",
      "        [1.2177e-03, 1.1615e-02, 2.7782e-01, 3.4294e-01, 2.3501e-01, 1.2638e-01,\n",
      "         1.8541e-03, 1.0548e-03, 1.0548e-03, 1.0548e-03],\n",
      "        [7.0647e-04, 2.4523e-03, 2.0356e-01, 3.8427e-01, 1.7535e-01, 2.1930e-01,\n",
      "         8.8429e-03, 1.8343e-03, 1.8343e-03, 1.8343e-03],\n",
      "        [2.5285e-03, 2.8726e-03, 6.3583e-02, 5.3251e-01, 6.9224e-02, 3.0420e-01,\n",
      "         1.3496e-02, 3.8631e-03, 3.8631e-03, 3.8631e-03],\n",
      "        [3.9828e-02, 2.1704e-02, 1.1948e-01, 3.8318e-01, 4.4276e-02, 2.1710e-01,\n",
      "         1.2299e-01, 1.7144e-02, 1.7144e-02, 1.7144e-02],\n",
      "        [1.2481e-02, 2.5128e-02, 4.2880e-02, 2.8473e-01, 3.2190e-02, 2.6356e-01,\n",
      "         2.1858e-01, 4.0149e-02, 4.0149e-02, 4.0149e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 4.00, Train Loss: 0.00, Val Loss: 4.36, Train BLEU: 0.00, Val BLEU: 6.87, Minutes Elapsed: 227.20\n",
      "Sampling from val predictions...\n",
      "Source: 你 的 下 一步 计划 是 什么 <EOS> <PAD> <PAD>\n",
      "Reference: what &apos;s the next one on your list ?\n",
      "Model: <SOS> what &apos;s you task task you <EOS> ? ?\n",
      "Attention Weights: tensor([[2.0138e-02, 6.6042e-01, 3.0395e-01, 1.5274e-02, 2.1113e-04, 7.9499e-06,\n",
      "         1.1487e-06, 1.0910e-07, 1.4005e-07, 1.4005e-07],\n",
      "        [5.1053e-02, 2.6580e-01, 5.4029e-01, 1.3751e-01, 5.2270e-03, 8.9231e-05,\n",
      "         2.9287e-05, 3.4194e-06, 2.7182e-06, 2.7182e-06],\n",
      "        [4.1523e-01, 1.4968e-01, 2.3015e-01, 1.4065e-01, 6.1354e-02, 1.9954e-03,\n",
      "         6.3091e-04, 1.4289e-04, 8.3180e-05, 8.3180e-05],\n",
      "        [1.8593e-01, 1.5374e-01, 2.3153e-01, 2.0024e-01, 2.1926e-01, 5.6906e-03,\n",
      "         1.5924e-03, 1.0043e-03, 5.0395e-04, 5.0395e-04],\n",
      "        [3.7924e-01, 7.0966e-02, 1.1002e-01, 1.0904e-01, 2.5578e-01, 2.8727e-02,\n",
      "         1.2716e-02, 1.7102e-02, 8.2080e-03, 8.2080e-03],\n",
      "        [7.0708e-01, 1.8164e-02, 3.8754e-02, 4.3416e-02, 1.2459e-01, 4.4699e-02,\n",
      "         6.2530e-03, 6.9555e-03, 5.0416e-03, 5.0416e-03],\n",
      "        [5.7016e-01, 4.8378e-02, 6.4521e-02, 7.3837e-02, 1.6061e-01, 3.4972e-02,\n",
      "         1.0446e-02, 1.7843e-02, 9.6138e-03, 9.6138e-03],\n",
      "        [5.9901e-01, 5.4540e-02, 7.7857e-02, 7.8336e-02, 1.0645e-01, 3.1448e-02,\n",
      "         1.0375e-02, 2.4392e-02, 8.7994e-03, 8.7994e-03],\n",
      "        [1.0232e-01, 1.0897e-02, 1.6728e-02, 3.8287e-02, 1.6947e-01, 3.6781e-01,\n",
      "         9.2896e-02, 1.6554e-01, 1.8026e-02, 1.8026e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 5.00, Train Loss: 0.00, Val Loss: 4.33, Train BLEU: 0.00, Val BLEU: 7.08, Minutes Elapsed: 284.11\n",
      "Sampling from val predictions...\n",
      "Source: 我们 与 客户 在 餐馆 和 酒吧 见面 <EOS> <PAD>\n",
      "Reference: we meet at the cafe . we meet at\n",
      "Model: <SOS> we &apos;re a to with and the we to\n",
      "Attention Weights: tensor([[9.2525e-01, 6.5840e-02, 8.3843e-03, 1.5745e-04, 2.4154e-04, 1.0393e-04,\n",
      "         1.1672e-05, 1.7020e-06, 7.3545e-07, 4.4537e-06],\n",
      "        [6.0399e-02, 7.8655e-01, 1.4204e-01, 5.0311e-03, 2.2471e-03, 3.1235e-03,\n",
      "         5.3992e-04, 5.5387e-05, 7.3338e-06, 4.6681e-06],\n",
      "        [5.6724e-02, 5.7747e-01, 2.5599e-01, 6.2051e-02, 2.0460e-02, 2.1700e-02,\n",
      "         4.6408e-03, 8.9746e-04, 2.6973e-05, 3.6256e-05],\n",
      "        [3.5064e-03, 3.4675e-01, 3.9277e-01, 6.7265e-02, 1.4182e-01, 3.2696e-02,\n",
      "         1.1315e-02, 2.3646e-03, 9.1239e-04, 6.0126e-04],\n",
      "        [1.1112e-03, 1.9350e-01, 1.4573e-01, 1.9175e-01, 3.9615e-01, 5.8583e-02,\n",
      "         8.7759e-03, 2.7940e-03, 1.0947e-03, 5.1224e-04],\n",
      "        [2.6916e-03, 3.7975e-02, 1.4999e-01, 1.8689e-01, 5.1639e-01, 9.2312e-02,\n",
      "         7.5076e-03, 2.6683e-03, 1.0202e-03, 2.5585e-03],\n",
      "        [7.0314e-03, 7.2059e-02, 1.8747e-01, 3.2229e-01, 3.3744e-01, 5.1970e-02,\n",
      "         6.1841e-03, 5.4788e-03, 2.0050e-03, 8.0825e-03],\n",
      "        [1.0933e-03, 2.7428e-02, 3.5846e-02, 2.0351e-01, 6.1356e-01, 8.9007e-02,\n",
      "         1.5634e-02, 1.0291e-02, 1.6211e-03, 2.0085e-03],\n",
      "        [2.9180e-03, 2.6635e-02, 1.4714e-02, 2.3196e-01, 2.2491e-01, 2.5256e-01,\n",
      "         9.3346e-02, 1.4231e-01, 8.6389e-03, 2.0094e-03]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6.00, Train Loss: 0.00, Val Loss: 4.32, Train BLEU: 0.00, Val BLEU: 7.11, Minutes Elapsed: 341.12\n",
      "Sampling from val predictions...\n",
      "Source: 每 一个 病例 的 情况 都 是 特殊 的 所以\n",
      "Reference: it manifests in each individual differently , hence why\n",
      "Model: <SOS> each every a , that a every a it\n",
      "Attention Weights: tensor([[9.9469e-01, 2.6427e-03, 7.7885e-04, 9.5503e-04, 8.7187e-04, 5.4283e-05,\n",
      "         5.2816e-06, 5.0130e-06, 2.3613e-07, 7.2512e-08],\n",
      "        [3.5431e-01, 1.2935e-01, 3.0058e-01, 8.7446e-02, 1.0326e-01, 1.5398e-02,\n",
      "         5.0570e-03, 4.5088e-03, 5.7602e-05, 2.3181e-05],\n",
      "        [9.0875e-02, 1.4581e-01, 5.2828e-01, 1.0289e-01, 6.2304e-02, 2.8460e-02,\n",
      "         3.1129e-02, 8.9435e-03, 9.9784e-04, 3.1226e-04],\n",
      "        [7.9444e-02, 3.8548e-02, 2.4427e-02, 2.1282e-02, 1.2395e-01, 3.0868e-02,\n",
      "         2.4388e-01, 3.4603e-01, 5.6266e-02, 3.5311e-02],\n",
      "        [3.8860e-01, 9.1560e-02, 3.5513e-02, 1.9836e-02, 9.5928e-02, 1.1131e-02,\n",
      "         5.7334e-02, 1.0057e-01, 8.3440e-02, 1.1608e-01],\n",
      "        [2.5436e-01, 8.6141e-02, 1.7802e-01, 4.8399e-02, 1.9540e-01, 2.4514e-02,\n",
      "         2.1896e-02, 8.3726e-02, 3.9059e-02, 6.8482e-02],\n",
      "        [3.1808e-01, 1.2331e-01, 1.5633e-01, 6.3227e-03, 3.0936e-02, 1.4764e-02,\n",
      "         2.3232e-02, 7.6119e-02, 5.0736e-02, 2.0017e-01],\n",
      "        [1.2374e-01, 8.4367e-02, 2.3732e-02, 3.0086e-03, 2.5828e-02, 4.0079e-02,\n",
      "         3.6722e-02, 1.5582e-01, 7.1857e-02, 4.3485e-01],\n",
      "        [4.5268e-02, 1.8177e-02, 3.5124e-02, 2.7006e-03, 1.0222e-02, 5.3311e-02,\n",
      "         6.6576e-02, 4.5325e-01, 9.4162e-02, 2.2121e-01]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 7.00, Train Loss: 0.00, Val Loss: 4.34, Train BLEU: 0.00, Val BLEU: 7.22, Minutes Elapsed: 398.03\n",
      "Sampling from val predictions...\n",
      "Source: 这些 光会 反射 射进 进入 房间 一些 会 返回 回到\n",
      "Reference: it &apos;s going to bounce , go inside the\n",
      "Model: <SOS> these these the to be the the the the\n",
      "Attention Weights: tensor([[3.2255e-01, 6.7579e-01, 1.5976e-03, 2.9814e-05, 1.9721e-05, 8.4000e-06,\n",
      "         3.5067e-06, 1.5673e-07, 2.0653e-07, 1.5467e-07],\n",
      "        [8.3327e-02, 4.7841e-01, 3.9039e-01, 1.1816e-02, 5.8268e-03, 2.3297e-02,\n",
      "         6.7166e-03, 1.4490e-04, 4.3962e-05, 2.1611e-05],\n",
      "        [8.8038e-02, 3.4900e-01, 3.9201e-01, 6.8143e-02, 4.4959e-02, 2.9370e-02,\n",
      "         2.6534e-02, 1.6204e-03, 1.9052e-04, 1.3371e-04],\n",
      "        [4.3120e-02, 1.6797e-01, 1.9635e-01, 1.2624e-01, 9.1811e-02, 2.1719e-01,\n",
      "         1.2724e-01, 1.0548e-02, 1.0607e-02, 8.9233e-03],\n",
      "        [1.1166e-01, 2.4229e-01, 1.5903e-01, 8.5099e-02, 1.0335e-01, 2.4311e-01,\n",
      "         4.6057e-02, 6.6500e-03, 1.5200e-03, 1.2354e-03],\n",
      "        [6.2526e-02, 3.4746e-01, 1.3387e-01, 4.0693e-02, 9.2342e-02, 2.3644e-01,\n",
      "         6.9436e-02, 6.0915e-03, 5.3018e-03, 5.8494e-03],\n",
      "        [1.6268e-02, 1.3870e-01, 2.4065e-01, 1.0706e-01, 5.0555e-02, 3.9347e-01,\n",
      "         4.3075e-02, 4.0759e-03, 3.3885e-03, 2.7620e-03],\n",
      "        [4.4790e-03, 1.0294e-01, 1.8923e-01, 5.1388e-02, 1.2961e-01, 4.5848e-01,\n",
      "         5.3745e-02, 6.9378e-03, 2.0264e-03, 1.1609e-03],\n",
      "        [1.2339e-02, 1.3763e-01, 1.2749e-01, 3.4493e-02, 1.4112e-01, 3.1922e-01,\n",
      "         1.9823e-01, 1.8877e-02, 6.3351e-03, 4.2535e-03]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 8.00, Train Loss: 0.00, Val Loss: 4.37, Train BLEU: 0.00, Val BLEU: 7.04, Minutes Elapsed: 454.97\n",
      "Sampling from val predictions...\n",
      "Source: 但是 总 需要 有人 维护 它 <EOS> <PAD> <PAD> <PAD>\n",
      "Reference: but you have to maintain it . <EOS> <PAD>\n",
      "Model: <SOS> but it needs to have somebody . <EOS> &apos;s\n",
      "Attention Weights: tensor([[6.7286e-04, 9.9906e-01, 2.6410e-04, 1.7693e-07, 2.2144e-09, 5.9457e-11,\n",
      "         1.7696e-12, 2.5672e-12, 2.5672e-12, 2.5672e-12],\n",
      "        [1.8168e-04, 9.9896e-01, 8.4999e-04, 4.0534e-06, 2.1384e-07, 2.4601e-09,\n",
      "         1.2354e-10, 8.2417e-10, 8.2417e-10, 8.2417e-10],\n",
      "        [6.7438e-02, 7.1108e-01, 1.7150e-01, 4.0736e-02, 8.5114e-03, 6.0487e-04,\n",
      "         3.9339e-05, 3.1616e-05, 3.1616e-05, 3.1616e-05],\n",
      "        [1.1308e-01, 2.1035e-01, 4.7098e-01, 1.6133e-01, 4.0204e-02, 2.1290e-03,\n",
      "         1.1515e-04, 6.0346e-04, 6.0346e-04, 6.0346e-04],\n",
      "        [1.7481e-02, 5.0726e-02, 9.5824e-02, 3.7113e-01, 3.7993e-01, 6.4231e-02,\n",
      "         7.0190e-03, 4.5515e-03, 4.5515e-03, 4.5515e-03],\n",
      "        [1.5731e-02, 1.8763e-02, 4.5178e-02, 5.6813e-01, 3.1675e-01, 2.8853e-02,\n",
      "         2.2125e-03, 1.4595e-03, 1.4595e-03, 1.4595e-03],\n",
      "        [6.8923e-03, 1.5312e-02, 1.7742e-02, 3.7690e-01, 4.3841e-01, 1.2651e-01,\n",
      "         8.3884e-03, 3.2804e-03, 3.2804e-03, 3.2804e-03],\n",
      "        [6.6127e-03, 2.2444e-02, 1.4041e-02, 1.9569e-01, 4.3847e-01, 2.6986e-01,\n",
      "         2.6187e-02, 8.8961e-03, 8.8961e-03, 8.8961e-03],\n",
      "        [6.9541e-03, 2.7922e-02, 1.0481e-02, 2.0639e-01, 1.8969e-01, 3.9108e-01,\n",
      "         1.4395e-01, 7.8457e-03, 7.8457e-03, 7.8457e-03]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 9.00, Train Loss: 0.00, Val Loss: 4.38, Train BLEU: 0.00, Val BLEU: 6.91, Minutes Elapsed: 512.31\n",
      "Sampling from val predictions...\n",
      "Source: 非常 非常感谢 感谢 肯尼 肯尼亚 尼亚 <UNK> <UNK> 在 我\n",
      "Reference: <UNK> <UNK> . it means in my language ,\n",
      "Model: <SOS> thank you thank thank you very for for for\n",
      "Attention Weights: tensor([[9.6754e-01, 3.1761e-02, 4.3267e-04, 9.5132e-05, 1.1327e-04, 1.1296e-05,\n",
      "         1.1094e-05, 6.8731e-06, 7.6063e-06, 1.9486e-05],\n",
      "        [8.1979e-01, 1.0771e-01, 5.6005e-02, 5.9746e-03, 6.8454e-03, 1.6348e-03,\n",
      "         6.2862e-04, 6.7542e-04, 4.5153e-04, 2.8509e-04],\n",
      "        [6.0993e-01, 1.3503e-01, 2.0685e-01, 1.2577e-02, 1.6759e-02, 1.3090e-02,\n",
      "         2.3360e-03, 1.4893e-03, 1.1425e-03, 8.0329e-04],\n",
      "        [5.3381e-02, 8.8701e-02, 2.6068e-01, 1.4910e-01, 2.3441e-01, 1.5699e-01,\n",
      "         1.8000e-02, 1.4559e-02, 1.6127e-02, 8.0564e-03],\n",
      "        [3.1902e-03, 2.1539e-02, 1.0292e-01, 4.6271e-02, 1.5708e-01, 1.8302e-01,\n",
      "         7.5986e-02, 1.1210e-01, 1.9937e-01, 9.8527e-02],\n",
      "        [4.4951e-02, 4.6103e-02, 2.1111e-01, 4.8554e-02, 1.3514e-01, 2.3680e-01,\n",
      "         8.8539e-02, 6.7837e-02, 5.6709e-02, 6.4252e-02],\n",
      "        [4.1128e-02, 1.2215e-01, 3.8273e-01, 1.2582e-01, 1.1509e-01, 9.0664e-02,\n",
      "         2.8628e-02, 3.5065e-02, 3.9406e-02, 1.9323e-02],\n",
      "        [1.6375e-02, 1.2989e-01, 4.2823e-01, 7.8354e-02, 1.3738e-01, 9.9454e-02,\n",
      "         3.7209e-02, 3.0395e-02, 2.7666e-02, 1.5041e-02],\n",
      "        [1.0853e-02, 2.4007e-02, 6.1101e-02, 7.1258e-02, 2.0596e-01, 4.0732e-01,\n",
      "         9.4110e-02, 7.3073e-02, 2.4966e-02, 2.7356e-02]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 10.00, Train Loss: 0.00, Val Loss: 4.42, Train BLEU: 0.00, Val BLEU: 6.85, Minutes Elapsed: 569.71\n",
      "Sampling from val predictions...\n",
      "Source: 我 不知 知道 那 张 是 怎么 怎么回事 回事 回事儿\n",
      "Reference: that was , i don &apos;t know what happened\n",
      "Model: <SOS> i don i i know &apos;t know what it\n",
      "Attention Weights: tensor([[9.9725e-01, 2.7089e-03, 2.0927e-05, 7.8424e-06, 5.3199e-06, 5.2723e-06,\n",
      "         4.8509e-07, 3.0765e-07, 2.5980e-07, 2.5683e-07],\n",
      "        [9.9393e-01, 5.8021e-03, 1.6737e-04, 3.9487e-05, 1.9891e-05, 1.7088e-05,\n",
      "         8.0477e-06, 5.3332e-06, 3.9512e-06, 3.9281e-06],\n",
      "        [9.5339e-01, 3.8298e-02, 6.9252e-03, 8.5200e-04, 2.6606e-04, 1.0643e-04,\n",
      "         5.8022e-05, 3.9530e-05, 3.8916e-05, 2.9687e-05],\n",
      "        [5.7906e-01, 1.3885e-01, 1.3150e-01, 9.7673e-02, 3.0237e-02, 1.2752e-02,\n",
      "         4.1563e-03, 1.8286e-03, 1.7086e-03, 2.2407e-03],\n",
      "        [3.6787e-01, 2.5638e-02, 7.4686e-02, 1.8324e-01, 1.0387e-01, 1.5859e-01,\n",
      "         5.8388e-02, 1.3139e-02, 6.3760e-03, 8.2041e-03],\n",
      "        [8.0499e-01, 5.9947e-02, 6.8399e-02, 3.1562e-02, 9.1446e-03, 1.3522e-02,\n",
      "         6.0880e-03, 3.2416e-03, 1.5522e-03, 1.5493e-03],\n",
      "        [3.1128e-03, 5.0909e-02, 4.5584e-01, 3.6655e-01, 9.5438e-02, 1.4929e-02,\n",
      "         7.1739e-03, 2.5087e-03, 1.6446e-03, 1.8924e-03],\n",
      "        [3.7836e-03, 4.2057e-03, 1.3491e-01, 4.5170e-01, 2.6671e-01, 9.6874e-02,\n",
      "         2.9675e-02, 9.0082e-03, 1.4911e-03, 1.6356e-03],\n",
      "        [3.7702e-04, 4.0213e-04, 5.8521e-03, 5.0193e-01, 3.0464e-01, 1.0202e-01,\n",
      "         6.0402e-02, 1.6888e-02, 3.5857e-03, 3.9001e-03]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "Epoch: 11.00, Train Loss: 0.00, Val Loss: 4.43, Train BLEU: 0.00, Val BLEU: 7.13, Minutes Elapsed: 626.64\n",
      "Sampling from val predictions...\n",
      "Source: 而 可持续性 持续 持续性 的 搞笑 之处 处在 在于 你\n",
      "Reference: the funny thing about sustainability , you have to\n",
      "Model: <SOS> and what thing about about is is is ,\n",
      "Attention Weights: tensor([[1.1556e-01, 7.7276e-01, 1.1057e-01, 9.4273e-04, 1.3071e-04, 1.9145e-05,\n",
      "         9.3275e-06, 1.1486e-06, 2.1415e-07, 3.4254e-09],\n",
      "        [3.5539e-02, 7.6772e-01, 1.7483e-01, 1.8657e-02, 1.6904e-03, 1.4645e-03,\n",
      "         8.7623e-05, 6.0713e-06, 5.1029e-06, 4.2592e-07],\n",
      "        [6.8419e-02, 2.8478e-01, 2.9484e-01, 2.0173e-01, 7.6325e-02, 5.4049e-02,\n",
      "         1.7465e-02, 1.6308e-03, 6.1048e-04, 1.5122e-04],\n",
      "        [4.8815e-02, 1.1747e-01, 4.2480e-01, 1.8564e-01, 1.2885e-01, 6.4418e-02,\n",
      "         2.5551e-02, 3.3022e-03, 8.5082e-04, 3.0202e-04],\n",
      "        [8.9991e-02, 1.4784e-01, 3.4335e-01, 1.0125e-01, 6.2491e-02, 5.5513e-02,\n",
      "         9.4381e-02, 3.8883e-02, 4.9950e-02, 1.6350e-02],\n",
      "        [1.4676e-01, 2.0667e-01, 4.1112e-01, 6.7573e-02, 3.2656e-02, 1.6846e-02,\n",
      "         2.2634e-02, 2.9152e-02, 4.7887e-02, 1.8699e-02],\n",
      "        [1.4118e-02, 4.9774e-02, 8.2506e-02, 3.5850e-02, 2.4408e-02, 2.5676e-02,\n",
      "         8.6256e-02, 3.5031e-01, 2.5721e-01, 7.3893e-02],\n",
      "        [6.0157e-04, 3.5722e-04, 3.8423e-04, 1.1241e-04, 2.8547e-04, 7.5746e-04,\n",
      "         1.4319e-02, 1.1525e-01, 7.3107e-01, 1.3687e-01],\n",
      "        [9.2391e-04, 1.9036e-03, 1.6231e-03, 8.5235e-04, 8.5656e-04, 2.0625e-03,\n",
      "         2.2321e-02, 1.0275e-01, 5.1398e-01, 3.5273e-01]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-73df5d68e3c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_full\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_minibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_minibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_minitrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders_minitrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     lazy_eval=True, print_attn=True, inspect_samples=1)\n\u001b[0m",
      "\u001b[0;32m~/nlp-translation/train_eval.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, loaders_full, loaders_minibatch, loaders_minitrain, params, vocab, lazy_eval, print_intermediate, save_checkpoint, save_to_log, inspect_samples, print_attn)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mtargets_for_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_for_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_for_nll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_grad_max_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, results = train_and_eval(\n",
    "    model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "    params=params, vocab=vocab, print_intermediate=True, save_checkpoint=True, save_to_log=True, \n",
    "    lazy_eval=True, print_attn=True, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(load_experiment_log())[['dt_created', 'num_epochs', 'learning_rate', 'clip_grad_max_norm', 'val_loss']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch: 199.00, Train Loss: 0.32, Val Loss: 13.19, Train BLEU: 98.94, Val BLEU: 0.27\n",
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with attention energies = v_broadcast.bmm(torch.tanh(self.attn(concat)).transpose(1, 2)) # switched order  \n",
    "# Epoch: 199.00, Train Loss: 0.63, Val Loss: 12.82, Train BLEU: 92.05, Val BLEU: 0.38\n",
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(vocab[SRC_LANG]['id2token']): \n",
    "    if i < 20: \n",
    "        print(\"{}: {}\".format(i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(vocab[TARG_LANG]['id2token']): \n",
    "    if i < 20: \n",
    "        print(\"{}: {}\".format(i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.arange(0, 3*5*10).view(3, 5, 10)\n",
    "print(x)\n",
    "y = x[1:, :, :]\n",
    "print(y)\n",
    "z = y.view(-1, 10)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(0, 2*5).view(5, 2)\n",
    "print(t)\n",
    "u = t.contiguous().view(-1)\n",
    "print(u)\n",
    "v = t.permute(1, 0)\n",
    "print(v)\n",
    "w = v.contiguous().view(-1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(0, 2*1*300)\n",
    "print(a)\n",
    "b = a.view(-1, 1, 300)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (src_idxs, targ_idxs, src_lens, targ_lens) in enumerate(full_loaders['train']):\n",
    "#     print(i)\n",
    "#     print(src_idxs.size())\n",
    "#     print(src_idxs)\n",
    "#     print(src_lens)\n",
    "#     print(targ_idxs.size())\n",
    "#     print(targ_idxs)\n",
    "#     print(targ_lens)\n",
    "    id2token = vocab[SRC_LANG]['id2token']\n",
    "    test_tensor = src_idxs\n",
    "    list_of_lists = test_tensor.numpy().astype(int).tolist()\n",
    "    to_token = lambda l: ' '.join([id2token[idx] for idx in l])\n",
    "    list_of_lists_tokens = [to_token(l) for l in list_of_lists] \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
