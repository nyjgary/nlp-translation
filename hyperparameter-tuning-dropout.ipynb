{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from data_processing import generate_vocab, process_data, create_dataloaders\n",
    "from model import get_pretrained_emb, EncoderRNN, DecoderRNN, DecoderAttnRNN, EncoderDecoder, EncoderDecoderAttn\n",
    "from train_eval import train_and_eval, count_parameters, summarize_results, plot_single_learning_curve, load_experiment_log\n",
    "import pickle as pkl \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params required for generating data loaders \n",
    "\n",
    "SRC_LANG = 'vi'\n",
    "TARG_LANG = 'en'\n",
    "\n",
    "SRC_MAX_SENTENCE_LEN = 10\n",
    "TARG_MAX_SENTENCE_LEN = 10\n",
    "SRC_VOCAB_SIZE = 30000 \n",
    "TARG_VOCAB_SIZE = 30000 \n",
    "\n",
    "BATCH_SIZE = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a long time to process, save to pickle for reimport in future \n",
    "# vocab = generate_vocab(SRC_LANG, TARG_LANG, SRC_VOCAB_SIZE, TARG_VOCAB_SIZE)\n",
    "# vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "# pkl.dump(vocab, open(vocab_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from pickle \n",
    "vocab_filename = \"{}-{}-vocab.p\".format(SRC_LANG, TARG_LANG)\n",
    "vocab = pkl.load(open(vocab_filename, \"rb\"))\n",
    "data = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, filter_long=False)\n",
    "data_minibatch = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=BATCH_SIZE, filter_long=False) \n",
    "data_minitrain = process_data(SRC_LANG, TARG_LANG, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, vocab, sample_limit=1000, filter_long=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders \n",
    "loaders_full = create_dataloaders(data, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minibatch = create_dataloaders(data_minibatch, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)\n",
    "loaders_minitrain = create_dataloaders(data_minitrain, SRC_MAX_SENTENCE_LEN, TARG_MAX_SENTENCE_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture params \n",
    "NETWORK_TYPE = 'rnn'\n",
    "RNN_CELL_TYPE = 'gru'\n",
    "NUM_LAYERS = 2 \n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 2 * ENC_HIDDEN_DIM \n",
    "TEACHER_FORCING_RATIO = 1\n",
    "CLIP_GRAD_MAX_NORM = 1\n",
    "ENC_DROPOUT = 0.2 \n",
    "DEC_DROPOUT = 0.2 \n",
    "ATTENTION_TYPE = 'additive'\n",
    "\n",
    "# training params  \n",
    "NUM_EPOCHS = 5\n",
    "LR = 0.0003 # 0.0005\n",
    "OPTIMIZER = 'Adam'\n",
    "LAZY_TRAIN = False\n",
    "\n",
    "# name the model and experiment \n",
    "EXPERIMENT_NAME = 'hyperparameter_tuning_dropout'\n",
    "if NETWORK_TYPE == 'rnn': \n",
    "    MODEL_NAME = '{}-rnn-{}-attn'.format(SRC_LANG, ATTENTION_TYPE)\n",
    "elif NETWORK_TYPE == 'cnn': \n",
    "    MODEL_NAME = '{}-cnn'.format(SRC_LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as dict to save to results later \n",
    "params = {'experiment_name': EXPERIMENT_NAME,'model_name': MODEL_NAME, 'src_lang': SRC_LANG, 'targ_lang': TARG_LANG, \n",
    "          'rnn_cell_type': RNN_CELL_TYPE, 'src_max_sentence_len': SRC_MAX_SENTENCE_LEN, \n",
    "          'targ_max_sentence_len': TARG_MAX_SENTENCE_LEN, 'src_vocab_size': SRC_VOCAB_SIZE, \n",
    "          'targ_vocab_size': TARG_VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'enc_hidden_dim': ENC_HIDDEN_DIM, \n",
    "          'dec_hidden_dim': DEC_HIDDEN_DIM, 'teacher_forcing_ratio': TEACHER_FORCING_RATIO, \n",
    "          'clip_grad_max_norm': CLIP_GRAD_MAX_NORM, 'enc_dropout': ENC_DROPOUT, 'dec_dropout': DEC_DROPOUT, \n",
    "          'attention_type': ATTENTION_TYPE, 'batch_size': BATCH_SIZE, 'num_epochs': NUM_EPOCHS, \n",
    "          'learning_rate': LR, 'optimizer': OPTIMIZER, 'lazy_train': LAZY_TRAIN} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with dropout = 0\n",
      "Epoch: 0.00, Train Loss: 0.00, Val Loss: 10.23, Train BLEU: 0.00, Val BLEU: 0.07, Minutes Elapsed: 0.12\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi không_thể chỉnh sửa nếu ảnh không được làm sạch\n",
      "Reference: we couldn &apos;t <UNK> the photo unless it was\n",
      "Model: <SOS> and the the the to to to to to\n",
      "Attention Weights: tensor([[0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023],\n",
      "        [0.0981, 0.0985, 0.0990, 0.0989, 0.0993, 0.1002, 0.1009, 0.1012, 0.1016,\n",
      "         0.1023]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.05, Train Loss: 0.00, Val Loss: 6.41, Train BLEU: 0.00, Val BLEU: 0.03, Minutes Elapsed: 2.02\n",
      "Sampling from val predictions...\n",
      "Source: khi trượt tay , tôi <UNK> nhớ lại người thợ_mỏ\n",
      "Reference: when my hand slips , i suddenly remember a\n",
      "Model: <SOS> and , , , , , , , ,\n",
      "Attention Weights: tensor([[5.1922e-05, 1.5019e-02, 5.3390e-02, 8.7278e-02, 1.1048e-01, 1.3662e-01,\n",
      "         1.8327e-01, 2.1258e-01, 1.7476e-01, 2.6558e-02],\n",
      "        [5.8412e-06, 2.9416e-03, 2.0450e-02, 4.7121e-02, 7.3414e-02, 1.1118e-01,\n",
      "         1.9768e-01, 2.8167e-01, 2.4148e-01, 2.4065e-02],\n",
      "        [1.4398e-05, 3.4683e-03, 2.1164e-02, 4.7087e-02, 7.2829e-02, 1.1000e-01,\n",
      "         1.9580e-01, 2.8066e-01, 2.4268e-01, 2.6297e-02],\n",
      "        [1.4583e-05, 3.4601e-03, 2.1071e-02, 4.6859e-02, 7.2455e-02, 1.0947e-01,\n",
      "         1.9522e-01, 2.8066e-01, 2.4398e-01, 2.6803e-02],\n",
      "        [1.4359e-05, 3.4351e-03, 2.0976e-02, 4.6701e-02, 7.2252e-02, 1.0922e-01,\n",
      "         1.9500e-01, 2.8073e-01, 2.4466e-01, 2.7019e-02],\n",
      "        [1.4234e-05, 3.4207e-03, 2.0919e-02, 4.6605e-02, 7.2128e-02, 1.0907e-01,\n",
      "         1.9485e-01, 2.8074e-01, 2.4508e-01, 2.7172e-02],\n",
      "        [1.4164e-05, 3.4119e-03, 2.0882e-02, 4.6539e-02, 7.2041e-02, 1.0896e-01,\n",
      "         1.9474e-01, 2.8074e-01, 2.4538e-01, 2.7295e-02],\n",
      "        [1.4121e-05, 3.4063e-03, 2.0856e-02, 4.6492e-02, 7.1977e-02, 1.0888e-01,\n",
      "         1.9465e-01, 2.8072e-01, 2.4561e-01, 2.7398e-02],\n",
      "        [1.4101e-05, 3.4031e-03, 2.0838e-02, 4.6458e-02, 7.1930e-02, 1.0881e-01,\n",
      "         1.9457e-01, 2.8069e-01, 2.4579e-01, 2.7485e-02]])\n",
      "\n",
      "Epoch: 0.10, Train Loss: 0.00, Val Loss: 6.32, Train BLEU: 0.00, Val BLEU: 1.21, Minutes Elapsed: 3.73\n",
      "Sampling from val predictions...\n",
      "Source: tôi luôn là người chụp ảnh . <EOS> <PAD> <PAD>\n",
      "Reference: i &apos;m always the one taking the picture .\n",
      "Model: <SOS> and &apos;s <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.0049, 0.0424, 0.0878, 0.1255, 0.1510, 0.1661, 0.1775, 0.2448, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0078, 0.0340, 0.0632, 0.0924, 0.1225, 0.1585, 0.2100, 0.3117, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0019, 0.0078, 0.0165, 0.0286, 0.0461, 0.0766, 0.1540, 0.6685, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0019, 0.0082, 0.0162, 0.0265, 0.0413, 0.0678, 0.1385, 0.6997, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0021, 0.0097, 0.0189, 0.0300, 0.0450, 0.0712, 0.1400, 0.6831, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0021, 0.0105, 0.0204, 0.0320, 0.0474, 0.0737, 0.1416, 0.6722, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0021, 0.0108, 0.0211, 0.0329, 0.0484, 0.0748, 0.1424, 0.6674, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0021, 0.0109, 0.0214, 0.0334, 0.0490, 0.0754, 0.1427, 0.6651, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0021, 0.0110, 0.0216, 0.0336, 0.0493, 0.0757, 0.1429, 0.6637, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.14, Train Loss: 0.00, Val Loss: 6.23, Train BLEU: 0.00, Val BLEU: 0.61, Minutes Elapsed: 5.44\n",
      "Sampling from val predictions...\n",
      "Source: đó là cách duy_nhất để chúng_tôi có_thể đi học .\n",
      "Reference: it was the only way we both could be\n",
      "Model: <SOS> and , , , , , , , ,\n",
      "Attention Weights: tensor([[0.2524, 0.2080, 0.1439, 0.0849, 0.0822, 0.0634, 0.0560, 0.0483, 0.0353,\n",
      "         0.0257],\n",
      "        [0.1252, 0.1335, 0.1301, 0.1127, 0.1108, 0.1020, 0.0946, 0.0833, 0.0634,\n",
      "         0.0444],\n",
      "        [0.0911, 0.1008, 0.1038, 0.1047, 0.1049, 0.1051, 0.1050, 0.1034, 0.0982,\n",
      "         0.0831],\n",
      "        [0.0783, 0.0952, 0.1017, 0.1050, 0.1051, 0.1063, 0.1066, 0.1056, 0.1028,\n",
      "         0.0933],\n",
      "        [0.0700, 0.0928, 0.1018, 0.1060, 0.1059, 0.1075, 0.1078, 0.1070, 0.1046,\n",
      "         0.0966],\n",
      "        [0.0621, 0.0906, 0.1021, 0.1073, 0.1070, 0.1089, 0.1092, 0.1084, 0.1062,\n",
      "         0.0982],\n",
      "        [0.0546, 0.0882, 0.1024, 0.1087, 0.1083, 0.1104, 0.1107, 0.1098, 0.1076,\n",
      "         0.0993],\n",
      "        [0.0479, 0.0857, 0.1026, 0.1100, 0.1095, 0.1119, 0.1123, 0.1112, 0.1088,\n",
      "         0.1001],\n",
      "        [0.0422, 0.0832, 0.1028, 0.1112, 0.1106, 0.1132, 0.1136, 0.1125, 0.1100,\n",
      "         0.1007]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.19, Train Loss: 0.00, Val Loss: 6.15, Train BLEU: 0.00, Val BLEU: 0.24, Minutes Elapsed: 7.13\n",
      "Sampling from val predictions...\n",
      "Source: công_nghiệp dệt may cũng thường được nghĩ đến khi nghe\n",
      "Reference: the textile industry is another one we often think\n",
      "Model: <SOS> so we we , , , , , ,\n",
      "Attention Weights: tensor([[0.9637, 0.0306, 0.0025, 0.0011, 0.0007, 0.0005, 0.0004, 0.0003, 0.0002,\n",
      "         0.0001],\n",
      "        [0.3676, 0.1829, 0.0980, 0.0751, 0.0644, 0.0569, 0.0511, 0.0453, 0.0353,\n",
      "         0.0233],\n",
      "        [0.2265, 0.1508, 0.1039, 0.0916, 0.0853, 0.0815, 0.0765, 0.0729, 0.0632,\n",
      "         0.0478],\n",
      "        [0.1219, 0.1354, 0.1121, 0.1032, 0.0988, 0.0965, 0.0930, 0.0899, 0.0824,\n",
      "         0.0667],\n",
      "        [0.0775, 0.1231, 0.1152, 0.1090, 0.1061, 0.1043, 0.1020, 0.0983, 0.0910,\n",
      "         0.0734],\n",
      "        [0.0480, 0.1069, 0.1165, 0.1139, 0.1125, 0.1114, 0.1101, 0.1055, 0.0977,\n",
      "         0.0775],\n",
      "        [0.0303, 0.0903, 0.1162, 0.1174, 0.1178, 0.1172, 0.1168, 0.1111, 0.1030,\n",
      "         0.0799],\n",
      "        [0.0206, 0.0767, 0.1149, 0.1196, 0.1215, 0.1215, 0.1219, 0.1152, 0.1070,\n",
      "         0.0810],\n",
      "        [0.0155, 0.0670, 0.1134, 0.1209, 0.1241, 0.1244, 0.1256, 0.1180, 0.1099,\n",
      "         0.0812]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.24, Train Loss: 0.00, Val Loss: 6.00, Train BLEU: 0.00, Val BLEU: 2.70, Minutes Elapsed: 8.84\n",
      "Sampling from val predictions...\n",
      "Source: tại tây phi có_một ngành buôn_bán kền_kền <UNK> kinh_khủng để\n",
      "Reference: in west africa , there &apos;s a horrific trade\n",
      "Model: <SOS> it &apos;s a a a , , , ,\n",
      "Attention Weights: tensor([[9.8392e-01, 1.5370e-02, 5.1550e-04, 1.0617e-04, 3.7389e-05, 1.6875e-05,\n",
      "         9.7165e-06, 8.6830e-06, 6.4099e-06, 4.6741e-06],\n",
      "        [2.4828e-01, 2.2104e-01, 1.7124e-01, 1.3725e-01, 7.9021e-02, 4.4680e-02,\n",
      "         3.0712e-02, 2.9121e-02, 2.1486e-02, 1.7167e-02],\n",
      "        [6.3238e-02, 9.8611e-02, 1.3943e-01, 1.3483e-01, 1.1997e-01, 1.0765e-01,\n",
      "         9.4088e-02, 8.4412e-02, 8.3641e-02, 7.4136e-02],\n",
      "        [6.6530e-03, 1.4940e-02, 5.4998e-02, 7.4857e-02, 1.6067e-01, 1.7166e-01,\n",
      "         1.4833e-01, 9.6149e-02, 1.4472e-01, 1.2703e-01],\n",
      "        [3.1114e-03, 7.8160e-03, 5.2068e-02, 7.4738e-02, 1.7170e-01, 1.7981e-01,\n",
      "         1.4923e-01, 9.1983e-02, 1.4527e-01, 1.2427e-01],\n",
      "        [1.8479e-03, 5.1398e-03, 5.0086e-02, 7.4565e-02, 1.7620e-01, 1.8390e-01,\n",
      "         1.5013e-01, 8.9770e-02, 1.4569e-01, 1.2268e-01],\n",
      "        [1.2362e-03, 3.7729e-03, 4.8706e-02, 7.4656e-02, 1.7917e-01, 1.8686e-01,\n",
      "         1.5076e-01, 8.7647e-02, 1.4598e-01, 1.2121e-01],\n",
      "        [8.7581e-04, 2.9260e-03, 4.7697e-02, 7.4934e-02, 1.8271e-01, 1.8990e-01,\n",
      "         1.5093e-01, 8.5339e-02, 1.4564e-01, 1.1905e-01],\n",
      "        [5.3448e-04, 2.0770e-03, 4.6361e-02, 7.5277e-02, 1.9091e-01, 1.9603e-01,\n",
      "         1.5039e-01, 8.0924e-02, 1.4383e-01, 1.1366e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.29, Train Loss: 0.00, Val Loss: 5.86, Train BLEU: 0.00, Val BLEU: 2.29, Minutes Elapsed: 10.54\n",
      "Sampling from val predictions...\n",
      "Source: ước_lượng rằng có hơn 4.000 trẻ_em đang làm nô_lệ ở\n",
      "Reference: it &apos;s estimated that more than 4,000 children are\n",
      "Model: <SOS> but &apos;s the a the the the the the\n",
      "Attention Weights: tensor([[9.1993e-01, 7.9449e-02, 6.0560e-04, 1.8425e-05, 6.0842e-07, 5.3712e-08,\n",
      "         2.4291e-08, 1.6161e-08, 9.9017e-09, 5.0075e-09],\n",
      "        [3.3896e-01, 2.4333e-01, 1.9856e-01, 1.1452e-01, 5.7431e-02, 1.6754e-02,\n",
      "         1.2012e-02, 9.0888e-03, 5.7611e-03, 3.5818e-03],\n",
      "        [1.9732e-01, 2.0352e-01, 2.4292e-01, 2.2976e-01, 9.5069e-02, 1.3301e-02,\n",
      "         8.1998e-03, 5.2710e-03, 2.9728e-03, 1.6625e-03],\n",
      "        [1.6540e-02, 2.6463e-02, 5.3261e-02, 2.2055e-01, 2.7285e-01, 1.2924e-01,\n",
      "         1.0400e-01, 8.6820e-02, 5.4707e-02, 3.5568e-02],\n",
      "        [5.6355e-04, 7.6358e-04, 1.5087e-03, 6.7336e-03, 1.0234e-01, 1.9165e-01,\n",
      "         2.0994e-01, 2.0556e-01, 1.7287e-01, 1.0807e-01],\n",
      "        [3.2774e-04, 4.2012e-04, 7.8557e-04, 2.8625e-03, 5.5472e-02, 1.5060e-01,\n",
      "         1.9500e-01, 2.1025e-01, 2.3510e-01, 1.4917e-01],\n",
      "        [2.4210e-04, 3.0466e-04, 5.3135e-04, 1.8107e-03, 3.6820e-02, 1.2166e-01,\n",
      "         1.7347e-01, 1.9845e-01, 2.7648e-01, 1.9023e-01],\n",
      "        [1.9436e-04, 2.4228e-04, 3.9230e-04, 1.3000e-03, 2.6499e-02, 1.0106e-01,\n",
      "         1.5609e-01, 1.8629e-01, 3.0262e-01, 2.2532e-01],\n",
      "        [1.7995e-04, 2.2273e-04, 3.4443e-04, 1.1193e-03, 2.1909e-02, 8.9637e-02,\n",
      "         1.4556e-01, 1.7798e-01, 3.1547e-01, 2.4758e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.34, Train Loss: 0.00, Val Loss: 5.71, Train BLEU: 0.00, Val BLEU: 2.36, Minutes Elapsed: 12.24\n",
      "Sampling from val predictions...\n",
      "Source: nhưng thay vì thế , chúng_tôi giết chết mọi thứ\n",
      "Reference: instead , everything we touched we killed . <EOS>\n",
      "Model: <SOS> but the , , , , we , ,\n",
      "Attention Weights: tensor([[9.5112e-01, 4.8201e-02, 6.6598e-04, 1.6083e-05, 6.1296e-07, 1.3232e-07,\n",
      "         4.5335e-08, 2.4618e-08, 1.3761e-08, 9.2039e-09],\n",
      "        [2.0362e-01, 3.2003e-01, 3.2274e-01, 1.1500e-01, 2.0969e-02, 9.4360e-03,\n",
      "         3.1868e-03, 2.2237e-03, 1.5340e-03, 1.2475e-03],\n",
      "        [4.1233e-02, 9.2034e-02, 2.1158e-01, 3.6046e-01, 1.8975e-01, 5.3750e-02,\n",
      "         1.8789e-02, 1.3673e-02, 1.0232e-02, 8.5050e-03],\n",
      "        [3.5170e-03, 8.4938e-03, 2.7927e-02, 1.9097e-01, 4.0862e-01, 1.9175e-01,\n",
      "         6.5685e-02, 4.4532e-02, 3.2579e-02, 2.5927e-02],\n",
      "        [1.4598e-03, 3.0055e-03, 7.7727e-03, 5.0559e-02, 2.5979e-01, 4.0285e-01,\n",
      "         1.2098e-01, 6.6304e-02, 4.7952e-02, 3.9327e-02],\n",
      "        [7.8263e-04, 1.4937e-03, 3.5421e-03, 1.9445e-02, 1.2185e-01, 4.9545e-01,\n",
      "         1.6870e-01, 8.4765e-02, 5.7887e-02, 4.6084e-02],\n",
      "        [4.0656e-04, 7.9499e-04, 1.7938e-03, 7.7635e-03, 4.2603e-02, 4.8051e-01,\n",
      "         2.0735e-01, 1.2025e-01, 7.8732e-02, 5.9797e-02],\n",
      "        [3.5511e-04, 6.9538e-04, 1.5185e-03, 5.6650e-03, 2.4680e-02, 4.3022e-01,\n",
      "         2.1865e-01, 1.4768e-01, 9.7539e-02, 7.2992e-02],\n",
      "        [2.9376e-04, 5.8187e-04, 1.2588e-03, 4.0446e-03, 1.3850e-02, 3.5531e-01,\n",
      "         2.1845e-01, 1.8099e-01, 1.2803e-01, 9.7182e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.38, Train Loss: 0.00, Val Loss: 5.58, Train BLEU: 0.00, Val BLEU: 3.92, Minutes Elapsed: 13.97\n",
      "Sampling from val predictions...\n",
      "Source: phụ_nữ thắng <UNK> phần_trăm trong quốc_hội nhà_nước ở những cuộc\n",
      "Reference: women won <UNK> percent of the national congress in\n",
      "Model: <SOS> so i i i the the the the the\n",
      "Attention Weights: tensor([[9.8251e-01, 1.7305e-02, 1.7893e-04, 8.1722e-07, 9.3665e-08, 5.4294e-08,\n",
      "         3.3722e-08, 2.8628e-08, 1.8766e-08, 1.9847e-08],\n",
      "        [4.3555e-01, 3.4302e-01, 1.8271e-01, 2.2261e-02, 5.8977e-03, 3.6223e-03,\n",
      "         2.1788e-03, 1.9292e-03, 1.3747e-03, 1.4567e-03],\n",
      "        [7.6531e-02, 1.6411e-01, 5.5999e-01, 1.3040e-01, 2.2292e-02, 1.4744e-02,\n",
      "         9.5908e-03, 8.5459e-03, 6.5584e-03, 7.2276e-03],\n",
      "        [9.0232e-03, 3.7056e-02, 3.2381e-01, 3.7811e-01, 8.7457e-02, 5.1439e-02,\n",
      "         3.3794e-02, 3.0245e-02, 2.3490e-02, 2.5568e-02],\n",
      "        [5.8037e-04, 1.2071e-03, 9.0604e-03, 1.3970e-01, 2.7345e-01, 1.8386e-01,\n",
      "         1.2435e-01, 1.0854e-01, 7.9133e-02, 8.0117e-02],\n",
      "        [5.9458e-05, 7.2263e-05, 2.2799e-04, 5.5825e-03, 8.3948e-02, 1.7252e-01,\n",
      "         1.7687e-01, 2.2562e-01, 1.5451e-01, 1.8059e-01],\n",
      "        [7.4822e-05, 9.0345e-05, 2.5559e-04, 4.0801e-03, 4.4295e-02, 1.0600e-01,\n",
      "         1.3013e-01, 2.3458e-01, 1.8524e-01, 2.9526e-01],\n",
      "        [1.1679e-04, 1.3838e-04, 3.7365e-04, 5.2100e-03, 4.5622e-02, 1.0035e-01,\n",
      "         1.2215e-01, 2.2119e-01, 1.8406e-01, 3.2080e-01],\n",
      "        [1.6374e-04, 1.9607e-04, 5.1863e-04, 6.4272e-03, 5.0492e-02, 1.0368e-01,\n",
      "         1.2411e-01, 2.1411e-01, 1.8348e-01, 3.1682e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.43, Train Loss: 0.00, Val Loss: 5.44, Train BLEU: 0.00, Val BLEU: 5.02, Minutes Elapsed: 15.79\n",
      "Sampling from val predictions...\n",
      "Source: cùng_nhau chúng_ta có_thể làm_cho giường ngủ của chúng_ta bàn_ăn và\n",
      "Reference: together we can make our beds , our dinner\n",
      "Model: <SOS> so we can to to and and and and\n",
      "Attention Weights: tensor([[9.9272e-01, 7.1981e-03, 8.1266e-05, 3.8654e-07, 5.3084e-08, 3.2372e-08,\n",
      "         9.9124e-09, 1.6008e-08, 5.6870e-09, 1.6889e-09],\n",
      "        [2.1286e-01, 7.1437e-01, 6.1219e-02, 4.6729e-03, 2.2150e-03, 1.7101e-03,\n",
      "         7.9733e-04, 1.1055e-03, 6.4607e-04, 4.0395e-04],\n",
      "        [1.5577e-02, 5.9019e-01, 3.6604e-01, 1.4603e-02, 4.2023e-03, 3.1677e-03,\n",
      "         1.7732e-03, 1.9892e-03, 1.4275e-03, 1.0359e-03],\n",
      "        [3.6262e-03, 7.3166e-02, 5.6080e-01, 1.6386e-01, 5.6889e-02, 4.5922e-02,\n",
      "         3.0607e-02, 2.2894e-02, 2.3865e-02, 1.8370e-02],\n",
      "        [1.4976e-04, 7.0458e-04, 1.2876e-02, 1.7903e-01, 2.0071e-01, 2.1499e-01,\n",
      "         1.3216e-01, 8.8432e-02, 1.0307e-01, 6.7878e-02],\n",
      "        [3.3767e-05, 1.0006e-04, 6.3841e-04, 2.3260e-02, 1.8513e-01, 3.1610e-01,\n",
      "         1.9122e-01, 1.0469e-01, 1.2076e-01, 5.8065e-02],\n",
      "        [1.9098e-05, 7.1769e-05, 2.0048e-04, 2.7991e-03, 6.0101e-02, 2.3751e-01,\n",
      "         2.2654e-01, 1.8566e-01, 2.1186e-01, 7.5241e-02],\n",
      "        [1.8080e-05, 8.2501e-05, 1.6651e-04, 1.2042e-03, 2.1106e-02, 1.3749e-01,\n",
      "         1.7317e-01, 2.5442e-01, 2.7831e-01, 1.3403e-01],\n",
      "        [2.1763e-05, 1.1247e-04, 2.1565e-04, 1.0163e-03, 1.2457e-02, 9.2941e-02,\n",
      "         1.3463e-01, 2.7896e-01, 2.9473e-01, 1.8491e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.48, Train Loss: 0.00, Val Loss: 5.33, Train BLEU: 0.00, Val BLEU: 4.81, Minutes Elapsed: 17.75\n",
      "Sampling from val predictions...\n",
      "Source: cám_ơn rất nhiều . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you very much . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it &apos;s . <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9106, 0.0892, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4299, 0.4689, 0.0854, 0.0128, 0.0029, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1224, 0.1897, 0.1911, 0.2713, 0.2256, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0714, 0.1022, 0.1585, 0.2470, 0.4209, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0582, 0.0688, 0.1048, 0.2186, 0.5496, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0810, 0.0951, 0.1177, 0.2305, 0.4757, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1233, 0.1455, 0.1286, 0.1954, 0.4071, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1565, 0.1748, 0.1389, 0.1718, 0.3580, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1595, 0.1755, 0.1459, 0.1713, 0.3478, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.53, Train Loss: 0.00, Val Loss: 5.26, Train BLEU: 0.00, Val BLEU: 6.31, Minutes Elapsed: 19.45\n",
      "Sampling from val predictions...\n",
      "Source: tại phòng_thí_nghiệm chúng_tôi đã thử qua nhiều mẫu , và\n",
      "Reference: we tested a lot of specimens there , and\n",
      "Model: <SOS> so we to to of the , , ,\n",
      "Attention Weights: tensor([[9.9159e-01, 8.3874e-03, 2.6217e-05, 2.1467e-07, 1.0928e-08, 4.2805e-09,\n",
      "         2.4576e-09, 1.4298e-09, 8.2246e-10, 3.4698e-10],\n",
      "        [3.4592e-01, 2.1866e-01, 4.1974e-01, 1.0367e-02, 2.0402e-03, 1.2582e-03,\n",
      "         8.0917e-04, 5.2998e-04, 3.9679e-04, 2.7466e-04],\n",
      "        [4.5531e-02, 8.2522e-02, 7.3514e-01, 1.0345e-01, 1.4262e-02, 9.1480e-03,\n",
      "         4.0067e-03, 2.4804e-03, 1.8610e-03, 1.5958e-03],\n",
      "        [1.5286e-03, 3.6302e-03, 6.3022e-02, 5.3352e-01, 1.2239e-01, 7.3827e-02,\n",
      "         6.6645e-02, 6.3320e-02, 4.6623e-02, 2.5499e-02],\n",
      "        [1.9094e-04, 3.7789e-04, 2.8091e-03, 1.9090e-01, 2.0295e-01, 1.6433e-01,\n",
      "         1.5149e-01, 1.4059e-01, 9.7718e-02, 4.8642e-02],\n",
      "        [2.4456e-04, 2.4676e-04, 6.9579e-04, 4.5151e-02, 1.6239e-01, 2.1160e-01,\n",
      "         2.0744e-01, 1.9196e-01, 1.2657e-01, 5.3702e-02],\n",
      "        [7.0454e-05, 7.5782e-05, 1.0097e-04, 2.0811e-03, 2.1919e-02, 1.7383e-01,\n",
      "         2.6959e-01, 2.7835e-01, 1.8886e-01, 6.5127e-02],\n",
      "        [3.5717e-04, 2.5702e-04, 6.1091e-04, 3.3121e-03, 2.0209e-02, 1.6456e-01,\n",
      "         2.3262e-01, 2.5925e-01, 2.1852e-01, 1.0031e-01],\n",
      "        [2.0545e-04, 1.5910e-04, 3.4001e-04, 1.6336e-03, 8.0835e-03, 6.4646e-02,\n",
      "         1.6088e-01, 2.7855e-01, 3.2608e-01, 1.5942e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.58, Train Loss: 0.00, Val Loss: 5.17, Train BLEU: 0.00, Val BLEU: 6.43, Minutes Elapsed: 21.16\n",
      "Sampling from val predictions...\n",
      "Source: 5 năm trước , tôi đã trải qua 1 chuyện\n",
      "Reference: five years ago , i experienced a bit of\n",
      "Model: <SOS> i years years years i i to a a\n",
      "Attention Weights: tensor([[9.9723e-01, 2.7141e-03, 4.9398e-05, 5.5068e-06, 1.4708e-06, 3.8293e-07,\n",
      "         1.8066e-07, 1.3277e-07, 1.3150e-07, 5.8499e-08],\n",
      "        [4.5803e-01, 5.0813e-01, 2.4355e-02, 4.0235e-03, 2.1279e-03, 1.0904e-03,\n",
      "         7.8051e-04, 6.3399e-04, 5.4000e-04, 2.8536e-04],\n",
      "        [5.9405e-02, 8.4209e-01, 8.3836e-02, 8.5493e-03, 2.6530e-03, 1.3090e-03,\n",
      "         7.8756e-04, 6.6477e-04, 5.0000e-04, 2.0564e-04],\n",
      "        [5.2587e-03, 6.1494e-01, 2.9303e-01, 6.2708e-02, 1.1733e-02, 4.4485e-03,\n",
      "         2.6892e-03, 2.3097e-03, 1.8109e-03, 1.0673e-03],\n",
      "        [1.7330e-03, 1.7411e-01, 2.9476e-01, 3.3706e-01, 1.1436e-01, 3.4311e-02,\n",
      "         1.7142e-02, 1.2263e-02, 9.2116e-03, 5.0500e-03],\n",
      "        [9.2946e-04, 1.8269e-02, 5.6929e-02, 3.1723e-01, 3.4589e-01, 1.2327e-01,\n",
      "         6.2996e-02, 4.1451e-02, 2.1762e-02, 1.1277e-02],\n",
      "        [1.6113e-03, 1.1941e-02, 2.7955e-02, 1.7635e-01, 2.9620e-01, 1.9367e-01,\n",
      "         1.1195e-01, 9.6285e-02, 5.9090e-02, 2.4936e-02],\n",
      "        [3.3431e-05, 3.1543e-04, 3.2581e-03, 8.3015e-02, 1.0644e-01, 2.0910e-01,\n",
      "         2.3417e-01, 1.8371e-01, 1.1854e-01, 6.1415e-02],\n",
      "        [3.0058e-05, 1.4662e-04, 7.8421e-04, 1.4676e-02, 4.3729e-02, 6.7105e-02,\n",
      "         1.4241e-01, 2.4028e-01, 2.8470e-01, 2.0614e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.62, Train Loss: 0.00, Val Loss: 5.08, Train BLEU: 0.00, Val BLEU: 7.39, Minutes Elapsed: 22.87\n",
      "Sampling from val predictions...\n",
      "Source: khi tôi viết bài phát_biểu này , tôi cảm_thấy rất\n",
      "Reference: so when i was writing this talk , i\n",
      "Model: <SOS> when i i i my , i i i\n",
      "Attention Weights: tensor([[9.9840e-01, 1.5952e-03, 4.9896e-07, 1.2185e-08, 1.5086e-09, 7.1725e-10,\n",
      "         3.7012e-10, 2.3630e-10, 9.7070e-11, 7.5952e-11],\n",
      "        [1.2761e-01, 8.4127e-01, 3.0137e-02, 6.4952e-04, 1.1477e-04, 5.6201e-05,\n",
      "         5.1214e-05, 4.5394e-05, 3.4391e-05, 3.2071e-05],\n",
      "        [1.2729e-02, 3.8334e-02, 9.1225e-01, 2.9943e-02, 2.3783e-03, 1.2346e-03,\n",
      "         9.5532e-04, 7.7065e-04, 7.2797e-04, 6.8163e-04],\n",
      "        [7.6005e-04, 1.2405e-03, 7.8044e-01, 2.0397e-01, 6.8069e-03, 2.4815e-03,\n",
      "         1.4503e-03, 1.0079e-03, 9.4002e-04, 9.0269e-04],\n",
      "        [9.0732e-04, 1.7617e-03, 7.4739e-01, 2.2067e-01, 1.4991e-02, 4.5125e-03,\n",
      "         3.2455e-03, 2.6855e-03, 2.0231e-03, 1.8211e-03],\n",
      "        [6.1943e-05, 1.6925e-04, 7.4271e-02, 4.5214e-01, 2.5186e-01, 1.0769e-01,\n",
      "         5.3006e-02, 2.6431e-02, 1.7491e-02, 1.6888e-02],\n",
      "        [7.7802e-05, 1.9523e-04, 4.8813e-03, 7.4982e-02, 2.7651e-01, 2.7178e-01,\n",
      "         1.6550e-01, 8.0258e-02, 6.3905e-02, 6.1916e-02],\n",
      "        [1.3828e-04, 1.9258e-04, 8.8680e-04, 5.9188e-03, 4.6275e-02, 1.3523e-01,\n",
      "         2.9305e-01, 2.8756e-01, 1.2334e-01, 1.0740e-01],\n",
      "        [1.9465e-04, 2.0129e-04, 1.1228e-03, 7.8697e-03, 5.0908e-02, 1.1757e-01,\n",
      "         2.5687e-01, 2.8530e-01, 1.4717e-01, 1.3280e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.67, Train Loss: 0.00, Val Loss: 5.02, Train BLEU: 0.00, Val BLEU: 6.81, Minutes Elapsed: 24.57\n",
      "Sampling from val predictions...\n",
      "Source: không bao_giờ có từ \" tôi , và từ \"\n",
      "Reference: never the word &quot; i , &quot; and the\n",
      "Model: <SOS> he &apos;s a , &quot; , and and ,\n",
      "Attention Weights: tensor([[9.9948e-01, 5.1529e-04, 8.3480e-06, 1.1831e-07, 2.9249e-09, 3.8749e-09,\n",
      "         9.4236e-10, 2.6717e-10, 2.9807e-10, 2.3947e-10],\n",
      "        [4.9437e-01, 3.5671e-01, 1.4376e-01, 4.5042e-03, 2.2653e-04, 2.6150e-04,\n",
      "         7.8674e-05, 3.1972e-05, 3.5863e-05, 2.6303e-05],\n",
      "        [1.3913e-01, 3.3354e-01, 4.2442e-01, 8.7109e-02, 7.9731e-03, 4.3986e-03,\n",
      "         1.8729e-03, 7.5896e-04, 4.6784e-04, 3.3160e-04],\n",
      "        [7.8936e-04, 3.3575e-02, 2.5529e-01, 4.0273e-01, 1.7695e-01, 8.5176e-02,\n",
      "         3.0963e-02, 8.3346e-03, 4.0421e-03, 2.1588e-03],\n",
      "        [2.9608e-04, 2.0698e-03, 1.9974e-02, 1.3800e-01, 4.3301e-01, 2.7376e-01,\n",
      "         9.0505e-02, 2.2703e-02, 1.2370e-02, 7.3179e-03],\n",
      "        [2.6140e-04, 3.9205e-04, 1.2032e-03, 5.5662e-03, 1.9599e-01, 4.5854e-01,\n",
      "         2.6129e-01, 4.0815e-02, 2.5074e-02, 1.0867e-02],\n",
      "        [1.6507e-03, 1.0586e-03, 2.3731e-03, 3.0786e-03, 5.6588e-02, 3.7980e-01,\n",
      "         3.4334e-01, 9.2580e-02, 8.3045e-02, 3.6494e-02],\n",
      "        [4.0728e-05, 1.4052e-04, 2.0298e-04, 1.4582e-03, 6.3801e-02, 2.3427e-01,\n",
      "         4.8126e-01, 1.1208e-01, 7.5577e-02, 3.1164e-02],\n",
      "        [2.7350e-03, 1.7840e-03, 3.9887e-03, 3.2265e-03, 2.9224e-02, 2.5389e-01,\n",
      "         3.1600e-01, 1.1382e-01, 1.5950e-01, 1.1582e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.72, Train Loss: 0.00, Val Loss: 4.95, Train BLEU: 0.00, Val BLEU: 8.12, Minutes Elapsed: 26.25\n",
      "Sampling from val predictions...\n",
      "Source: nó thật_là kì_cục . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: that was awkward . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> it &apos;s the . <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9996, 0.0004, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2595, 0.7223, 0.0178, 0.0003, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0938, 0.7159, 0.1851, 0.0046, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0156, 0.2083, 0.6771, 0.0809, 0.0181, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0069, 0.0294, 0.7008, 0.1652, 0.0977, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0278, 0.0590, 0.2885, 0.2692, 0.3554, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1558, 0.1886, 0.2522, 0.1654, 0.2380, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1661, 0.2055, 0.2136, 0.1568, 0.2580, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0261, 0.0733, 0.2230, 0.2753, 0.4023, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.77, Train Loss: 0.00, Val Loss: 4.92, Train BLEU: 0.00, Val BLEU: 7.98, Minutes Elapsed: 27.95\n",
      "Sampling from val predictions...\n",
      "Source: tôi chắc rằng bạn trong số những người đang nghe\n",
      "Reference: i promise you there are several people listening to\n",
      "Model: <SOS> i &apos;m that you in the you you you\n",
      "Attention Weights: tensor([[9.9996e-01, 3.9105e-05, 5.3797e-08, 4.0193e-09, 8.1217e-10, 4.2523e-10,\n",
      "         2.5031e-10, 2.0006e-10, 1.4880e-10, 1.1661e-10],\n",
      "        [4.9606e-02, 9.3736e-01, 1.1298e-02, 1.0061e-03, 2.3135e-04, 1.7527e-04,\n",
      "         1.1040e-04, 8.1684e-05, 7.1943e-05, 5.7011e-05],\n",
      "        [1.3542e-02, 8.2655e-01, 1.4933e-01, 7.2143e-03, 1.9168e-03, 7.3743e-04,\n",
      "         2.8431e-04, 1.8751e-04, 1.3694e-04, 9.1794e-05],\n",
      "        [1.2479e-04, 8.1035e-02, 7.0371e-01, 1.2723e-01, 5.8795e-02, 1.6877e-02,\n",
      "         4.6711e-03, 3.3775e-03, 2.1807e-03, 2.0055e-03],\n",
      "        [8.0127e-06, 3.7272e-04, 1.0276e-02, 2.8234e-01, 3.9118e-01, 2.2172e-01,\n",
      "         4.7461e-02, 2.4306e-02, 1.3329e-02, 9.0101e-03],\n",
      "        [3.6463e-07, 1.7613e-05, 2.7012e-04, 7.9321e-03, 2.0564e-01, 2.8346e-01,\n",
      "         2.0647e-01, 1.5618e-01, 8.6319e-02, 5.3705e-02],\n",
      "        [4.7847e-07, 2.5720e-05, 2.5604e-04, 2.1103e-03, 2.0157e-02, 6.0728e-02,\n",
      "         1.3611e-01, 1.8742e-01, 2.8961e-01, 3.0358e-01],\n",
      "        [1.1962e-06, 5.0834e-05, 4.7248e-04, 3.7664e-03, 1.6192e-02, 4.2211e-02,\n",
      "         1.0538e-01, 1.6441e-01, 2.9029e-01, 3.7722e-01],\n",
      "        [6.4486e-06, 2.1949e-04, 9.6504e-04, 6.9522e-03, 1.4496e-02, 3.4329e-02,\n",
      "         9.4576e-02, 1.4604e-01, 2.9099e-01, 4.1143e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.82, Train Loss: 0.00, Val Loss: 4.87, Train BLEU: 0.00, Val BLEU: 8.70, Minutes Elapsed: 29.66\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi chụp_ảnh liên_tục . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: we take photos constantly . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> we &apos;re the . <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9984, 0.0016, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0387, 0.9423, 0.0185, 0.0005, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0058, 0.7729, 0.2089, 0.0115, 0.0009, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0003, 0.0355, 0.5941, 0.2952, 0.0749, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0018, 0.0431, 0.1952, 0.4375, 0.3224, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0129, 0.1275, 0.1525, 0.3347, 0.3724, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0237, 0.2521, 0.1619, 0.2715, 0.2908, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0430, 0.5874, 0.1693, 0.1122, 0.0881, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0327, 0.5309, 0.1917, 0.1362, 0.1085, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.86, Train Loss: 0.00, Val Loss: 4.81, Train BLEU: 0.00, Val BLEU: 8.80, Minutes Elapsed: 31.40\n",
      "Sampling from val predictions...\n",
      "Source: và tôi có được một công_tắc để có_thể tắt và\n",
      "Reference: and i got a switch where i can switch\n",
      "Model: <SOS> and i had a a to to and to\n",
      "Attention Weights: tensor([[3.1774e-02, 9.6633e-01, 1.8888e-03, 4.3983e-06, 1.4235e-07, 2.5741e-08,\n",
      "         5.7088e-09, 1.8459e-09, 2.7150e-10, 3.7006e-11],\n",
      "        [5.0049e-03, 9.7801e-01, 1.6602e-02, 3.0684e-04, 4.0418e-05, 1.5832e-05,\n",
      "         1.0747e-05, 5.2071e-06, 2.0523e-06, 7.1309e-07],\n",
      "        [5.3961e-04, 1.9048e-02, 7.8966e-01, 1.7794e-01, 9.4362e-03, 2.1946e-03,\n",
      "         8.0309e-04, 2.5937e-04, 9.1126e-05, 2.5521e-05],\n",
      "        [1.7195e-04, 1.8023e-03, 1.6105e-01, 4.7726e-01, 2.7552e-01, 6.7399e-02,\n",
      "         1.4540e-02, 1.7791e-03, 3.7992e-04, 9.2603e-05],\n",
      "        [5.9657e-05, 4.1242e-05, 1.4126e-03, 1.1897e-01, 3.8275e-01, 3.4883e-01,\n",
      "         1.1408e-01, 1.7336e-02, 1.3328e-02, 3.1926e-03],\n",
      "        [1.6790e-05, 8.7074e-06, 5.8257e-05, 4.0654e-03, 6.6203e-02, 3.8393e-01,\n",
      "         3.7626e-01, 9.6609e-02, 6.2699e-02, 1.0147e-02],\n",
      "        [1.2582e-05, 1.2968e-05, 3.5160e-05, 3.8835e-04, 1.2902e-02, 1.2163e-01,\n",
      "         4.6532e-01, 2.1813e-01, 1.6045e-01, 2.1125e-02],\n",
      "        [7.0907e-06, 6.6563e-06, 1.1821e-05, 1.2767e-04, 2.3245e-03, 2.0280e-02,\n",
      "         1.6783e-01, 4.4991e-01, 3.1276e-01, 4.6745e-02],\n",
      "        [1.3061e-05, 1.1875e-05, 4.1865e-05, 2.5799e-04, 2.7675e-03, 1.8014e-02,\n",
      "         1.2976e-01, 4.4463e-01, 3.0121e-01, 1.0330e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.91, Train Loss: 0.00, Val Loss: 4.76, Train BLEU: 0.00, Val BLEU: 8.32, Minutes Elapsed: 33.12\n",
      "Sampling from val predictions...\n",
      "Source: đối_với tôi , afghanistan là một đất_nước của hy_vọng ,\n",
      "Reference: to me , afghanistan is a country of hope\n",
      "Model: <SOS> my i , a was a a , ,\n",
      "Attention Weights: tensor([[8.1936e-01, 1.8006e-01, 5.7759e-04, 8.1275e-08, 2.0820e-10, 2.0479e-11,\n",
      "         2.8208e-12, 1.3780e-12, 1.0480e-12, 9.8937e-13],\n",
      "        [2.6197e-01, 4.7730e-01, 2.0980e-01, 5.0012e-02, 6.4569e-04, 1.4947e-04,\n",
      "         4.4590e-05, 3.1158e-05, 2.1478e-05, 2.1117e-05],\n",
      "        [1.7142e-02, 1.2415e-02, 3.2420e-01, 6.3276e-01, 1.1710e-02, 1.1752e-03,\n",
      "         3.3891e-04, 1.3714e-04, 7.5942e-05, 4.4673e-05],\n",
      "        [9.6100e-04, 3.2500e-04, 4.3948e-02, 8.9632e-01, 4.8454e-02, 5.9782e-03,\n",
      "         2.3187e-03, 9.9087e-04, 4.5377e-04, 2.4578e-04],\n",
      "        [2.2881e-04, 2.0450e-04, 2.9011e-03, 5.8958e-01, 3.1518e-01, 5.2358e-02,\n",
      "         2.3102e-02, 1.0211e-02, 4.2499e-03, 1.9763e-03],\n",
      "        [9.6907e-04, 5.9043e-04, 7.7827e-03, 3.3324e-01, 3.1505e-01, 2.0804e-01,\n",
      "         8.5658e-02, 3.0545e-02, 1.3980e-02, 4.1558e-03],\n",
      "        [1.2937e-04, 5.8145e-05, 2.5263e-04, 4.7423e-02, 8.7586e-02, 1.8852e-01,\n",
      "         3.6446e-01, 1.5377e-01, 1.2987e-01, 2.7922e-02],\n",
      "        [3.7996e-05, 3.2139e-05, 2.3475e-05, 1.2449e-03, 8.1368e-03, 3.3810e-02,\n",
      "         3.7457e-01, 2.9051e-01, 2.2782e-01, 6.3820e-02],\n",
      "        [3.8656e-04, 4.6895e-04, 8.6547e-05, 1.5664e-03, 5.6249e-03, 4.1077e-02,\n",
      "         2.0428e-01, 2.2458e-01, 2.9471e-01, 2.2722e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.96, Train Loss: 0.00, Val Loss: 4.73, Train BLEU: 0.00, Val BLEU: 8.79, Minutes Elapsed: 34.86\n",
      "Sampling from val predictions...\n",
      "Source: nên tối đó , tôi truy_cập facebook và hỏi một_vài\n",
      "Reference: so that evening , i just reached out on\n",
      "Model: <SOS> so i i i i i to and and\n",
      "Attention Weights: tensor([[7.8656e-01, 1.6597e-01, 4.7456e-02, 1.0817e-05, 6.9755e-07, 6.6376e-09,\n",
      "         2.5788e-09, 1.3112e-10, 1.0312e-10, 5.6021e-11],\n",
      "        [1.1658e-01, 6.2211e-01, 2.5818e-01, 1.9542e-03, 9.0341e-04, 1.4782e-04,\n",
      "         7.7283e-05, 1.4553e-05, 1.6466e-05, 1.4547e-05],\n",
      "        [8.8900e-02, 3.1238e-01, 4.6973e-01, 7.4753e-02, 4.5816e-02, 5.8667e-03,\n",
      "         2.0043e-03, 1.8098e-04, 1.8012e-04, 1.8494e-04],\n",
      "        [5.2350e-02, 1.4253e-01, 2.0572e-01, 2.5832e-01, 2.1103e-01, 1.1496e-01,\n",
      "         1.2573e-02, 6.9845e-04, 9.2463e-04, 8.9406e-04],\n",
      "        [2.7977e-03, 9.3068e-03, 2.5580e-02, 2.2905e-01, 4.8225e-01, 2.1811e-01,\n",
      "         2.8844e-02, 1.5119e-03, 1.3137e-03, 1.2322e-03],\n",
      "        [4.0891e-04, 2.0651e-03, 6.5351e-03, 5.7113e-02, 6.6059e-01, 1.9274e-01,\n",
      "         7.1524e-02, 2.6552e-03, 3.0308e-03, 3.3299e-03],\n",
      "        [1.1792e-03, 3.3460e-03, 7.6836e-03, 3.2437e-02, 2.2718e-01, 4.4704e-01,\n",
      "         2.4822e-01, 9.0863e-03, 1.1423e-02, 1.2406e-02],\n",
      "        [5.6809e-06, 6.1449e-06, 4.1667e-05, 3.6935e-04, 1.3169e-03, 4.2994e-01,\n",
      "         5.4035e-01, 1.2171e-02, 8.0191e-03, 7.7823e-03],\n",
      "        [2.1057e-05, 2.1069e-05, 1.1021e-04, 3.7563e-04, 2.9192e-03, 8.8459e-02,\n",
      "         6.2276e-01, 9.5554e-02, 9.7018e-02, 9.2763e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.00, Train Loss: 0.00, Val Loss: 4.73, Train BLEU: 0.00, Val BLEU: 8.69, Minutes Elapsed: 36.31\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi có những quả cà_chua rất tuyệt_vời . tại ý\n",
      "Reference: we had these magnificent tomatoes . in italy ,\n",
      "Model: <SOS> we have have the the . <EOS> &apos;s .\n",
      "Attention Weights: tensor([[9.9973e-01, 2.6689e-04, 1.8986e-06, 5.5424e-08, 6.4761e-09, 1.1885e-08,\n",
      "         3.2351e-09, 1.4462e-09, 2.1355e-09, 1.1785e-09],\n",
      "        [1.0173e-01, 8.3139e-01, 6.3989e-02, 1.7140e-03, 3.4219e-04, 4.5520e-04,\n",
      "         1.5410e-04, 6.1604e-05, 9.1998e-05, 6.8583e-05],\n",
      "        [2.1887e-02, 4.2946e-01, 5.2638e-01, 1.9926e-02, 1.0747e-03, 8.9445e-04,\n",
      "         1.7310e-04, 5.5470e-05, 8.2002e-05, 6.7698e-05],\n",
      "        [4.2320e-05, 1.0800e-03, 2.6902e-01, 6.2635e-01, 6.5786e-02, 2.4393e-02,\n",
      "         9.6589e-03, 1.7205e-03, 1.3648e-03, 5.8277e-04],\n",
      "        [4.1402e-06, 9.2299e-05, 5.5873e-02, 7.0833e-01, 1.3066e-01, 7.7672e-02,\n",
      "         1.8664e-02, 3.8474e-03, 3.3996e-03, 1.4535e-03],\n",
      "        [8.9291e-07, 4.1903e-05, 4.5224e-03, 7.8406e-02, 2.4272e-01, 5.1302e-01,\n",
      "         1.2568e-01, 1.3412e-02, 1.6428e-02, 5.7651e-03],\n",
      "        [1.1185e-05, 7.8278e-05, 9.4721e-04, 7.0731e-03, 5.8278e-02, 3.8562e-01,\n",
      "         1.8366e-01, 9.6250e-02, 2.0962e-01, 5.8455e-02],\n",
      "        [4.9080e-05, 1.4839e-04, 1.7036e-03, 5.3326e-03, 3.2039e-02, 3.1020e-01,\n",
      "         1.0786e-01, 6.8920e-02, 3.4860e-01, 1.2516e-01],\n",
      "        [7.6120e-04, 1.4528e-02, 1.2008e-02, 9.4441e-03, 5.6986e-02, 3.1437e-01,\n",
      "         1.0137e-01, 6.4967e-02, 2.2622e-01, 1.9934e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.05, Train Loss: 0.00, Val Loss: 4.68, Train BLEU: 0.00, Val BLEU: 9.85, Minutes Elapsed: 37.98\n",
      "Sampling from val predictions...\n",
      "Source: bỗng_nhiên , tôi chẳng có một đất_nước nào để có_thể\n",
      "Reference: suddenly , there was no country i could proudly\n",
      "Model: <SOS> now , i i i to to i to\n",
      "Attention Weights: tensor([[9.9975e-01, 2.4688e-04, 4.4651e-06, 1.2501e-07, 5.1281e-09, 5.9680e-10,\n",
      "         1.2138e-10, 1.3881e-10, 9.4156e-11, 4.2250e-11],\n",
      "        [7.9726e-01, 1.1631e-01, 5.5580e-02, 2.7228e-02, 2.3831e-03, 6.0683e-04,\n",
      "         2.0120e-04, 1.9412e-04, 1.5171e-04, 8.5230e-05],\n",
      "        [2.5232e-01, 1.9219e-01, 3.5139e-01, 1.9118e-01, 8.9511e-03, 1.8191e-03,\n",
      "         6.5606e-04, 6.3848e-04, 5.3311e-04, 3.1185e-04],\n",
      "        [1.7393e-02, 4.4153e-02, 1.8755e-01, 7.2399e-01, 2.2126e-02, 2.6139e-03,\n",
      "         7.6137e-04, 6.5636e-04, 4.7746e-04, 2.7820e-04],\n",
      "        [4.6725e-03, 8.0790e-03, 2.6900e-02, 8.4621e-01, 9.8718e-02, 1.0692e-02,\n",
      "         2.3910e-03, 1.4459e-03, 5.7303e-04, 3.1689e-04],\n",
      "        [2.2789e-03, 7.1605e-03, 1.5303e-02, 8.0699e-01, 1.2518e-01, 2.5352e-02,\n",
      "         1.0145e-02, 5.1721e-03, 1.6397e-03, 7.7717e-04],\n",
      "        [4.5611e-04, 6.8622e-03, 4.7379e-03, 4.0014e-01, 1.5359e-01, 1.1278e-01,\n",
      "         1.5330e-01, 1.2989e-01, 2.8630e-02, 9.6087e-03],\n",
      "        [5.3117e-05, 1.9314e-04, 1.9921e-04, 1.6570e-02, 1.2585e-02, 3.0378e-02,\n",
      "         1.8740e-01, 5.2781e-01, 1.9772e-01, 2.7090e-02],\n",
      "        [7.0894e-05, 4.5327e-05, 6.3964e-05, 7.4274e-03, 1.5479e-02, 4.1394e-02,\n",
      "         8.7007e-02, 3.7390e-01, 3.8106e-01, 9.3551e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.10, Train Loss: 0.00, Val Loss: 4.64, Train BLEU: 0.00, Val BLEU: 10.31, Minutes Elapsed: 40.33\n",
      "Sampling from val predictions...\n",
      "Source: và cái chúng_tôi làm là trở_thành bạn của nhau ,\n",
      "Reference: and what we do , we become friends ,\n",
      "Model: <SOS> and what we do do is is a ,\n",
      "Attention Weights: tensor([[1.0570e-02, 9.7885e-01, 1.0519e-02, 5.9779e-05, 1.0222e-07, 2.3582e-09,\n",
      "         8.0455e-10, 1.0500e-10, 9.2477e-11, 2.7676e-11],\n",
      "        [8.7811e-04, 9.8549e-01, 1.3498e-02, 1.2821e-04, 4.4830e-06, 1.0154e-06,\n",
      "         6.3463e-07, 1.1761e-07, 8.0349e-08, 4.1168e-08],\n",
      "        [1.6430e-03, 2.6242e-01, 5.1566e-01, 2.1487e-01, 4.5464e-03, 5.3959e-04,\n",
      "         2.2192e-04, 4.7355e-05, 3.8731e-05, 1.4452e-05],\n",
      "        [9.7185e-05, 4.8491e-03, 4.8789e-02, 7.6769e-01, 1.5848e-01, 1.6398e-02,\n",
      "         2.9962e-03, 3.2994e-04, 2.7863e-04, 8.5104e-05],\n",
      "        [1.0125e-04, 1.1918e-03, 5.1137e-03, 2.6883e-01, 4.2261e-01, 2.4069e-01,\n",
      "         5.4087e-02, 4.5921e-03, 2.2886e-03, 4.9117e-04],\n",
      "        [1.5899e-05, 2.7644e-05, 2.8406e-05, 4.3099e-03, 1.3035e-01, 4.0154e-01,\n",
      "         3.0864e-01, 1.0671e-01, 4.0823e-02, 7.5478e-03],\n",
      "        [3.0655e-06, 4.8714e-06, 4.8460e-06, 4.5255e-04, 8.1732e-03, 8.6722e-02,\n",
      "         2.9373e-01, 2.1403e-01, 3.0568e-01, 9.1201e-02],\n",
      "        [2.3599e-05, 1.5005e-04, 2.8641e-04, 6.8711e-03, 1.3672e-02, 8.2913e-02,\n",
      "         4.2878e-01, 9.8019e-02, 2.5758e-01, 1.1171e-01],\n",
      "        [3.5859e-06, 5.2854e-06, 1.2644e-06, 3.0057e-05, 6.8532e-04, 1.0533e-02,\n",
      "         8.7506e-02, 1.7937e-01, 5.0301e-01, 2.1886e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.14, Train Loss: 0.00, Val Loss: 4.66, Train BLEU: 0.00, Val BLEU: 9.90, Minutes Elapsed: 42.74\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi bắt_đầu chỉnh sửa những tấm ảnh này . <EOS>\n",
      "Reference: so we started <UNK> photos . <EOS> <PAD> <PAD>\n",
      "Model: <SOS> we started we to the the <EOS> . the\n",
      "Attention Weights: tensor([[9.9977e-01, 2.2551e-04, 2.8294e-07, 8.5282e-09, 1.4492e-09, 6.4891e-10,\n",
      "         4.7019e-10, 2.5082e-10, 1.4052e-10, 8.0801e-11],\n",
      "        [1.7462e-01, 7.8866e-01, 3.3711e-02, 1.8840e-03, 5.7379e-04, 2.5076e-04,\n",
      "         1.7664e-04, 7.8109e-05, 2.8769e-05, 1.7072e-05],\n",
      "        [3.4526e-02, 4.5386e-01, 4.5539e-01, 5.2631e-02, 2.4006e-03, 5.5762e-04,\n",
      "         4.2324e-04, 1.2343e-04, 5.8982e-05, 2.7927e-05],\n",
      "        [5.8575e-02, 4.3746e-01, 4.2218e-01, 7.3898e-02, 5.8659e-03, 1.0560e-03,\n",
      "         6.6635e-04, 1.8844e-04, 7.2501e-05, 3.2176e-05],\n",
      "        [8.2292e-05, 6.5622e-03, 3.1905e-01, 4.7354e-01, 1.5262e-01, 3.1656e-02,\n",
      "         1.3071e-02, 1.8888e-03, 1.0678e-03, 4.6409e-04],\n",
      "        [2.7833e-05, 1.7039e-03, 8.0043e-02, 4.6651e-01, 2.6585e-01, 9.7833e-02,\n",
      "         7.5401e-02, 8.2467e-03, 3.2330e-03, 1.1516e-03],\n",
      "        [4.8158e-05, 4.2724e-04, 4.4391e-03, 1.0083e-01, 2.0016e-01, 1.6214e-01,\n",
      "         2.9191e-01, 1.2115e-01, 8.6582e-02, 3.2322e-02],\n",
      "        [1.7082e-04, 1.4363e-03, 1.0339e-02, 9.5219e-02, 1.6299e-01, 1.5749e-01,\n",
      "         2.7588e-01, 1.4282e-01, 9.7996e-02, 5.5663e-02],\n",
      "        [1.4455e-03, 5.6987e-03, 2.2325e-02, 1.0850e-01, 2.6032e-01, 1.6819e-01,\n",
      "         2.3616e-01, 8.5196e-02, 6.6791e-02, 4.5370e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.19, Train Loss: 0.00, Val Loss: 4.61, Train BLEU: 0.00, Val BLEU: 10.03, Minutes Elapsed: 45.09\n",
      "Sampling from val predictions...\n",
      "Source: chúng_tôi nói với những người zambia , \" chúa ơi\n",
      "Reference: and we said to the <UNK> , &quot; my\n",
      "Model: <SOS> we say we about the , , , ,\n",
      "Attention Weights: tensor([[9.9993e-01, 7.1876e-05, 1.3267e-06, 7.1259e-08, 2.2601e-08, 8.6704e-09,\n",
      "         3.3331e-09, 2.7924e-09, 3.2221e-09, 1.8688e-09],\n",
      "        [1.0640e-01, 8.2811e-01, 5.1420e-02, 7.6415e-03, 2.8403e-03, 1.2388e-03,\n",
      "         5.4922e-04, 4.2748e-04, 8.8009e-04, 4.9531e-04],\n",
      "        [1.3423e-02, 3.4853e-01, 5.6585e-01, 4.6273e-02, 1.5036e-02, 7.5709e-03,\n",
      "         1.2734e-03, 7.0257e-04, 8.1744e-04, 5.2459e-04],\n",
      "        [5.9565e-02, 3.6290e-01, 5.2264e-01, 3.8479e-02, 9.4773e-03, 5.1059e-03,\n",
      "         7.0924e-04, 3.3031e-04, 5.0928e-04, 2.7971e-04],\n",
      "        [1.4986e-05, 3.0623e-03, 3.2783e-01, 2.9837e-01, 2.1020e-01, 1.2540e-01,\n",
      "         2.2728e-02, 5.0331e-03, 4.4092e-03, 2.9523e-03],\n",
      "        [2.3268e-05, 8.3521e-04, 6.4684e-02, 2.5002e-01, 2.4814e-01, 2.8302e-01,\n",
      "         1.1140e-01, 1.2677e-02, 2.1438e-02, 7.7573e-03],\n",
      "        [2.1430e-05, 3.3878e-04, 1.0127e-02, 6.3668e-02, 1.8043e-01, 2.9972e-01,\n",
      "         3.3284e-01, 3.6717e-02, 5.8640e-02, 1.7493e-02],\n",
      "        [1.1136e-03, 1.6771e-03, 1.1293e-02, 4.8484e-02, 1.2592e-01, 1.8508e-01,\n",
      "         3.8903e-01, 7.2829e-02, 1.3435e-01, 3.0221e-02],\n",
      "        [4.3265e-03, 1.5618e-03, 7.3581e-03, 2.6575e-02, 6.4944e-02, 8.6604e-02,\n",
      "         1.3957e-01, 1.3051e-01, 4.4466e-01, 9.3894e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.24, Train Loss: 0.00, Val Loss: 4.56, Train BLEU: 0.00, Val BLEU: 10.55, Minutes Elapsed: 47.63\n",
      "Sampling from val predictions...\n",
      "Source: khi một cái gì đó trở_nên cực rẻ , nó\n",
      "Reference: when something becomes ultra-low cost , it becomes massively\n",
      "Model: <SOS> as a was the , , it it it\n",
      "Attention Weights: tensor([[9.9374e-01, 5.0528e-03, 1.1606e-03, 4.3043e-05, 7.0949e-06, 9.6905e-08,\n",
      "         1.0774e-08, 4.1916e-09, 5.4010e-10, 3.1842e-10],\n",
      "        [1.8897e-02, 8.3300e-01, 1.3029e-01, 9.6462e-03, 7.3328e-03, 6.0909e-04,\n",
      "         1.3911e-04, 5.4435e-05, 1.7362e-05, 1.6818e-05],\n",
      "        [1.2320e-02, 8.7958e-02, 3.3402e-01, 1.1668e-01, 2.1421e-01, 1.9034e-01,\n",
      "         3.5147e-02, 8.6831e-03, 3.2313e-04, 3.2053e-04],\n",
      "        [1.1143e-03, 6.9106e-03, 5.7495e-02, 4.4069e-02, 1.5627e-01, 4.4006e-01,\n",
      "         1.9575e-01, 9.6406e-02, 1.1395e-03, 7.8547e-04],\n",
      "        [1.5839e-05, 2.3020e-04, 1.6625e-03, 4.6137e-03, 2.6330e-02, 2.7249e-01,\n",
      "         4.0015e-01, 2.9036e-01, 2.6166e-03, 1.5338e-03],\n",
      "        [5.9175e-05, 1.6554e-04, 3.5717e-04, 1.2752e-03, 6.1805e-03, 3.4869e-02,\n",
      "         1.1367e-01, 5.0813e-01, 1.7640e-01, 1.5890e-01],\n",
      "        [7.2985e-04, 1.8304e-03, 2.6138e-03, 4.5123e-03, 1.4645e-02, 1.0693e-02,\n",
      "         1.5903e-02, 7.2724e-02, 1.1793e-01, 7.5842e-01],\n",
      "        [1.9763e-04, 4.2531e-04, 7.7464e-04, 1.1425e-03, 4.0689e-03, 7.2099e-03,\n",
      "         1.0070e-02, 5.5757e-02, 9.5696e-02, 8.2466e-01],\n",
      "        [1.3737e-03, 2.8152e-03, 1.1335e-02, 1.2442e-02, 3.3692e-02, 5.0257e-02,\n",
      "         5.1393e-02, 1.4022e-01, 1.0853e-01, 5.8794e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.29, Train Loss: 0.00, Val Loss: 4.57, Train BLEU: 0.00, Val BLEU: 10.40, Minutes Elapsed: 50.14\n",
      "Sampling from val predictions...\n",
      "Source: ít_hơn 6 % phụ_nữ tuổi_tôi được học sau trung_học phổ_thông\n",
      "Reference: fewer than six percent of women my age have\n",
      "Model: <SOS> in of of percent of the are the in\n",
      "Attention Weights: tensor([[9.9993e-01, 6.5739e-05, 1.1610e-06, 4.0498e-09, 5.6860e-10, 1.1610e-10,\n",
      "         5.6005e-11, 2.8073e-11, 1.8143e-11, 1.6844e-11],\n",
      "        [8.3034e-02, 4.1110e-01, 4.9410e-01, 7.3477e-03, 2.6752e-03, 7.5058e-04,\n",
      "         4.1087e-04, 2.9269e-04, 1.5444e-04, 1.3497e-04],\n",
      "        [1.4659e-02, 7.3205e-02, 8.3992e-01, 4.9737e-02, 1.4576e-02, 4.0121e-03,\n",
      "         1.7126e-03, 1.2677e-03, 4.6135e-04, 4.5005e-04],\n",
      "        [5.7597e-04, 4.3994e-03, 6.3649e-01, 1.9317e-01, 1.0737e-01, 3.2220e-02,\n",
      "         1.3525e-02, 8.9031e-03, 1.6639e-03, 1.6885e-03],\n",
      "        [3.1599e-04, 7.6687e-04, 1.5861e-01, 2.1164e-01, 2.7811e-01, 2.0983e-01,\n",
      "         8.7639e-02, 4.2982e-02, 5.2946e-03, 4.8194e-03],\n",
      "        [6.5233e-05, 1.0052e-04, 1.4920e-02, 6.4223e-02, 1.5974e-01, 3.3036e-01,\n",
      "         2.3987e-01, 1.6164e-01, 1.5975e-02, 1.3104e-02],\n",
      "        [3.9587e-05, 5.1588e-05, 5.5776e-03, 4.4659e-02, 1.2352e-01, 2.6985e-01,\n",
      "         2.0827e-01, 2.4902e-01, 5.4001e-02, 4.5015e-02],\n",
      "        [4.6313e-05, 4.1842e-05, 2.3251e-03, 2.0341e-02, 6.3953e-02, 2.9161e-01,\n",
      "         2.4144e-01, 2.9377e-01, 4.6115e-02, 4.0362e-02],\n",
      "        [1.0709e-05, 2.9414e-05, 1.6505e-03, 1.5531e-02, 6.3533e-02, 1.5118e-01,\n",
      "         2.0011e-01, 3.2395e-01, 1.3072e-01, 1.1328e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.34, Train Loss: 0.00, Val Loss: 4.52, Train BLEU: 0.00, Val BLEU: 10.48, Minutes Elapsed: 52.64\n",
      "Sampling from val predictions...\n",
      "Source: vì_thế chúng_tôi có_thể nhận sự trợ_giúp từ cộng_đồng quốc_tế cho\n",
      "Reference: so we can benefit from the international community for\n",
      "Model: <SOS> so we we make from from from from from\n",
      "Attention Weights: tensor([[9.9709e-01, 2.9074e-03, 3.5503e-06, 4.9036e-08, 6.8215e-10, 9.5937e-11,\n",
      "         1.4066e-11, 2.3131e-12, 8.5408e-13, 7.6203e-13],\n",
      "        [1.9496e-01, 7.2370e-01, 7.5623e-02, 5.1407e-03, 3.8938e-04, 1.2411e-04,\n",
      "         3.7519e-05, 1.9700e-05, 8.2929e-06, 5.0193e-06],\n",
      "        [2.3567e-02, 1.1315e-01, 6.2469e-01, 2.2295e-01, 1.1915e-02, 2.4551e-03,\n",
      "         6.0333e-04, 3.6883e-04, 1.6288e-04, 1.3260e-04],\n",
      "        [3.7029e-03, 8.3690e-03, 1.4669e-01, 7.0520e-01, 1.2239e-01, 1.1966e-02,\n",
      "         9.4730e-04, 4.6197e-04, 1.8171e-04, 9.1703e-05],\n",
      "        [6.0100e-04, 2.3213e-04, 1.0260e-03, 1.5116e-01, 5.4650e-01, 2.8831e-01,\n",
      "         8.0560e-03, 3.0262e-03, 6.9655e-04, 3.9182e-04],\n",
      "        [1.6121e-04, 2.5694e-04, 1.6480e-03, 1.0905e-01, 2.3779e-01, 5.5555e-01,\n",
      "         6.7384e-02, 2.1259e-02, 4.4866e-03, 2.4097e-03],\n",
      "        [9.5538e-04, 6.8839e-04, 1.9783e-03, 6.4711e-03, 2.0476e-02, 3.9637e-01,\n",
      "         2.2700e-01, 2.8215e-01, 4.5783e-02, 1.8127e-02],\n",
      "        [2.2575e-03, 9.3410e-04, 1.3162e-03, 2.4013e-03, 4.0318e-03, 8.3561e-02,\n",
      "         1.4019e-01, 5.2481e-01, 1.6677e-01, 7.3723e-02],\n",
      "        [2.1958e-02, 6.2512e-03, 3.3279e-03, 2.5121e-03, 3.4977e-03, 4.8704e-02,\n",
      "         1.0980e-01, 4.7559e-01, 2.0236e-01, 1.2600e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.38, Train Loss: 0.00, Val Loss: 4.49, Train BLEU: 0.00, Val BLEU: 11.07, Minutes Elapsed: 55.14\n",
      "Sampling from val predictions...\n",
      "Source: một slide như thế này , không_những nhàm_chán mà_còn phụ_thuộc\n",
      "Reference: a slide like this is not only boring ,\n",
      "Model: <SOS> a a , this , , , , ,\n",
      "Attention Weights: tensor([[9.9522e-01, 4.3422e-03, 4.3032e-04, 5.7515e-06, 8.1584e-07, 1.0456e-07,\n",
      "         8.1676e-08, 2.5157e-08, 6.5550e-09, 2.9150e-09],\n",
      "        [4.0790e-01, 4.0326e-01, 1.5570e-01, 2.7007e-02, 2.4025e-03, 9.9934e-04,\n",
      "         1.3769e-03, 7.4195e-04, 4.0112e-04, 2.1225e-04],\n",
      "        [1.8455e-01, 1.8515e-01, 3.9971e-01, 1.8804e-01, 2.5199e-02, 5.9351e-03,\n",
      "         6.9473e-03, 2.6037e-03, 1.3239e-03, 5.3556e-04],\n",
      "        [1.1747e-02, 3.5753e-02, 3.3470e-01, 4.1784e-01, 7.8275e-02, 6.7028e-02,\n",
      "         3.6518e-02, 1.0160e-02, 5.8902e-03, 2.0877e-03],\n",
      "        [3.7411e-04, 1.7051e-03, 2.1985e-02, 1.6834e-01, 6.9596e-02, 2.9965e-01,\n",
      "         3.4571e-01, 6.8005e-02, 2.0075e-02, 4.5598e-03],\n",
      "        [1.1182e-04, 2.6967e-04, 4.6375e-03, 3.1635e-02, 1.3947e-02, 1.8756e-01,\n",
      "         5.5196e-01, 1.6140e-01, 4.0535e-02, 7.9526e-03],\n",
      "        [1.0303e-05, 5.0653e-05, 1.0213e-03, 4.2547e-03, 3.0816e-03, 6.2558e-02,\n",
      "         4.3535e-01, 2.8470e-01, 1.5949e-01, 4.9486e-02],\n",
      "        [1.2391e-05, 7.1643e-05, 9.1731e-04, 4.8477e-03, 4.4946e-03, 1.2002e-01,\n",
      "         3.6262e-01, 2.7310e-01, 1.7326e-01, 6.0664e-02],\n",
      "        [6.2435e-06, 4.4654e-05, 5.1011e-04, 2.5431e-03, 2.6579e-03, 7.5016e-02,\n",
      "         3.5658e-01, 2.8419e-01, 2.0020e-01, 7.8250e-02]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.43, Train Loss: 0.00, Val Loss: 4.46, Train BLEU: 0.00, Val BLEU: 11.16, Minutes Elapsed: 57.64\n",
      "Sampling from val predictions...\n",
      "Source: cám_ơn . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: thank you . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> thank . . <EOS> . <EOS> <EOS> <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9998, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.9543, 0.0392, 0.0065, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.6570, 0.1559, 0.1871, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.4738, 0.1213, 0.4049, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5679, 0.1512, 0.2809, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.7235, 0.1722, 0.1044, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.6817, 0.1993, 0.1190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5396, 0.1883, 0.2721, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.5129, 0.1950, 0.2921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.48, Train Loss: 0.00, Val Loss: 4.44, Train BLEU: 0.00, Val BLEU: 11.02, Minutes Elapsed: 60.12\n",
      "Sampling from val predictions...\n",
      "Source: tuy_nhiên , sau khi chúng_tôi qua được biên_giới , gia_đình\n",
      "Reference: but even after we got past the border ,\n",
      "Model: <SOS> however , , when we when , , ,\n",
      "Attention Weights: tensor([[9.9859e-01, 1.4093e-03, 8.0289e-07, 2.0302e-08, 1.1705e-09, 2.5788e-10,\n",
      "         5.5442e-11, 2.9751e-11, 5.9946e-12, 3.4534e-12],\n",
      "        [4.8279e-01, 4.4383e-01, 6.3945e-02, 6.1303e-03, 1.7545e-03, 7.4494e-04,\n",
      "         3.8457e-04, 2.4313e-04, 9.4576e-05, 8.0722e-05],\n",
      "        [2.2330e-01, 3.3036e-01, 3.3713e-01, 9.2573e-02, 8.7723e-03, 4.5201e-03,\n",
      "         2.0207e-03, 8.1031e-04, 2.6007e-04, 2.6393e-04],\n",
      "        [6.5365e-03, 4.9148e-02, 3.5103e-01, 4.2441e-01, 1.2442e-01, 2.6643e-02,\n",
      "         9.8913e-03, 3.8870e-03, 1.8439e-03, 2.1818e-03],\n",
      "        [1.8729e-03, 4.3023e-03, 3.0964e-02, 2.1997e-01, 3.5890e-01, 2.9077e-01,\n",
      "         6.4104e-02, 1.7136e-02, 6.0290e-03, 5.9455e-03],\n",
      "        [6.3082e-04, 1.0863e-03, 9.6727e-03, 1.0160e-01, 1.9602e-01, 4.8858e-01,\n",
      "         1.6223e-01, 2.7042e-02, 6.7960e-03, 6.3439e-03],\n",
      "        [4.7636e-05, 5.6995e-05, 2.3126e-04, 2.7744e-03, 1.1287e-02, 1.1597e-01,\n",
      "         3.4727e-01, 3.3608e-01, 1.1876e-01, 6.7527e-02],\n",
      "        [1.1734e-04, 5.7699e-05, 2.3940e-04, 2.3405e-03, 6.3393e-03, 5.3417e-02,\n",
      "         1.6540e-01, 2.2853e-01, 3.6069e-01, 1.8286e-01],\n",
      "        [2.8053e-04, 5.9760e-05, 3.2042e-04, 1.5785e-03, 2.6752e-03, 2.7927e-02,\n",
      "         4.1594e-02, 1.9523e-02, 9.5917e-02, 8.1012e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.53, Train Loss: 0.00, Val Loss: 4.45, Train BLEU: 0.00, Val BLEU: 11.29, Minutes Elapsed: 62.62\n",
      "Sampling from val predictions...\n",
      "Source: tôi đã bị sốc . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "Reference: i was so shocked . <EOS> <PAD> <PAD> <PAD>\n",
      "Model: <SOS> i was still . . <EOS> . <EOS> <EOS>\n",
      "Attention Weights: tensor([[0.9819, 0.0170, 0.0010, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0071, 0.3391, 0.6224, 0.0306, 0.0007, 0.0001, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.0802, 0.6689, 0.2405, 0.0069, 0.0005, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0006, 0.0041, 0.1314, 0.7947, 0.0413, 0.0280, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0016, 0.0065, 0.1165, 0.3631, 0.1962, 0.3160, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0031, 0.0489, 0.2273, 0.2104, 0.0810, 0.4293, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0115, 0.0903, 0.3893, 0.2270, 0.0564, 0.2255, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0143, 0.1774, 0.5199, 0.1781, 0.0291, 0.0812, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0148, 0.1099, 0.4193, 0.2329, 0.0439, 0.1793, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.58, Train Loss: 0.00, Val Loss: 4.40, Train BLEU: 0.00, Val BLEU: 11.69, Minutes Elapsed: 65.07\n",
      "Sampling from val predictions...\n",
      "Source: tất_cả những điều đó hoàn_toàn có_thể chỉ với chiếc điện_thoại_di_động\n",
      "Reference: all this is possible with your mobile phone .\n",
      "Model: <SOS> all of that all to to to to to\n",
      "Attention Weights: tensor([[4.6944e-01, 3.7211e-01, 1.5693e-01, 1.5048e-03, 2.0223e-05, 3.2773e-07,\n",
      "         3.1045e-08, 6.1928e-09, 2.2108e-09, 1.3880e-09],\n",
      "        [6.7837e-02, 5.1354e-01, 3.6263e-01, 3.9730e-02, 1.5146e-02, 5.7818e-04,\n",
      "         3.5798e-04, 1.0559e-04, 4.3536e-05, 2.9426e-05],\n",
      "        [3.2121e-02, 3.8652e-01, 2.1117e-01, 1.3108e-01, 2.2063e-01, 1.1209e-02,\n",
      "         5.2995e-03, 1.3128e-03, 4.0486e-04, 2.4301e-04],\n",
      "        [3.9306e-03, 4.7715e-02, 3.3299e-02, 9.1246e-02, 6.5966e-01, 6.6258e-02,\n",
      "         8.4978e-02, 1.0015e-02, 1.9685e-03, 9.3447e-04],\n",
      "        [6.7521e-04, 7.0094e-03, 9.9645e-03, 2.1527e-02, 4.5303e-01, 9.1135e-02,\n",
      "         2.8971e-01, 1.0740e-01, 1.4949e-02, 4.5907e-03],\n",
      "        [9.4030e-05, 8.6433e-04, 2.3771e-03, 2.7240e-03, 9.6231e-02, 4.1732e-02,\n",
      "         3.5025e-01, 3.6322e-01, 1.0792e-01, 3.4585e-02],\n",
      "        [2.3151e-06, 1.1348e-05, 3.4405e-05, 5.3141e-05, 4.8090e-03, 9.1852e-03,\n",
      "         2.0027e-01, 3.9108e-01, 2.5009e-01, 1.4447e-01],\n",
      "        [1.3258e-06, 3.2535e-06, 2.0273e-05, 2.1082e-05, 4.6521e-04, 2.8344e-03,\n",
      "         5.8140e-02, 3.6375e-01, 2.5332e-01, 3.2144e-01],\n",
      "        [2.0910e-06, 3.1278e-06, 2.2110e-05, 2.1759e-05, 1.5809e-04, 6.1609e-04,\n",
      "         1.3597e-02, 1.4465e-01, 2.6539e-01, 5.7554e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.62, Train Loss: 0.00, Val Loss: 4.36, Train BLEU: 0.00, Val BLEU: 12.22, Minutes Elapsed: 67.55\n",
      "Sampling from val predictions...\n",
      "Source: không có đạo_diễn nghệ_thuật , không có nhà tạo mẫu\n",
      "Reference: there were no art directors , no <UNK> ,\n",
      "Model: <SOS> there &apos;s no , , no no &apos;s no\n",
      "Attention Weights: tensor([[9.9924e-01, 7.3552e-04, 2.7496e-05, 1.5152e-06, 1.4876e-07, 8.1551e-08,\n",
      "         3.4223e-08, 1.8255e-08, 1.0682e-08, 1.1487e-08],\n",
      "        [4.0242e-01, 1.9920e-01, 3.2816e-01, 6.3303e-02, 1.9295e-03, 2.3042e-03,\n",
      "         7.6198e-04, 8.4073e-04, 5.9900e-04, 4.7696e-04],\n",
      "        [2.5574e-01, 1.0452e-01, 4.4169e-01, 1.6885e-01, 9.7285e-03, 1.2906e-02,\n",
      "         2.8376e-03, 1.7940e-03, 1.1116e-03, 8.2146e-04],\n",
      "        [2.1976e-02, 3.3590e-02, 6.4399e-01, 2.7184e-01, 1.2712e-02, 8.4235e-03,\n",
      "         3.1707e-03, 2.1536e-03, 1.2227e-03, 9.2028e-04],\n",
      "        [3.0774e-03, 6.3533e-03, 2.4833e-01, 5.3265e-01, 1.2282e-01, 5.2327e-02,\n",
      "         7.9131e-03, 1.2418e-02, 7.4897e-03, 6.6155e-03],\n",
      "        [5.3793e-04, 9.5694e-04, 2.6164e-02, 1.8772e-01, 3.8222e-01, 2.8102e-01,\n",
      "         5.7249e-02, 3.1071e-02, 1.7570e-02, 1.5494e-02],\n",
      "        [1.0740e-03, 2.0709e-03, 9.9351e-03, 3.6396e-02, 6.1880e-02, 2.7033e-01,\n",
      "         1.5649e-01, 2.1412e-01, 1.3043e-01, 1.1727e-01],\n",
      "        [2.4970e-04, 5.5765e-04, 4.0945e-03, 1.2378e-02, 1.5936e-02, 7.6054e-02,\n",
      "         9.3868e-02, 2.9999e-01, 2.2727e-01, 2.6960e-01],\n",
      "        [9.1033e-03, 5.6470e-03, 1.4305e-02, 2.3397e-02, 1.6478e-02, 2.2540e-01,\n",
      "         1.5752e-01, 2.5475e-01, 1.4278e-01, 1.5062e-01]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.67, Train Loss: 0.00, Val Loss: 4.37, Train BLEU: 0.00, Val BLEU: 11.43, Minutes Elapsed: 70.04\n",
      "Sampling from val predictions...\n",
      "Source: và cuối_cùng , tôi đã có một thoả_thuận với họ\n",
      "Reference: so at the end , i had a settlement\n",
      "Model: <SOS> and finally , , , i had a a\n",
      "Attention Weights: tensor([[2.0793e-02, 9.7721e-01, 1.9746e-03, 2.2227e-05, 3.2071e-06, 3.6737e-07,\n",
      "         1.3984e-08, 1.0584e-09, 5.3013e-09, 1.1298e-09],\n",
      "        [3.2284e-03, 9.8454e-01, 1.1101e-02, 7.9759e-04, 2.4356e-04, 7.3196e-05,\n",
      "         1.0422e-05, 3.3624e-06, 2.7502e-06, 1.4081e-06],\n",
      "        [9.5506e-03, 7.4993e-01, 1.9630e-01, 1.7733e-02, 1.5063e-02, 8.9483e-03,\n",
      "         1.7433e-03, 3.6707e-04, 2.6143e-04, 1.0971e-04],\n",
      "        [5.7078e-03, 8.2584e-02, 2.0385e-01, 2.3488e-01, 3.2442e-01, 1.2894e-01,\n",
      "         1.3861e-02, 2.7580e-03, 1.9197e-03, 1.0743e-03],\n",
      "        [3.1338e-03, 1.8165e-02, 4.7098e-01, 2.4799e-01, 1.5908e-01, 7.3851e-02,\n",
      "         1.6335e-02, 4.0443e-03, 3.8091e-03, 2.6084e-03],\n",
      "        [1.7030e-03, 8.6527e-03, 3.9757e-01, 3.4858e-01, 1.5584e-01, 5.1450e-02,\n",
      "         1.6736e-02, 5.8849e-03, 8.4047e-03, 5.1709e-03],\n",
      "        [9.1474e-04, 3.2420e-03, 5.9750e-02, 3.0569e-01, 3.6487e-01, 1.9564e-01,\n",
      "         3.8357e-02, 1.2688e-02, 1.4024e-02, 4.8185e-03],\n",
      "        [1.2177e-04, 3.7101e-04, 2.1049e-03, 1.0143e-02, 1.2996e-01, 6.5747e-01,\n",
      "         1.7915e-01, 1.5110e-02, 4.0008e-03, 1.5708e-03],\n",
      "        [1.1138e-05, 2.7176e-05, 8.8331e-05, 2.1260e-04, 6.0832e-04, 9.5418e-03,\n",
      "         1.8001e-01, 6.1219e-01, 1.7911e-01, 1.8205e-02]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for candidate in [0, 0.2, 0.5]: \n",
    "    \n",
    "    print(\"Training with dropout = {}\".format(candidate))\n",
    "    \n",
    "    # overwrite relevant key-value in params \n",
    "    params['enc_dropout'] = candidate \n",
    "    params['dec_dropout'] = candidate\n",
    "    params['model_name'] = '{}-rnn-{}-attn-{}-dropout'.format(SRC_LANG, ATTENTION_TYPE, candidate)\n",
    "    \n",
    "    # instantiate model \n",
    "    encoder = EncoderRNN(rnn_cell_type=RNN_CELL_TYPE, enc_hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, \n",
    "                         src_max_sentence_len=SRC_MAX_SENTENCE_LEN, enc_dropout=candidate, \n",
    "                         pretrained_word2vec=get_pretrained_emb(vocab[SRC_LANG]['word2vec'], vocab[SRC_LANG]['token2id']))\n",
    "    decoder = DecoderAttnRNN(rnn_cell_type=RNN_CELL_TYPE, dec_hidden_dim=DEC_HIDDEN_DIM, enc_hidden_dim=ENC_HIDDEN_DIM, \n",
    "                             num_layers=NUM_LAYERS, targ_vocab_size=TARG_VOCAB_SIZE, \n",
    "                             src_max_sentence_len=SRC_MAX_SENTENCE_LEN, targ_max_sentence_len=TARG_MAX_SENTENCE_LEN, \n",
    "                             dec_dropout=candidate, attention_type=ATTENTION_TYPE,\n",
    "                             pretrained_word2vec=get_pretrained_emb(vocab[TARG_LANG]['word2vec'], \n",
    "                                                                    vocab[TARG_LANG]['token2id']))\n",
    "    model = EncoderDecoderAttn(encoder, decoder, vocab[TARG_LANG]['token2id']).to(device) \n",
    "    \n",
    "    # train and eval \n",
    "    model, results = train_and_eval(\n",
    "        model=model, loaders_full=loaders_full, loaders_minibatch=loaders_minibatch, loaders_minitrain=loaders_minitrain, \n",
    "        params=params, vocab=vocab, print_intermediate=100, save_checkpoint=True, save_to_log=True, \n",
    "        lazy_eval=True, print_attn=True, inspect_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_results = load_experiment_log(experiment_name=EXPERIMENT_NAME)\n",
    "# summarize_results(experiment_results)[['model_name', 'enc_dropout', 'dec_dropout', 'best_val_loss', 'best_val_bleu', \n",
    "#                                        'total_params', 'trainable_params', 'runtime']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
